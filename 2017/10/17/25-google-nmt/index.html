<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>谷歌RNN翻译模型 | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="论文笔记,RNN,神经机器翻译">
    <meta name="description" content="谷歌神经机器翻译系统，Transformer之前最强的基于RNN的翻译模型">
<meta name="keywords" content="论文笔记,RNN,神经机器翻译">
<meta property="og:type" content="article">
<meta property="og:title" content="谷歌RNN翻译模型">
<meta property="og:url" content="http://plmsmile.github.io/2017/10/17/25-google-nmt/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="谷歌神经机器翻译系统，Transformer之前最强的基于RNN的翻译模型">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/05_gnmt.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/06_residual_LSTM.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/33_bidir_lstm.png">
<meta property="og:updated_time" content="2018-12-13T08:34:31.547Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="谷歌RNN翻译模型">
<meta name="twitter:description" content="谷歌神经机器翻译系统，Transformer之前最强的基于RNN的翻译模型">
<meta name="twitter:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/05_gnmt.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/">
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives/">
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags/">
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories/">
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about/">
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">谷歌RNN翻译模型</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">谷歌RNN翻译模型</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-10-17T05:25:38.000Z" itemprop="datePublished" class="page-time">
  2017-10-17
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/论文笔记/">论文笔记</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#简介"><span class="post-toc-number">1.</span> <span class="post-toc-text">简介</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#传统nmt缺点"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">传统NMT缺点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#gnmt的模型优点"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">GNMT的模型优点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#先进技术"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">先进技术</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#系统架构"><span class="post-toc-number">2.</span> <span class="post-toc-text">系统架构</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#系统总览"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">系统总览</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#残差连接"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">残差连接</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#双向encoder"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">双向Encoder</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#模型并行性"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">模型并行性</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#分割技巧"><span class="post-toc-number">3.</span> <span class="post-toc-text">分割技巧</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#wordpiece-模型"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">Wordpiece 模型</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-25-google-nmt" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">谷歌RNN翻译模型</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-10-17 13:25:38" datetime="2017-10-17T05:25:38.000Z" itemprop="datePublished">2017-10-17</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/论文笔记/">论文笔记</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <blockquote>
<p><a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="noopener">谷歌神经机器翻译系统</a>，Transformer之前最强的基于RNN的翻译模型</p>
</blockquote>
<a id="more"></a>
<p><img src="" style="display:block; margin:auto" width="60%"></p>
<h1 id="简介">简介</h1>
<p><code>神经机器翻译</code>是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点</p>
<ul>
<li>训练和翻译都太慢了，花费代价很大</li>
<li>缺乏鲁棒性，特别是输入句子包含生僻词汇</li>
<li>精确度和速度也不行</li>
</ul>
<h2 id="传统nmt缺点">传统NMT缺点</h2>
<p>神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。</p>
<p>NMT避开了传统基于短语的翻译模型的很多缺点。但是，在实际中，NMT的准确度要比基于短语的翻译模型更差些。</p>
<p>NMT有3个主要的缺点：训练和推理速度太慢，不能有效处理稀有词汇，有时不能完全翻译原句子。</p>
<p><strong>训练和推理速度太慢</strong></p>
<p>训练大数据集，需要大量时间和资源；反馈太慢周期太长。加了一个小技巧，看结果要等很长时间。推理翻译的时候，要使用大量的参数去计算，也很慢。</p>
<p><strong>不能有效处理稀有词汇</strong></p>
<p>有两个方法去复制稀有单词：</p>
<ul>
<li>模仿传统对齐模型去训练1个<code>copy model</code></li>
<li>使用注意力机制去复制</li>
</ul>
<p>但是效果都不是很好，都不可靠，不同语言的对齐效果差；在网络很深的时候，注意力机制的对齐向量也不稳定。而且，简单的复制过去也不是最好的办法，比如需要直译的时候。</p>
<p><strong>不能完整翻译整个句子</strong></p>
<p>不能覆盖整个输入句子的内容，然后会导致一些奇怪的翻译结果。</p>
<h2 id="gnmt的模型优点">GNMT的模型优点</h2>
<p>采用的模型：<strong>深层LSTM</strong> 、<strong>Encoder8层</strong>、<strong>Decoder8层</strong> 。<a href="https://plmsmile.github.io/2017/10/18/rnn/">我的LSTM笔记</a>。</p>
<p>各层之间使用<strong>残差连接</strong>促进梯度流，顶层Enocder到底层Decoder使用<strong>注意力连接</strong>，提高并行性。</p>
<p>进行翻译推断的时候，使用低精度算法，去加速翻译。</p>
<p>处理<strong>稀有词汇</strong>：使用<strong>sub-word单元</strong>，也称作wordpieces方法。把单词划分到有限的<strong>sub-word</strong> (wordpieces)单元集合，输入输出都这样。sub-word结合了字符分割模型的弹性和单词分割模型的效率。</p>
<p><code>Beam Search</code> 使用<strong>长度规范化</strong>和<strong>覆盖惩罚</strong>。覆盖惩罚就是说，希望，翻译的结果句子，尽量多地包含输入句子中的所有词汇。</p>
<p>使用强化学习去优化模型， 优化翻译的<code>BLEU</code> 分数。</p>
<h2 id="先进技术">先进技术</h2>
<p>有很多先进的技术来提高NMT，下面这些都有论文的。</p>
<ul>
<li>利用attention去处理稀有词汇</li>
<li>建立翻译覆盖的机制</li>
<li>多任务和半监督训练，去合并使用更多数据</li>
<li>字符分割的encoder和decoder</li>
<li>使用subword单元处理稀疏的输出</li>
</ul>
<h1 id="系统架构">系统架构</h1>
<h2 id="系统总览">系统总览</h2>
<p><strong>架构</strong></p>
<p>有3个模块：Encoder，Decoder，Attention。</p>
<p>Encdoer：把句子转换成一系列的向量，每一个向量代表一个输入词汇（符号）。</p>
<p>Decoder：根据这些向量，每一时刻会生成一个目标词汇，直到EOS。</p>
<p>Attention：连接Encoder和Decoder，在解码的过程中，可以让Decoder有权重的有选择的关注输入句子的部分区域。</p>
<p><strong>符号说明</strong></p>
<ul>
<li>加粗小写代表<strong>向量</strong>，如<span class="math inline">\(\boldsymbol {v, o_i}\)</span></li>
<li>加粗大写，<strong>矩阵</strong>，如<span class="math inline">\(\boldsymbol {U, W}\)</span> 和<span class="math inline">\(\mathbf{U, W}\)</span></li>
<li>手写体，<strong>集合</strong>，如<span class="math inline">\(\mathcal{V, F}\)</span></li>
<li>大写字母，<strong>句子</strong>，如<span class="math inline">\(X, Y\)</span></li>
<li>小写字母，<strong>单个符号</strong>，如<span class="math inline">\(x_1, x_2\)</span></li>
</ul>
<p><strong>Encoder</strong></p>
<p>输入句子和目标句子组成一个Pair <span class="math inline">\((X, Y)\)</span>，其中输入句子<span class="math inline">\(X = x_1, x_2, \cdots, x_M\)</span> ，<span class="math inline">\(M\)</span> 个单词，翻译的输出目标句子<span class="math inline">\(Y = y_1, y_2, \cdots, y_N\)</span> ，有<span class="math inline">\(N\)</span>个单词。</p>
<p>Encoder其实就是一个转换函数，得到<span class="math inline">\(M\)</span>个长度固定的向量，也就是其中Encoder对各个<span class="math inline">\(x_i\)</span>的<strong>编码向量 <span class="math inline">\(\mathbf{x_i}\)</span> </strong>： <span class="math display">\[
\mathbf {x_1, x_2, \cdots, x_M} = \mathit{EncoderRNN} (x_1, x_2, \cdots, x_n)
\]</span> 使用链式条件概率可得到翻译概率<span class="math inline">\(\color{blue} {P (Y \mid X)}\)</span> ，其中<span class="math inline">\(y_0\)</span>是起始符号<span class="math inline">\(SOS\)</span> 。 <span class="math display">\[
\begin{align}
P(Y \mid X) 
&amp; = P(Y \mid \mathbf{x_1, x_2, \cdots, x_M}) \\
&amp; =\prod_{i=1}^N P(y_i \mid y_0, \cdots, y_{i-1}; \mathbf{x_1, x_2, \cdots, x_M}) \\
\end{align}
\]</span> <strong>Decoder</strong></p>
<p>在翻译<span class="math inline">\(y_i\)</span>的时候， 利用Encoder得到的编码向量<span class="math inline">\(\mathbf{x_i}\)</span> 和 <span class="math inline">\(y_0 \sim y_{i-1}\)</span> 来进行计算概率翻译 <span class="math display">\[
P(y_i \mid y_0, \cdots, y_{i-1}; \mathbf{x_1, x_2, \cdots, x_M})
\]</span> Decoder是由<strong>RNN+Softmax</strong>构成的。会得到一个<strong>隐状态<span class="math inline">\(\mathbf{y_i}\)</span> 向量</strong>，有2个作用：</p>
<ul>
<li>作为下一个RNN的输入</li>
<li><span class="math inline">\(\mathbf{y_i}\)</span>经过softmax得到概率分布， 选出<span class="math inline">\(y_i\)</span> 输出符号</li>
</ul>
<p><strong>Attention</strong></p>
<p>在之前的文章里有介绍<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/">论文</a> 和 <a href="https://plmsmile.github.io/2017/10/10/attention-model/">通俗理解</a>，其实就是<strong>影响力模型</strong>。原句子的各个单词对翻译当前单词分别有多少的影响力，也叫作<strong>对齐概率</strong>吧。使用decoder-RNN的输出<span class="math inline">\(\mathbf{y_{t-1}}\)</span> 向量作为时刻<span class="math inline">\(t\)</span>的输入。</p>
<p>时刻<span class="math inline">\(t\)</span>，给定<span class="math inline">\(\mathbf{y_{t-1}}\)</span></p>
<p>有3个符号定义：</p>
<ul>
<li><span class="math inline">\(s_i\)</span> ： <strong><span class="math inline">\(y_t\)</span>与<span class="math inline">\(x_i\)</span>的得分</strong>，在luong论文里面有3种计算方式，分别是dot, general和concat。</li>
<li><span class="math inline">\(p_i\)</span> ：<strong><span class="math inline">\(y_t\)</span>与<span class="math inline">\(x_i\)</span>的对齐概率</strong>，<span class="math inline">\((p_1, p_2, \cdots, p_M)\)</span> 联合起来就是<span class="math inline">\(y_t\)</span>与<span class="math inline">\(X\)</span>的<strong>对齐向量</strong>。其实就是对得分softmax。</li>
<li><span class="math inline">\(\mathbf{a_t}\)</span> ：<strong>带注意力的语义向量</strong>。对于所有的<span class="math inline">\(x_i\)</span>，使用<span class="math inline">\(y_t\)</span>与它的对齐概率<span class="math inline">\(p_i\)</span>乘以本身的编码向量<span class="math inline">\(\mathbf{x_i}\)</span>，得到<span class="math inline">\(x_i\)</span>传达的语义，再对所有的语义求和，即得到总体的带有注意力的语义。</li>
</ul>
<p>整体详细计算的流程，如下面的公式： <span class="math display">\[
\begin {align}
&amp;  s_i = \mathit{AttentionFunction} (\mathbf{y_{t-1}}, \mathbf{x_i}), \quad i \in [1, M] \\
&amp; p_i = \frac {\exp (s_i)}{\sum_{j=1}^M \exp(s_j)}  \quad i \in [1, M] \\
&amp;  \mathbf{a_t} = \sum_{i=1}^M p_i \cdot \mathbf{x_i} \quad \color{blue}{对所有带注意力的x_i的语义求和得总体的语义}
\end{align}
\]</span> 计算打分的函数即<span class="math inline">\(\mathit{AttentionFunction}\)</span>是一个有隐藏层的前馈网络！实现是Badh这个人的，不是Luong的。</p>
<p><strong>系统架构图说明</strong></p>
<p>架构图如下</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/05_gnmt.png" style="display:block; margin:auto" width="80%"></p>
<p>Encoder是8层的LSTM：最底层是双向的LSTM，得到两个方向的信息；上面7层都是单向的。Encoder和Decoder的残差连接都是从第3层开始的。</p>
<p>训练时，会让Encoder最底层的双向的LSTM开始训练，完成之后，再训练别的层，每层都用单独的GPU。</p>
<p>为了提高并行性，Decoder最底层，只是为了用来计算Attention Context。带注意力的语义计算好之后，会单独发给其它的各个层。</p>
<p><strong>经验说明</strong></p>
<p>实验结果得到，要想NMT有好效果，<strong>Encoder和Decoder的网络层数一定要够深</strong>，才能发现2种语言之间的细微异常规则。和这个同理，深层LSTM比浅层LSTM明显效果好。每加一层，会大约减少10%的perplexity。所以使用deep stacked LSTM。</p>
<h2 id="残差连接">残差连接</h2>
<p><a href="http://www.zhuanzhi.ai/#/document/79d7c640238540eba497a7267a07d805" target="_blank" rel="noopener">残差网络讲解</a> 。</p>
<p>虽然深层LSTM比浅层LSTM效果好，但是如果只是简单堆积的话，只在几个少数层效果才可以。经过试验，4层的话估计效果还可以，6层大部分都不好，8层的话，效果就相当差了。这是因为网络会变得很慢和很难训练，很大程度是因为梯度爆炸和梯度消失的问题。</p>
<p>根据在中间层和目标之间建立差别的思想，引入<strong>残差连接</strong>，如下图右边所示。其实就是<strong>把之前层的输入和当前的输出合并起来，作为下一层的输入</strong>。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/06_residual_LSTM.png" style="display:block; margin:auto" width="75%"></p>
<p>一些参数和符号说明，一下均是时刻<span class="math inline">\(t\)</span></p>
<ul>
<li><span class="math inline">\(\mathbf{x^i_t}\)</span> : 第<span class="math inline">\(i+1\)</span>层 <span class="math inline">\(\mathit{LSTM}_{i+1}\)</span>的输入。 即<strong>上标代表LSTM的层数，下标代表时间</strong>。</li>
<li><span class="math inline">\(\mathbf{W} ^i\)</span> : 第<span class="math inline">\(i\)</span>层LSTM的参数</li>
<li><span class="math inline">\(\mathbf {h} _t^i\)</span> : 第<span class="math inline">\(i\)</span>层输出隐状态</li>
<li><span class="math inline">\(\mathbf {c} ^i_t\)</span> : 第<span class="math inline">\(i\)</span>层输出单元状态</li>
</ul>
<p>那么<span class="math inline">\(LSTM_i\)</span>和<span class="math inline">\(LSTM_{i+1}\)</span>是这样交互的。即<strong>层层纵向传递输入，时间横向传递隐状态和单元状态</strong>。 <span class="math display">\[
\begin{align}
&amp;  \mathbf{c}_t^i, \mathbf{h}_t^i 
= LSTM_i(\mathbf{c}_{t-1}^i, \mathbf{h}_{t-1}^i, \mathbf x_{t}^{i-1} ; \; \mathbf W^i ) \\
&amp;  \mathbf x_t^i = \mathbf h_t^i \quad\quad\quad\quad  \color{blue}{普通连接：i+1层输入=i层隐层输出} \\
&amp; \mathbf{x}_t^i 
= \mathbf h_t^i + \mathbf{x}_t^{i-1} \quad \color{blue} {残差连接：第i+1层的输入=i层输入+i层隐层输出} \\
&amp; \mathbf{c}_t^{i+1}, \mathbf{h}_t^{i+1} 
= LSTM_{i+1} (\mathbf c_{t-1}^{i+1}, \mathbf {h} _{t-1}^{i+1}, x_{t}^i ; \; \mathbf W ^{i+1}) \\
\end{align}
\]</span> 残差连接可以在反向传播的时候大幅度提升梯度流，这样就可以训练很深的网络。</p>
<h2 id="双向encoder">双向Encoder</h2>
<p>一般输入的句子是从左到右，输出也是。但是由于语言的复杂性，有助于翻译的关键信息可能在原句子的不同地方。为了在Encoder中的每一个点都有最好的上下文语义，所以需要使用双向LSTM。</p>
<p>这里只在Encoder的最底层使用双向LSTM，其余各层均使用单向的LSTM。双向LSTM训练完成之后，再训练别的层。</p>
<p><span class="math inline">\(LSTM_f\)</span>从左到右处理句子，<span class="math inline">\(LSTM_b\)</span>从右到左处理句子。<strong>把两个方向的信息<span class="math inline">\(\mathbf{x^f_i}\)</span>和<span class="math inline">\(\mathbf{x}^b_i\)</span>concat起来，传递给下一层。</strong></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/33_bidir_lstm.png" style="display:block; margin:auto" width="50%"></p>
<h2 id="模型并行性">模型并行性</h2>
<p>模型很复杂，所以使用模型并行和数据并行，来加速。</p>
<p><strong>数据并行</strong></p>
<p>数据并行很简单，使用<a href="https://wlypku.github.io/2016/10/06/Downpour-SGD/" target="_blank" rel="noopener">大规模分布式深度网络(Downpour SGD)</a> 同时训练<span class="math inline">\(n\)</span>个模型副本，它们都使用相同的模型参数，但是每个副本会使用Adam和SGD去异步地更新参数。每个模型副本一次处理m个句子。一般实验中，<span class="math inline">\(n=10, m=128\)</span>。</p>
<p><strong>模型并行</strong></p>
<p>除了数据并行以外，模型并行也会加速每个副本的梯度计算。Encoder和Decoder会进行深度去划分，一般每一层会放在一个单独的GPU上。除了第一层的Encoder之外，所有的层都是单向的，所以第<span class="math inline">\(i+1\)</span>层可以提前运行，不必等到第<span class="math inline">\(i\)</span>层完全训练好了才进行训练。Softmax也会进行划分，每个处理一部分的单词。</p>
<p><strong>并行带来的约束</strong></p>
<p>由于要并行计算，所以我们不能够在Encoder的所有层上使用双向LSTM。因为如果使用了双向的，上面层必须等到下面层前向后向完全训练好之后才能开始训练，就不能并行计算。在Attention上，我们也只能使用最顶层的Encoder和最底层的Decoder进行对齐计算。如果使用顶层Encoder和顶层Decoder，那么整个Decoder将没有任何并行性，也就享受不到多个GPU的快乐了。</p>
<h1 id="分割技巧">分割技巧</h1>
<p>一般NMT都是的词汇表都是定长的，但是实际上词汇表却是开放的。比如人名、地名和日期等等。一般有两种方法去处理OOV(out-of-vocabulary)单词，复制策略和sub-word单元策略。GNMT是使用sub-word单元策略，也称为wordpiece模型。</p>
<p><strong>复制策略</strong></p>
<p>有下面几种复制策略</p>
<ul>
<li>把稀有词汇直接复制到目标句子中，因为大部分都是人名和地名</li>
<li>使用注意力模型，添加特别的注意力</li>
<li>使用一个外部的对齐模型，去处理稀有词汇</li>
<li>使用一个复杂的带有特殊目的的指出网络，去指出稀有词汇</li>
</ul>
<p><strong>sub-word单元</strong></p>
<p>比如字符，混合单词和字符，更加智能的sub-words。</p>
<h2 id="wordpiece-模型">Wordpiece 模型</h2>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-12-13T08:34:31.547Z" itemprop="dateUpdated">2018-12-13 16:34:31</time>
</span><br>


        
        <br>原始链接：<a href="/2017/10/17/25-google-nmt/" target="_blank" rel="external">http://plmsmile.github.io/2017/10/17/25-google-nmt/</a>
        
    </div>
    
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/">RNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经机器翻译/">神经机器翻译</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/论文笔记/">论文笔记</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/10/17/25-google-nmt/&title=《谷歌RNN翻译模型》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/10/17/25-google-nmt/&title=《谷歌RNN翻译模型》 — PLM's Notes&source=
谷歌神经机器翻译系统，Transformer之前最强的基于RNN的翻译模型
" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/10/17/25-google-nmt/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《谷歌RNN翻译模型》 — PLM's Notes&url=http://plmsmile.github.io/2017/10/17/25-google-nmt/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/10/17/25-google-nmt/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/10/18/rnn/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">循环神经网络</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/10/16/24-hexo-problems/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">博客搭建及相关问题</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'true' == 'true',
            verify: 'false' == 'true',
            appId: "kR8nND4dcsWgqDWIjpiH4YFj-gzGzoHsz",
            appKey: "il7PLkcJCfDBXMR6XirLdO2K",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->







</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check" data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="bottom">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            <span>
            PLM's Notes &nbsp; &copy; &nbsp
            </span>
            2016 - 2018
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/10/17/25-google-nmt/&title=《谷歌RNN翻译模型》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/10/17/25-google-nmt/&title=《谷歌RNN翻译模型》 — PLM's Notes&source=
谷歌神经机器翻译系统，Transformer之前最强的基于RNN的翻译模型
" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/10/17/25-google-nmt/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《谷歌RNN翻译模型》 — PLM's Notes&url=http://plmsmile.github.io/2017/10/17/25-google-nmt/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/10/17/25-google-nmt/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACL0lEQVR42u3ay07DMBAF0P7/T5dtJWh67zgg4hyvUKvGPlkM8/DjEa/nyzr+5Pu3r5+3zzx5YWBgXJbxPFzHGxz/6h0vOUN+NgwMjPsw3j26PcoMub4XBgYGRh4ok/QxSRkxMDAwzgq4bbGaIzEwMDBmRexKK61NIn+xFsfAwLggI++6//3fvzLfwMDAuBTjWa6VAJ2/oPpUGBgYWzPyANc2yPJnzgIuBgbGfRhJadoWkG2peQxILnBgYGDch9EOANr2/ezKRXIeDAyMvRn5g1rwLIDmKSYGBsZ9GPmAcHgBYuEFFd9iYGBszZiFvKTczQNuvmPUjMPAwNiU0aZlyQWv2Wghv3D2tumGgYGxKaNt8a+XrLMdZy8aAwNjP0Ye7NprFnkRuxK4MTAw9mMk7bZ2YNkmeW0R+5htjIGBcVlGW6DmR1lp4bWXPzAwMO7DyMPobNC4UtAWLxoDA2M7xiyNW2m3Jc+pB6gYGBhbM5LgmB86TzTzgrkopDEwMG7AWClo2xdxVqPth4CLgYFxA8bxlrMDJeVuvmMR0TEwMLZmtCnarCidDSY/FLEYGBjbMZ7lajfIE8q2tVdnphgYGJdlnBCtR4VuMjQ97c4IBgbGFox2JDCrHHPMUhGLgYGxNSMPfEnkbpv++X+AD3kuBgYGRjluXBmODrNXDAwMjIVDr19aLeYbGBgYmzLaqxVtU+ys4vaEdhsGBsYFGbO8Kzlifuh2OHFa+oiBgfHfGV8yiiLgHwtiWwAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>

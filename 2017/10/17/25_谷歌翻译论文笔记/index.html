<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>谷歌翻译论文笔记 | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="论文笔记,神经机器翻译">
    <meta name="description" content="谷歌神经机器翻译系统   简介神经机器翻译是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点  训练和翻译都太慢了，花费代价很大 缺乏鲁棒性，特别是输入句子包含生僻词汇 精确度和速度也不行  传统NMT缺点神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。">
<meta name="keywords" content="论文笔记,神经机器翻译">
<meta property="og:type" content="article">
<meta property="og:title" content="谷歌翻译论文笔记">
<meta property="og:url" content="http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="谷歌神经机器翻译系统   简介神经机器翻译是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点  训练和翻译都太慢了，花费代价很大 缺乏鲁棒性，特别是输入句子包含生僻词汇 精确度和速度也不行  传统NMT缺点神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/05_gnmt.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/06_residual_LSTM.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/33_bidir_lstm.png">
<meta property="og:updated_time" content="2018-11-25T08:30:08.202Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="谷歌翻译论文笔记">
<meta name="twitter:description" content="谷歌神经机器翻译系统   简介神经机器翻译是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点  训练和翻译都太慢了，花费代价很大 缺乏鲁棒性，特别是输入句子包含生僻词汇 精确度和速度也不行  传统NMT缺点神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。">
<meta name="twitter:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/05_gnmt.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/">
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives">
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags">
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories">
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about">
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">谷歌翻译论文笔记</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">谷歌翻译论文笔记</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-10-17T05:25:38.000Z" itemprop="datePublished" class="page-time">
  2017-10-17
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/论文笔记/">论文笔记</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#简介"><span class="post-toc-number">1.</span> <span class="post-toc-text">简介</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#传统NMT缺点"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">传统NMT缺点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#GNMT的模型优点"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">GNMT的模型优点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#先进技术"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">先进技术</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#系统架构"><span class="post-toc-number">2.</span> <span class="post-toc-text">系统架构</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#系统总览"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">系统总览</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#残差连接"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">残差连接</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#双向Encoder"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">双向Encoder</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#模型并行性"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">模型并行性</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#分割技巧"><span class="post-toc-number">3.</span> <span class="post-toc-text">分割技巧</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Wordpiece-模型"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">Wordpiece 模型</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-25_谷歌翻译论文笔记" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">谷歌翻译论文笔记</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-10-17 13:25:38" datetime="2017-10-17T05:25:38.000Z" itemprop="datePublished">2017-10-17</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/论文笔记/">论文笔记</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <blockquote>
<p><a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="noopener">谷歌神经机器翻译系统</a></p>
</blockquote>
<p><img src="" style="display:block; margin:auto" width="60%"></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><code>神经机器翻译</code>是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点</p>
<ul>
<li>训练和翻译都太慢了，花费代价很大</li>
<li>缺乏鲁棒性，特别是输入句子包含生僻词汇</li>
<li>精确度和速度也不行</li>
</ul>
<h2 id="传统NMT缺点"><a href="#传统NMT缺点" class="headerlink" title="传统NMT缺点"></a>传统NMT缺点</h2><p>神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。</p>
<p>NMT避开了传统基于短语的翻译模型的很多缺点。但是，在实际中，NMT的准确度要比基于短语的翻译模型更差些。</p>
<p>NMT有3个主要的缺点：训练和推理速度太慢，不能有效处理稀有词汇，有时不能完全翻译原句子。</p>
<p><strong>训练和推理速度太慢</strong></p>
<p>训练大数据集，需要大量时间和资源；反馈太慢周期太长。加了一个小技巧，看结果要等很长时间。推理翻译的时候，要使用大量的参数去计算，也很慢。</p>
<p><strong>不能有效处理稀有词汇</strong></p>
<p>有两个方法去复制稀有单词：</p>
<ul>
<li>模仿传统对齐模型去训练1个<code>copy model</code> </li>
<li>使用注意力机制去复制</li>
</ul>
<p>但是效果都不是很好，都不可靠，不同语言的对齐效果差；在网络很深的时候，注意力机制的对齐向量也不稳定。而且，简单的复制过去也不是最好的办法，比如需要直译的时候。</p>
<p><strong>不能完整翻译整个句子</strong></p>
<p>不能覆盖整个输入句子的内容，然后会导致一些奇怪的翻译结果。</p>
<h2 id="GNMT的模型优点"><a href="#GNMT的模型优点" class="headerlink" title="GNMT的模型优点"></a>GNMT的模型优点</h2><p>采用的模型：<strong>深层LSTM</strong> 、<strong>Encoder8层</strong>、<strong>Decoder8层</strong>  。<a href="https://plmsmile.github.io/2017/10/18/rnn/">我的LSTM笔记</a>。   </p>
<p>各层之间使用<strong>残差连接</strong>促进梯度流，顶层Enocder到底层Decoder使用<strong>注意力连接</strong>，提高并行性。</p>
<p>进行翻译推断的时候，使用低精度算法，去加速翻译。</p>
<p>处理<strong>稀有词汇</strong>：使用<strong>sub-word单元</strong>，也称作wordpieces方法。把单词划分到有限的<strong>sub-word</strong> (wordpieces)单元集合，输入输出都这样。sub-word结合了字符分割模型的弹性和单词分割模型的效率。</p>
<p><code>Beam Search</code> 使用<strong>长度规范化</strong>和<strong>覆盖惩罚</strong>。覆盖惩罚就是说，希望，翻译的结果句子，尽量多地包含输入句子中的所有词汇。</p>
<p>使用强化学习去优化模型， 优化翻译的<code>BLEU</code> 分数。</p>
<h2 id="先进技术"><a href="#先进技术" class="headerlink" title="先进技术"></a>先进技术</h2><p>有很多先进的技术来提高NMT，下面这些都有论文的。</p>
<ul>
<li>利用attention去处理稀有词汇</li>
<li>建立翻译覆盖的机制</li>
<li>多任务和半监督训练，去合并使用更多数据</li>
<li>字符分割的encoder和decoder</li>
<li>使用subword单元处理稀疏的输出</li>
</ul>
<h1 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h1><h2 id="系统总览"><a href="#系统总览" class="headerlink" title="系统总览"></a>系统总览</h2><p><strong>架构</strong></p>
<p>有3个模块：Encoder，Decoder，Attention。</p>
<p>Encdoer：把句子转换成一系列的向量，每一个向量代表一个输入词汇（符号）。</p>
<p>Decoder：根据这些向量，每一时刻会生成一个目标词汇，直到EOS。</p>
<p>Attention：连接Encoder和Decoder，在解码的过程中，可以让Decoder有权重的有选择的关注输入句子的部分区域。</p>
<p><strong>符号说明</strong></p>
<ul>
<li>加粗小写代表<strong>向量</strong>，如$\boldsymbol {v, o_i}$</li>
<li>加粗大写，<strong>矩阵</strong>，如$\boldsymbol {U, W}$  和$\mathbf{U, W}$</li>
<li>手写体，<strong>集合</strong>，如$\mathcal{V, F}$</li>
<li>大写字母，<strong>句子</strong>，如$X, Y$</li>
<li>小写字母，<strong>单个符号</strong>，如$x_1, x_2$</li>
</ul>
<p><strong>Encoder</strong></p>
<p>输入句子和目标句子组成一个Pair $(X, Y)$，其中输入句子$X = x_1, x_2, \cdots, x_M$ ，$M$ 个单词，翻译的输出目标句子$Y = y_1, y_2, \cdots, y_N$ ，有$N$个单词。</p>
<p>Encoder其实就是一个转换函数，得到$M$个长度固定的向量，也就是其中Encoder对各个$x_i$的<strong>编码向量 $\mathbf{x_i}$ </strong>：<br>$$<br>\mathbf {x_1, x_2, \cdots, x_M} = \mathit{EncoderRNN} (x_1, x_2, \cdots, x_n)<br>$$<br>使用链式条件概率可得到翻译概率$\color{blue} {P (Y \mid X)}$ ，其中$y_0$是起始符号$SOS$ 。<br>$$<br>\begin{align}<br>P(Y \mid X)<br>&amp; = P(Y \mid \mathbf{x_1, x_2, \cdots, x_M}) \<br>&amp; =\prod_{i=1}^N P(y_i \mid y_0, \cdots, y_{i-1}; \mathbf{x_1, x_2, \cdots, x_M}) \<br>\end{align}<br>$$<br><strong>Decoder</strong></p>
<p>在翻译$y_i$的时候， 利用Encoder得到的编码向量$\mathbf{x_i}$ 和 $y_0 \sim y_{i-1}$ 来进行计算概率翻译<br>$$<br>P(y_i \mid y_0, \cdots, y_{i-1}; \mathbf{x_1, x_2, \cdots, x_M})<br>$$<br>Decoder是由<strong>RNN+Softmax</strong>构成的。会得到一个<strong>隐状态$\mathbf{y_i}$ 向量</strong>，有2个作用：</p>
<ul>
<li>作为下一个RNN的输入</li>
<li>$\mathbf{y_i}$经过softmax得到概率分布， 选出$y_i$ 输出符号</li>
</ul>
<p><strong>Attention</strong></p>
<p>在之前的文章里有介绍<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/">论文</a> 和 <a href="https://plmsmile.github.io/2017/10/10/attention-model/">通俗理解</a>，其实就是<strong>影响力模型</strong>。原句子的各个单词对翻译当前单词分别有多少的影响力，也叫作<strong>对齐概率</strong>吧。使用decoder-RNN的输出$\mathbf{y_{t-1}}$ 向量作为时刻$t$的输入。</p>
<p>时刻$t$，给定$\mathbf{y_{t-1}}$ </p>
<p>有3个符号定义：</p>
<ul>
<li>$s_i$ ： <strong>$y_t$与$x_i$的得分</strong>，在luong论文里面有3种计算方式，分别是dot, general和concat。 </li>
<li>$p_i$ ：<strong>$y_t$与$x_i$的对齐概率</strong>，$(p_1, p_2, \cdots, p_M)$ 联合起来就是$y_t$与$X$的<strong>对齐向量</strong>。其实就是对得分softmax。</li>
<li>$\mathbf{a_t}$ ：<strong>带注意力的语义向量</strong>。对于所有的$x_i$，使用$y_t$与它的对齐概率$p_i$乘以本身的编码向量$\mathbf{x_i}$，得到$x_i$传达的语义，再对所有的语义求和，即得到总体的带有注意力的语义。   </li>
</ul>
<p>整体详细计算的流程，如下面的公式：<br>$$<br>\begin {align}<br>&amp;  s_i = \mathit{AttentionFunction} (\mathbf{y_{t-1}}, \mathbf{x_i}), \quad i \in [1, M] \<br>&amp; p_i = \frac {\exp (s_i)}{\sum_{j=1}^M \exp(s_j)}  \quad i \in [1, M] \<br>&amp;  \mathbf{a_t} = \sum_{i=1}^M p_i \cdot \mathbf{x_i} \quad \color{blue}{对所有带注意力的x_i的语义求和得总体的语义}<br>\end{align}<br>$$<br>计算打分的函数即$\mathit{AttentionFunction}$是一个有隐藏层的前馈网络！实现是Badh这个人的，不是Luong的。</p>
<p><strong>系统架构图说明</strong></p>
<p>架构图如下</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/05_gnmt.png" style="display:block; margin:auto" width="80%"></p>
<p>Encoder是8层的LSTM：最底层是双向的LSTM，得到两个方向的信息；上面7层都是单向的。Encoder和Decoder的残差连接都是从第3层开始的。</p>
<p>训练时，会让Encoder最底层的双向的LSTM开始训练，完成之后，再训练别的层，每层都用单独的GPU。</p>
<p>为了提高并行性，Decoder最底层，只是为了用来计算Attention Context。带注意力的语义计算好之后，会单独发给其它的各个层。</p>
<p><strong>经验说明</strong></p>
<p>实验结果得到，要想NMT有好效果，<strong>Encoder和Decoder的网络层数一定要够深</strong>，才能发现2种语言之间的细微异常规则。和这个同理，深层LSTM比浅层LSTM明显效果好。每加一层，会大约减少10%的perplexity。所以使用deep stacked LSTM。</p>
<h2 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h2><p><a href="http://www.zhuanzhi.ai/#/document/79d7c640238540eba497a7267a07d805" target="_blank" rel="noopener">残差网络讲解</a> 。</p>
<p>虽然深层LSTM比浅层LSTM效果好，但是如果只是简单堆积的话，只在几个少数层效果才可以。经过试验，4层的话估计效果还可以，6层大部分都不好，8层的话，效果就相当差了。这是因为网络会变得很慢和很难训练，很大程度是因为梯度爆炸和梯度消失的问题。</p>
<p>根据在中间层和目标之间建立差别的思想，引入<strong>残差连接</strong>，如下图右边所示。其实就是<strong>把之前层的输入和当前的输出合并起来，作为下一层的输入</strong>。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/06_residual_LSTM.png" style="display:block; margin:auto" width="75%"></p>
<p>一些参数和符号说明，一下均是时刻$t$</p>
<ul>
<li>$\mathbf{x^i_t}$ : 第$i+1$层 $\mathit{LSTM}_{i+1}$的输入。  即<strong>上标代表LSTM的层数，下标代表时间</strong>。</li>
<li>$\mathbf{W} ^i$ : 第$i$层LSTM的参数</li>
<li>$\mathbf {h} _t^i$  : 第$i$层输出隐状态</li>
<li>$\mathbf {c} ^i_t$ : 第$i$层输出单元状态</li>
</ul>
<p>那么$LSTM_i$和$LSTM_{i+1}$是这样交互的。即<strong>层层纵向传递输入，时间横向传递隐状态和单元状态</strong>。<br>$$<br>\begin{align}<br>&amp;  \mathbf{c}_t^i, \mathbf{h}_t^i<br>= LSTM_i(\mathbf{c}<em>{t-1}^i, \mathbf{h}</em>{t-1}^i, \mathbf x_{t}^{i-1} ; \; \mathbf W^i ) \<br>&amp;  \mathbf x_t^i = \mathbf h_t^i \quad\quad\quad\quad  \color{blue}{普通连接：i+1层输入=i层隐层输出} \<br>&amp; \mathbf{x}_t^i<br>= \mathbf h_t^i + \mathbf{x}_t^{i-1} \quad \color{blue} {残差连接：第i+1层的输入=i层输入+i层隐层输出} \<br>&amp; \mathbf{c}_t^{i+1}, \mathbf{h}<em>t^{i+1}<br>= LSTM</em>{i+1} (\mathbf c_{t-1}^{i+1}, \mathbf {h} <em>{t-1}^{i+1}, x</em>{t}^i ; \; \mathbf W ^{i+1}) \<br>\end{align}<br>$$<br> 残差连接可以在反向传播的时候大幅度提升梯度流，这样就可以训练很深的网络。</p>
<h2 id="双向Encoder"><a href="#双向Encoder" class="headerlink" title="双向Encoder"></a>双向Encoder</h2><p>一般输入的句子是从左到右，输出也是。但是由于语言的复杂性，有助于翻译的关键信息可能在原句子的不同地方。为了在Encoder中的每一个点都有最好的上下文语义，所以需要使用双向LSTM。</p>
<p>这里只在Encoder的最底层使用双向LSTM，其余各层均使用单向的LSTM。双向LSTM训练完成之后，再训练别的层。</p>
<p>$LSTM_f$从左到右处理句子，$LSTM_b$从右到左处理句子。<strong>把两个方向的信息$\mathbf{x^f_i}$和$\mathbf{x}^b_i$concat起来，传递给下一层。</strong>    </p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/paper/33_bidir_lstm.png" style="display:block; margin:auto" width="50%"></p>
<h2 id="模型并行性"><a href="#模型并行性" class="headerlink" title="模型并行性"></a>模型并行性</h2><p>模型很复杂，所以使用模型并行和数据并行，来加速。</p>
<p><strong>数据并行</strong></p>
<p>数据并行很简单，使用<a href="https://wlypku.github.io/2016/10/06/Downpour-SGD/" target="_blank" rel="noopener">大规模分布式深度网络(Downpour SGD)</a> 同时训练$n$个模型副本，它们都使用相同的模型参数，但是每个副本会使用Adam和SGD去异步地更新参数。每个模型副本一次处理m个句子。一般实验中，$n=10, m=128$。 </p>
<p><strong>模型并行</strong></p>
<p>除了数据并行以外，模型并行也会加速每个副本的梯度计算。Encoder和Decoder会进行深度去划分，一般每一层会放在一个单独的GPU上。除了第一层的Encoder之外，所有的层都是单向的，所以第$i+1$层可以提前运行，不必等到第$i$层完全训练好了才进行训练。Softmax也会进行划分，每个处理一部分的单词。</p>
<p><strong>并行带来的约束</strong></p>
<p>由于要并行计算，所以我们不能够在Encoder的所有层上使用双向LSTM。因为如果使用了双向的，上面层必须等到下面层前向后向完全训练好之后才能开始训练，就不能并行计算。在Attention上，我们也只能使用最顶层的Encoder和最底层的Decoder进行对齐计算。如果使用顶层Encoder和顶层Decoder，那么整个Decoder将没有任何并行性，也就享受不到多个GPU的快乐了。</p>
<h1 id="分割技巧"><a href="#分割技巧" class="headerlink" title="分割技巧"></a>分割技巧</h1><p>一般NMT都是的词汇表都是定长的，但是实际上词汇表却是开放的。比如人名、地名和日期等等。一般有两种方法去处理OOV(out-of-vocabulary)单词，复制策略和sub-word单元策略。GNMT是使用sub-word单元策略，也称为wordpiece模型。</p>
<p><strong>复制策略</strong></p>
<p>有下面几种复制策略</p>
<ul>
<li>把稀有词汇直接复制到目标句子中，因为大部分都是人名和地名</li>
<li>使用注意力模型，添加特别的注意力</li>
<li>使用一个外部的对齐模型，去处理稀有词汇</li>
<li>使用一个复杂的带有特殊目的的指出网络，去指出稀有词汇</li>
</ul>
<p><strong>sub-word单元</strong></p>
<p>比如字符，混合单词和字符，更加智能的sub-words。</p>
<h2 id="Wordpiece-模型"><a href="#Wordpiece-模型" class="headerlink" title="Wordpiece 模型"></a>Wordpiece 模型</h2>
        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-11-25T08:30:08.202Z" itemprop="dateUpdated">2018-11-25 16:30:08</time>
</span><br>


        
        <br>原始链接：<a href="/2017/10/17/25_谷歌翻译论文笔记/" target="_blank" rel="external">http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/</a>
        
    </div>
    
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经机器翻译/">神经机器翻译</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/论文笔记/">论文笔记</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/&title=《谷歌翻译论文笔记》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/&title=《谷歌翻译论文笔记》 — PLM's Notes&source=NLP, DL, MRC." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《谷歌翻译论文笔记》 — PLM's Notes&url=http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/10/18/rnn/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">循环神经网络</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/10/17/谷歌翻译论文笔记/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">谷歌翻译论文笔记</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'true' == 'true',
            verify: 'false' == 'true',
            appId: "kR8nND4dcsWgqDWIjpiH4YFj-gzGzoHsz",
            appKey: "il7PLkcJCfDBXMR6XirLdO2K",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->







</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check" data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="bottom">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            <span>
            PLM's Notes &nbsp; &copy; &nbsp
            </span>
            2016 - 2018
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/&title=《谷歌翻译论文笔记》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/&title=《谷歌翻译论文笔记》 — PLM's Notes&source=NLP, DL, MRC." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《谷歌翻译论文笔记》 — PLM's Notes&url=http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACr0lEQVR42u3aMW7DMAwF0Nz/0unaIrDzSYqFh6epMFxZT4PEkHy94vH+NT6fX72TvH//5v07xwYeHh7eYOmf4+qd/H/zjahuR/JdPDw8vD3e1YmaHPH5Uu5P72Qr8zXj4eHhPZN3v/T758kM1fAaDw8P75m8KvtUgI6Hh4f3NF51uvxiqMJ6YfeBXAseHh5ezMurSM/5e6W+h4eHhzeuqicFreSI781/YLV4eHh4C7z8wK22Bcwbp6oB9+WbeHh4eEd598d9ssRq+ap3uOcb+ucJHh4e3hovT7N+uWGCtoDqJTFZCR4eHt4Gb9IOVWVMCmbV7cDDw8Pb4yUfK+9QsZGrN1vyBA8PD2+DN/qpP05hVPHV0hoeHh7eHi9HJmmLfHGTdEN1PXh4eHineL2DuJeSqAbx86IaHh4e3h6vWv7vpV97W9C7SKKJ8PDw8Fq8ZvQ9TlJUU8OTliw8PDy8PV5elJoXsd6tcb/1UQEMDw8Pb8xLSkfVcLkaslfTH/dpi8sbDw8PD2+BN+lTKjdCjZuuojnx8PDwFnjJIdsrYu2VwcpXFB4eHt5RXv7haoIgCXmjsHiyKXh4eHgLvFMFsGrzVm+51eQyHh4e3h4v//FfLUfN0w15SeyVDzw8PLwxr4epJlurLVO9C+ZLrgUPDw9vgVdNofaapU61JpQvBjw8PLyjvElDQJ6MSNi9bcLDw8P7f96pRG3eWHDg0E8SuHh4eHhHee/iSMLcXitVHtAXimF4eHh4C7xqSSw/0PPAvbqtk8YsPDw8vFO83tQH0gTFYltyIV3+YsDDw8Nb4PWC4DwRXA3QJ9uEh4eH90xeL8DtheD51/Hw8PCezJskT3vp2mZCBA8PD2+NVw18k6N5kuqtFti+XAx4eHh4R3mTo/xsSayX8miS8PDw8Dq8H2USnHEgD/uQAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>

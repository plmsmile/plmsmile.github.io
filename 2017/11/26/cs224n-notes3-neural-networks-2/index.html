<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>神经网络-过拟合-预处理-BN | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="数据预处理,正则化,范数,Dropout,PCA,白化,BN">
    <meta name="description" content="过拟合过拟合训练数据很少，或者训练次数很多，会导致过拟合。避免过拟合的方法有如下几种：  early stop 数据集扩增 正则化 （L1， L2权重衰减） Dropout 决策树剪枝（尽管不属于神经网络）  现在一般用L2正则化+Dropout。 过拟合时，拟合系数一般都很大。过拟合需要顾及到所有的数据点，意味着拟合函数波动很大。  看到，在某些很小的区间内里，函数值的变化很剧烈。意味着这些小区">
<meta name="keywords" content="数据预处理,正则化,范数,Dropout,PCA,白化,BN">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络-过拟合-预处理-BN">
<meta property="og:url" content="http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="过拟合过拟合训练数据很少，或者训练次数很多，会导致过拟合。避免过拟合的方法有如下几种：  early stop 数据集扩增 正则化 （L1， L2权重衰减） Dropout 决策树剪枝（尽管不属于神经网络）  现在一般用L2正则化+Dropout。 过拟合时，拟合系数一般都很大。过拟合需要顾及到所有的数据点，意味着拟合函数波动很大。  看到，在某些很小的区间内里，函数值的变化很剧烈。意味着这些小区">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/overfit.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/compare-neurons-num.jpg">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/compare-lambda-regularize.jpg">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/dropout.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/data-process.jpg">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/data-pca-process.jpg">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB1.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB2.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB3.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB4.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB5.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB6.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB8.png">
<meta property="og:updated_time" content="2018-11-25T08:30:09.997Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络-过拟合-预处理-BN">
<meta name="twitter:description" content="过拟合过拟合训练数据很少，或者训练次数很多，会导致过拟合。避免过拟合的方法有如下几种：  early stop 数据集扩增 正则化 （L1， L2权重衰减） Dropout 决策树剪枝（尽管不属于神经网络）  现在一般用L2正则化+Dropout。 过拟合时，拟合系数一般都很大。过拟合需要顾及到所有的数据点，意味着拟合函数波动很大。  看到，在某些很小的区间内里，函数值的变化很剧烈。意味着这些小区">
<meta name="twitter:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/overfit.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/">
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives">
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags">
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories">
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about">
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">神经网络-过拟合-预处理-BN</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">神经网络-过拟合-预处理-BN</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-11-26T08:21:23.000Z" itemprop="datePublished" class="page-time">
  2017-11-26
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#过拟合"><span class="post-toc-number">1.</span> <span class="post-toc-text">过拟合</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#过拟合-1"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">过拟合</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#范数"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">范数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#L2正则化权重衰减"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">L2正则化权重衰减</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#实例说明"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">实例说明</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#L1正则化"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">L1正则化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#随机失活Dropout"><span class="post-toc-number">1.6.</span> <span class="post-toc-text">随机失活Dropout</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#数据预处理"><span class="post-toc-number">2.</span> <span class="post-toc-text">数据预处理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#中心化"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">中心化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#标准化"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">标准化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#PCA"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">PCA</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#白化"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">白化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#权重初始化"><span class="post-toc-number">3.</span> <span class="post-toc-text">权重初始化</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#小随机数初始化"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">小随机数初始化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#稀疏和偏置初始化"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">稀疏和偏置初始化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Batch-Normalization"><span class="post-toc-number">4.</span> <span class="post-toc-text">Batch Normalization</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#训练速度分析"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">训练速度分析</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#敏感度问题"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">敏感度问题</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#BN算法"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">BN算法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#BN的优点"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">BN的优点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#效果图片展示"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">效果图片展示</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-cs224n-notes3-neural-networks-2" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">神经网络-过拟合-预处理-BN</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-11-26 16:21:23" datetime="2017-11-26T08:21:23.000Z" itemprop="datePublished">2017-11-26</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h1><h2 id="过拟合-1"><a href="#过拟合-1" class="headerlink" title="过拟合"></a>过拟合</h2><p>训练数据很少，或者训练次数很多，会导致<code>过拟合</code>。避免过拟合的方法有如下几种：</p>
<ul>
<li>early stop</li>
<li>数据集扩增</li>
<li>正则化 （L1， <strong>L2权重衰减</strong>）</li>
<li>Dropout</li>
<li>决策树剪枝（尽管不属于神经网络）</li>
</ul>
<p>现在一般用L2正则化+Dropout。</p>
<p><strong>过拟合时</strong>，<strong>拟合系数一般都很大</strong>。过拟合需要顾及到所有的数据点，意味着<strong>拟合函数波动很大</strong>。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/overfit.png" style="display:block; margin:auto" width="50%"></p>
<p>看到，在某些很小的区间内里，函数值的<strong>变化很剧烈</strong>。意味着这些小区间的<strong>导数值（绝对值）非常大</strong>。由于自变量值可大可小，所以只有<strong>系数足够大</strong>，才能保证导数值足够大。</p>
<p>所以：<strong>过拟合时，参数一般都很大</strong>。<strong>参数较小时，意味着模型复杂度更低，对数据的拟合刚刚好</strong>， 这也是<code>奥卡姆剃刀</code>法则。 </p>
<h2 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h2><p><strong>向量范数</strong></p>
<p>$x \in \mathbb {R}^d$</p>
<table>
<thead>
<tr>
<th>范数</th>
<th>定义</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-范数</td>
<td>$\left \</td>
<td>x\right\</td>
<td>_1 = \sum_i^d \</td>
<td>x_i\</td>
<td>$， <strong>绝对值之和</strong></td>
</tr>
<tr>
<td>2-范数</td>
<td>$\left \</td>
<td>x\right\</td>
<td>_2 = \left(\sum_i^d \vert x_i\vert^2\right)^{\frac{1}{2}}$， <strong>绝对值平方之和再开方</strong></td>
</tr>
<tr>
<td>p-范数</td>
<td>$\left \</td>
<td>x\right\</td>
<td>_p = \left(\sum_i^d \</td>
<td>x_i\</td>
<td>^p\right)^{\frac{1}{p}}$， <strong>绝对值的p次方之和的$\frac{1}{p}​$次幂</strong></td>
</tr>
<tr>
<td>$\infty$-范数</td>
<td>$\left \</td>
<td>x\right\</td>
<td><em>\infty = \max</em>\limits i \</td>
<td>x_i\</td>
<td>$   ，绝对值的最大值</td>
</tr>
<tr>
<td>-$\infty$-范数</td>
<td>$\left \</td>
<td>x\right\</td>
<td><em>{-\infty} = \min</em>\limits i \</td>
<td>x_i\</td>
<td>$  ，绝对值的最小值</td>
</tr>
</tbody>
</table>
<p><strong>矩阵范数</strong></p>
<p>$A \in \mathbb R^{m \times n}$</p>
<table>
<thead>
<tr>
<th>范数</th>
<th>定义</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-范数</td>
<td>$\left \</td>
<td>A\right\</td>
<td><em>1 =  \max \limits</em>{j}\sum_i^m \</td>
<td>a_{ij}\</td>
<td>$，<strong>列和范数</strong>，矩阵列向量绝对值之和的最大值。</td>
</tr>
<tr>
<td>$\infty$-范数</td>
<td>$\left \</td>
<td>A\right\</td>
<td><em>\infty = \max</em>\limits i \sum_{j}^{n}\</td>
<td>a_{ij}\</td>
<td>$ ，<strong>行和范数</strong>，所有行向量绝对值之和的最大值。</td>
</tr>
<tr>
<td>2-范数</td>
<td>$\left \</td>
<td>A\right\</td>
<td><em>2 = \sqrt{\lambda</em>{m}}$ ， 其中$\lambda_m$是$A^TA$的<strong>最大特征值</strong>。</td>
</tr>
<tr>
<td>F-范数</td>
<td>$\left \</td>
<td>A\right\</td>
<td>_F = \left(\sum_i^m \sum_j^n a_{ij}^2\right)^{\frac{1}{2}}$，<strong>所有元素的平方之和，再开方。或者不开方</strong>， L2正则化就直接平方，不开方。</td>
</tr>
</tbody>
</table>
<h2 id="L2正则化权重衰减"><a href="#L2正则化权重衰减" class="headerlink" title="L2正则化权重衰减"></a>L2正则化权重衰减</h2><p><strong>为了避免过拟合</strong>，使用L2正则化参数。$\lambda$是正则项系数，<strong>用来权衡正则项和默认损失的比重</strong>。$\lambda$ 的选取很重要。<br>$$<br>J_R = J + \lambda \sum_{i=1}^L \left | W^{(i)}\right |_F<br>$$<br>L2惩罚更倾向于<strong>更小更分散</strong>的权重向量，鼓励使用所有维度的特征，而不是只依赖其中的几个，这也避免了过拟合。</p>
<p><strong>标准L2正则化</strong></p>
<p>$\lambda$ 是<code>正则项系数</code>，$n$是数据数量，$w$是模型的参数。<br>$$<br>C = C_0 + \frac {\lambda} {2n} \sum_w w^2<br>$$<br>$C$对参数$w$和$b$的<code>偏导</code>：<br>$$<br>\begin {align}<br>&amp; \frac{\partial C}{\partial w}  =  \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} w  \<br>&amp; \frac{\partial C}{\partial b}  =  \frac{\partial C_0}{\partial b} \<br>\end{align}<br>$$<br><strong>更新参数</strong> ：可以看出，正则化$C$对$w$有影响，对$b$无影响。<br>$$<br>\begin{align}<br>w<br>&amp;= w - \alpha \cdot  \frac{\partial C}{\partial w} \<br>&amp;=  (1 - \frac{\alpha \lambda}{n})w - \alpha \frac{\partial C_0}{\partial w} \<br>\end{align}<br>$$<br>从上式可以看出：</p>
<ul>
<li>不使用正则化时，$w$的系数是1</li>
<li>使用正则化时<ul>
<li>$w$的系数是$1 - \frac{\alpha \lambda}{n} &lt; 1$ ，效果是减小$w$， 所以是<strong>权重衰减</strong> <code>weight decay</code> </li>
<li>当然，$w$具体增大或减小，还取决于后面的导数项</li>
</ul>
</li>
</ul>
<p><strong>mini-batch随机梯度下降</strong></p>
<p>设$m$ 是这个batch的样本个数，有更新参数如下，即求<strong>batch个C对w的平均偏导值</strong><br>$$<br>\begin {align}<br>&amp; w =  (1 - \frac{\alpha \lambda}{n})w - \frac{\alpha}{m} \cdot \sum_{i=1}^{m}\frac{\partial C_i}{\partial w} \<br>&amp; b = b - \frac{\alpha}{m} \cdot \sum_{i=1}^{m}\frac{\partial C_i}{\partial b} \<br>\end{align}<br>$$<br>所以，权重衰减后一般可以减小过拟合。  L2正则化比L1正则化<strong>更加发散</strong>，权值也会被限制的更小。 一般使用L2正则化。</p>
<p>还有一种方法是<code>最大范数限制</code>：给范数一个上界$\left |  w \right | &lt; c$ ，  可以在学习率太高的时候网络不会爆炸，因为更新总是有界的。 </p>
<h2 id="实例说明"><a href="#实例说明" class="headerlink" title="实例说明"></a>实例说明</h2><p>增加网络的层的数量和尺寸时，网络的容量上升，多个神经元一起合作，可以表达各种复杂的函数。</p>
<p>如下图，2分类问题，有噪声数据。</p>
<p>一个隐藏层。神经元数量分别是3、6、20。很明显20<strong>过拟合</strong>了，拟合了所有的数据。<code>正则化</code>就是<strong>处理过拟合</strong>的非常好的办法。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/compare-neurons-num.jpg" style="display:block; margin:auto" width="60%"></p>
<p>对20个神经元的网络，使用正则化，解决过拟合问题。正则化强度$\lambda$很重要。 </p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/compare-lambda-regularize.jpg" style="display:block; margin:auto" width="60%"></p>
<h2 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h2><p>正则化loss如下：<br>$$<br>C = C_0 + \frac {\lambda} {n} \sum_w |w|<br>$$<br>对$w$的<strong>偏导</strong>， 其中$\rm{sgn}(w)$是<code>符号函数</code>：<br>$$<br>\frac{\partial C}{\partial w}  =  \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} \cdot \rm{sgn}(w)<br>$$<br>更新参数：<br>$$<br>w<br>=  w  - \frac{\alpha \lambda}{n} \cdot \rm{sgn}(w) - \alpha \frac{\partial C_0}{\partial w}<br>$$<br>分析：$w$为正，减小；$w$为负，增大。所以<strong>L1正则化就是使参数向0靠近</strong>，是权重尽可能为0，减小网络复杂度，防止过拟合。   </p>
<p>特别地：当$w=0$时，不可导，就不要正则化项了。L1正则化更加稀疏。</p>
<h2 id="随机失活Dropout"><a href="#随机失活Dropout" class="headerlink" title="随机失活Dropout"></a>随机失活Dropout</h2><p>Dropout是<strong>非常有用</strong>的<strong>正则化</strong>的办法，它<strong>改变了网络结构</strong>。一般采用<strong>L2正则化+Dropout来防止过拟合</strong>。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/dropout.png" style="display:block; margin:auto" width="50%"></p>
<p>训练的时候，输出不变，<strong>随机以概率$p$保留神经元，$1-p$删除神经元</strong>（<strong>置位0</strong>）。每次迭代删除的神经元都不一样。</p>
<p>BP的时候，<strong>置位0的神经元的参数就不再更新</strong>， <strong>只更新前向时alive的神经元</strong>。</p>
<p>预测的时候，要保留所有的神经元，即不使用Dropout。</p>
<p>相当于训练了很多个（<strong>指数级数量</strong>）小网络（<code>半数网络</code>），在预测的时候综合它们的结果。随着训练的进行，大部分的半数网络都可以给出正确的分类结果。</p>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>用的很多的是0中心化。CNN中很少用PCA和白化。</p>
<p>应该：线划分训练、验证、测试集，<strong>只是从训练集中求平均值</strong>！<strong>然后各个集再减去这个平均值</strong>。</p>
<h2 id="中心化"><a href="#中心化" class="headerlink" title="中心化"></a>中心化</h2><p>也称作<code>均值减法</code>， 把数据所有维度变成<code>0均值</code>，其实就是减去均值。就是将<strong>数据迁移到原点</strong>。<br>$$<br>x = x - \rm{avg}(x) = x - \bar x<br>$$</p>
<h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><p>也称作<code>归一化</code>， 数据所有维度都归一化，使其数值<strong>变化范围都近似相等</strong>。</p>
<ul>
<li><strong>除以标准差</strong></li>
<li>最大值和最小值按照比例缩放到$(-1 ,1)$ 之间 </li>
</ul>
<p><code>方差</code>$s^2  = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2$ ，<code>标准差</code>就是$s$ 。数据<strong>除以标准差</strong>，接近<code>标准高斯分布</code>。<br>$$<br>x = \frac{x}{s}<br>$$<br><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/data-process.jpg" style="display:block; margin:auto" width="70%"></p>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p><a href="http://ufldl.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90" target="_blank" rel="noopener">斯坦福PCA</a> ，<a href="http://blog.csdn.net/wangjian1204/article/details/50642732" target="_blank" rel="noopener">CSDNPCA和SVD的区别和联系</a></p>
<p><strong>协方差</strong></p>
<p><code>协方差</code>就是<strong>乘积的期望-期望的乘积</strong>。<br>$$<br>\rm{Cov}(X, Y) = E(XY) - E(X)E(Y)<br>$$<br>协方差的性质如下：<br>$$<br>\begin{align}<br>&amp; \rm{Cov}(X, Y) = \rm{Conv}(Y, X) \ \<br>&amp; \rm{Cov}(aX, bY) = ab \cdot \rm{Conv}(Y, X) \ \<br>&amp; \rm{Cov}(X, X) = E(X^2) - E^2(X) = D(X) , \quad \text{三方公式}\ \<br>&amp; \rm{Cov}(X, C)  = 0 \ \<br>&amp; \rm{Cov}(X, Y) = 0 \leftrightarrow X与Y独立<br>\end{align}<br>$$<br>还有别的性质就看考研笔记吧。  </p>
<p><strong>奇异值分解</strong><br>$$<br>A_{m \times n} = U_{m \times m} \Sigma_{m \times n}  V^T_{n \times n}<br>$$<br>$V_{n \times n}$ ：$V$的列，一组对A正交输入或分析的基向量（线性无关）。这些向量是$M^TM$ 的特征向量。</p>
<p>$U_{m \times m}$ ：$U$的列，一组对A正交输出的<code>基向量</code> 。是$MM^T$的特征向量。 </p>
<p>$\Sigma_{m \times n}$：<code>对角矩阵</code>。对角元素按照从小到大排列，这些<strong>对角元素</strong>称为<code>奇异值</code>。  是$M^TM, MM^T$ 的特征值的非负平方根，并且与U和V的行向量对应。</p>
<p>记$r$是<strong>非0奇异值的个数</strong>，则A中仅有<strong>$r$个重要特征</strong>，其余特征都是噪声和冗余特征。</p>
<p><a href="https://www.zhihu.com/question/22237507" target="_blank" rel="noopener">奇异值的物理意义</a> </p>
<p><strong>利用SVD进行PCA</strong>  </p>
<p><strong>先将数据中心化</strong>。输入是$X \in \mathbb R^ {N \times D}$ ，则<code>协方差矩阵</code> 如下：<br>$$<br>\mathrm{Cov}(X) = \frac{X^TX}{N}  \;  \in  \mathbb R^{D \times D}<br>$$<br>比如X有a和b两维，<strong>均值均是0</strong>。那么$\rm{Cov}(ab)=E(ab)-0=(a_0b_0+a_1b_1+\cdots + a_nb_n) /n$ ，就得到了协方差值。</p>
<ul>
<li>中心化</li>
<li>计算$x$的<code>协方差矩阵cov</code></li>
<li>对协方差矩阵cov进行<code>svd分解</code>，得到<code>u, s, v</code></li>
<li><strong>去除x的相关性，旋转</strong>，$xrot = x \cdot u$ ，此时xrot的协方差矩阵只有对角线才有值，其余均为0</li>
<li>选出大于0的奇异值</li>
<li>数据降维</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_pca</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.random.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="comment"># 中心化</span></span><br><span class="line">    x -= np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">print</span> (x.shape)</span><br><span class="line">    <span class="comment"># 协方差</span></span><br><span class="line">    conv = np.dot(x.T, x) / x.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">print</span> (conv.shape)</span><br><span class="line">    <span class="keyword">print</span> (conv)</span><br><span class="line">    u, s, v = np.linalg.svd(conv)</span><br><span class="line">    <span class="keyword">print</span> (s)</span><br><span class="line">    <span class="keyword">print</span> (u.shape, s.shape, v.shape)</span><br><span class="line">    <span class="comment"># 大于0的奇异值</span></span><br><span class="line">    n_sv = np.where(s &gt; <span class="number">1e-5</span>)[<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">    print(n_sv)</span><br><span class="line">    <span class="comment"># 对数据去除相关性</span></span><br><span class="line">    xrot = np.dot(x, u)</span><br><span class="line">    <span class="keyword">print</span> (xrot.shape)</span><br><span class="line">    <span class="comment"># 数据降维</span></span><br><span class="line">    xrot_reduced = np.dot(x, u[:, :n_sv])</span><br><span class="line">    <span class="comment"># 降到了4维</span></span><br><span class="line">    <span class="keyword">print</span> (xrot_reduced.shape)</span><br></pre></td></tr></table></figure>
<h2 id="白化"><a href="#白化" class="headerlink" title="白化"></a>白化</h2><p><a href="http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96" target="_blank" rel="noopener">斯坦福白化</a> </p>
<p>白化希望特征之间的相关性较低，所有特征具有相同的协方差。白化后，得到均值为0，协方差相等的矩阵。对$xrot$除以特征值。<br>$$<br>x_{white} = \frac{x_{rot}}{\sqrt{\lambda + \epsilon}}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_white = xrot / np.sqrt(s + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>
<p>缺陷是：可能会夸大数据中的早上，因为把所有维度都拉伸到了相同的数值范围。可能有一些极少差异性（方差小）但大多数是噪声的维度。可以使用平滑来解决。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/data-pca-process.jpg" style="display:block; margin:auto" width="60%"></p>
<h1 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h1><p>如果数据恰当归一化以后，可以假设所有权重数值中大约一半为正数，一半为负数。所以期望参数值是0。</p>
<p><strong>千万不能够全零初始化</strong>。因为每个神经元的输出相同，BP时梯度也相同，参数更新也<strong>相同</strong>。神经元之间就<strong>失去了不对称性的源头</strong>。</p>
<h2 id="小随机数初始化"><a href="#小随机数初始化" class="headerlink" title="小随机数初始化"></a>小随机数初始化</h2><blockquote>
<p>如果神经元刚开始的时候是随机且不相等的，那么它们将<strong>计算出不同的更新</strong>，并<strong>成为网络的不同部分</strong>。</p>
</blockquote>
<p>参数接近于0单不等于0。使用<strong>零均值和标准差的高斯分布</strong>来生成随机数初始化参数，这样就<strong>打破了对称性</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,H)</span><br></pre></td></tr></table></figure>
<p>注意：不是参数值初始越小就一定好。参数小，意味着会减小BP中的梯度信号，在深度网络中，就会有问题。</p>
<p><strong>校准方差</strong></p>
<p>随着数据集的增长，随机初始化的神经元的<strong>输出数据分布的方差也会增大</strong>。可以<strong>使用$\frac{1}{\sqrt{n}}$ 校准方差</strong>。n是数据的数量。这样就能保证网络中<strong>所有神经元起始时有近似同样的输出分布</strong>。这样也能够提高收敛的速度。 感觉实际上就是做了一个归一化。</p>
<p>数学详细推导见<code>cs231n</code> ， $s = \sum_{i}^nw_ix_i$ ，假设w和x都服从同样的分布。想要输出s和输入x有同样的方差。<br>$$<br>\begin {align}<br>&amp; \because D(s) = n \cdot D(w)D(x),  \; D(s) = D(x)    \<br>&amp; \therefore D(w) = \frac{1}{n} \<br>&amp; \because D(w_{old}) = 1, \; D(aX) = a^2 D(X) \<br>&amp; \therefore D(w) = \frac{1}{n}D(w_{old}) = D(\frac{1}{\sqrt n} w_{old}) \<br>&amp; \therefore w = \frac{1}{\sqrt n} w_{old}<br>\end{align}<br>$$<br>所以要使用$\frac{1}{\sqrt{n}}$来标准化参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,H)/ sqrt(n)</span><br></pre></td></tr></table></figure>
<p><strong>经验公式</strong></p>
<p>对于某一层的方差，应该取决于两层的输入和输出神经元的数量，如下：<br>$$<br>\rm{D}(w) = \frac{2}{n_{in} + n_{out}}<br>$$<br><code>ReLU</code>来说，<strong>方差应该是$\frac{2}{n}$</strong>  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,H) * sqrt(<span class="number">2.0</span> / n)</span><br></pre></td></tr></table></figure>
<h2 id="稀疏和偏置初始化"><a href="#稀疏和偏置初始化" class="headerlink" title="稀疏和偏置初始化"></a>稀疏和偏置初始化</h2><p>一般稀疏初始化用的比较少。一般偏置都初始化为0。</p>
<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><p><a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-08-batch-normalization/" target="_blank" rel="noopener">莫凡python BN讲解</a> 和 <a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="noopener">CSDN-BN论文介绍</a> 。Batch Normalization和普通数据标准化类似，是将分散的数据标准化。</p>
<p><code>Batch Normalization</code>在神经网络非常流行，<strong>已经成为一个标准了</strong>。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB1.png" style="display:block; margin:auto" width="40%"></p>
<h2 id="训练速度分析"><a href="#训练速度分析" class="headerlink" title="训练速度分析"></a>训练速度分析</h2><p>网络训练的时候，每一层<strong>网络参数更新</strong>，会导致<strong>下一层输入数据分布的变化</strong>。这个称为<code>Internal Convariate Shift</code>。</p>
<p>需要对<strong>数据归一化的原因</strong> ：</p>
<ul>
<li>神经网络的本质是<strong>学习数据分布</strong>。如果训练数据与测试数据的分布不同，那么<code>泛化能力</code>也大大降低</li>
<li>如果每个batch数据分布不同（batch 梯度下降），每次迭代都要去<strong>学习适应不同的分布</strong>，会大大<strong>降低训练速度</strong></li>
</ul>
<p>深度网络，前几层数据微小变化，后面几层数据<strong>差距会积累放大</strong>。</p>
<p>一旦某一层网络输入<strong>数据发生改变</strong>，这层网络就需要去<strong>适应学习这个新的数据分布</strong>。如果训练数据的分布一直变化，那么就会<strong>影响网络的训练速度</strong>。</p>
<h2 id="敏感度问题"><a href="#敏感度问题" class="headerlink" title="敏感度问题"></a>敏感度问题</h2><p>神经网络中，如果使用<code>tanh</code>激活函数，初始权值是0.1。</p>
<p>输入$x=1$， 正常更新：<br>$$<br>z = wx = 0.1, \quad a(z_1) = 0.1 \quad  \to \quad a^\prime(z) = 0.99<br>$$<br>但是如果一开始输入 $x=20$ ，会导致梯度消失，不更新参数。<br>$$<br>z = wx = 2 ,\quad a(z) \approx 1  \quad \to \quad   a^\prime(z) = 0<br>$$<br>同样地，如果再输入$x=100$ ，神经元的输出依然是接近于1，不更新参数。<br>$$<br>z = wx = 10 ,\quad a(z) \approx 1  \quad \to \quad   a^\prime(z) = 0<br>$$<br>对于一个<strong>变化范围比较大</strong>特征维度，神经网络在初始阶段<strong>对它已经不敏感没有区分度</strong>了！</p>
<p>这样的问题，在神经网络的输入层和中间层都存在。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB2.png" style="display:block; margin:auto" width="50%"></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB3.png" style="display:block; margin:auto" width="50%"></p>
<h2 id="BN算法"><a href="#BN算法" class="headerlink" title="BN算法"></a>BN算法</h2><p>BN算法在每一次迭代中，对每一层的输入都进行归一化。<strong>把数据转换为均值为0、方差为1的高斯分布</strong>。<br>$$<br>\hat x = \frac{x - E(x)} {\sqrt{D(x) + \epsilon}}<br>$$<br>非常大的<code>缺陷</code>：<strong>强行归一化会破坏掉刚刚学习到的特征</strong>。 把每层的数据分布都固定了，但不一定是前面一层学习到的数据分布。</p>
<p><code>牛逼的地方</code> ：设置两个可以学习的变量<code>扩展参数</code>$\gamma$ ，和<code>平移参数</code> $\beta$ ，<strong>用这两个变量去还原上一层应该学习到的数据分布</strong>。（但是芳芳说，这一步其实可能没那么重要，要不要都行，CNN的本身会处理得更好）。<br>$$<br>y = \gamma \hat x+ \beta<br>$$<br>这样理解：用这两个参数，让神经网络自己去学习琢磨是前面的标准化是否有优化作用，<strong>如果没有优化效果，就用$\gamma, \beta$来抵消标准化的操作。</strong> </p>
<p>这样，BN就把原来不固定的数据分布，全部转换为固定的数据分布，而这种数据分布恰恰就是要学习到的分布。从而<strong>加速了网络的训练</strong>。 </p>
<p>对一个<code>mini-batch</code>进行更新， 输入一个$batchsize=m$的数据，学习两个参数，输出$y$<br>$$<br>\begin{align}<br>&amp; \mu = \frac{1}{m} \sum_{i=1}^m x_i &amp;  \text{求均值} \<br>&amp; \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 &amp; \text{求方差} \<br>&amp; \hat x = \frac{x - E(x)} {\sqrt{\sigma^2 + \epsilon}} &amp;  \text{标准化} \<br>&amp; y =  \gamma \hat x+ \beta &amp; \text{scale and shfit}<br> \end{align}<br>$$<br>其实就是对输入数据做个归一化：<br>$$<br>z = wx+b \to z = \rm{BN}(wx + b) \to a = f(z)<br>$$<br>一般<strong>在全连接层和激活函数之间</strong>添加BN层。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB4.png" style="display:block; margin:auto" width="50%"></p>
<p>在测试的时候，由于是没有batch，所以使用固定的均值和标准差，也就是对训练的各个batch的均值和标准差做批处理。<br>$$<br>E(x) = E(\mu), \quad D(x) = \frac{b}{b-1} E(\sigma^2)<br>$$</p>
<h2 id="BN的优点"><a href="#BN的优点" class="headerlink" title="BN的优点"></a>BN的优点</h2><p><strong>1 训练速度快</strong></p>
<p><strong>2 选择大的初始学习率</strong></p>
<p>初始大学习率，学习率的衰减也很快。<strong>快速训练收敛</strong>。小的学习率也可以。</p>
<p><strong>3 不再需要Dropout</strong> </p>
<p>BN本身就可以<strong>提高网络泛化能力</strong>，可以不需要Dropout和L2正则化。源神说，<strong>现在主流的网络都没有dropout了</strong>。但是<strong>会使用L2正则化</strong>，比较小的正则化。</p>
<p><strong>4 不再需要局部相应归一化</strong></p>
<p><strong>5 可以把训练数据彻底打乱</strong></p>
<h2 id="效果图片展示"><a href="#效果图片展示" class="headerlink" title="效果图片展示"></a>效果图片展示</h2><p>对所有数据标准化到一个范围，这样<strong>大部分的激活值都不会饱和</strong>，都不是-1或者1。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB5.png" style="display:block; margin:auto" width="50%"></p>
<p>大部分的激活值在各个分布区间都有值。再传递到后面，数据更有价值。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB6.png" style="display:block; margin:auto" width="50%"></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/NB8.png" style="display:block; margin:auto" width="50%"></p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-11-25T08:30:09.997Z" itemprop="dateUpdated">2018-11-25 16:30:09</time>
</span><br>


        
        <br>原始链接：<a href="/2017/11/26/cs224n-notes3-neural-networks-2/" target="_blank" rel="external">http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/</a>
        
    </div>
    
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BN/">BN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dropout/">Dropout</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PCA/">PCA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据预处理/">数据预处理</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/正则化/">正则化</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/白化/">白化</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/范数/">范数</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&title=《神经网络-过拟合-预处理-BN》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&title=《神经网络-过拟合-预处理-BN》 — PLM's Notes&source=NLP, DL, MRC." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《神经网络-过拟合-预处理-BN》 — PLM's Notes&url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/11/27/cs231n-linear-notes/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">cs231n线性分类器和损失函数</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/11/23/cs224n-notes3-neural-networks/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">神经网络基础-反向传播-激活函数</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'true' == 'true',
            verify: 'false' == 'true',
            appId: "kR8nND4dcsWgqDWIjpiH4YFj-gzGzoHsz",
            appKey: "il7PLkcJCfDBXMR6XirLdO2K",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->







</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check" data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="bottom">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            <span>
            PLM's Notes &nbsp; &copy; &nbsp
            </span>
            2016 - 2018
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&title=《神经网络-过拟合-预处理-BN》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&title=《神经网络-过拟合-预处理-BN》 — PLM's Notes&source=NLP, DL, MRC." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《神经网络-过拟合-预处理-BN》 — PLM's Notes&url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACuElEQVR42u3awWrkMBAE0Pn/n87CsoeF4KGqpSYDeT6FiW3p+SA11Xq94uvr75X/8vT3/798v56e/X7P5QsPDw9vNPVkyHbg/PfkzvejP84EDw8Pb403e3X7hmQbeL+2tx8ODw8P72d5SWmbTB0PDw/vt/GS4dt72m0DDw8P7xN4ebSabBV5WJDHGetZCx4eHl7Maxtgn/D3Yn8PDw8P76Crnl/tsYDZ++un8PDw8BZ4bVybl7BtMJEEH/mBhmEajYeHhxfw3r8oB59MPT9ekIweJcd4eHh4l3izOLUNL3LGSXCMh4eHt8fLU9B8WW/T1PNSuz0WhoeHh3fOmw3TdpruHsAq8Hh4eHgLvLZ91ZbO5yEvHh4e3ufw8iX7ZOpt6NDO7fEpPDw8vAVe/rr2v0lzq42Mj2IRPDw8vKu8vGnUhgt52ywJMtoQBA8PD2+DlxS7+RGo2XTbjztMVvDw8PAu8fIS9mRjmIXCbYMt6u/h4eHhXeLlsezs0EB7xCr/pYiG8fDw8K7ykiW7PTI1m1wbbURbDh4eHt4Cb9awT4KAdvhZPBGV1Hh4eHhXeW07v40wcvZsg3ndHQwPDw+vbIAlxWs7xaRAv3uAAA8PD2+bd6tV324J7XTzD/HvTjw8PLxl3q1FP3/qVszxOC4eHh7eAi95RX6YICm+W8BJzoCHh4e3wWuX6dmk25Z/W3Y/zhMPDw9vgdc2nE4ChXyPqovj5Ck8PDy8S7yv8mqPDuQfKI9xiy0BDw8Pb4HXLrtteywvkfOCOy+y8fDw8PZ4+VGnk1J7trXMNoxi98PDw8M74M0OXZ1POgEcZS14eHh4H8DLm/p5iTw7oBB9Gjw8PLwf5bXsE1LbbIsODeDh4eFd5eVhRLt8J58vz5vbqAIPDw9vg9c2wGbHp9qYOI+Mj2B4eHh43f1/AC1IIhbhOc9gAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>

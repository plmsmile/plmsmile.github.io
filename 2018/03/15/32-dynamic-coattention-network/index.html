<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>Dynamic Coattention Network (Plus) | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="DCN,coattention,QA">
    <meta name="description" content="先放四张图，分别是DCN的Encoder、Decoder，DCN+的Encoder和Objective。后面再详细总结">
<meta name="keywords" content="DCN,coattention,QA">
<meta property="og:type" content="article">
<meta property="og:title" content="Dynamic Coattention Network (Plus)">
<meta property="og:url" content="http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="先放四张图，分别是DCN的Encoder、Decoder，DCN+的Encoder和Objective。后面再详细总结">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcn.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcn-decoder.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/hmn.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/deep-residual-coattention-encoder.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/deep-residual-coattention-encoder.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcnplus-loss.png">
<meta property="og:updated_time" content="2018-12-13T08:08:07.295Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dynamic Coattention Network (Plus)">
<meta name="twitter:description" content="先放四张图，分别是DCN的Encoder、Decoder，DCN+的Encoder和Objective。后面再详细总结">
<meta name="twitter:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcn.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/">
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives/">
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags/">
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories/">
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about/">
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Dynamic Coattention Network (Plus)</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Dynamic Coattention Network (Plus)</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-03-15T00:33:16.000Z" itemprop="datePublished" class="page-time">
  2018-03-15
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#dcn"><span class="post-toc-number">1.</span> <span class="post-toc-text">DCN</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#coattention-encoder"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">Coattention Encoder</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#dynamic-pointing-decoder"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">Dynamic Pointing Decoder</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#hmn"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">HMN</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#dcn-1"><span class="post-toc-number">2.</span> <span class="post-toc-text">DCN+</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#dcn的问题"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">DCN的问题</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#dcn的优化点"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">DCN+的优化点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#deep-residual-encoder"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">Deep Residual Encoder</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#coattention深层理解"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">Coattention深层理解</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#coattention-encoder总结"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">Coattention Encoder总结</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#mixed-objective"><span class="post-toc-number">2.6.</span> <span class="post-toc-text">Mixed Objective</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-32-dynamic-coattention-network" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Dynamic Coattention Network (Plus)</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-03-15 08:33:16" datetime="2018-03-15T00:33:16.000Z" itemprop="datePublished">2018-03-15</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <blockquote>
<p>先放四张图，分别是DCN的Encoder、Decoder，DCN+的Encoder和Objective。后面再详细总结</p>
</blockquote>
<a id="more"></a>
<p><a href="https://arxiv.org/abs/1611.01604" target="_blank" rel="noopener">Dynamic Coattention Networks For Question Answering</a></p>
<p><a href="https://arxiv.org/pdf/1711.00106" target="_blank" rel="noopener">DCN+: Mixed Objective and Deep Residual Coattention for Question Answering</a></p>
<p><img src="" style="display:block; margin:auto" width="80%"></p>
<h1 id="dcn">DCN</h1>
<h2 id="coattention-encoder">Coattention Encoder</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcn.png" style="display:block; margin:auto" width="100%"></p>
<h2 id="dynamic-pointing-decoder">Dynamic Pointing Decoder</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcn-decoder.png" style="display:block; margin:auto" width="100%"></p>
<h2 id="hmn">HMN</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/hmn.png" style="display:block; margin:auto" width="60%"></p>
<h1 id="dcn-1">DCN+</h1>
<h2 id="dcn的问题">DCN的问题</h2>
<p><strong>loss没有判断真正的意义</strong></p>
<p>DCN使用传统交叉熵去优化<code>optimization</code>，只考虑答案字符串的匹配程度。但是实际上人的评判<code>evaluation</code>却是看回答的意义。如果只考虑span，则有下面两个问题：</p>
<ul>
<li>精确答案：没影响</li>
<li>但是对正确答案周围重叠的单词，却可能认为是错误的。</li>
</ul>
<p>句子：<code>Some believe that the Golden State Warriors team of 2017 is one of the greatest teams in NBA history</code></p>
<p>问题：<code>which team is considered to be one of the greatest teams in NBA history</code></p>
<p>正确答案：<code>the Golden State Warriors team of 2017</code></p>
<p>其实<code>Warriors</code>也是正确答案， 但是传统交叉熵却认为它还不如<code>history</code>。</p>
<p>DCN没有建立起<code>Optimization</code>和 <code>evaluation</code>的联系。 这也是Word Overlap。</p>
<p><strong>单层coattention表达力不强</strong></p>
<h2 id="dcn的优化点">DCN+的优化点</h2>
<p><strong>Mixed Loss</strong></p>
<p>交叉熵+自我批评学习（强化学习）。Word真正<strong>意义相似</strong>才会给一个好的<code>reward</code>。</p>
<ul>
<li>强化学习会鼓励意义相近的词语，而dis不相近的词语</li>
<li>交叉熵让强化学习朝着正确的轨迹发展</li>
</ul>
<p><strong>Deep Residual Coattention Encoder</strong></p>
<p>多层表达能力更强，详细看下面的优点。</p>
<h2 id="deep-residual-encoder">Deep Residual Encoder</h2>
<p><strong>优点</strong></p>
<p>两个别人得出的重要结论：</p>
<ul>
<li><code>stacked self-attention</code> 可以加速信号传递</li>
<li>减少信号传递路径，可以增加长依赖</li>
</ul>
<p>比DCN的两个优化点：</p>
<ul>
<li><code>coattention with self-attention</code>和多层<code>coattention</code> 。可以对输入有<code>richer representations</code></li>
<li>对每层的<code>coattention outputs</code>进行残差连接。缩短了信息传递路径。</li>
</ul>
<h2 id="coattention深层理解">Coattention深层理解</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/deep-residual-coattention-encoder.png" style="display:block; margin:auto" width="90%"></p>
<blockquote>
<p>当时理解了很久都不懂，后来一个下午，一直看，结合机器翻译实现和实际例子矩阵计算，终于理解了Attention、Coattention。</p>
</blockquote>
<p>参考了我的下面三篇笔记。</p>
<ul>
<li><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#pytorch%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91">注意力机制在机器翻译中的思想和代码实现总结</a></li>
<li><a href="https://plmsmile.github.io/2017/10/10/attention-model/">图文介绍Attention</a></li>
<li><a href="https://plmsmile.github.io/2018/03/14/31-co-attention-vqa/#co-attention">Coattention的两种形式</a></li>
</ul>
<p><strong>单个Coattention层计算</strong></p>
<p>经过双向RNN后，得到两个语义编码：文档<span class="math inline">\(E_0^D \in \mathbb R^{m\times e}\)</span>， 问题编码<span class="math inline">\(E^Q_0 \in \mathbb R ^{n \times h}\)</span> 。 <span class="math display">\[
E_1^D = \rm{biGRU_1}(E_0^D) \quad\in \mathbb R^{m \times h}
\]</span></p>
<p><span class="math display">\[
E_1^Q = \tanh(\rm{W \; \rm{biGRU_1(Q_E)+b)}} \quad \in \mathbb R^{n \times h}
\]</span></p>
<p>计算<code>关联得分矩阵</code>A <span class="math display">\[
A = E_1^D (E_1^Q)^T \in \mathbb R^{m \times n}
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix} 
0 &amp; 0 \\ 
2 &amp; 3 \\
0 &amp; 2 \\ 
1 &amp; 1 \\
3 &amp; 3 \\
\end{bmatrix}_{5 \times 2}
\cdot 
\begin{bmatrix} 
1&amp; 3 \\ 
1 &amp; 1 \\
1&amp; 3 \\ 
\end{bmatrix}_{3 \times 2}^T
=
\begin{bmatrix} 
0&amp; 0 &amp;0 \\ 
11&amp; 5 &amp;11 \\ 
6&amp; 2 &amp;6 \\ 
4&amp; 2 &amp;4 \\ 
12&amp; 6 &amp;12 \\ 
\end{bmatrix}_{5\times 3}
\]</span></p>
<p>做<code>行Softmax</code>，得到Q对D的权值分配概率<span class="math inline">\(A^Q\)</span>， <code>attention_weights</code></p>
<ul>
<li>每一行是一个文档单词w</li>
<li>元素值是所有问句单词对当前文档单词w的注意力分配权值</li>
<li>元素值是每个问句单词的权值概率</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix} 
0.3333 &amp; 0.3333 &amp; 0.3333 \\ 
0.4994  &amp;0.0012 &amp; 0.4994 \\ 
0.4955  &amp;0.0091  &amp;0.4955 \\ 
0.4683  &amp;0.0634  &amp;0.4683\\ 
0.4994  &amp;0.0012  &amp;0.4994 \\ 
\end{bmatrix}_{5\times 3}
\]</span></p>
<p>计算D的summary， <span class="math inline">\(S^D = A^Q \cdot Q\)</span> <span class="math display">\[
S^D = A^Q \cdot Q
\]</span></p>
<ul>
<li>D所需要的新的语义，参考机器翻译的<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#%E8%AE%A1%E7%AE%97%E6%96%B0%E7%9A%84%E8%AF%AD%E4%B9%89">新语义理解</a></li>
<li><span class="math inline">\(A^Q\)</span>的每一行去乘以Q的每一列去表达单词w</li>
<li>用Q去表达D，每个<span class="math inline">\(D_w\)</span>都是<strong>Q的所有单词对w的线性表达</strong>，权值就是<span class="math inline">\(A^Q\)</span><br>
</li>
<li>所以<span class="math inline">\(S^D\)</span>也是D的<code>summary</code>， 也称作D需要<code>context</code></li>
</ul>
<p>同理，对<code>列做softmax</code>， 得到D对Q的权值分配概率<span class="math inline">\(A^D\)</span>， 得到Q的<code>summary</code>， <span class="math inline">\(S^Q = A^D \cdot D\)</span></p>
<p>这时，借鉴<a href="https://plmsmile.github.io/2018/03/14/31-co-attention-vqa/#alternating-co-attention">alternation-coattention思想</a> 去计算对D的<code>Coattention context</code><span class="math inline">\(C^D\)</span> ： <span class="math display">\[
C^D = S^Q \cdot A^Q
\]</span> 实际上，<span class="math inline">\(C^D\)</span>与<span class="math inline">\(S^D\)</span>类似，都是<code>Summary</code>， 都是<code>context</code>。 只是<span class="math inline">\(C^D\)</span>使用的是新的<span class="math inline">\(S^Q\)</span>， 而不是<span class="math inline">\(E^Q_1\)</span>。</p>
<h2 id="coattention-encoder总结">Coattention Encoder总结</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/deep-residual-coattention-encoder.png" style="display:block; margin:auto" width="90%"></p>
<p>使用两层<code>coattention</code>， 最后再残差连接，经过LSTM输出。</p>
<p><strong>第一层</strong> <span class="math display">\[
E_1^D = \rm{biGRU_1}(E_0^D) \quad\in \mathbb R^{m \times h} \\
E_1^Q = \tanh(\rm{W \cdot \rm{biGRU_1(E_0^Q)+b)}} \quad \in \mathbb R^{n \times h}
\]</span></p>
<p><span class="math display">\[
\rm{coattn_1} (E_1^D, E_1^Q) =  S_1^D, S_1^Q, C_1^Q \\
\]</span></p>
<p><strong>第二层</strong> <span class="math display">\[
E_2^D = \rm{biGRU_2}(E_1^D) \quad\in \mathbb R^{m \times h} \\
E_2^Q = \tanh (W \cdot \rm{biGRU_2}(E_1^Q) + b) \quad\in \mathbb R^{m \times h}
\]</span></p>
<p><span class="math display">\[
\rm{coattn_2} (E_2^D, E_2^Q) =  S_2^D, S_2^Q, C_2^Q \\
\]</span></p>
<p><strong>残差连接所有的D</strong> <span class="math display">\[
c = \rm {concat}((E_1^D, E_2^D, S_1^D, S_2^D, C_1^D, C_2^D)
\]</span> <strong>LSTM编码输出，得到Encoder的输出</strong> <span class="math display">\[
U = \rm{biGRU}(c) \quad \in \mathbb R^{m \times 2h}
\]</span></p>
<h2 id="mixed-objective">Mixed Objective</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcnplus-loss.png" style="display:block; margin:auto" width="100%"></p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-12-13T08:08:07.295Z" itemprop="dateUpdated">2018-12-13 16:08:07</time>
</span><br>


        
        <br>原始链接：<a href="/2018/03/15/32-dynamic-coattention-network/" target="_blank" rel="external">http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/</a>
        
    </div>
    
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DCN/">DCN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/QA/">QA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/coattention/">coattention</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/&title=《Dynamic Coattention Network (Plus)》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/&title=《Dynamic Coattention Network (Plus)》 — PLM's Notes&source=
先放四张图，分别是DCN的Encoder、Decoder，DCN+的Encoder和Objective。后面再详细总结
" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Dynamic Coattention Network (Plus)》 — PLM's Notes&url=http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2018/03/25/33-attention-summary/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">各种注意力总结</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/03/14/31-co-attention-vqa/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">协同注意力简介</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'true' == 'true',
            verify: 'false' == 'true',
            appId: "kR8nND4dcsWgqDWIjpiH4YFj-gzGzoHsz",
            appKey: "il7PLkcJCfDBXMR6XirLdO2K",
            avatar: "retro",
            placeholder: "快来评论吧！",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->







</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check" data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="bottom">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            <span>
            PLM's Notes &nbsp; &copy; &nbsp
            </span>
            2016 - 2018
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/&title=《Dynamic Coattention Network (Plus)》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/&title=《Dynamic Coattention Network (Plus)》 — PLM's Notes&source=
先放四张图，分别是DCN的Encoder、Decoder，DCN+的Encoder和Objective。后面再详细总结
" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Dynamic Coattention Network (Plus)》 — PLM's Notes&url=http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACr0lEQVR42u3aQY4bMQwEQP//0841QWK7m5SwDlBzMrzjkWoOFJfk4xFfzxdXcuff3z/Ka/NbPDw8vBu859ur5f1+f/I52XS+n388DQ8PD+8aL9niq2VyZPsq33/zfkU8PDy8b+blSfaGioeHh/f/8vIKQFJQSFbBw8PD+05eEqCTl5L/tS1SXK+14OHh4cW8NtB/w+cr/T08PDy8dVd9FnbbUsKs5BHtEw8PD+8CL/9X/32BIC8ftCn7gcQaDw8P7wJv3/JvD4DNC9qMMuDh4eGd5eXpcluEbRtXLTvaPx4eHt4hXv6gzcGQjyC0x8CHZB0PDw/vAi8pNyRBOY/G7Vptb6ueAsPDw8MbRcs8rM/69bPNtc+p3wEeHh7eiJe3kfL2WLuh5GmrMi4eHh7eBd6scLAZJ63z/f3oAB4eHt413qbYuhkLmIX7ooSBh4eHd4g3a+TngwJ5QXb2+oozBA8PD+8QbxPo21LFrGg7w+Ph4eHd5uXgPOjPkun8BW3GDvDw8PA2vLyQemr5NkVuDy08PDy827xNutyS9il1WwTBw8PDu8GbNe/bJtmmNLwZeMXDw8O7wWvb/G1Yn92TY6JxBzw8PLyjvDaFbUu0myJFXv54eT8eHh7eD/HaVtYmp23T5aiPh4eHh3eU1w48bZLszdGyKo7g4eHhHeXlSyaLFeNQo/GsfG9/pNR4eHh4R3ltktoeAzNe25ArWnd4eHh4a96zvJIUfDbC1SbNUXkCDw8P7wJvM2L1PpTnpNmhsjlI8PDw8Pa8WUDP798E97wh96GYi4eHh3eBlyxzoJC6KNQOn4OHh4f3o7xT2Xqbjs9Kw3h4eHjfwMvb/G1iPTsAijYYHh4e3gVe2/Q6O3I6G0uNdoWHh4d3gTdLgttfJUfIppxxoL+Hh4eH95n3C+PY5ClerJF6AAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>

<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>Transformer | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="论文笔记,Transformer,机器翻译,Self-Attention,多头注意力,残差连接,层归一化,位置编码">
    <meta name="description" content="大名鼎鼎的Transformer，Attention Is All You Need">
<meta name="keywords" content="论文笔记,Transformer,机器翻译,Self-Attention,多头注意力,残差连接,层归一化,位置编码">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="大名鼎鼎的Transformer，Attention Is All You Need">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/02-multi-head-attention.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/01-transformer.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/30-transformer_resideual_layer_norm_3.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/03-the_transformer_3.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/04-The_transformer_encoders_decoders.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/05-The_transformer_encoder_decoder_stack.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/06-Transformer_encoder.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/10-encoder_with_tensors_2.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/07-Transformer_decoder.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/08-embeddings.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/09-encoder_with_tensors.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/10-encoder_with_tensors_2.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/12-transformer_self_attention_vectors.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/13-transformer_self_attention_score.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/14-self-attention_softmax.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/15-self-attention-output.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/11-transformer_self-attention_visualization.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/16-self-attention-matrix-calculation.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/17-self-attention-matrix-calculation-2.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/18-transformer_attention_heads_qkv.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/19-transformer_attention_heads_z.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/20-transformer_attention_heads_weight_matrix_o.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/21-transformer_multi-headed_self-attention-recap.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/22-transformer_self-attention_visualization_2.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/23-transformer_self-attention_visualization_3.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/24-transformer_positional_encoding_vectors.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/26-transformer_positional_encoding_example.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/27-transformer_positional_encoding_large_example.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/28-transformer_resideual_layer_norm.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/29-transformer_resideual_layer_norm_2.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/30-transformer_resideual_layer_norm_3.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/31-transformer_decoding_1.gif">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/32-transformer_decoding_2.gif">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/33-transformer_decoder_output_softmax.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/34-vocabulary.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/35-one-hot-vocabulary-example.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/36-output_target_probability_distributions.png">
<meta property="og:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/37-output_trained_model_probability_distributions.png">
<meta property="og:updated_time" content="2018-11-25T08:30:09.233Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Transformer">
<meta name="twitter:description" content="大名鼎鼎的Transformer，Attention Is All You Need">
<meta name="twitter:image" content="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/02-multi-head-attention.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/">
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives/">
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags/">
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories/">
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about/">
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Transformer</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Transformer</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-08-29T07:41:32.000Z" itemprop="datePublished" class="page-time">
  2018-08-29
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#transformer概览"><span class="post-toc-number">1.</span> <span class="post-toc-text">Transformer概览</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#论文结构"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">论文结构</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#总览结构"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">总览结构</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#图解总览"><span class="post-toc-number">2.</span> <span class="post-toc-text">图解总览</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#encoder"><span class="post-toc-number">3.</span> <span class="post-toc-text">Encoder</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#总体结构"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">总体结构</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#编码实例"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">编码实例</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#attention"><span class="post-toc-number">4.</span> <span class="post-toc-text">Attention</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#self-attention"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">Self-Attention</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#矩阵形式"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">矩阵形式</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#多头注意力"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">多头注意力</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#注意力总结"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">注意力总结</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#attention图示"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">Attention图示</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#前向神经网络"><span class="post-toc-number">5.</span> <span class="post-toc-text">前向神经网络</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#位置编码"><span class="post-toc-number">6.</span> <span class="post-toc-text">位置编码</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#encoder-block"><span class="post-toc-number">7.</span> <span class="post-toc-text">Encoder-Block</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#残差连接"><span class="post-toc-number">7.1.</span> <span class="post-toc-text">残差连接</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#层归一化"><span class="post-toc-number">7.2.</span> <span class="post-toc-text">层归一化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#总览"><span class="post-toc-number">8.</span> <span class="post-toc-text">总览</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#decoder"><span class="post-toc-number">9.</span> <span class="post-toc-text">Decoder</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#单步"><span class="post-toc-number">9.1.</span> <span class="post-toc-text">单步</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#多步"><span class="post-toc-number">9.2.</span> <span class="post-toc-text">多步</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#linear-softmax"><span class="post-toc-number">9.3.</span> <span class="post-toc-text">Linear-Softmax</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#模型样例"><span class="post-toc-number">10.</span> <span class="post-toc-text">模型样例</span></a></li></ol>
        </nav>
    </aside>


<article id="post-48-attention-is-all-you-need" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Transformer</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-08-29 15:41:32" datetime="2018-08-29T07:41:32.000Z" itemprop="datePublished">2018-08-29</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>大名鼎鼎的Transformer，<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a><a id="more"></a></p>
<p><img src="" style="display:block; margin:auto" width="60%"></p>
<h1 id="transformer概览">Transformer概览</h1>
<h2 id="论文结构">论文结构</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/02-multi-head-attention.png" style="display:block; margin:auto" width="60%"></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/01-transformer.png" style="display:block; margin:auto" width="40%"></p>
<h2 id="总览结构">总览结构</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/30-transformer_resideual_layer_norm_3.png" style="display:block; margin:auto" width="80%"></p>
<h1 id="图解总览">图解总览</h1>
<p>其实也是一个<code>Encoder-Decoder</code>的翻译模型。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/03-the_transformer_3.png" style="display:block; margin:auto" width="70%"></p>
<p>由一个Encoders和一个Decoders组成。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/04-The_transformer_encoders_decoders.png" style="display:block; margin:auto" width="70%"></p>
<p>Encoders由多个Encoder块组成。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/05-The_transformer_encoder_decoder_stack.png" style="display:block; margin:auto" width="70%"></p>
<h1 id="encoder">Encoder</h1>
<h2 id="总体结构">总体结构</h2>
<p><strong>1个Encoder由Self-Attention和FFN组成</strong></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/06-Transformer_encoder.png" style="display:block; margin:auto" width="70%"></p>
<p><strong>一个Encoder的结果再给到下一个Encoder</strong></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/10-encoder_with_tensors_2.png" style="display:block; margin:auto" width="70%"></p>
<p><strong>Encoder-Decoder</strong></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/07-Transformer_decoder.png" style="display:block; margin:auto" width="70%"></p>
<h2 id="编码实例">编码实例</h2>
<p>对一个句子进行编码</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/08-embeddings.png" style="display:block; margin:auto" width="60%"></p>
<p><code>Self-Attention</code>会对每一个单词进行编码，得到对应的向量。<span class="math inline">\(\mathbf{x_1}, \mathbf{x_2}, \mathbf{x_3} \to \mathbf{z_1}, \mathbf{z_2}, \mathbf{z_3}\)</span>，再给到FFN，会得到一个Encoder的结果<span class="math inline">\(\mathbf{r_1}, \mathbf{r_2}, \mathbf{r_3}\)</span>， 再继续给到下一个Encoder</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/09-encoder_with_tensors.png" style="display:block; margin:auto" width="70%"></p>
<h1 id="attention">Attention</h1>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/10-encoder_with_tensors_2.png" style="display:block; margin:auto" width="70%"></p>
<h2 id="self-attention">Self-Attention</h2>
<p><strong>1. 乘以3个矩阵生成3个向量：Query、Key、Value</strong></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/12-transformer_self_attention_vectors.png" style="display:block; margin:auto" width="70%"></p>
<p><strong>2. 计算与每个位置的score</strong></p>
<p>编码一个单词时，会计算它与句子中其他单词的得分。会得到每个单词对于当前单词的关注程度。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/13-transformer_self_attention_score.png" style="display:block; margin:auto" width="70%"></p>
<p><strong>3. 归一化和softmax得到每个概率</strong></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/14-self-attention_softmax.png" style="display:block; margin:auto" width="70%"></p>
<p><strong>4. 依概率结合每个单词的向量</strong></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/15-self-attention-output.png" style="display:block; margin:auto" width="70%"></p>
<p><strong>Attention图示</strong></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/11-transformer_self-attention_visualization.png" style="display:block; margin:auto" width="60%"></p>
<h2 id="矩阵形式">矩阵形式</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/16-self-attention-matrix-calculation.png" style="display:block; margin:auto" width="60%"></p>
<p><strong>其实就是一个注意力矩阵公式</strong></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/17-self-attention-matrix-calculation-2.png" style="display:block; margin:auto" width="60%"></p>
<h2 id="多头注意力">多头注意力</h2>
<p>其实就是多个KV注意力。从两个方面提升了<code>Attention Layer</code>的优点</p>
<ul>
<li>让模型能够关注到句子中的各个不同位置</li>
<li>Attention Layer可以有多个不同的表示子空间<code>representation subspaces</code></li>
</ul>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/18-transformer_attention_heads_qkv.png" style="display:block; margin:auto" width="70%"></p>
<p><strong>多头注意力矩阵形式</strong></p>
<p>经过多头注意力映射，会生成多个注意力<span class="math inline">\(Z_0, Z_1, \cdots, Z_7\)</span>。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/19-transformer_attention_heads_z.png" style="display:block; margin:auto" width="60%"></p>
<p>把这些注意力头拼接起来，再乘以一个大矩阵，最终融合得到一个信息矩阵。会给到FFN进行计算。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/20-transformer_attention_heads_weight_matrix_o.png" style="display:block; margin:auto" width="70%"></p>
<h2 id="注意力总结">注意力总结</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/21-transformer_multi-headed_self-attention-recap.png" style="display:block; margin:auto" width="80%"></p>
<h2 id="attention图示">Attention图示</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/22-transformer_self-attention_visualization_2.png" style="display:block; margin:auto" width="40%"></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/23-transformer_self-attention_visualization_3.png" style="display:block; margin:auto" width="40%"></p>
<h1 id="前向神经网络">前向神经网络</h1>
<p><code>Position-wise Feed-Forward Network</code>，会对每个位置过两个线性层，其中使用<a href="https://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/#relu">ReLU</a>作为激活函数。 <span class="math display">\[
\rm{FFN}(x) = \rm{Linear}(\rm{ReLU}(\rm{Linear}(x))) = \rm{max}(0, xW_1 + b_1)W_2 + b_2
\]</span></p>
<h1 id="位置编码">位置编码</h1>
<p><strong>词向量+位置信息=带位置信息的词向量</strong> <span class="math display">\[
\rm{PE}(pos, 2i) = \sin (\rm{pos} / 10000^{\frac{2i}{d}})
\]</span></p>
<p><span class="math display">\[
\rm{PE}(pos, \rm{2i+1}) = \cos (\rm{pos} / 10000^{\frac{2i}{d}})
\]</span></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/24-transformer_positional_encoding_vectors.png" style="display:block; margin:auto" width="70%"></p>
<p><strong>示例</strong></p>
<p>再把sin和cos的两个值拼接起来，就得到如下图所示。</p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/26-transformer_positional_encoding_example.png" style="display:block; margin:auto" width="70%"></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/27-transformer_positional_encoding_large_example.png" style="display:block; margin:auto" width="60%"></p>
<h1 id="encoder-block">Encoder-Block</h1>
<h2 id="残差连接">残差连接</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/28-transformer_resideual_layer_norm.png" style="display:block; margin:auto" width="50%"></p>
<h2 id="层归一化">层归一化</h2>
<p><a href="https://plmsmile.github.io/2018/03/30/35-nerual-network-optim/#%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96">层归一化</a></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/29-transformer_resideual_layer_norm_2.png" style="display:block; margin:auto" width="50%"></p>
<h1 id="总览">总览</h1>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/30-transformer_resideual_layer_norm_3.png" style="display:block; margin:auto" width="80%"></p>
<h1 id="decoder">Decoder</h1>
<h2 id="单步">单步</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/31-transformer_decoding_1.gif" style="display:block; margin:auto" width="60%"></p>
<h2 id="多步">多步</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/32-transformer_decoding_2.gif" style="display:block; margin:auto" width="60%"></p>
<h2 id="linear-softmax">Linear-Softmax</h2>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/33-transformer_decoder_output_softmax.png" style="display:block; margin:auto" width="50%"></p>
<h1 id="模型样例">模型样例</h1>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/34-vocabulary.png" style="display:block; margin:auto" width="60%"></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/35-one-hot-vocabulary-example.png" style="display:block; margin:auto" width="60%"></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/36-output_target_probability_distributions.png" style="display:block; margin:auto" width="60%"></p>
<p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/37-output_trained_model_probability_distributions.png" style="display:block; margin:auto" width="60%"></p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-11-25T08:30:09.233Z" itemprop="dateUpdated">2018-11-25 16:30:09</time>
</span><br>


        
        <br>原始链接：<a href="/2018/08/29/48-attention-is-all-you-need/" target="_blank" rel="external">http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/</a>
        
    </div>
    
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Self-Attention/">Self-Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/位置编码/">位置编码</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/多头注意力/">多头注意力</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/层归一化/">层归一化</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器翻译/">机器翻译</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/残差连接/">残差连接</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/论文笔记/">论文笔记</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/&title=《Transformer》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/&title=《Transformer》 — PLM's Notes&source=大名鼎鼎的Transformer，Attention Is All You Need" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Transformer》 — PLM's Notes&url=http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2018/08/30/49-qanet/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">QANet</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/05/22/47-bidaf/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Bidirectional Attention Flow</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'false' == 'true',
            verify: 'false' == 'true',
            appId: "kR8nND4dcsWgqDWIjpiH4YFj-gzGzoHsz",
            appKey: "il7PLkcJCfDBXMR6XirLdO2K",
            avatar: "wavatar",
            placeholder: "快来评论吧~",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->







</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check" data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="bottom">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            <span>
            PLM's Notes &nbsp; &copy; &nbsp
            </span>
            2016 - 2018
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/&title=《Transformer》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/&title=《Transformer》 — PLM's Notes&source=大名鼎鼎的Transformer，Attention Is All You Need" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Transformer》 — PLM's Notes&url=http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACrUlEQVR42u3aS27CQBAFQO5/aSJlFyWY158BFJVXkQEz5UjuR0/fbvFx/z4enbn/Oq7PX18zv8LagYeHh9da+qMjweSfegS7Xk91DXh4eHiv4V0Xg6QwJMvN2b1vxMPDw/s0XhJqr9+Zx2I8PDy8/8HrfU0Cvm5YXJcWPDw8vHfxkrZC8uqkcVDlLfda8PDw8GJeNeB+wt8H9/fw8PDwBrvq1WKwdcvyMYIn68TDw8M7wOsttLplVQ2++YBXVDDw8PDwXsjL26/VW9A70xx0wMPDw1vlzX/258ME8+GAclHBw8PDO8bbGiftsXstj+gfgIeHh3eMN3nQT+JyPmpQbXbg4eHhneYll8i3pvLyUy0nycDWw/09PDw8vGO8yXLz6/TGTMtlDA8PD+8AL2kK5BdNGgrVR//CsBceHh7eKm8SjvNmRF4qeqWrMBOBh4eHt8SrNmF7sTt53FcHDp7cPjw8PLwDvK1GbW80qpD642j+x3k8PDy8VV6+YT9pyPY+2wv3eHh4eK/kTcJrEpGrw1t5sH4yOoCHh4e3yqtuRG01Z/NHf36jC2EdDw8Pb8yrxuJCn+PAzSq3lfHw8PBWeSeaC9XbkSy6Gq/x8PDwzvESaq/Ju7WFNmmI4OHh4Z3jVX/e5w/0fPtq0rxoBms8PDy8AS//qV+IsMWQnY9qjSI1Hh4e3hKvOtg035SaR/NCCxgPDw/vGG/hQkvButq8KFQ8PDw8vDHvXjwm0TZpW8yLx49X8fDw8A7wqs/Y6oJ67Y/dQoWHh4d3gtcrBsn7J6NX1eiPh4eH93peHoJ7oTl/oPdqV2GaDA8PD+9NvF5hyLfNJnEcDw8P73N4yeBUrwmbF4lqkwIPDw/vHK/6oJ8vrldOlgcI8PDw8I4deTMiLyE9fDlS4+Hh4W3yvgCL+MU6Utx2WgAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>

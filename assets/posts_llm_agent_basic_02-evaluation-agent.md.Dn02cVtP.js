import{_ as e,c as a,o as l,ah as i}from"./chunks/framework.CvbyeFFO.js";const g=JSON.parse('{"title":"Agent 评估 Benchmarks","description":"","frontmatter":{"title":"Agent 评估 Benchmarks","date":"2025-05-14T15:42:09.000Z","create":"2025-05-14T15:42:09.000Z","categories":["agent"],"tags":["agent evalution"]},"headers":[],"relativePath":"posts/llm/agent/basic/02-evaluation-agent.md","filePath":"posts/llm/agent/basic/02-evaluation-agent.md","lastUpdated":1753200077000}'),n={name:"posts/llm/agent/basic/02-evaluation-agent.md"};function o(r,t,d,c,s,u){return l(),a("div",null,t[0]||(t[0]=[i('<div class="custom-block tip"><div class="custom-block-title">本文概览</div><p>Agent Evaluation 相关内容</p></div><h2 id="一图概览-来自论文" tabindex="-1">一图概览(来自论文) <a class="header-anchor" href="#一图概览-来自论文" aria-label="Permalink to &quot;一图概览(来自论文)&quot;">​</a></h2><img src="https://paper-assets.alphaxiv.org/figures/2503.16416/img-0.jpeg" style="display:block;margin:auto;" width="70%"><h2 id="智能体能力评估" tabindex="-1">智能体能力评估 <a class="header-anchor" href="#智能体能力评估" aria-label="Permalink to &quot;智能体能力评估&quot;">​</a></h2><h3 id="plan-multi-step-reasoning" tabindex="-1">Plan &amp; Multi-step Reasoning <a class="header-anchor" href="#plan-multi-step-reasoning" aria-label="Permalink to &quot;Plan &amp; Multi-step Reasoning&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">Plan and Multi-step Reasoning / Multi-step Planning</div><p>这是LLM-Agent的基本能力，要求他们能<code>把复杂任务分解成更小更容易管理的子任务</code>，并<code>执行一系列action</code>来完成任务。</p></div><p>以下是一些基准，这些基准都突出了Agent Plan所需要的关键能力：</p><ul><li>任务分解能力(<code>task decomposition</code>)：分解复杂问题</li><li>状态跟踪和信念维护能力(<code>state tracking and belief maintenance</code>)：用于准确的多步推理</li><li>自我修正能力(<code>self-correction</code>)：用于检测错误和还原回溯</li><li>因果理解(<code>casual understanding</code>)：预测action结果</li><li>元规划(<code>meta-planing</code>)：改进规划策略</li></ul><table tabindex="0"><thead><tr><th>类型<img width="100/"></th><th>名称<img width="250/"></th><th>备注</th></tr></thead><tbody><tr><td>数学推理</td><td>(2021)GMS8k、(2021)MATH、(2017)QAUA-RAT</td><td></td></tr><tr><td>多跳问答</td><td>(2017)<code>HotpotQA</code>、(2021)StrategyQA、(2018)MultiRC</td><td></td></tr><tr><td>科学推理</td><td>(2018)ARC</td><td></td></tr><tr><td>逻辑推理</td><td>(2024)FOLIO、(2022)P-FOLIO</td><td></td></tr><tr><td>常识推理</td><td>(2023)MUSR</td><td></td></tr><tr><td>挑战型推理</td><td>(2022)BBH</td><td></td></tr><tr><td>综合型推理</td><td>(2023)PlanBench</td><td>评估不同领域LLM的规划能力，表明短期规划ok，长期规划不ok</td></tr><tr><td>日常场景推理</td><td>(2023)AutoPlanBench</td><td>评估日常场景中的规划能力</td></tr><tr><td>工作流</td><td>(2024)FlowBench</td><td>评估工作流程规划能力，重点关注知识密集型任务</td></tr><tr><td>核心推理</td><td>(2024)ACPBench</td><td>评估LLM核心推理技能</td></tr><tr><td>现实世界</td><td>(2024)<mark>Natural Plan Benchmark</mark></td><td>评估现实世界的规划任务</td></tr><tr><td>工具规划推理</td><td>(2023)ToolEmu</td><td></td></tr></tbody></table><h3 id="function-calling-tool-use" tabindex="-1">Function Calling &amp; Tool Use <a class="header-anchor" href="#function-calling-tool-use" aria-label="Permalink to &quot;Function Calling &amp; Tool Use&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">函数调用</div><p>调用外部工具，是构建实时且准确回复Agent的重要能力。函数调用设计多个子任务协作，包括以下几个流程</p><ul><li>意图识别：根据query识别何时需要某个函数，确定使用哪个工具</li><li>参数提取：从对话中提取函数参数</li><li>函数调用：调用外部函数获取结果</li><li>LLM回复： 把结果整合到输入中，给到LLM做回复</li></ul></div><p>整体有如下Bench：</p><ul><li>早期：侧重简单、提供明确参数的但不交互 <ul><li>(2023)ToolAlpaca、(2025)APIBench、(2023)ToolBench</li><li>(2024)<code>BFCL</code> v1(实时性)、v2(组织工具)、<code>v3</code>(多轮、多步)，(<mark>Berkeley Function Calling Leaderboard</mark> )</li></ul></li><li>演变：拓展评估领域 <ul><li>(2024) ToolSandbox：结合状态、隐式依赖关系等。</li><li>(2024) Seal-Tools：采用self-instruct来生成嵌套的工具调用。</li><li>(2023) API-Bank：对话、真实API评估。</li><li>(2024) API-Blend：真实场景。</li><li>(2023) RestBench、(2024) APIGen、(2024) StableToolBench。</li></ul></li><li>多步骤交互： <ul><li>(2025) ComplexFUncBench：需要隐式参数推断、用户约束、长上下文处理的场景。</li></ul></li></ul><h3 id="self-reflection-反思" tabindex="-1">Self-Reflection 反思 <a class="header-anchor" href="#self-reflection-反思" aria-label="Permalink to &quot;Self-Reflection 反思&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">self-refelection</div><p>agent能够自我反思、通过交互式反馈来提升推理能力，从而减少错误。</p></div><ul><li>早期：间接评估，将已有的推理/规划任务重新用于多轮反馈，查看模型能否根据外部反馈纠正自身错误 <ul><li>(2023) AGIEval、(2022)MedMCQA、(2021) ALFWorld</li></ul></li><li>中期：交互式自我反思 基准 <ul><li>(2023) LLF-Bench：扩展各种决策任务</li><li>(2024) LLM-Evolve：</li><li>(2024) LiveCodeBench：交互式设置</li></ul></li><li>认知科学角度： <ul><li>(2024) Reflection-Bench：评估LLM的认知反思能力。将其分解为： <ul><li>新信息感知：new information perception</li><li>记忆使用：memory usage</li><li>信念更新：belief updating following surprise</li><li>决策调整：decesion-making adjustments</li><li>反事实推理：counterfactual reasoning</li><li>Meta-reflection：</li></ul></li></ul></li></ul><h3 id="memory" tabindex="-1">Memory <a class="header-anchor" href="#memory" aria-label="Permalink to &quot;Memory&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">Memory</div><p>记忆力机制可以在交互机制中保持一定的上下文。</p><ul><li><mark>短期记忆</mark> ：助于<mark>实时响应</mark></li><li><mark>长期记忆</mark> ：助于更<mark>深入的理解</mark>和<mark>长期应用知识</mark></li></ul></div><ul><li><p>长上下文评估：通过memory来增强长上下文或检索相关的推理。</p><ul><li>工作：(2024)ReadAgent、(2024)MemGPT、(2025)A-Mem</li><li>Bench：(2021)Quality、(2018) NarrativeQA、(2021) QMSum、(2024) LoCoMo、(2024) NaturalQuestions-Open</li></ul></li><li><p>情景记忆评估</p><ul><li>(2025) Episode Memories：评估LLM如何生成和管理memories</li></ul></li><li><p>外部记忆结合评估</p><ul><li>(2024) <code>StreamBench</code>：评估利用外部memory(反馈)来持续提高效果，在HotpotQA/ToolBench/Spider等多数据集上测。</li></ul></li><li><p>实时决策和学习评估：<code>优化action</code></p><ul><li>(2024)<code>LTMBench</code>：通过扩展的多任务交互、频繁上下文切换，来评估对话agent的长期记忆和信息整合能力。</li></ul></li></ul><h2 id="特定智能体评估" tabindex="-1">特定智能体评估 <a class="header-anchor" href="#特定智能体评估" aria-label="Permalink to &quot;特定智能体评估&quot;">​</a></h2><h3 id="web-agents" tabindex="-1">Web Agents <a class="header-anchor" href="#web-agents" aria-label="Permalink to &quot;Web Agents&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">Web Agents</div><p>通过网络交互来完成任务的AI 系统，例如订机票、购物等。</p></div><ul><li>早期：静态。 <ul><li>(2022) WebShop、(2023) Mind2Web、(2024) WebVoyoger</li></ul></li><li>近期：动态、更真实的场景 <ul><li>适应网页动态变化：(2024) WebLInx</li><li>解释包含视觉信息：(2023) WebArea、(2024)Visual-WebArea</li><li>办公室复杂任务：(2024) WorkArea、(2025) WorkArea++</li><li>多模态/多站点：(2024) MMInA、(2024) AssistantBench、(2024) WebCanvas</li><li>动静态结合：(2024) ST-WebAgentBench</li></ul></li></ul><h3 id="software-engineering-agents" tabindex="-1">Software Engineering Agents <a class="header-anchor" href="#software-engineering-agents" aria-label="Permalink to &quot;Software Engineering Agents&quot;">​</a></h3><ul><li>最主要 <ul><li>SWEBench系列：(2023) SWEBench、(2024) SWEBench-Lite、(2024) SWEBench Multimodal</li><li>AgentBench：评估SWE Agent的交互能力</li><li>(2025) SWELancer：代表最新趋势，把性能和货币价值结合起来，凸显诸多挑战。</li></ul></li></ul><h3 id="scientfic-agents" tabindex="-1">Scientfic Agents <a class="header-anchor" href="#scientfic-agents" aria-label="Permalink to &quot;Scientfic Agents&quot;">​</a></h3><ul><li>早期：强调科学知识回忆和推理 <ul><li>(2017) ACR Clark、(2022) Science QA、(2022) Science World等等。</li></ul></li><li>近期：强调加速科学研究，单一任务为主 <ul><li><code>科学构思</code>：<mark>产生新颖、专家级的想法</mark>。</li><li><code>实验设计</code>：</li><li><code>实验代码生成</code>： (2024) SciCode、(2025) ScienceAgentBench、(2024) <code>CORE-Bench</code>等。</li><li><code>同行评审生成</code>：</li></ul></li><li>近期/未来：由单一向统一集成转变 <ul><li>(2025) <code>AAAR-1.0</code>：同时评估方程推理、实验设计、论文缺陷识别和评审4项任务。</li><li>(2025) <code>MLGym</code>：健身房环境，13个挑战</li><li>(2024)<mark>DiscoverWorld</mark>：模拟120个不同任务完整科学发现周期</li><li>(2024) LabBench：生物学研究领域评估</li></ul></li></ul><h3 id="对话-agents" tabindex="-1">对话 Agents <a class="header-anchor" href="#对话-agents" aria-label="Permalink to &quot;对话 Agents&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">提示</div><p>Conversational agents 处理用户请求，完成多轮对话，涉及调用工具等。</p></div><p>主要有：</p><ul><li>(2021) Action-Based Conversations Datasets(ABCD)：10k对话，55种意图</li><li>(2024) ALMITA Bench：客服领域，14个意图，192个对话，1420个测试</li><li>(2024) <mark>τ -Bench</mark>：航空和零售领域</li><li>(2025) <mark>IntellAgent</mark>：一个自动评估对话agent的框架，以数据库/公司政策为输入，基于事件和用户测试</li></ul><h3 id="data-agents" tabindex="-1">Data Agents <a class="header-anchor" href="#data-agents" aria-label="Permalink to &quot;Data Agents&quot;">​</a></h3><p>(2502)DABStep</p><div class="custom-block tip"><div class="custom-block-title">DABStep</div><ul><li>(2025)<a href="https://huggingface.co/blog/dabstep" target="_blank" rel="noreferrer">DABStep</a>，Data Agent Benchmark for Multi-Step Reasoning</li><li><a href="https://huggingface.co/spaces/adyen/DABstep/tree/main/baseline" target="_blank" rel="noreferrer">Baseline</a>、<a href="https://huggingface.co/spaces/adyen/DABstep" target="_blank" rel="noreferrer">DABStep Leaderboard</a>、<a href="https://colab.research.google.com/drive/1pXi5ffBFNJQ5nn1111SnIfjfKCOlunxu" target="_blank" rel="noreferrer">QuickStart</a></li></ul><p>DABStep是一个多步推理的数据Benchmark，包括450个数据分析任务。它要求模型</p><ul><li>深入数据细节、保持严谨无幻觉</li><li>对结构化和非结构化数据进行推理。</li><li>连接到真实实际应用场景中。是分析师日常面料的真实挑战0</li></ul></div><p><strong>数据示例</strong></p><p>数据由多种金融文件组成。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/llm/agent/02-evaluation-agent/20250519102532.jpg" style="display:block;margin:auto;" width="80%"><p>问题示例：</p><p>包括：<mark>问题、难度、Guidelines</mark>(说明如何去解析答案结构来评估正确性)</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/llm/agent/02-evaluation-agent/20250519102645.jpg" style="display:block;margin:auto;" width="80%"><p>对于难度1，很多acc达到90%，但<mark>对于难度2，o3 mini、DSR1也只有10+的准确率。</mark></p><h2 id="通用智能体评估" tabindex="-1">通用智能体评估 <a class="header-anchor" href="#通用智能体评估" aria-label="Permalink to &quot;通用智能体评估&quot;">​</a></h2><div class="custom-block tip"><div class="custom-block-title">Generalist Agent</div><p>由单一能力向综合转变，整合LLM、网络导航、信息检索、代码执行等能力，处理复杂任务。</p></div><p>主要有：</p><ul><li>一般能力评估：<mark>多步推理、交互式解决问题、工具使用等</mark>。 <ul><li>(2023) <mark>GAIA</mark>：466个真实问题，测试推理、多模理解、网页导航、工具使用等。</li><li>(2025) Galileo’s Agent Leaderboard：强调实际应用中函数调用的能力。</li><li>(2023) AgentBench：交互式环境，操作系统、SQL、数字游戏、家务任务等。</li></ul></li><li>超越一般评估：<mark>强调在完整计算机系统中的表现</mark><ul><li>(2024) OSWorld、(2024) OminiAct、(2024) AppWorld。需编写调试代码， 保证系统稳定运行</li></ul></li><li>数字工作环境评估：像人类工作一样评估 <ul><li>(2024) <mark>TheAgentCompany</mark>：浏览内部网站、编写代码、与同事沟通。</li><li>(2025) CRMArena：客户关系管理</li></ul></li><li>标准化评估平台 <ul><li>(2025) Holistic Agent Leaderboard：一个标准化汇评估平台，汇总多个bench</li></ul></li></ul><h2 id="agent评估框架" tabindex="-1">Agent评估框架 <a class="header-anchor" href="#agent评估框架" aria-label="Permalink to &quot;Agent评估框架&quot;">​</a></h2><table tabindex="0"><thead><tr><th>时期</th><th>特点</th></tr></thead><tbody><tr><td>早期</td><td>单轮交互、<mark>评估任务完成度</mark></td></tr><tr><td>最近</td><td><mark>多步推理、轨迹分析、特定agent评估</mark>(如tool use)等。</td></tr></tbody></table><h3 id="主要框架" tabindex="-1">主要框架 <a class="header-anchor" href="#主要框架" aria-label="Permalink to &quot;主要框架&quot;">​</a></h3><p>主要包括：</p><table tabindex="0"><thead><tr><th>名称</th><th>内容</th></tr></thead><tbody><tr><td>(2023) LangSmith</td><td>Langchain的</td></tr><tr><td>(2023) LangFuse</td><td></td></tr><tr><td>(2025) LangChain AgentsEvals</td><td>Langchain的</td></tr><tr><td>(2025) Google Vertex AI Evaluation</td><td></td></tr><tr><td>(2025) Arize AI&#39;s Evaluation Framework</td><td></td></tr><tr><td>(2025) Galileo Agentic Evaluation</td><td></td></tr><tr><td>(2023) Databricsks Mosaic AI Agent Evaluation</td><td>主要forRAG任务</td></tr><tr><td>(2025) Botpress Multi-Agent Evaluation System</td><td>Mulit-Agent</td></tr><tr><td>(2024) AutoGen</td><td>Multi-Agent</td></tr></tbody></table><h3 id="评估维度" tabindex="-1">评估维度 <a class="header-anchor" href="#评估维度" aria-label="Permalink to &quot;评估维度&quot;">​</a></h3><table tabindex="0"><thead><tr><th><strong>名称</strong></th><th><strong>内容</strong></th></tr></thead><tbody><tr><td><mark>Final Response 评估</mark></td><td>事先定义好<code>评估标准</code>，再使用<code>LLM-based judeges</code>来评估。</td></tr><tr><td><mark>Stepwise 评估</mark></td><td><code>细粒度评估每个action</code>，分析错误原因。比如<code>工具选择</code>、执行等。<br>如Galieo Agentic Evaluation提供 <code>action advancement metric</code>，来评估action是否有贡献等。<br>但问题是，缺乏泛化通用judge，很多都是task-specific的。</td></tr><tr><td>Traj-Based 评估</td><td>评估决策过程相对预期最佳路径所采取的步骤顺序。</td></tr></tbody></table><h2 id="benchmarks" tabindex="-1">Benchmarks <a class="header-anchor" href="#benchmarks" aria-label="Permalink to &quot;Benchmarks&quot;">​</a></h2><ul><li><a href="https://huggingface.co/spaces/smolagents/smolagents-leaderboard" target="_blank" rel="noreferrer">Smolagents Ledearboard</a></li><li><a href="https://huggingface.co/blog/dabstep" target="_blank" rel="noreferrer">DABStep Data Agent Benchmark for Multi-step Reasoning</a></li></ul>',54)]))}const m=e(n,[["render",o]]);export{g as __pageData,m as default};

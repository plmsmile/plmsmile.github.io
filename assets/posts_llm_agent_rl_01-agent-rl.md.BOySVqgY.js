import{_ as t,c as o,o as e,ah as a}from"./chunks/framework.CvbyeFFO.js";const p=JSON.parse('{"title":"Agent-RL 综述型笔记","description":"","frontmatter":{"title":"Agent-RL 综述型笔记","date":"2025-05-21T11:42:56.000Z","create":"2025-05-21T11:42:56.000Z","categories":["agent-rl"],"tags":["agent-rl"]},"headers":[],"relativePath":"posts/llm/agent/rl/01-agent-rl.md","filePath":"posts/llm/agent/rl/01-agent-rl.md","lastUpdated":null}'),i={name:"posts/llm/agent/rl/01-agent-rl.md"};function r(n,l,s,c,d,g){return e(),o("div",null,l[0]||(l[0]=[a('<div class="custom-block important"><div class="custom-block-title">文章概要</div><ul><li>记录整理学习他人的<mark>Agent-RL综述/方向型</mark>的内容笔记。</li></ul></div><h2 id="前言-✨" tabindex="-1">前言 ✨ <a class="header-anchor" href="#前言-✨" aria-label="Permalink to &quot;前言 ✨&quot;">​</a></h2><p>在学习<code>Agent-RL</code>过程中，发现很多有意思的文章，本想放到一篇博客整理，但发现太多，于是对其进行拆开，整体目前分为4个部分：</p><ul><li>《Agent-Search-RL 笔记》：记录<mark>搜索浏览</mark>相关内容。</li><li>《Agent-Tool-RL 笔记》：记录<mark>工具调用</mark>相关内容，目前代码数学问题居多。</li><li>《Agent-Interaction-RL 笔记》：侧重于<mark>环境多轮交互</mark>，目前游戏居多。</li><li>《Agent-RL 综合性笔记》(本文)：不属于上述类型的内容，偏综述型的内容。</li></ul><h2 id="技术文章-📚" tabindex="-1">技术文章 📚 <a class="header-anchor" href="#技术文章-📚" aria-label="Permalink to &quot;技术文章 📚&quot;">​</a></h2><p>技术达人写的文章，笔记。</p><h3 id="_2505-是念-2025年大模型agent-rl训练多轮planning技术torl-toolrl-otc-skyrl-v0-gigpo-tool-n1-artist-zerotir-grpo" tabindex="-1">(2505) 是念：2025年大模型agent rl训练多轮planning技术TORL,ToolRL,OTC,SkyRL-v0, GiGPO,Tool-N1 ,ARTIST, ZeroTIR, GRPO <a class="header-anchor" href="#_2505-是念-2025年大模型agent-rl训练多轮planning技术torl-toolrl-otc-skyrl-v0-gigpo-tool-n1-artist-zerotir-grpo" aria-label="Permalink to &quot;(2505) 是念：2025年大模型agent rl训练多轮planning技术TORL,ToolRL,OTC,SkyRL-v0, GiGPO,Tool-N1 ,ARTIST, ZeroTIR, GRPO&quot;">​</a></h3><div class="custom-block important"><div class="custom-block-title">概要</div><ul><li><strong>AgentRL优点</strong>： 通过工具交互能获取外部知识。</li><li><strong>AgentRL缺点</strong>： 目前<mark>交互次数少</mark>、<mark>多工具混合研究少</mark>，解决复杂问题仍有挑战。</li><li>DeepSeek R1 带火了 RL技术。</li><li>列举了相关流行工作，见拆解文章。</li><li>原文链接：<a href="https://zhuanlan.zhihu.com/p/1902381952998281700" target="_blank" rel="noreferrer">是念:2025大模型agentrl...</a></li></ul></div><p><strong>Agent RL 优缺点分析</strong> 🧐</p><div class="custom-block tip"><div class="custom-block-title">Agent RL 优缺点 🛠️</div><p>🏴 <strong>背景</strong></p><ul><li><code>Agentic tool use learning</code> 也开始用上了 GRPO 等 RL 算法，让 LLM 学会使用 <code>code-intepreter</code>、<code>web-search</code> 等工具，增强模型数学及推理能力，包括单轮/多轮 tool-use。</li></ul><p>🌟 <strong>Agent RL 优点</strong></p><ul><li><mark>通过 tool 交互获取外部知识</mark>，进一步提升模型准确率。</li><li>PPO 系列是一个 <code>online-rl</code> 方法，<mark>需要的数据量小很多</mark>，而传统 DPO 需要大量数据进行训练。 <ul><li><mark>每次通过 sampling 生成样本，然后进行训练提升</mark>。</li></ul></li></ul><p>⚠️ <strong>Agent RL 缺点</strong></p><ul><li>真正复杂任务可能需要 30-100 个 step 才能完成，目前 RL 框架集中解决 10 个 step 左右就能完成的任务，<mark>距离真正解决复杂问题仍有一段距离</mark>。 <ul><li>受限于 LLM 处理长序列效果下降、计算效率低等原因。</li></ul></li><li>GRPO rule-based 方法虽已简化流程，<mark>仍需要标注数据、精心设计 reward、调参及数据，才能得到好效果</mark>。</li><li>RL 依赖环境训练，一般速度较慢（仿真环境），如何跟上 GPU 计算 RL 训练，仍是一个问题。</li><li>Agent-RL <mark>研究单一工具居多</mark>（code, web-search），而<mark>多工具混合、多轮调用研究较少</mark>。</li></ul></div><img src="https://pic3.zhimg.com/v2-abc23ce6d62f9fceba4931742e2240da_1440w.jpg" style="display:block;margin:auto;" width="80%"><p><strong>DeepSeek 技术分析</strong> 🔍</p><ul><li><code>MoE</code>：降低了训练成本、提高了推理效率</li><li><code>Multi-Head Latent Attention</code>：减少了注意力部分的KV缓存、Low Rank</li><li><code>Multi-Token Prediction</code>：提高模型性能(准确性)</li><li><code>DualPipe</code>：提高了大规模GPU集群的计算与通信比率和效率</li><li><code>FP8 Training</code>：采样低精度训练进一步降低训练成本</li><li><code>DeepSeek-R1</code>：采样GRPO和多阶段训练。</li></ul><div class="custom-block caution"><div class="custom-block-title">GRPO vs PPO ⚠️</div><p>DeepSeek R1 GRPO 带火了RL技术路线，其中GRPO和PPO相差较小。主要区别是advantage是sampling过程产生样本的reward 求均值求方差得到的。</p></div><img src="https://pica.zhimg.com/v2-e696727d297071de37dbe8e6fb13eaf4_1440w.jpg" style="display:block;margin:auto;" width="80%"><p>其他具体内容见拆分后的笔记。</p><h3 id="_2505-亚里随笔-toolrl探路者——万字长文总结llm-toolrl系列近期工作-✍️" tabindex="-1">(2505) 亚里随笔：ToolRL探路者——万字长文总结LLM ToolRL系列近期工作 ✍️ <a class="header-anchor" href="#_2505-亚里随笔-toolrl探路者——万字长文总结llm-toolrl系列近期工作-✍️" aria-label="Permalink to &quot;(2505) 亚里随笔：ToolRL探路者——万字长文总结LLM ToolRL系列近期工作 ✍️&quot;">​</a></h3><div class="custom-block important"><div class="custom-block-title">概要</div><ul><li>原文：<a href="https://zhuanlan.zhihu.com/p/1904999390919259656" target="_blank" rel="noreferrer">亚里随笔：TooRL...</a></li><li>有搜索，具体看拆解文章。</li></ul></div><div class="custom-block warning"><div class="custom-block-title">问题背景 🚧</div><ul><li>纯文本推理具有局限性：面对复杂计算等场景，有<strong>工具调用需求</strong>。</li><li>工具集成推理的<strong>现有问题</strong>：<mark>SFT/Prompt方法不具备泛化能力</mark>，难以发现最优策略，限制了模型探索。</li><li>RL的挑战 (<strong>偏搜索</strong>) <ul><li>如何<mark>将搜索引擎集成到RL并保持优化稳定</mark></li><li>LLM难以实现迭代推理和搜索引擎调用，无法<mark>根据问题复杂性动态调整检索策略</mark></li><li>有效的<mark>搜索/推理奖励设计困难</mark>，简单基于结果的奖励可能不足以引导LLM学习有意义的搜索行为</li></ul></li><li>工具使用<strong>效率问题</strong>：当前方法通常鼓励无节制工具使用，训练/推理存在问题。</li><li>现有<strong>训练数据和方法不足</strong>：为增强工具调用能力，现研究大都合成工具使用数据来简单微调，但缺乏推理步骤，训练难以对过程指导，容易导致伪推理</li></ul></div><img src="https://pic1.zhimg.com/70/v2-52ec96b8baaf23c8890c2c658779c867_1440w.avis?source=172ae18b&amp;biz_tag=Post" style="display:block;margin:auto;" width="80%"><p>其他具体内容见拆分后的笔记。</p>',21)]))}const k=t(i,[["render",r]]);export{p as __pageData,k as default};

import{_ as o,c as i,o as e,ah as c}from"./chunks/framework.CvbyeFFO.js";const p=JSON.parse('{"title":"Code 任务Bench相关","description":"","frontmatter":{"title":"Code 任务Bench相关","date":"2025-12-14T17:37:49.000Z","create":"2025-12-14T17:37:49.000Z","categories":["code-task"],"tags":["metrics","task"]},"headers":[],"relativePath":"posts/llm/industry/codellm/02-eval-task-benchmark.md","filePath":"posts/llm/industry/codellm/02-eval-task-benchmark.md","lastUpdated":null}'),t={name:"posts/llm/industry/codellm/02-eval-task-benchmark.md"};function s(d,l,n,a,r,u){return e(),i("div",null,l[0]||(l[0]=[c('<h2 id="参考文章" tabindex="-1">参考文章 <a class="header-anchor" href="#参考文章" aria-label="Permalink to &quot;参考文章&quot;">​</a></h2><div class="custom-block caution"><div class="custom-block-title">参考文章</div><ul><li><a href="https://www.alphaxiv.org/abs/2511.18538" target="_blank" rel="noreferrer">survey paper</a></li></ul></div><h2 id="评估指标" tabindex="-1">评估指标 <a class="header-anchor" href="#评估指标" aria-label="Permalink to &quot;评估指标&quot;">​</a></h2><div class="custom-block info"><div class="custom-block-title">传统指标</div><p><strong>1. CodeBLEU</strong></p><ul><li>看<code>词汇重合度</code>。</li></ul><p><strong>2. CodeBERTScore / RTC</strong></p><ul><li>CodeBERTScore：<code>向量相似度</code>。</li><li>RTC(Round-Trip Correctness)：把代码翻译成另一种形式，再翻译回来，对比前后一致性。</li></ul><p><strong>3. Pass@k</strong></p><ul><li>评估<code>功能正确性</code>，实际执行。<code>给k次机会</code>，只要有1个成功，就算成功。</li></ul></div><div class="custom-block info"><div class="custom-block-title">LLM-as-a-Judge 指标</div><p><strong>核心思想</strong></p><ul><li>让LLM来评估是否写得好。</li></ul><p><strong>1. ICE-Score</strong></p><ul><li>引入<code>Rubrics打分细则</code>，输出1个量化的分数。 <ul><li>比如完全没用大0分、语法错误打1分、解决问题打4分。</li></ul></li></ul><p><strong>2. CodeJudge/CodeJudgeBench</strong></p><ul><li>输入task和task_code，让模型<code>先思考</code>（哪里写得好/哪里有bug），<code>再打分</code>。</li><li>缺点：不稳定。</li></ul><p><strong>3. BigCodeReward (多模态评估)</strong></p><ul><li>输入<code>代码</code>和<code>代码运行结果</code>，去做打分，<code>更关心实际效果</code>。 <ul><li>运行结果：报错日志、图标、网页截图等。</li></ul></li><li><code>偏好打分</code>：给2个方案及运行结果，判断谁更好。</li></ul></div><div class="custom-block info"><div class="custom-block-title">执行指标</div><p><strong>1. ProbGen/差异测试</strong></p><ul><li>背景：判断代码<code>和标准答案是否一致</code>。</li><li>让llm自动生成<code>多个输入数据</code>(探针)，分别喂给<code>2段代码</code>；若输出<code>结果不同</code>，则<code>功能不同</code>。</li></ul><p><strong>2. REFUTE/反例生成</strong></p><ul><li>给一段有bug的代码，生成输入，让代码跑崩溃。</li></ul><p><strong>3. EvaLooop/循环转换测试</strong></p><ul><li>代码需求循环：需求 -&gt; 代码 -&gt; 需求 -&gt; 代码 -&gt; ...</li><li>多语言转换循环：Python -&gt; Java -&gt; Python -&gt; Java -&gt; ...</li><li>评估标准：如果能坚持多轮，则效果稳健ASL分数高；否则，分数低。</li></ul></div><div class="custom-block info"><div class="custom-block-title">Multi-Agent</div><p><strong>1. MCTS-Judge</strong></p><ul><li>搜索输，每个节点代表一个特定检查视角(边界条件/异常/需求规范等等)。</li><li>沿推理路径做预测和模拟执行，如果推理和执行结果一致，则给一个奖励。</li></ul><p><strong>2. CodeVisionary</strong></p><ul><li><code>让多个模型一起打分</code>。先收集情报，再让n个LLM进行打分。</li><li>指标：共识分数，降低评估方差。</li></ul></div><div class="custom-block info"><div class="custom-block-title">稳定性测试</div><p><strong>1. Incoherence/不连贯性/不一致性</strong></p><ul><li>1个任务，让模型生成2个程序，观察输出是否一致。 <ul><li>高Incoherence：模型随机性很强，不可靠。</li><li>低Incoherence：模型很稳定，一致性好。</li></ul></li></ul><p><strong>2. Mean Absolute Deviation (MAD)/鲁棒性测试</strong></p><ul><li>先测原始代的准确率，做一些表面修改(如变量命名)，计算评分准确率。</li><li>判断两个准确率之间的差距。MAD越小越好。</li></ul></div><div class="custom-block info"><div class="custom-block-title">其他</div><p><strong>1. SBC 语义相似性</strong></p><ul><li>原始需求 -&gt; 模型 -&gt; 代码，再把代码 -&gt; 新需求。</li><li>判断<code>原始需求描述</code>和<code>新需求描述</code>是否一致。</li></ul><p><strong>2. Copilot Arena / BigCodeArena</strong></p><ul><li>系统同时给模型A和B的代码，开发者采纳谁，就算赢。计算排名。</li></ul><p><strong>3. CodeCriticBench</strong></p><ul><li>给一段代码，具体两个指标由大模型来判断 <ul><li><code>Acc准确率</code>：模型对不对</li><li><code>Checklist</code>：比如安全性达标吗、命名规范吗、效率高吗？</li></ul></li><li><code>人类均方误差</code>：LLM打分和人类打分偏离多少。</li></ul></div><h2 id="片段级任务" tabindex="-1">片段级任务 <a class="header-anchor" href="#片段级任务" aria-label="Permalink to &quot;片段级任务&quot;">​</a></h2><h3 id="代码补全和fim" tabindex="-1">代码补全和FIM <a class="header-anchor" href="#代码补全和fim" aria-label="Permalink to &quot;代码补全和FIM&quot;">​</a></h3><p>给定部分上下文，评估模型<code>预测正确代码片段</code>的能力。</p><div class="custom-block caution"><div class="custom-block-title">代码补全</div><p><strong>核心点</strong></p><ul><li>利用上文前缀Prefix，<code>继续编写</code>或<code>修复代码</code>。</li></ul><p><strong>Statement-Level(语句级)</strong></p><ul><li>基于上文预测单行代码或表达式，关注局部语法。</li><li>CodeXGLUE：下一行补全，6种语言，<code>精准匹配</code>作为指标。</li><li>HumanEval-Infill/Qwencoder2.5：给定文档，<code>可执行</code>、<code>单元测试</code>作为指标。</li></ul><p><strong>Function-level(函数级)</strong></p><ul><li><code>给定函数签名</code>或部分代码，<code>生成完整函数</code>，关注算法逻辑。</li><li>BigCodeBench：从github上提取<code>真实代码</code>，做<code>函数还原</code>。评估：<code>正确性</code>和复杂度。</li><li>MultiPL-E：同一个功能，可以有<code>多个写法</code>。</li></ul><p><strong>Class-Level(类级)</strong></p><ul><li>给定部分类定义中，实现整个类、多个方法。</li><li>ClassEval：不仅写一个函数，要写整个类，关注<code>全局类结构理解能力</code>。</li><li>ClassEval-T：口占了Java和CPP，更强调面向对象。</li><li>OOP：关注设计模式。</li></ul></div><div class="custom-block caution"><div class="custom-block-title">FIM</div><p><strong>核心</strong></p><ul><li>利用前缀prefix和后缀suffix，来预测中间部分。</li><li>FIM是一个预训练任务，现在单独出来，评估是否好用。</li></ul><p><strong>主要工作</strong></p><ul><li>CCCI：强调上下文 <ul><li>喂外部知识，如数据库结构关系等；模型懂了业务逻辑，补全代码质量更高。</li></ul></li><li>SAFIM：强调语法感知的评估 <ul><li>挖空的是完整的语法块，避免随机mask。</li><li>如果预训练效果不好，则SAFIM能力就会差。</li></ul></li><li>ASR-FIM <ul><li>也是避免随机mask。</li><li>使用抽象语法树(AST)来做掩码，遮住完整的语法结构。</li><li>强迫模型去预测整个逻辑结构。</li></ul></li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181133.jpg" style="display:block;margin:auto;" width="70%"><h3 id="代码生成" tabindex="-1">代码生成 <a class="header-anchor" href="#代码生成" aria-label="Permalink to &quot;代码生成&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">Funtion-Level</div><p><strong>早期-简单函数生成</strong></p><ul><li>CodeXPLUG,</li><li><strong>HumanEval&amp;MBPP</strong> (<code>最流行</code>)：<code>Prompt</code> -&gt; <code>Code</code> -&gt; <code>单元测试</code></li><li>缺点：测试用你太少，<code>容易过拟合</code></li></ul><p><strong>增强测试严谨性</strong></p><ul><li>HumanEval+, MBPP+：<code>自动生成测试用例</code>，去验证是否真的懂，避免过拟合</li><li>HumanEval Prom MBPP PRo：<code>增加难度、扩大题库</code>、改进评估方式等。</li></ul><p><strong>多语言扩展</strong></p><ul><li>MBXP/Multi-HumanEval：把测试用例扩展至Java/C++/JS等语言。</li><li>HumanEval-XL：测试自然语言和编程语言的交叉组合。英语中文俄语、python/java 等混搭组合。</li></ul><p><strong>真实场景/上下文/复杂性</strong></p><ul><li><strong>McEval/BigCodeBench</strong>：强调代码不是孤立的，需要结合<code>代码库上下文</code>、显示世界等。</li><li>MERA Code：超越简单函数到复杂系统的<code>全方位评估框架</code>。</li><li>CodeFuseEval：<code>中文指令</code>、多任务能力。</li></ul></div><div class="custom-block note"><div class="custom-block-title">Class-Level</div><ul><li>ClassEval：100个python类，需要内部多个方法调用</li><li>ClassEvalT：</li><li>OOP：面向对象</li></ul></div><div class="custom-block note"><div class="custom-block-title">编程</div><p><strong>核心思想</strong></p><ul><li>不仅要代码能跑通，还要逻辑强、效率高。</li></ul><p><strong>主要工作</strong></p><ul><li>LiveCodeBench, LiveCodeBench Pro：收集模型训练截止日期之后的题目，测试OOD题目。</li><li>CruxEval：让模型<code>写代码</code>、还<code>预测输入输出</code>，测试解题能力。</li><li>CodeNet：多语言数据集。</li><li>APPS：竞赛问题+人类写的代码。</li><li>MultiPL-E：要求模型为每个问题生成多个答案，评估多样性。</li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181216.jpg" style="display:block;margin:auto;" width="70%"><div class="custom-block note"><div class="custom-block-title">领域Benchmark</div><p><strong>垂直技术栈</strong></p><ul><li>Deep-Bench：写pytorch/tensorflow的能力。</li><li>KernelBench/TritonBench：测试GPU编程。</li><li>SciCode：科学计算。</li></ul><p><strong>复杂结构和全栈</strong></p><ul><li>FullStackBench：测试全栈能力(前端、后端、数据库)</li><li>YABLoCo：长上下文，读几千行代码，写新代码。一般用作老旧项目维护。</li></ul><p><strong>评测方法创新</strong></p><ul><li>Meta-Benching/AutoCodeBench：设计框架<code>自动构建Bench</code>，让AI自动<code>抓取代码</code>、<code>生成测试用例</code>。</li><li>RAG/CodeRAG-Bench：考察<code>看文档写代码</code>的能力。</li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181145.jpg" style="display:block;margin:auto;" width="70%"><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181226.jpg" style="display:block;margin:auto;" width="70%"><h3 id="代码编辑和修bug" tabindex="-1">代码编辑和修bug <a class="header-anchor" href="#代码编辑和修bug" aria-label="Permalink to &quot;代码编辑和修bug&quot;">​</a></h3><p>改原始代码。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181237.jpg" style="display:block;margin:auto;" width="70%"><div class="custom-block info"><div class="custom-block-title">代码编辑&amp;修bug</div><p><strong>语句级Bench</strong></p><ul><li>早期数据集(版本控制的提交历史) <ul><li>MegaDiff：66.3万个Java变更。</li><li>TSSB-3M：300w个python合成的单语句错误。</li><li>特定类型错误：TFix (10.5万个js修复)、FixJS(32.4万个js)、PyTer(python错误)</li></ul></li><li>后来的工作：规模、可执行和语言改进。 <ul><li>RunBugRun：8种语言，45w个可执行错误修复Pair。</li><li>xCodeEval：450个多语言样本，10+语言。</li><li>DebugBench：4.2k的debug任务和配套测试，c++,python,java</li></ul></li><li>最近数据集：把调试集成到指令微调中 <ul><li>HumanEvalPack：把HumanEval扩展到6种语言，并增加调试变体</li><li>MDEval：18种语言、3513个调试任务</li></ul></li></ul><p><strong>交互迭代式Bench</strong></p><ul><li>传统：一次性修复</li><li><strong>迭代与反馈</strong><ul><li>SWT-Bench：1900个，可执行测试用例，真实Github错误</li><li>FeedbackEval：带有结构化外部提示的迭代修复</li><li><code>写代码</code> -&gt; <code>报错</code> -&gt;<code> 改代码</code> -&gt; <code>跑通</code></li></ul></li><li><strong>Agentic &amp; Tool Use</strong><ul><li><strong>CodeEditorBench</strong>：模拟<code>IDE环境</code>，涉及增量编辑和文件间依赖。</li><li><strong>DebugEval</strong>：自我调试，<code>模型生成调试信号</code>来指导多步修复。</li><li><strong>Debug-gym</strong>： <ul><li>教模型<code>使用debug工具(pdb)</code>，包括打断点、单步执行、查看变量值等。</li><li>通过<code>SFT/RL</code>教模型使用<code>有状态工具</code>，解决预训练此类数据稀缺问题。</li></ul></li></ul></li></ul></div><h3 id="代码效率" tabindex="-1">代码效率 <a class="header-anchor" href="#代码效率" aria-label="Permalink to &quot;代码效率&quot;">​</a></h3><div class="custom-block warning"><div class="custom-block-title">代码效率</div><p><strong>背景</strong></p><ul><li>LLM 写的代码可能效果好，但<code>效率很低</code>，希望写对代码-&gt;<code>写好代码</code>。</li><li>效率：运行时间、内存使用、算法复杂度、能耗。</li></ul><p><strong>性能和复杂度测试</strong></p><ul><li>EffiBench：1k+算法任务，GPT4代码比人类代码慢3倍、内存占用多14-44倍。</li><li>Mercury：扩展并提出多维框架，测量各种编程问题在最佳/最差下的执行时间、内存占用和复杂度。</li><li>EffiBench-x：多种编程语言。</li><li>BigO/DynCode：侧重算法复杂度，是否达到算法复杂度。</li><li>COFFE：强调在现实执行负载下的实际运行时间评估，提供了互补的系统级视角。</li><li>EvalPerf：引入了一个差分性能评估框架，评估 LLM代码在性能挑战任务上的效果。</li></ul><p><strong>能耗和效率优化</strong></p><ul><li>ECCO/Solovyeva/Cappendijk：量化了LLMCode和人类Code的能耗开销，探索微调策略降低功耗。</li></ul></div><div class="custom-block warning"><div class="custom-block-title">把效率纳入建模/训练过程的方法</div><p><strong>训练时干预 (Training Objective)</strong></p><ul><li><strong>EffiCoder</strong> 试图在训练阶段就告诉模型：“写得快的代码才是好代码”，<code>把效率信号加入损失函数</code>。</li></ul><p><strong>强化学习 (RL)：</strong></p><ul><li><strong>ACECode</strong> 使用 RL，如果代码运行速度快，则给奖励，来激励模型优化代码。</li></ul><p><strong>事后修补 (Post-generation Refinement)：</strong></p><ul><li><strong>Rethinking Code Refinement</strong> ：先让模型写，再用专门步骤去检查“哪里写慢了”，再重构优化。</li></ul></div><h3 id="代码偏好" tabindex="-1">代码偏好 <a class="header-anchor" href="#代码偏好" aria-label="Permalink to &quot;代码偏好&quot;">​</a></h3><div class="custom-block caution"><div class="custom-block-title">代码偏好</div><p><strong>背景</strong></p><ul><li>不仅是写对代码，还要评估谁的代码更好，更符合人类标准：(<code>正确</code>、<code>快</code>、<code>安全</code>、<code>易读</code>).</li></ul><p><strong>方法1：硬指标打分法</strong></p><ul><li>CodeArena：题越难，分数越高；运行时间越短，分数越高。</li><li>Long CodeArena：项目级，看能否在大项目里理解项目架构。</li></ul><p><strong>方法2：竞技场LLM-as-a-Judge法</strong></p><ul><li>找强力模型做裁判。</li><li><strong>CodePrefBench</strong>：直接测模型有无鉴赏力，能不能挑出好代码。</li><li><strong>AutoCodeArena</strong>：自动化擂台。裁判看2者的<code>代码</code>+<code>各自沙盒运行结果</code>，来打分。</li></ul></div><h3 id="代码推理和问答" tabindex="-1">代码推理和问答 <a class="header-anchor" href="#代码推理和问答" aria-label="Permalink to &quot;代码推理和问答&quot;">​</a></h3><div class="custom-block important"><div class="custom-block-title">代码推理和问答</div><p><strong>背景</strong></p><ul><li>评估模型理解分析代码的能力。</li></ul><p><strong>基于QA的评估</strong></p><ul><li>CodeQA：<code>把代码注释</code>转换成<code>问答对</code>，10w的数据集。</li><li>CS1QA：从python入门课程的聊天记录中收集问答对。</li><li><strong>CodeMMLU</strong>：多种任务，包括代码修复、执行推断和填空挑战。</li><li><strong>CodeSense</strong>：<code>真实软件项目</code>中<code>细粒度的代码语义推理</code>任务。 <ul><li>现有Bench依赖合成数据或教育性编程。</li></ul></li></ul><p><strong>基于代码语义推理的评估 (更具体的任务)</strong></p><ul><li>SpecEval：4个任务来评估，规范判断、选择、补全和生成。</li><li>CRUXEval/CRUXEval-X：执行预测，<code>输出预测</code>和<code>输入预测</code>s。</li><li>EquiBench：程序等价性检查，两段代码，修改了语法结构，看是否一致。</li><li>CORE：通过静态分析来判断，如数据流（Data Flow）和控制流（Control Flow）。 <ul><li>变量 A 的值是否依赖于变量 B？，这段代码的死代码（Dead Code）在哪里？</li></ul></li></ul><p><strong>多模态维度</strong></p><ul><li><strong>ScratchEval</strong> 非常新颖。Scratch 是图形化编程（拖拽积木块），代码是“画”出来的。 <ul><li>这测试了多模态模型（如 GPT-4V）能否看懂<strong>视觉化的逻辑结构</strong>。</li></ul></li></ul></div><h3 id="代码翻译" tabindex="-1">代码翻译 <a class="header-anchor" href="#代码翻译" aria-label="Permalink to &quot;代码翻译&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">Code Translation</div><p><strong>早期方法：语法结构驱动</strong></p><ul><li>AST： 抽象语法树，类似先把英文句子分析成主谓宾结构树，再把树变成中文结构。难以处理复杂结构。</li><li>TransCoder：只给模型看大量的中文书和大量的英文书（单语语料），模型也能通过无监督学习学会翻译</li></ul><p><strong>基于执行反馈的方法 (测试用例)</strong></p><ul><li>保留原有代码结果，LLM翻译成新代码，执行新代码的单元测试， <ul><li>如果结果一致，说明正确；否则，把不同信息给到，让LLM继续改。</li></ul></li></ul><p><strong>Prompt工程</strong></p><ul><li>先解释后翻译，效果好。</li></ul><p><strong>安全</strong></p><ul><li>使用<strong>差分模糊测试（differential fuzzing</strong>来验证等价性。 <ul><li>如c++ -&gt; rust，内存不安全-&gt;内存安全语言。</li></ul></li></ul><p><strong>数据与评估的进化</strong></p><ul><li><strong>数据合成：</strong> 很难找到完全对应的代码对，用 <strong>Back-Translation（回译）</strong> 来造数据。 <ul><li>Python -&gt; Java, 再Java -&gt; Python，看能否复原，以此来造数据，</li></ul></li><li><strong>超越 BLEU：</strong> 之前看BLEU（看词重叠率，发现没用，现在看<strong>功能等价性</strong>（能不能跑通测试）和<strong>复杂度分级</strong>（是简单的改变量名，还是复杂的算法重写）。</li></ul></div><h3 id="测试用例生成" tabindex="-1">测试用例生成 <a class="header-anchor" href="#测试用例生成" aria-label="Permalink to &quot;测试用例生成&quot;">​</a></h3><div class="custom-block important"><div class="custom-block-title">测试用例生成</div><p><strong>面向软件工程</strong></p><ul><li>SWT-Bench/TestGenEval <ul><li>转换了SWE-Bench代码，提供了有bug的代码和对应的补丁。</li><li>要求生成的测试用例，在bug代码不通过、在修复后的代码上通过。</li></ul></li></ul><p><strong>面向程序竞赛</strong></p><ul><li><strong>TestEval</strong>：LeetCode 210个问题；其评估仍然局限于覆盖率指标。</li><li><strong>CodeForce-SAGA</strong>、<strong>TCGBench</strong>、<strong>TestCase-Eval</strong><ul><li>收集了大量的<strong>错误</strong>和<strong>正确</strong>的代码提交，以进行端到端的评估。</li><li>核心指标：测试用例能否<strong>拒绝错误代码</strong>同时<strong>通过正确代码</strong>的比例。</li></ul></li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181256.jpg" style="display:block;margin:auto;" width="70%"><h2 id="repo-level-任务" tabindex="-1">Repo-Level 任务 <a class="header-anchor" href="#repo-level-任务" aria-label="Permalink to &quot;Repo-Level 任务&quot;">​</a></h2><h3 id="代码生成和补全" tabindex="-1">代码生成和补全 <a class="header-anchor" href="#代码生成和补全" aria-label="Permalink to &quot;代码生成和补全&quot;">​</a></h3><div class="custom-block important"><div class="custom-block-title">仓库级代码生成和补全</div><ul><li>由<code>单一代码刷题</code> -&gt; <code>写工程代码</code>。</li><li><code>整个代码库上下文</code>(多<strong>文件/项目结构/依赖关系/跨文件关系</strong>），来<code>预测/补全或生成代码片段</code>。</li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181306.jpg" style="display:block;margin:auto;" width="70%"><div class="custom-block danger"><div class="custom-block-title">相关Bench</div><ul><li><strong>RepoBench</strong><ul><li><strong>仓库级</strong>，在检索和<code>补全任务</code>上评估模型。</li><li>训练集：<code>22年之前</code>的代码；测试集：<code>22年之后</code>的代码。</li></ul></li><li><strong>RepoEval</strong><ul><li>处理仓库级时：RAG(检索+生成) &gt; 纯模型。<code>RepoCoder</code>利用<code>完整仓库上下文</code>，效果好于其他模型。</li><li>使用<code>单元测试</code>来评估<code>14个</code>真实代码仓库的<code>代码补全</code>。</li></ul></li><li>Execrepobench <ul><li>重新生成<code>多粒度</code>(表达式/语句/函数)的<code>masked spans</code>，来评估 LLM 的<code>仓库级补全能力</code></li><li>通过<code>仓库测试文件</code>进行验证；并在其上<code>微调Qwen2.5-Coder</code>以增强补全性能。</li></ul></li><li><strong>CoderEval</strong><ul><li>专门测试<code>非独立函数</code>，需要调用<code>其他文件代码</code>等。 <ul><li>模型效果：非独立函数 弱于 独立函数。460 个 Java/Python问题。</li></ul></li></ul></li><li><strong>CrossCodeEval</strong> ：多语言bench，10k，通过静态分析测试<code>跨文件上下文</code>的需求。</li><li><strong>M2rc-Eval</strong>：多语言Bench，使用<code>AST(抽象语法树)</code>来做<code>标注</code>的。</li><li><strong>Codev-Bench</strong>：使用<code>工业数据</code>评估<code>repo补全</code>能力。 <ul><li>CodeLLM 优于通用模型，但在<code>不完整后缀</code>场景<code>效果都不好</code>。</li></ul></li><li><strong>RepoCod</strong>：Python Bench，11个项目，980个任务，50%都需要仓库级上下文。</li><li><strong>DI-Bench</strong>：<code>评估依赖推理</code>，4语言，581仓库。顶级模型效果也不好。</li><li><strong>DependEval</strong>：<code>分层评估</code> <code>仓库级依赖</code>，8种语言，15k仓库。</li><li><strong>REPOST</strong>： <code>沙盒测试</code>构建仓库级环境。 <ul><li><code>REPOST-TRAIN</code>训练的模型在<code>HumanEval/RepoEval</code> 有效果</li><li>但在 <code>REPOST-EVAL 效果一般</code>。</li></ul></li><li><strong>SecRepoBench</strong>：在仓库级别评估<code>安全代码生成</code>（318 个任务，C/C++ 中的 15 种 CWE 类型）。</li><li><strong>DevEval</strong>：<code>人工注释的Bench</code>（1.8k样本，117 仓库）；有难度，当前 LLM 表现很差。</li></ul></div><h3 id="领域复杂代码生成" tabindex="-1">领域复杂代码生成 <a class="header-anchor" href="#领域复杂代码生成" aria-label="Permalink to &quot;领域复杂代码生成&quot;">​</a></h3><p>需要特别的<strong>领域知识</strong>，<code>逻辑复杂</code>、<code>多重依赖</code>等。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181328.jpg" style="display:block;margin:auto;" width="70%"><div class="custom-block danger"><div class="custom-block-title">相关Bench</div><ul><li><p>BioCoder</p><ul><li><code>生物学代码</code>生成Bench，1.7k生物学仓库、2.2k高质量代码问题。</li></ul></li><li><p>PaperBench</p><ul><li>要求模型<code>去复现论文</code>，20篇ICML2024的论文。 <ul><li>论文写公式原理，模型需深刻理解，才能写出和跑通代码。</li></ul></li><li><code>评分树(Rubric Tree)</code>：不只看最终结果，而是把<code>大任务</code>拆解为<code>8.3k个小步骤子任务</code>，一步步打分。 <ul><li>如数据预处理、模型定义、loss函数等。</li></ul></li><li>也有一个llm-as-a-judge方法</li></ul></li><li><p>Commit0</p><ul><li>输入<code>文档</code>和<code>空函数体</code>，<code>从0开始编写代码库</code>，实现所有函数，通过<code>单元测试</code>。54个python库。</li></ul></li><li><p>HackerRank-ASTRA</p><ul><li>评估跨领域、多文件的能力。每个问题运行32次做评估。</li></ul></li><li><p>ProjectEval</p><ul><li>模拟<code>用户交互</code>，来测试repo-level 代码生成能力，284个测试用例，共3级输入 <ul><li>Level1：<code>NL Prompt</code>；Level2：<code>NL Checklist</code>；Level3：<code>Skeleton</code>。</li></ul></li><li>模型在需要<code>系统性思维</code>和<code>全项目理解</code>的任务上表现很差。</li></ul></li><li><p>DA-Code</p><ul><li>数据科学上的代码生成。500个任务。存在挑战。</li></ul></li></ul></div><h3 id="代码编辑重构agent合作" tabindex="-1">代码编辑重构Agent合作 <a class="header-anchor" href="#代码编辑重构agent合作" aria-label="Permalink to &quot;代码编辑重构Agent合作&quot;">​</a></h3><div class="custom-block important"><div class="custom-block-title">**代码编辑、重构和智能体协作**</div><ul><li>对代码进行修改、重组以改进质量。</li><li>并由多个Agent协同工作以完成编程目标。</li><li>从简单代码补全插件 -&gt; 能干脏活的agent。</li><li>可靠性(不能影响别的代码)、长代码不懒惰、大局观(复杂计划、跨文件依赖处理)。</li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181345.jpg" style="display:block;margin:auto;" width="70%"><div class="custom-block warning"><div class="custom-block-title">代码编辑重构Bench</div><ul><li><p>Aider Edit Bench</p><ul><li><code>编辑python源文件</code>，完成133个Exercism的<code>编程练习</code>。</li></ul></li><li><p>Aider Refactor Bench</p><ul><li>重构大python类的<code>89个大方法</code>。</li><li>需要<code>长上下文能力</code>。</li></ul></li><li><p>Aider Polyglot Bench</p><ul><li>基于edit bench，扩展了多种语言，共225个case。</li></ul></li><li><p>RES-Q</p><ul><li>100个真实repo的评估bench，在重构、编辑和问答中都有测试用例。</li></ul></li><li><p>LiveRepoReflection</p><ul><li>提出一个<code>系统</code>来<code>收集仓库数据</code>，RepoReflect数据集，包含580个reflection实例。</li><li>集成执行反馈，解决当前llm的局限性(复杂/长上下文等)</li></ul></li><li><p>RepoExec</p><ul><li>仓库级代码生成测试，构建<code>可执行环境</code>来分析<code>上下文</code>如何<code>影响代码的质量</code>。</li><li>指标：pass@k, 依赖调用率(DIR)。</li><li>结果：完整依赖能提高模型效果，指令微调模型比base模型更善于使用依赖。</li></ul></li><li><p>CodePlan</p><ul><li><p>解决复杂任务，<code>Planning</code> + <code>Multi-step Edits</code>。</p><ul><li>先生成<code>多步计划</code>，再<code>一步步编辑</code>，确保每一步的依赖关系都正确。</li></ul></li><li><p>仓库级编码的框架。自动化了需在<code>整个仓库</code>中进行<code>广泛编辑</code>的任务。</p></li></ul></li></ul></div><h3 id="commit-message-生成" tabindex="-1">Commit Message 生成 <a class="header-anchor" href="#commit-message-生成" aria-label="Permalink to &quot;Commit Message 生成&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">提交信息生成</div><p><strong>任务</strong></p><ul><li>根据<code>代码差异</code>，自动生成<code>简单</code>、<code>信息丰富</code>的文本描述。</li></ul><p><strong>早早期方法：统计和规则</strong></p><ul><li>猜词，用朴素贝叶斯猜提交是fix还是add。</li></ul><p><strong>早期方法：检索&amp;NMT</strong></p><ul><li>NMT：把diff使用翻译模型翻译成英文</li><li>检索：去数据库里检索一个最像的diff，抄这个message。</li></ul><p><strong>Bert/大模型</strong></p><ul><li>CommitBert：</li><li>CoRec：</li><li>ExGroFi：整合链接的issue描述，增强消息的rationale</li><li>CommitChronicle：利用提交历史，提高对时间和风格的连贯性。</li></ul><p><strong>评估的挑战</strong></p><ul><li>BELU有局限：传统指标看词重叠，一般commit消息比较短暂，稍微换个词，意思一样，但bleu分数会很低。</li><li>B-Norm：通过平滑和忽略大小写，尽量让打分接近人类直观感受。</li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181359.jpg" style="display:block;margin:auto;" width="70%"><h3 id="swe-任务" tabindex="-1">SWE 任务 <a class="header-anchor" href="#swe-任务" aria-label="Permalink to &quot;SWE 任务&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">软件工程任务</div><ul><li>来源真实的github开源项目，给定真实的issue，</li><li>徐自己去读代码、复现bug、定位文件、修改代码并通过测试。</li></ul></div><div class="custom-block danger"><div class="custom-block-title">SWE-Bench</div><p><strong>SWE-Bench 家族</strong></p><ul><li><code>SWE-bench</code>：2.2k <code>issuse-PR对</code>，真实github python <code>问题</code> ，<code>fail-to-pass</code> <code>单元测试</code>。</li><li>Java bench：SWE-Bench扩展至Java。</li><li>SWE-bench-light：为降低计算成本，仅保留300个任务。</li><li>SWE-bench-Multilingual：9种编程语言、42个仓库、300个手动验证的任务。</li><li>SWE-bench-Multimodal：517个基于图像的问题，测试是否解释视觉线索(屏幕截图/GUI错误等)。</li><li><code>SWE-bench-verified</code>：<code>500个人工策划</code>的例子，基于docker评估，确保<code>一致性</code>。</li></ul><p><strong>SWE-Bench 扩展</strong></p><ul><li>SWE-bench-Live：持续纳入新的github问题，164仓库、1565任务，更动态、比静态数据难。</li><li>SWE-rebench：自动化大规模问题收集(&gt;21k任务)，实现时间纵向评估。</li><li><strong>SWE-dev</strong>：从bug修复转到<code>功能实现</code>，<code>1.4万个训练数据</code>和<code>500个评估数据</code>。</li><li><strong>BugPilot</strong>：让<code>LLM添加新功能</code>而非直接插入bug，来<code>合成更逼真的bug</code>。 <ul><li>使用<code>Qwen3</code>在swe-bench-verified达到<code>SOTA</code>。</li></ul></li><li>Gistfy：要求agent把仓库提炼为复制其运行时行为的<code>最小单文件代码</code>。</li><li>SWE-PolyBench：跨语言推理能力，21仓库，2110任务。</li><li>Multi-SWE-Bench：7语言，1632 issue，c/c++/rust性能最弱。</li><li>SWE-Bench+：包括截止日期后的仓库，避免数据泄露/</li><li>SWE-bench-M：<code>视觉调试</code>，17个JS项目，619个视觉issue。很有挑战。</li></ul><p><strong>超越SWE-Bench</strong></p><ul><li>SWE-Lancer：经济价值，1.4k真实Upwork任务(100万美元)，</li><li>FAUN-Eval：300个github细粒度问题。</li><li>FEA-Bench：跨83个仓库的仓库级功能实现，最好模型仅10%</li><li>SwingArea/CoreCodeBench：分别把评估扩展至持续集成和复合任务。</li><li>AgentIssue-Bench：agent的自我维护能力，201个问题、50个任务，长期记忆挑战。</li></ul></div><h3 id="综合软件开发" tabindex="-1">综合软件开发 <a class="header-anchor" href="#综合软件开发" aria-label="Permalink to &quot;综合软件开发&quot;">​</a></h3><div class="custom-block caution"><div class="custom-block-title">综合软件开发</div><ul><li>代码只是一小部分。</li><li>还需要：写文档、看图修bug、用git、跟进项目等。</li></ul></div><div class="custom-block info"><div class="custom-block-title">关键Bench</div><ul><li>README Eval：文档生成，根据项目的issue和commits等数据生成readme</li><li>OminiGIRL：评估github issue能力，引入多模态挑战，错误包括图像。</li><li>GitGoodBench：版本控制。</li><li>EvoCodeBench：动态范式，不断演进真实的仓库。</li><li>Stack-Repo：策划<code>大规模</code>、<code>去重的源代码</code>数据，对提高真实任务性能很重要。</li></ul></div><h3 id="repo-level和长上下文理解" tabindex="-1">Repo-Level和长上下文理解 <a class="header-anchor" href="#repo-level和长上下文理解" aria-label="Permalink to &quot;Repo-Level和长上下文理解&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">仓库级和长上下文理解</div><p><strong>思想</strong></p><ul><li>在整个代码库或大量文档做理解推理，需要跨越多个文件、可能需保持高达数百万token的上下文。</li></ul><p><strong>基准测试：海量代码大海捞针</strong></p><ul><li>RepoQA：仓库级问答，需对<code>多个文件</code>进行<code>检索和推理</code>。</li><li>CodeRepoQA：仓库级问答</li><li>CoreQA：真实github仓库问题和评论构建的数据集</li><li>LongCodeU：更全面和挑战的任务，四个方面来评估： <ul><li>代码单元感知、代码单元内部理解、代码单元关系理解、长文档理解</li></ul></li></ul><p><strong>长上下文压缩</strong></p><ul><li>LongCodeZip：减少上下文长度，高效压缩长代码上下文 <ul><li><a href="https://plmsmile.github.io/posts/llm/basic/01-lm-define-information-theory.html#%E5%9B%B0%E6%83%91%E5%BA%A6" target="_blank" rel="noreferrer">困惑度</a>：如果一段代码对当前问题重要，去掉其以后，模型对该问题的困惑度会飙升。</li><li>粗粒度压缩：以函数为单位，计算每个函数对当问题的贡献度(互信息)，只保留贡献大的函数。</li><li>细粒度压缩：基于困惑度的边界检测，把保留的函数分割成快，使用背包算法进行token预算优化</li></ul></li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251214181418.jpg" style="display:block;margin:auto;" width="70%"><h2 id="agentic-system" tabindex="-1">Agentic System <a class="header-anchor" href="#agentic-system" aria-label="Permalink to &quot;Agentic System&quot;">​</a></h2><h3 id="agentic-tool-use" tabindex="-1">Agentic Tool Use <a class="header-anchor" href="#agentic-tool-use" aria-label="Permalink to &quot;Agentic Tool Use&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">Agentic Tool Use</div><p><strong>核心</strong></p><ul><li>考察选择和使用工具的能力。</li></ul><p><strong>入门级</strong></p><ul><li>API-Bank：天气如何，意图识别。</li><li>ToolBench</li></ul><p><strong>多轮交互</strong></p><ul><li>BFCL：多步多轮交互。</li><li>还有AppWorld。</li></ul><p><strong>领域任务</strong></p><ul><li>TauBench：复杂的公司内部流程(客服)</li></ul></div><h3 id="deep-research-bench" tabindex="-1">Deep Research Bench <a class="header-anchor" href="#deep-research-bench" aria-label="Permalink to &quot;Deep Research Bench&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">DeepResearch</div><p><strong>核心</strong></p><ul><li>信息检索、深度推理。</li></ul><p><strong>Bench</strong></p><ul><li>GAIA：真实问题，结合推理、网页浏览、工具使用。</li><li>xbench：人才搜寻和市场营销领域。</li><li>DeepResearch Bench：学术标准，需达phd-level，自主研究和生成报告。</li></ul></div><h3 id="websearch-bench" tabindex="-1">WebSearch Bench <a class="header-anchor" href="#websearch-bench" aria-label="Permalink to &quot;WebSearch Bench&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">WebSearch</div><p><strong>核心</strong></p><ul><li>开放的、不受约束的网络环境。需要持久、适应和过滤噪声和矛盾信息的能力。</li></ul><p><strong>Bench</strong></p><ul><li>BrowseComp：把信息检索构建为组合推理任务，要求聚合多个网站上的碎片化信息。</li><li>BrowseComp-ZH：多语言版本。</li><li>BrowseComp-Plus：推理和检索解耦，分为<code>找信息</code>和<code>根据信息推理</code>2步。</li><li>WebWalker-QA：强调<code>网页之间的跳转</code>，像人逛维基百科一样。</li><li>Widesearch：大规模、开放域的信息综合synthesis。</li></ul></div><h3 id="gui-bench" tabindex="-1">GUI Bench <a class="header-anchor" href="#gui-bench" aria-label="Permalink to &quot;GUI Bench&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">GUI Bench</div><p><strong>核心</strong></p><ul><li>要求理解图像用户界面。</li><li>把NL指令变成Grounding的UI动作，多步计划、泛化性仍然存在挑战。</li></ul><p><strong>前端导航Bench</strong></p><ul><li>WebShop/Mind2Web：数千个目标导向的任务，要求在真实网站中操作。</li><li>OminiACT：扩展，引入跨网页、桌面和移动设置的bench。</li><li>WebChoreArena：长期记忆和计算的复合家务任务。</li><li>PersonalWAB：用户历史和偏好引入个性化。</li><li>Sphinx/NovelScreenSpot：GUI分解子技能，如目标理解、UI落地和规划。</li></ul><p><strong>前端开发Bench</strong></p><ul><li>Design2Code/WebCode2M：设计和代码实现的pair数据集。看图写代码。</li><li>Sketch2Code：看草图写代码</li><li>Interaction2Code：生成动态交互式的网站。</li><li>WebGen-Bench：从0开始生成多文件网站。</li><li>Web-Bench：模拟了具有顺序依赖任务的真实软件开发，不仅是一次生成，还有后续修改和迭代。</li></ul></div><h3 id="terminal-use" tabindex="-1">Terminal Use <a class="header-anchor" href="#terminal-use" aria-label="Permalink to &quot;Terminal Use&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">Terminal Use</div><p><strong>核心</strong></p><ul><li>在终端环境中做真实任务的自主操作能力。</li><li>超越了自定义环境，还需要在真实系统里做操作。 <ul><li>SWE-Bench：定义好环境，边界清晰。</li><li>Terminal-Bench：无边界，整个系统都是环境。</li></ul></li></ul><p><strong>Bench</strong></p><ul><li>Terminal-Bench： <ul><li>需要系统级开发能力，探索系统、执行shell和各种命令行工具，完成复杂系统任务。 <ul><li>比如从源码编译和启动一个完整的linux内核。</li></ul></li><li>整个环境都是待解的空间。</li></ul></li></ul></div>',75)]))}const h=o(t,[["render",s]]);export{p as __pageData,h as default};

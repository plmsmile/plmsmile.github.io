import{_ as e,c as i,o,ah as a}from"./chunks/framework.CvbyeFFO.js";const p=JSON.parse('{"title":"Code RL 任务","description":"","frontmatter":{"title":"Code RL 任务","date":"2025-12-17T17:10:17.000Z","create":"2025-12-17T17:10:17.000Z","categories":[],"tags":[]},"headers":[],"relativePath":"posts/llm/industry/codellm/03-rl-task.md","filePath":"posts/llm/industry/codellm/03-rl-task.md","lastUpdated":null}'),r={name:"posts/llm/industry/codellm/03-rl-task.md"};function t(s,l,c,d,n,u){return o(),i("div",null,l[0]||(l[0]=[a('<h2 id="参考文章" tabindex="-1">参考文章 <a class="header-anchor" href="#参考文章" aria-label="Permalink to &quot;参考文章&quot;">​</a></h2><div class="custom-block caution"><div class="custom-block-title">参考文章</div><ul><li><a href="https://www.alphaxiv.org/abs/2511.18538" target="_blank" rel="noreferrer">survey paper</a></li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251216162243.jpg" style="display:block;margin:auto;" width="70%"><h2 id="代码生成-生成-补全-翻译" tabindex="-1">代码生成(生成+补全+翻译) <a class="header-anchor" href="#代码生成-生成-补全-翻译" aria-label="Permalink to &quot;代码生成(生成+补全+翻译)&quot;">​</a></h2><h3 id="代码合成" tabindex="-1">代码合成 <a class="header-anchor" href="#代码合成" aria-label="Permalink to &quot;代码合成&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">代码合成 NL2Code</div><p><strong>PPO 时代</strong></p><ul><li>(2022) CodeRL：PPO算法，训练Critic预估每步价值，在APPS和MBPP效果不错。</li></ul><p><strong>DPO 时代</strong></p><ul><li>(2410) CodeDPO：引入PageRank算法生成高质量偏好数据对</li><li>(2502) Focused-DPO：针对容易出错的地方进行优化</li></ul><p><strong>RLVR 时代</strong></p><ul><li>(2502) <a href="https://github.com/ganler/code-r1" target="_blank" rel="noreferrer">CodeR1</a>：证明可靠奖励信号很重要</li><li>(2507) <a href="https://www.alphaxiv.org/abs/2507.09075" target="_blank" rel="noreferrer">OpenR1</a>/<a href="https://huggingface.co/datasets/nvidia/OpenCodeReasoning-2" target="_blank" rel="noreferrer">nvidia/OpenCodeReasoning-2</a>：大规模数据集，<code>多个沙盒</code>实现<code>不同奖励函数</code></li><li>(25) <a href="https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51" target="_blank" rel="noreferrer">DeepCoder</a>: <code>14B+RL训练</code>，在<code>LiveCodeBench</code>上达到<code>O3-mini</code>效果。</li></ul></div><h3 id="代码补全" tabindex="-1">代码补全 <a class="header-anchor" href="#代码补全" aria-label="Permalink to &quot;代码补全&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">代码补全</div><p><strong>检索/RL</strong></p><ul><li>(2407) <a href="https://github.com/DeepSoftwareAnalytics/RLCoder" target="_blank" rel="noreferrer">RLCoder</a>：<code>仓库级代码补全</code>，学习<code>检索有用信息</code>，在CrossCodeEval和RepoEval上提升12.2%</li><li>(2409) <a href="https://arxiv.org/abs/2409.13122" target="_blank" rel="noreferrer">Repogenreflex</a>：仓库级代码补全，RL + RAG，动态<code>优化检索</code>。</li><li>(2401) <a href="https://arxiv.org/abs/2401.16637" target="_blank" rel="noreferrer">IRCoco</a>：提供即时奖励。</li><li>(2503) <a href="https://arxiv.org/abs/2503.15301" target="_blank" rel="noreferrer">aiXcoder-7B-v2</a>：RL监督信号去解决LLM忽略长距离上下文的问题。</li></ul><p><strong>DPO</strong></p><ul><li>(2508) <a href="https://arxiv.org/abs/2508.15495" target="_blank" rel="noreferrer">SynthCoder</a>：课程学习 + DPO，在FIM上达SOTA。</li></ul><p><strong>工业应用</strong></p><ul><li>(2509) <a href="https://cursor.com/cn/blog/tab-rl" target="_blank" rel="noreferrer">Cusor-Tab-Online-RL</a>：每天4亿请求，通过用户反馈持续优化模型。</li><li>(2412) <a href="https://www.augmentcode.com/blog/reinforcement-learning-from-developer-behaviors" target="_blank" rel="noreferrer">AugmentCode RLDB</a>：学习开发者-IDE的交互，相当于模型参数翻倍的提升。</li></ul></div><h3 id="代码翻译" tabindex="-1">代码翻译 <a class="header-anchor" href="#代码翻译" aria-label="Permalink to &quot;代码翻译&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">代码翻译</div><ul><li>CoTran</li><li>TransAgent</li></ul></div><h2 id="代码理解-摘要-注释-搜索" tabindex="-1">代码理解(摘要+注释+搜索) <a class="header-anchor" href="#代码理解-摘要-注释-搜索" aria-label="Permalink to &quot;代码理解(摘要+注释+搜索)&quot;">​</a></h2><div class="custom-block tip"><div class="custom-block-title">代码理解</div><p><strong>摘要</strong></p><ul><li>难点：暴露偏差、结构信息丢失。</li><li>方法： <ul><li>Actor-Critic + AST</li><li>ICSER：先理解找关键部分，再用RL去生成摘要。</li></ul></li></ul><p><strong>注释生成</strong></p><ul><li>核心：解释具体实现逻辑、参数含义、API用法等。</li><li>利用类型信息：TAG，分层RL</li><li>关注块结构/块注释：RL-BlockCom</li><li>CoRAL：面向下游任务优化：认为好的注释能帮助改进代码。 <ul><li>奖励函数：语义相似度 + 是否能作为输入帮助模型完成代码优化</li></ul></li></ul><p><strong>搜索</strong></p><ul><li>任务：输入NL，找到对应的代码片段</li><li>CoaCor：希望RL<code>生成提高检索区分度的注释</code>。</li><li>QueCos：<code>改写扩充query</code><ul><li>奖励信号：<code>搜索结果的准确性</code>。</li></ul></li></ul></div><h2 id="swe-issue修复-重构-评审-优化" tabindex="-1">SWE (Issue修复+重构+评审+优化) <a class="header-anchor" href="#swe-issue修复-重构-评审-优化" aria-label="Permalink to &quot;SWE (Issue修复+重构+评审+优化)&quot;">​</a></h2><h3 id="issue修复" tabindex="-1">Issue修复 <a class="header-anchor" href="#issue修复" aria-label="Permalink to &quot;Issue修复&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">Issue修复</div><p><strong>解决github Issue核心挑战</strong></p><ul><li>环境复杂：需要在成百上千的文件中找到出错的那一行。</li><li>步骤繁琐：需要理解issue描述、复现bug、修改代码、跑测试、确定修好了再提交。</li><li>反馈稀疏：只有测试通过，才算成功。中间没有明确指导</li></ul><p><strong>RL 工作</strong></p><ul><li>(2502) <a href="https://www.alphaxiv.org/abs/2502.18449" target="_blank" rel="noreferrer">SWE-RL</a>：开源仓库+RL+提升推理能力，能够有效解决Github Issue。</li><li>(2507) <a href="https://www.together.ai/blog/deepswe" target="_blank" rel="noreferrer">DeepSWE</a>：扩展RL规模，完全开源，SWE-Bench SOTA。</li><li>(2508) <a href="https://www.notion.so/SWE-Swiss-A-Multi-Task-Fine-Tuning-and-RL-Recipe-for-High-Performance-Issue-Resolution-21e174dedd4880ea829ed4c861c44f88" target="_blank" rel="noreferrer">SWE-Swiss</a>：<code>2阶段训练策略</code><ul><li>多任务SFT：增加定位、修复和单元测试的能力。</li><li>RL：优化repair技能</li></ul></li><li>(2505) <a href="https://www.alphaxiv.org/abs/2505.23604" target="_blank" rel="noreferrer">Satori-SWE</a>：EvoScale，进化式TestTimeScaling， <ul><li>自我进化：生成一个补丁没过，继续分析为什么没过，生成下一个补丁，继续迭代</li><li>RL作用：训练模型自我纠错能力，教会模型如何反馈不断迭代，直到正确。</li><li>效果：较少样本、多轮迭代，32B达到100B效果</li></ul></li></ul></div><h3 id="代码重构" tabindex="-1">代码重构 <a class="header-anchor" href="#代码重构" aria-label="Permalink to &quot;代码重构&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">代码重构</div><ul><li>(2412) Refactor-RL <ul><li>识别代码块 -&gt; 提取成新函数 -&gt; 起个好名字</li><li>PPO 算法，奖励信号：编译成功、是否重构。</li><li>效果：RL比SFT强，BLEU/CodeBLEU指标、单元测试通过率提升。</li></ul></li></ul></div><h3 id="code-review" tabindex="-1">Code Review <a class="header-anchor" href="#code-review" aria-label="Permalink to &quot;Code Review&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">Code Review</div><ul><li>Attn+PG+RL</li><li>CodeMentor</li></ul></div><h3 id="代码优化" tabindex="-1">代码优化 <a class="header-anchor" href="#代码优化" aria-label="Permalink to &quot;代码优化&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">代码优化</div><ul><li>Pearl</li></ul></div><h2 id="代码安全" tabindex="-1">代码安全 <a class="header-anchor" href="#代码安全" aria-label="Permalink to &quot;代码安全&quot;">​</a></h2><h3 id="bug-检测" tabindex="-1">Bug 检测 <a class="header-anchor" href="#bug-检测" aria-label="Permalink to &quot;Bug 检测&quot;">​</a></h3><div class="custom-block info"><div class="custom-block-title">Bug检测</div><p><strong>挑战</strong></p><ul><li>调试信息有限、bug搜索空间大</li></ul><p><strong>工作</strong></p><ul><li>(20) RecBI：RL应用于编译器隔离，智能引导诊断测试程序生成</li><li>(24) RLocator： <ul><li>用户报bug，需要定位找到文件；传统方法：基于相似度找文件。</li><li>看作排序问题，把MAP指标当做奖励函数，引导把正确答案排在前面。</li></ul></li></ul></div><h3 id="bug-修复" tabindex="-1">Bug 修复 <a class="header-anchor" href="#bug-修复" aria-label="Permalink to &quot;Bug 修复&quot;">​</a></h3><div class="custom-block info"><div class="custom-block-title">Bug修复</div><ul><li>(24) Repair：过程监督，引入Critic为每一步操作打分</li><li>(2507) <a href="https://arxiv.org/abs/2507.22853" target="_blank" rel="noreferrer">Repair-R1</a>：左右互搏，边修bug 边写测试用例，协同训练。</li><li>(2506) <a href="https://arxiv.org/abs/2506.03921" target="_blank" rel="noreferrer">Repairity</a>：GPT4蒸馏 + SFT + RL</li></ul></div><h2 id="代码测试" tabindex="-1">代码测试 <a class="header-anchor" href="#代码测试" aria-label="Permalink to &quot;代码测试&quot;">​</a></h2><h3 id="测试生成" tabindex="-1">测试生成 <a class="header-anchor" href="#测试生成" aria-label="Permalink to &quot;测试生成&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">测试生成</div><p><strong>背景</strong></p><ul><li>AI写出高质量测试，能够<code>自动评估代码</code>、反过来<code>帮助训练模型</code>。</li></ul><p><strong>关键工作</strong></p><ul><li>(2506) <a href="https://arxiv.org/abs/2506.03136" target="_blank" rel="noreferrer">Co-Evolving</a>：<code>协同优化</code>，<code>代码生成</code>和<code>单元测试生成</code>的能力。</li><li>(2508) <a href="https://arxiv.org/abs/2508.21107" target="_blank" rel="noreferrer">UTRL</a>：<code>对抗训练</code>，测试用例生成(找bug) + 代码生成(写出健壮代码)。</li></ul></div><h3 id="模糊测试" tabindex="-1">模糊测试 <a class="header-anchor" href="#模糊测试" aria-label="Permalink to &quot;模糊测试&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">模糊测试</div><p><strong>背景</strong></p><ul><li>自动化软件测试，向程序输入随机数据来找出bug。</li></ul><p><strong>关键工作</strong></p><ul><li>(2019) REINAM：无种子，利用RL去猜输入。</li><li>(2022) RLF：区块链安全。</li><li>(2024) CovRL-Fuzz：JS，LLM负责生成复杂代码片段，RL负责探索路径，找bug。</li></ul></div><h2 id="算法相关" tabindex="-1">算法相关 <a class="header-anchor" href="#算法相关" aria-label="Permalink to &quot;算法相关&quot;">​</a></h2><h3 id="rl-算法" tabindex="-1">RL 算法 <a class="header-anchor" href="#rl-算法" aria-label="Permalink to &quot;RL 算法&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">大致发展</div><p><strong>整体流程</strong></p><ul><li>流程：编译 -&gt; 运行 -&gt; 测试，<code>生成 -&gt; 运行 -&gt; 调试</code></li><li>奖励：单元测试通过率，语法错误，验证反馈等。</li></ul><p><strong>主要算法</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/rl/theory/09-policy-trpo-ppo.html#ppo" target="_blank" rel="noreferrer">PPO</a>：KL约束，限制策略更新幅度，实现在线探索。结合循环，提高pass@k指标。</li><li><a href="https://plmsmile.github.io/posts/llm/rl/theory/09-policy-trpo-ppo.html#dpo" target="_blank" rel="noreferrer">DPO</a>：无法在线采样。</li><li>RAIF：利用强模型提供dense/结构化的奖励信号。</li><li>O1/R1：执行反馈、验证器、自我迭代修正。</li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251216162229.jpg" style="display:block;margin:auto;" width="70%"><div class="custom-block danger"><div class="custom-block-title">PPO及其变体</div><ul><li><a href="https://plmsmile.github.io/posts/llm/rl/theory/09-policy-trpo-ppo.html#ppo" target="_blank" rel="noreferrer">PPO</a>：CLIP限制幅度/GAE平衡偏差方差。 <ul><li>但PPO在稀疏奖励场景效果一般，因为Critic不好训，价值崩塌，难以区分状态好坏，输出几乎相同。</li></ul></li><li><a href="https://plmsmile.github.io/posts/llm/rl/theory/09-policy-trpo-ppo.html#grpo" target="_blank" rel="noreferrer">GRPO</a>：不学Critic，使用组相对优势。 <ul><li>降低显存，稳定更新。</li><li>优化方向：group baseline、rollout效率、group多样性。</li></ul></li><li><a href="https://plmsmile.github.io/posts/llm/rl/theory/11-grpo-series.html#_2503-dr-grpo" target="_blank" rel="noreferrer">(2503)DR.GRPO</a>：解决长度和问题难度偏差，样本间平均以及不除以标准差。</li><li><a href="https://plmsmile.github.io/posts/llm/rl/theory/11-grpo-series.html#_2504-dapo-seed" target="_blank" rel="noreferrer">(2504) DAPO</a>：CLip-Higher, 动态采样，Token-level Loss，超长Reward奖励</li><li><a href="https://plmsmile.github.io/posts/llm/rl/theory/10-ppo-series.html#_2504-vapo-seed" target="_blank" rel="noreferrer">(2504) VAPO</a>：解决价值偏差、长度不一致、奖励稀疏问题。 <ul><li>价值预训练、解耦GAE、长度自适应GAE、Token-Level Loss、CLip-HIgher、正样本LMLoss、分组采样。</li></ul></li><li><a href="https://plmsmile.github.io/posts/llm/rl/theory/10-ppo-series.html#_2501-reinforce" target="_blank" rel="noreferrer">(2501) REINFORCE++</a>：全局归一化、PPO训练目标无需Critic</li></ul></div><div class="custom-block danger"><div class="custom-block-title">DPO 及其变体</div><ul><li><a href="https://plmsmile.github.io/posts/llm/rl/theory/09-policy-trpo-ppo.html#dpo" target="_blank" rel="noreferrer">DPO</a>：偏好建模，直接优化策略，纯依赖离线偏好数据。 <ul><li>缺点：无在线采样数据、优化不稳定、依赖SFT模型和数据质量。</li><li>优点：简单、快速、成本低，无需reward或value model</li></ul></li><li>(2503) DPO-VP：集成数学和code，把正确输出的作为正样本。</li><li>Iterative DPO：多轮迭代、Offline-RL，仍使用DPO Loss <ul><li>多轮迭代：模型生成数据、验证打分，挑选出好的，构造Pair数据</li></ul></li><li>(2410) CodeDPO：Code+DPO，代码和测试用例同时创建评估，PageRank更新分数</li></ul></div><div class="custom-block important"><div class="custom-block-title">RLAIF</div><ul><li>直接使用高级LLM做打分，来给反馈。 <ul><li>自己训的RM可能会失真，reward hacking。</li></ul></li><li>LLM-based Critic：自我修正、CoT Review、宪法原则，提供结构化反馈。 <ul><li>Skywork-OR1，CRITIC、宪法AI等。</li></ul></li><li>挑战：reward hacking、bias 传播、计算成本</li></ul></div><h3 id="reward-设计" tabindex="-1">Reward 设计 <a class="header-anchor" href="#reward-设计" aria-label="Permalink to &quot;Reward 设计&quot;">​</a></h3><h4 id="面向正确性" tabindex="-1">面向正确性 <a class="header-anchor" href="#面向正确性" aria-label="Permalink to &quot;面向正确性&quot;">​</a></h4><div class="custom-block warning"><div class="custom-block-title">Reward Shaping</div><p><strong>背景</strong></p><ul><li>期望模型：功能正确、安全、可维护、符合软件工程标准。</li><li>四个维度：正确性、任务对齐、安全性、结构质量。</li></ul><p><strong>三个层次</strong></p><ul><li>人类偏好：RLHF。 <ul><li>解决：主观质量、可读性、风格等问题。</li></ul></li><li>ORM(Outcome-superviesd RM) <ul><li>解决：正确性问题。</li><li>局限性：<code>奖励稀疏问题</code>，<code>只看结果</code>，有时候代码逻辑正确、仅最后一步错误，可能<code>会判0分</code>。</li></ul></li><li>PRM(Process RM) <ul><li>解决：ORM局限性问题，在每一步都做一个评估。</li></ul></li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251216162628.jpg" style="display:block;margin:auto;" width="70%"><div class="custom-block warning"><div class="custom-block-title">面向正确性的奖励</div><p><strong>面向任务正确的奖励</strong></p><ul><li><strong>静态分析奖励</strong>：检查<code>语法和规范</code><ul><li>基于Pylint,Flake8等编译器，检查语法、拼写、类型等错误。</li><li>优点：快、安全、即时。</li></ul></li><li><strong>测试用例奖励</strong>：检查<code>逻辑和功能</code><ul><li>RLVR最核心部分。</li><li>测试用例：可以由AI或专门工具生成，自我验证。</li></ul></li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251216162640.jpg" style="display:block;margin:auto;" width="70%"><h4 id="面向代码质量" tabindex="-1">面向代码质量 <a class="header-anchor" href="#面向代码质量" aria-label="Permalink to &quot;面向代码质量&quot;">​</a></h4><p>除正确性以外，还需满足软件工程标准。</p><div class="custom-block warning"><div class="custom-block-title">高质量代码</div><ul><li>任务对齐(Task Alignment)：指令跟随、听懂人话。</li><li>安全性 <ul><li>避免漏洞：避免已知安全缺陷(SQL注入等)，可给奖励。</li><li>执行访问控制：权限检查、认证机制时，可给奖励。</li></ul></li><li>结构好：代码整洁 <ul><li>模块化可重用性：职责清晰、函数模块定义较好、可读性好，可给奖励。</li><li>接口兼容：遵循现有API、类结构、可无缝集成等， 可给奖励。</li></ul></li></ul></div><div class="custom-block warning"><div class="custom-block-title">两种方法</div><p><strong>硬指标奖励</strong></p><ul><li>直接给奖励。</li></ul><p><strong>软指标奖励：RLHF</strong></p><ul><li>正确性奖励难以捕捉<code>可读性</code>、<code>清晰度</code>、<code>风格</code>等<code>审美维度</code>。</li><li>让人类进行标注：A 和 B <code>偏好比较</code></li><li>DPO、PPO。</li></ul><p><strong>过程奖励：PRM</strong></p><ul><li>ORM存在<code>奖励稀疏问题</code>。 <ul><li>写了几百行代码，第3行错误，全部都给0分。但模型不知道是第3行错误。</li><li>PRM 能减少探索空间，支持渐进式学习。</li></ul></li><li>设计原则： <ul><li>局部有效性：如当前token的<code>语法正确</code>性。</li><li>全局一致性：是否符合全局问题逻辑。</li></ul></li><li>实现机制 <ul><li>Token-Level PRM：预测每个token的奖励。</li><li>State-Tracking PRM：维护内部状态，评估部分代码是否满足结构约束。</li></ul></li></ul></div><h2 id="数据相关" tabindex="-1">数据相关 <a class="header-anchor" href="#数据相关" aria-label="Permalink to &quot;数据相关&quot;">​</a></h2><h3 id="可用数据集" tabindex="-1">可用数据集 <a class="header-anchor" href="#可用数据集" aria-label="Permalink to &quot;可用数据集&quot;">​</a></h3><div class="custom-block danger"><div class="custom-block-title">数据集</div><ul><li>CodeContests：1.33w，程序竞赛题，早期基石。</li><li>CodeContets+：Seed扩充了高质量测试用例。</li><li>TACO：算法代码数据，2.5w训练 + 1k测试，总计155w个答案。</li><li>Eurus-2-RL：2.7w编程+45w数学题。</li><li>IOI：国际奥林匹克竞赛，229问题，高级挑战。</li><li>ACECode-87k：合成代码数据集，技术：自动测试用例生成。 <ul><li>GPT-4o-mini为每个问题生成了16个测试用例；并提出无效的。</li></ul></li><li>Synthetic-1：140w任务，数学、编程、软件工程等多个领域。14.4w代码合成和理解问题。</li><li>Synthetic2：400w，更大。</li><li>KodCode：48.4k，<code>问题+解决方案+测试</code> 三元组。 <ul><li>领域广泛：基础算法、经典数据结构、复杂特定领域等。</li></ul></li><li>HardTests：4.71w问题，合成测试用例增强。</li><li>Code-R1-12k：精选RL数据集，2k leetcode + 1w TACO筛选。</li><li>LeetCodeDataset：python数据集，90%leetcode问题。每个问题有难度、标签，有100测试集。</li><li>Klear-CodeTest：快手，2.7w编程题。通过生成-检查器构建的。</li></ul></div><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/codellm/code-survey/20251216162539.jpg" style="display:block;margin:auto;" width="70%">',52)]))}const g=e(r,[["render",t]]);export{p as __pageData,g as default};

import{_ as o,c as l,o as d,ah as c}from"./chunks/framework.CvbyeFFO.js";const u=JSON.parse('{"title":"SWE 总结索引","description":"","frontmatter":{"title":"SWE 总结索引","date":"2026-01-05T14:19:49.000Z","create":"2026-01-05T14:19:49.000Z","categories":["swe"],"tags":["Self-play SWE-RL","写Bug修Bug自我博弈JointRL","SKyRL-Agent","AST工具增强","增加环境提示信息","留一法估计优势","InfoCode","对抗生成代码和测试","Kimi-Dev","Agentless训练","SWE-Agent适配","MidTrain","CodeEditRL","SWE-Swiss","3任务SFT","2阶段课程RL","NEBIUS-SWE","Mask错误动作SFT","DeepSWE","GRPO++","Devstral2","Devstral","SWE-RL","Patch相似度奖励信号","SWE-Agent","ACI","Agent-Computer-Interface","SWE-Lego","Mask错误动作","SFT课程学习","BugPilot","FeatAddBug","SWE-Mirror","Issue迁移","生成测试用例","生成Bug源码，Issue描述生成","AgentSFT 数据蒸馏","SWE-Mirror-LM-32B","Skywork-SWE","SWE-rebench","自动Issue-PR 收集","SWE-smith","SWE-Agent-LM-32B","Agent安装环境","4策略合成Bug","PR Mirror","执行验证","逆向合成Issue","R2E-Gym","Hybrid TTS","挖掘Commit数据","SWE-Gym","tts","scaffold"]},"headers":[],"relativePath":"posts/llm/industry/codellm/10-swe-summary.md","filePath":"posts/llm/industry/codellm/10-swe-summary.md","lastUpdated":null}'),r={name:"posts/llm/industry/codellm/10-swe-summary.md"};function t(i,e,s,a,n,g){return d(),l("div",null,e[0]||(e[0]=[c('<h2 id="swe-训练工作" tabindex="-1">SWE 训练工作 <a class="header-anchor" href="#swe-训练工作" aria-label="Permalink to &quot;SWE 训练工作&quot;">​</a></h2><h3 id="_2512-self-play-swe-rl-51-4分-meta" tabindex="-1">(2512) Self-Play SWE-RL (51.4分, Meta) <a class="header-anchor" href="#_2512-self-play-swe-rl-51-4分-meta" aria-label="Permalink to &quot;(2512) Self-Play SWE-RL (51.4分, Meta)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">Self-Play SWE-RL 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2512.18552" target="_blank" rel="noreferrer">paper</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2512-self-play-swe-rl-51-4%E5%88%86-meta" target="_blank" rel="noreferrer">论文笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>Self-Play SWE-RL框架</code><ul><li>给定<code>仓库+环境</code>，通过<code>写Bug</code>+<code>修Bug</code> <code>自我博弈联合RL训练</code>。<code>无需人工Issue</code></li></ul></li><li><code>仓库数据</code>：<code>未知</code></li><li><code>CWM scaffold</code>：<code>bash</code> + <code>search-replace 编辑器</code></li></ul><p><strong>模型效果(CWM-32B-sft)</strong></p><ul><li>在SWE-V和SWE-Pro上，<code>SSR方法</code>都超过<code>RL+人类Issue</code>训练的模型，但<code>也没高多少</code>。</li><li><code>SWE-V</code>达<code>51.4分</code>，<code>SWE-P</code>达<code>28.9分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>Self-Play RL</code>比<code>Repair/Injection-Only RL</code> 性能<code>更好</code>，<code>Inject-Only</code> 效果最差。</li><li><code>大幅删除代码的Bug</code>更好比<code>仅改一行代码的Bug</code>的好。<code>后者太简单</code>，学习信号弱。</li><li>由于共享1个Policy，<code>Solver解决率信号</code> <code>对训练效果影响不大</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li><code>Self-Play SWE-RL 思想</code>，很有<code>启发意义的工作</code>。</li></ul></div><h3 id="_2511-skyrl-agent-39分" tabindex="-1">(2511) SkyRL-Agent(39分) <a class="header-anchor" href="#_2511-skyrl-agent-39分" aria-label="Permalink to &quot;(2511) SkyRL-Agent(39分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SkyRL-Agent 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2511.16108" target="_blank" rel="noreferrer">paper</a>, <a href="https://github.com/NovaSky-AI/skyrl/tree/main/skyrl-train/examples/mini_swe_agent" target="_blank" rel="noreferrer">SkyRL代码</a>, <a href="https://huggingface.co/NovaSky-AI/SA-SWE-32B" target="_blank" rel="noreferrer">SA-SWE-32B</a>, <a href="https://skyrl.readthedocs.io/en/latest/examples/mini_swe_agent.html" target="_blank" rel="noreferrer">doc</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-r2e-gym-34-4%E5%88%86" target="_blank" rel="noreferrer">R2E-Gym</a>, <a href="http://plmsmile.github.io/posts/llm/industry/mainllm/15-skywork-series.html#_2511-skyrl-agent" target="_blank" rel="noreferrer">论文笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SkyRL-Agent 框架</code>：<code>Tool-接口</code> + <code>异步Dispatcher</code> + <code>桥接后端</code></li><li><code>SWE-RL实验</code>：<code>AST工具增强 鼓励检索</code> + <code>增加环境提示信息</code> + <code>On-Policy</code> + <code>留一法优势估计</code></li><li><code>数据</code>：<code>4.5k R2E-Gym</code> ，<code>Scaffold</code>：<code>Simple ReAct Agent</code></li></ul><p><strong>模型效果(Qwen3-32B + RL)</strong></p><ul><li><code>纯RL</code>，SWE <code>pass@1 达 39分</code>，相比基模<code>提升15pt</code>。</li><li>超过DeepSWE <code>36分</code> (报告42分)，训练成本降一半。</li><li><code>弱于蒸馏模型</code> SWE-Agent-LM-32B <code>38分</code>。</li><li>泛化性：Terminal-Bench(+2.5%), BrowseComp-Plus(+1.3%), WebArena(+1.2 turns)</li></ul><p><strong>重要结论</strong></p><p><strong>关键贡献</strong></p><ul><li><code>SKyRL-Agent</code> 框架。<code>SkyRL-Agent-SWE 开源实现</code>。</li></ul></div><h3 id="_2511-infcode-没训练模型" tabindex="-1">(2511) InfCode(没训练模型) <a class="header-anchor" href="#_2511-infcode-没训练模型" aria-label="Permalink to &quot;(2511) InfCode(没训练模型)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">InfCode 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2511.16004" target="_blank" rel="noreferrer">paper</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2511-infcode-%E6%B2%A1%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B" target="_blank" rel="noreferrer">论文笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><p><code>框架</code>：<code>对抗式PatchGeneration</code> + <code>Patch Selection</code></p><ul><li><code>对抗</code>生成<code>代码</code>和<code>单元测试</code>：<code>TestGenerator</code> + <code>CodeGenerator</code>，</li></ul></li><li><p><code>没有训练模型</code>。</p></li></ul><p><strong>模型效果</strong></p><ul><li><code>Claude4.5</code> + <code>InfCode</code>：SWE-Verified <code>79.4分</code>。不知尝试了多少次。</li><li>轻微超过<code>TRAE</code>+<code>DoubaoSeedCode</code> <code>78.8分</code></li></ul><p><strong>重要结论</strong></p><ul><li>对抗生成贡献4pt，选择贡献8pt。</li></ul><p><strong>关键贡献</strong></p><ul><li>对抗<code>Bug修复</code>和<code>测试生成</code>的迭代修复框架。</li><li>虽然没有训练模型，但思路挺好的。</li><li>后来的<code>Self-Play SWE-RL</code> 就和其思路相同，但区别是<code>使用了RL训练</code>。</li></ul></div><h3 id="_2509-kimi-dev-48分" tabindex="-1">(2509) Kimi-Dev(48分) <a class="header-anchor" href="#_2509-kimi-dev-48分" aria-label="Permalink to &quot;(2509) Kimi-Dev(48分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">Kimi-Dev 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2509.23045" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/moonshotai/Kimi-Dev-72B" target="_blank" rel="noreferrer">Kimi-Dev-72B</a>, <a href="https://github.com/MoonshotAI/Kimi-Dev" target="_blank" rel="noreferrer">Kimi-Dev</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2509-kimi-dev-48%E5%88%86" target="_blank" rel="noreferrer">论文笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><p><code>Agentless 训练</code>(3阶段) + <code>SWE-Agent适配</code>(SFT)。</p></li><li><p>Agentless训练：<code>BugFixer</code> + <code>TestWriter</code></p><ul><li><code>MidTrain</code>：<code>Diff Patch</code> + <code>PR Commit</code> + <code>定位推理合成数据</code> +<code>agent交互合成数据</code></li><li><code>CoT SFT</code> ：DeepSeek-R1 蒸馏(SWE-Gym, SWE-bench-extra)</li><li><code>CodeEdit RL</code>：<code>执行结果奖励</code> + <code>难度课程学习</code> + <code>正样本强化</code></li></ul></li><li><p>SWE-Agent适配：<a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-swe-smith-swe-agent-lm" target="_blank" rel="noreferrer">5.7k SWE-smith 轨迹数据</a> 做SFT</p></li><li><p><code>训练数据</code>：<code>是不可能开源的</code>。</p></li></ul><p><strong>模型效果(Qwen2.5-72B-Base)</strong></p><ul><li><code>Agentless 训练</code> SWE-verified <code>Pass@1 48分</code>，<code>TTS(40) 达60分</code>。</li><li><code>SWE-Agent SFT适配</code><ul><li><code>Pass@1 48分</code>，优于<a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#%E5%85%B3%E9%94%AE%E7%BB%93%E6%9E%9C-swe-agent-lm-32b" target="_blank" rel="noreferrer">SWE-Agent-LM-32B </a><code>40.2分</code>；</li><li><code>Pass@10达74分</code>，优于Agentless <code>Pass@30 73.8分</code>，推理次数仅1/3。</li></ul></li></ul><p><strong>重要结论</strong></p><ul><li>Agentless训练可以带来<code>Skill Priors</code>，更好<code>适配SWE-Agent</code></li><li><code>RL的先验最强</code>：做SFT学的快好、做RL效果也更好。</li></ul><p><strong>关键贡献</strong></p><ul><li><code>多阶段CodeAgent训练方法论</code><ul><li><code>Agentless 训练</code>(MT+SFT+RL) + <code>SWE-Agent适配</code>(SFT)。</li><li>先从Agentless打基础，再逐步做Agent，模型不偏科、适应性强。</li></ul></li></ul></div><h3 id="_2508-swe-swiss-45分" tabindex="-1">(2508) SWE-Swiss(45分) <a class="header-anchor" href="#_2508-swe-swiss-45分" aria-label="Permalink to &quot;(2508) SWE-Swiss(45分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Swiss 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.notion.so/SWE-Swiss-A-Multi-Task-Fine-Tuning-and-RL-Recipe-for-High-Performance-Issue-Resolution-21e174dedd4880ea829ed4c861c44f88" target="_blank" rel="noreferrer">SWE-Swiss Blog</a>, <a href="https://github.com/zhenyuhe00/SWE-Swiss" target="_blank" rel="noreferrer">SWE-Siwss</a>, <a href="https://huggingface.co/SWE-Swiss/datasets" target="_blank" rel="noreferrer">datasets</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2508-swe-swiss-45%E5%88%86" target="_blank" rel="noreferrer">论文笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>3任务SFT数据构建</code>：<code>问题定位</code>+ <code>问题修复</code>+ <code>测试生成</code></li><li><code>2阶段训练方法</code>：<code>3任务SFT</code> + <code>2阶段RL 课程学习</code>，难样本：过滤<code>正确率&gt;90</code>的数据。</li><li><code>3任务-SFT</code> <code>10k轨迹</code> (<code>蒸馏DSR1</code>)，<code>Bug修复-RL</code> <code>12k</code>，来自<code>SWE-Gym</code>,<code>SWE-smith</code>等。</li><li><code>TTS方法</code>：EM + <code>GT代码相似度</code>。</li><li><code>Scaffold</code>：<code>Agentless</code>，<code>不是Agent</code></li></ul><p><strong>模型效果(Qwen2.5-32B-Instruct, SFT+RL)</strong></p><ul><li>SWE-Verified <code>SFT达36</code>，<code>RL达45</code>，<code>RL提升9pt</code>，增加TTS(best-120) 达60分。</li><li>在<code>通用任务</code>、<code>Math任务</code>、<code>代码生成任务</code>上，<code>均有提升</code>。</li></ul><p><strong>重要结论</strong></p><ul><li>虽然<code>训练3任务用SFT</code>，但也<code>可用RL做定位</code>，<code>也很有效果</code>，后续可以基于此。</li></ul><p><strong>关键贡献</strong></p><ul><li><code>开源数据代码</code></li></ul></div><h3 id="_2508-nebius-swe-agent-39分-筛选swe-rebench数据" tabindex="-1">(2508) NEBIUS SWE-Agent (39分, 筛选SWE-rebench数据) <a class="header-anchor" href="#_2508-nebius-swe-agent-39分-筛选swe-rebench数据" aria-label="Permalink to &quot;(2508) NEBIUS SWE-Agent (39分, 筛选SWE-rebench数据)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">NEBIUS-SWE论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2508.03501" target="_blank" rel="noreferrer">paper</a>, <a href="https://nebius.com/blog/posts/training-and-search-for-software-engineering-agents" target="_blank" rel="noreferrer">blog</a>, <a href="https://huggingface.co/nebius" target="_blank" rel="noreferrer">nebius datasets</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2508-nebius-swe-agent-39%E5%88%86-%E7%AD%9B%E9%80%89swe-rebench%E6%95%B0%E6%8D%AE" target="_blank" rel="noreferrer">笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><p><code>SWE-rebench数据筛选</code>：<code>过滤有误数据</code>+<code>控制复杂度</code>+<code>LLM质量评估</code>+<code>确定性测试</code></p></li><li><p><code>数据</code>：<code>7k任务</code> + <code>自蒸馏6.5k轨迹数据</code> + <code>Verified-50</code>做快速验证</p></li><li><p><code>RFT冷启动</code>： <code>Mask错误格式动作</code>，仅<code>学习有效动作</code>。</p></li><li><p><code>2阶段RL课程学习</code></p><ul><li><code>65k</code> -&gt; <code>131k</code>，<code>7k全部样本</code> -&gt; <code>2k难度样本</code></li><li><code>难样本</code>：过滤阶段1 <code>正确率 &gt; 2/3</code>、<code>正确率=0</code>的样本</li></ul></li><li><p><code>DAPO技巧</code></p><ul><li><code>超长步数惩罚</code> + <code>去掉0优势样本</code> + <code>Token-level Loss</code>，<code>阶段2减小CLIP-Higher</code></li><li><code>步数惩罚</code>：鼓励高效和<code>惩罚死循环</code>动作</li></ul></li><li><p><code>Scaffold</code>：<code>SWE-Agent</code></p></li></ul><p><strong>模型效果 (Qwen2.5-72B-Inst, SFT+2RL)</strong></p><ul><li>训练后，SWE <code>pass@1达39分</code>，<code>pass@10达58分</code>，<code>持平DeeepSeek-V3-0324</code></li></ul><p><strong>重要结论</strong></p><ul><li><code>不要过滤超长样本</code>，<code>要惩罚死循环</code>。</li><li><code>训推不一致</code>：采样<code>topk, topp</code>导致<code>词表被截断</code>，解法：<code>关闭filter</code>。</li><li>未来难题方向：<code>长程信用分配问题</code>、<code>盲目自信问题</code>。</li></ul></div><h3 id="_2508-deepswe-42分-agentic" tabindex="-1">(2508) DeepSWE (42分, Agentic) <a class="header-anchor" href="#_2508-deepswe-42分-agentic" aria-label="Permalink to &quot;(2508) DeepSWE (42分, Agentic)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">DeepSWE 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33" target="_blank" rel="noreferrer">blog</a>, <a href="https://huggingface.co/agentica-org/DeepSWE-Preview" target="_blank" rel="noreferrer">DeepSWE</a>, <a href="https://rllm-project.readthedocs.io/en/latest/examples/swe/" target="_blank" rel="noreferrer">rllm-deepswe</a>, <a href="https://huggingface.co/datasets/R2E-Gym/R2E-Gym-Subset" target="_blank" rel="noreferrer">R2E-Gym-Subset</a>, <a href="https://github.com/agentica-project/rllm" target="_blank" rel="noreferrer">rllm</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2508-deepswe-42%E5%88%86-agentic" target="_blank" rel="noreferrer">笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>Kubernates R2E环境集群</code> + <code>R2E-Gym 4.5k数据</code> + <code>环境执行反馈</code></li><li><code>GRPO++算法</code>： <ul><li>DAPO技巧：<code>Clip-Higher</code>+<code>去除KLloss</code>+ <code>去除熵loss</code> + <code>compact过滤</code></li><li>Dr.GRPO技巧：<code>优势不除以标准差</code> + <code>去掉序列内Token平均</code></li><li>RLOO技巧：<code>留一法计算优势</code></li></ul></li><li><code>Hybrid TTS</code>：执行验证 + 免执行验证</li><li><code>SWE-Agent</code></li></ul><p><strong>模型效果(Qwen3-32B, RL)</strong></p><ul><li>Qwen3-32B 经<code>GRPO++</code>优化后，SWE-verified 达<code>42分</code>，<code>TTS达59分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li>用Claude蒸馏来<code>SFT模型</code>，<code>SWE仅34分</code>，低于<a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#%E5%85%B3%E9%94%AE%E7%BB%93%E6%9E%9C-swe-agent-lm-32b" target="_blank" rel="noreferrer">SWE-Agent-LM 40分</a>。</li><li>用<code>SWE-Smith</code>和<code>SWE-Gym</code>数据做RL，<code>提升有限</code>。</li><li><code>R2E-Gym</code> 很适合做RL，<code>较好课程学习</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li>开源。</li></ul></div><h3 id="_2512-devstral2-72-2分" tabindex="-1">(2512) Devstral2(72.2分) <a class="header-anchor" href="#_2512-devstral2-72-2分" aria-label="Permalink to &quot;(2512) Devstral2(72.2分)&quot;">​</a></h3><div class="custom-block danger"><div class="custom-block-title">Devstral2 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512" target="_blank" rel="noreferrer">Devstral-Small-2-24B-Instruct-2512</a>, <a href="https://mistral.ai/news/devstral-2-vibe-cli" target="_blank" rel="noreferrer">devstral-2-vibe-cli</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2512-devstral2-72-2%E5%88%86" target="_blank" rel="noreferrer">笔记</a></li></ul><p><strong>模型效果</strong></p><ul><li><code>模型小</code>且<code>效果好</code><ul><li><code>256k</code>、<code>Dense模型</code>，比Kimi/DeepSeek<code>都小很多</code>。</li><li>Devstral2：<code>123B</code>，<code>72.2 SWE-verified</code>。</li><li>Devstral Small2：<code>24B</code>，<code>68 SWE-verified</code>。</li></ul></li><li>但<code>仍落后于闭源模型</code>。</li></ul><p><strong>关键结论</strong></p><ul><li>支持<code>探索代码库</code>、<code>跨文件协调更改</code>、<code>架构级上下文</code></li><li>支持 <code>Mistral Vibe CLI 工具</code>。</li></ul></div><h3 id="_2505-devstral-46分-tts3指标" tabindex="-1">(2505) Devstral(46分, tts3指标) <a class="header-anchor" href="#_2505-devstral-46分-tts3指标" aria-label="Permalink to &quot;(2505) Devstral(46分, tts3指标)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">Devstral 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://mistral.ai/news/devstral" target="_blank" rel="noreferrer">devstral</a>, <a href="https://huggingface.co/mistralai/Devstral-Small-2505" target="_blank" rel="noreferrer">mistralai/Devstral-Small-2505</a>, <a href="https://www.alphaxiv.org/abs/2509.25193" target="_blank" rel="noreferrer">paper</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2505-devstral-46%E5%88%86-tts3%E6%8C%87%E6%A0%87" target="_blank" rel="noreferrer">笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li>SFT轨迹数据合成方法：基于<code>环境探索</code>+<code>单元测试验证</code>， 保留<code>正确轨迹</code><ul><li>模式：<code>CoT</code>+<code>代码执行</code>，<code>OpenHands</code> + <code>SWE-Gym</code></li><li>具体数据没细讲，类似 <a href="http://plmsmile.github.io/posts/llm/industry/mainllm/02-deepseek-series.html#%E8%87%AA%E8%92%B8%E9%A6%8F%E5%86%B7%E5%90%AF%E5%8A%A8" target="_blank" rel="noreferrer">DeepSeekV3.2 自蒸馏冷启动</a></li></ul></li><li><code>Post-Training方法</code>：<code>简单过滤SFT</code>、<code>严格过滤SFT</code>、<code>RL训练</code>。</li><li><code>OpenHands</code></li></ul><p><strong>模型效果</strong></p><ul><li>Devstral-small-24B模型，<code>SWE达46分</code>，<code>迭代式 Best-of-3</code>指标。</li></ul></div><h3 id="_2502-swe-rl-meta" tabindex="-1">(2502) SWE-RL (Meta) <a class="header-anchor" href="#_2502-swe-rl-meta" aria-label="Permalink to &quot;(2502) SWE-RL (Meta)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-RL 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2502.18449" target="_blank" rel="noreferrer">paper</a>, <a href="https://github.com/facebookresearch/swe-rl" target="_blank" rel="noreferrer">swe-rl</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2502-swe-rl-meta" target="_blank" rel="noreferrer">笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>GithubPR数据收集构建方法</code>：<code>仓库事件克隆</code> + <code>PR聚合</code> + <code>预测相关文件</code> + <code>数据过滤</code><ul><li>SWE-RL PR数据：<code>27.3w </code></li></ul></li><li><code>AgentSFT数据合成方法</code>：<code>PR种子筛选</code> + <code>定位数据合成</code> + <code>编辑数据合成</code></li><li>SWE-RL方法：<code>LLama3-70B</code> + <code>GRPO</code>，<code>不执行环境</code>，采用<code>Patch相似度</code>来做<code>奖励信号</code></li><li><code>Agentless Scaffold</code></li></ul><p><strong>模型效果(LLaMA3-70B, RL, SWE-Verified)</strong></p><ul><li>LLama3-SWE-RL-70B：<code>SWE-Verified 41分</code>，在<code>100B模型下效果最好</code>，</li><li><code>SFT 达36.2分</code>，效果也不错。</li><li><code>未使用闭源LLM蒸馏技术</code>，<code>纯开源数据</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>RL比SFT效果好</code>。</li><li><code>Best-of-N</code> 越大越好，但后期逐渐收敛。</li><li><code>DenseReward</code> 比Sparse Reward好。</li></ul></div><h3 id="_2405-swe-agent" tabindex="-1">(2405) SWE-agent <a class="header-anchor" href="#_2405-swe-agent" aria-label="Permalink to &quot;(2405) SWE-agent&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-agent 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2405-swe-agent" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2405.15793" target="_blank" rel="noreferrer">paper</a></li></ul><p><strong>核心方法</strong></p><ul><li>设计<code>Agent-Computer-Interface 范式</code></li></ul><p><strong>模型效果</strong></p><ul><li>基于<code>SWE-Agent框架</code>，GPT4-Turbo，<code>SWE-Full-12分</code>，<code>Light-18分</code></li><li><code>SWE-Agent</code>比<code>标准Shell提高7pt</code>，<code>比RAG提高16pt</code>。</li></ul></div><h2 id="swe-数据工作" tabindex="-1">SWE 数据工作 <a class="header-anchor" href="#swe-数据工作" aria-label="Permalink to &quot;SWE 数据工作&quot;">​</a></h2><h3 id="_2601-swe-lego-52-6分" tabindex="-1">(2601) SWE-Lego (52.6分) <a class="header-anchor" href="#_2601-swe-lego-52-6分" aria-label="Permalink to &quot;(2601) SWE-Lego (52.6分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Lego 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2601-swe-lego-52-6%E5%88%86" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2601.01426?chatId=019bc120-7bc9-7898-9d4a-d66201136d67" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/SWE-Lego" target="_blank" rel="noreferrer">SWE-lego</a>, <a href="https://github.com/SWE-Lego/SWE-Lego" target="_blank" rel="noreferrer">代码</a></li></ul><p><strong>核心方法</strong></p><ul><li><p><code>SWE-lego数据集</code>：<code>3.2k仓库</code>+<code>32k任务</code>+<code>18k轨迹</code>，来源<code>SWE-rebench</code></p></li><li><p><code>数据集构造方法</code>：<code>真实PR</code> + <code>合成任务</code> + <code>Qwen3Coder蒸馏轨迹</code></p></li><li><p><code>Refine SFT方法</code>：<code>Mask错误动作</code> + <code>3难度课程学习</code>，<code>难度为交互轮次</code></p></li></ul><p><strong>模型效果(Qwen3-32B + SFT)</strong></p><ul><li><code>SWE-V</code> <code>达52.6分</code>，<code>TTS-16</code> <code>达58.8分</code>，<code>8B</code> <code>达42.2分</code>。</li><li><code>Refine SFT</code> 比<code>普通 SFT(48.8分)</code> <code>高 3.8pt</code></li><li><code>没有Git Hacking</code>的结果，让Agent <code>不能查看git log</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>精细化SFT数据</code> 效果可以<code>超过复杂训练方法</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li><code>SWE-lego数据集</code>，<code>开源代码</code></li></ul></div><h3 id="_2510-bugpilot-54-9分" tabindex="-1">(2510) BugPilot(54.9分) <a class="header-anchor" href="#_2510-bugpilot-54-9分" aria-label="Permalink to &quot;(2510) BugPilot(54.9分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">BugPilot 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2510-bugpilot" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2510.19898?chatId=019bb759-e3a3-7c3c-8a37-b430712af950" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/microsoft/FrogBoss-32B-2510" target="_blank" rel="noreferrer">microsoft/FrogBoss-32B-2510</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>1套Bug合成框架</code>：<code>SWE-Agent</code>开发Feature，<code>引入无意的FeatAdd-Bug</code></li><li><code>数据集-9k轨迹</code>：<code>R2E-Gym</code> + <code>SWE-Smith</code> + <code>FeatAdd轨迹/任务</code>(<code>未开源</code>)</li><li><code>2种训练方法</code>：<code>SFT全数据训练</code>，<code>SFT冷启动</code>+<code>RL训练</code>。</li><li><code>R2E-Gym 脚手架</code></li></ul><p><strong>模型效果(Qwen3-32B + SFT, SWE-Verified)</strong></p><ul><li><code>BaseMix5.8k-SFT</code> <code>pass@1</code> <code>达49分</code>，即<code>SWE-Gym</code> + <code>SWE-smith</code> <code>蒸馏数据</code></li><li>增加<code>FeatAdd-1.2k-轨迹 SFT</code> 达<code>51.9分</code>；增加<code>FeatAdd-Bug RL</code>达<code>52.4分</code>。</li><li>使用<code>全9k蒸馏数据 SFT </code>达<code>54.9分</code>，高于<code>SWE-Mirror-60k-SFT 52分</code>。<code>14B也达45分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>FeatAdd-Bug比较好</code><ul><li><code>解决率低</code>(相比规则SWE-Smith)，平均<code>修改4.2个文件</code>，<code>Bug类型更均匀</code>。</li><li><code>无意Bug</code>比<code>故意Bug</code> <code>效果好</code>。</li></ul></li></ul><p><strong>关键贡献</strong></p><ul><li><code>FeatAdd 无意引入的Bug</code> <code>这种思想</code></li><li>仅开源模型，并<code>未开源</code> <code>数据集</code>和<code>代码</code>。</li></ul></div><h3 id="_2509-swe-mirror-52分-seed" tabindex="-1">(2509) SWE-Mirror(52分, Seed) <a class="header-anchor" href="#_2509-swe-mirror-52分-seed" aria-label="Permalink to &quot;(2509) SWE-Mirror(52分, Seed)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Mirror 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2509-swe-mirror-seed" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2509.08724" target="_blank" rel="noreferrer">paper</a></li></ul><p><strong>核心方法</strong></p><ul><li><p>1套<code>SWE任务合成移植方法</code>：<code>任务选择</code> + <code>任务移植</code> + <code>任务验证</code></p><ul><li><code>Bug移植</code>：<code>生成测试用例</code> + <code>生成Bug源代码</code> + <code>生成Issue描述</code></li></ul></li><li><p><code>SWE-mirror-60k 数据</code>：<code>4语言</code>+<code>40 仓库</code>+<code>60k任务</code>+<code>6.3k蒸馏轨迹</code></p><ul><li><code>数据未开源</code>，python为主，来自<code>SWE-Gym</code>, <code>SWE-rebench</code>, <code>Multi-SWE-RL</code></li></ul></li><li><p><code>SFT方法</code>：<code>Mask错误动作</code></p></li><li><p><code>Scaffold</code>：<code>OpenHands</code>+<code>MopenHands</code></p></li></ul><p><strong>模型效果(Qwen2.5-Coder-Instruct-32B + SFT)</strong></p><ul><li>SWE-verified 达<code>52分</code>。Multi-SWE-Bench-Flash 达21分。</li></ul><p><strong>重要结论</strong></p><ul><li><code>Mask错误动作</code> SFT 效果比不Mask或片段剪辑掉的好。</li><li>SFT <code>Data Scaling有效</code>：<code>4k</code>轨迹训练，6-&gt;<code>35分</code>；<code>12k</code>训练，达<code>52分</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li>SWE-Mirror-60k 任务，没开源，也不算贡献吧。</li></ul></div><h3 id="_2506-skywork-swe-36分" tabindex="-1">(2506) Skywork-SWE(36分) <a class="header-anchor" href="#_2506-skywork-swe-36分" aria-label="Permalink to &quot;(2506) Skywork-SWE(36分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">Skywork-SWE 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/mainllm/15-skywork-series.html#_2506-skywork-swe" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2506.19290" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/Skywork/Skywork-SWE-32B" target="_blank" rel="noreferrer">Skywork-SWE-32B</a>, <a href="https://quixotic-sting-239.notion.site/Skywork-SWE-Unveiling-Data-Scaling-Laws-for-Software-Engineering-in-LLMs-eb17f379610040ceb54da5d5d24065bd" target="_blank" rel="noreferrer">blog</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SWE任务收集构建方法</code><ul><li><code>Repo+PR 收集</code> + <code>统一环境安装</code> + <code>执行验证</code>等。</li><li>基于<code>真实环境执行</code>来做<code>数据验证</code>，<code>3层增量式镜像</code> (基础+环境+实例镜像)。</li></ul></li><li><code>Skywork-SWE数据</code>：<code>10k任务</code> + <code>2.5k仓库</code> + <code>8k蒸馏轨迹</code>。<code>没开源数据</code></li><li><code>Scaffold</code>：<code>Openhands</code></li></ul><p><strong>模型效果 (Qwen-2.5-Coder-32B + SFT)</strong></p><ul><li>SWE-verified 达<code>36分</code>，<code>TTS-3</code> 达<code>47分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li>SWE <code>Data-Scaling</code>, <code>Test-Time-Scaling</code>, <code>轮数Scaling</code> Law 得到验证。</li><li>经过<code>单元测试验证的数据</code>比<code>SWE-smith合成数据</code> 靠谱，提升6.8%</li></ul><p><strong>关键贡献</strong></p><ul><li>仅开源模型，<code>未开源代码和数据</code>。</li></ul></div><h3 id="_2505-swe-rebench" tabindex="-1">(2505) SWE-rebench <a class="header-anchor" href="#_2505-swe-rebench" aria-label="Permalink to &quot;(2505) SWE-rebench&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-rebench 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2505-swe-rebench" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2505.20411" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/datasets/nebius/SWE-rebench-openhands-trajectories" target="_blank" rel="noreferrer">NEBIUS-SWE-rebench-轨迹数据</a>, <a href="https://huggingface.co/datasets/nebius/SWE-rebench-openhands-trajectories" target="_blank" rel="noreferrer">nebius/SWE-rebench</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>自动</code> <code>SWE Issue-PR任务</code> <code>收集工具</code></li></ul><p><strong>关键贡献</strong></p><ul><li>SWE-rebench 数据集：<code>21k python任务</code></li><li><a href="https://swe-rebench.com/" target="_blank" rel="noreferrer">SWE-rebench Benchmark 排行榜</a></li></ul></div><h3 id="_2504-swe-smith-40分-swe-agent-lm" tabindex="-1">(2504) SWE-smith (40分, SWE-Agent-LM) <a class="header-anchor" href="#_2504-swe-smith-40分-swe-agent-lm" aria-label="Permalink to &quot;(2504) SWE-smith (40分, SWE-Agent-LM)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-smith 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-swe-smith-swe-agent-lm" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2504.21798" target="_blank" rel="noreferrer">paper</a>, <a href="https://github.com/SWE-bench/SWE-smith" target="_blank" rel="noreferrer">SWE-smith</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SWE任务合成方法</code>：<code>Agent安装环境</code> + <code>4策略合成候选任务</code> + <code>执行验证</code> + <code>逆向合成Issue</code></li><li><code>SWE-smith数据</code>：<code>128仓库</code>+<code>50k任务</code>+<code>5k蒸馏轨迹</code></li><li><code>SWE-Agent</code></li></ul><p><strong>模型效果 (Qwen2.5-Coder-32B)</strong></p><ul><li>使用<code>轨迹数据SFT</code>，<code>SWE-verified 达40</code>，<code>提升33pt</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>任务Scaling有效</code>，<code>多样性很重要</code>，<code>PR-Mirror</code>, <code>LM-Rewrite</code>的任务比较好。</li></ul><p><strong>关键贡献</strong></p><ul><li>开源代码、<code>任务</code>、<code>环境</code>、<code>轨迹</code>，<code>真开源！</code></li><li><a href="https://huggingface.co/datasets/SWE-bench/SWE-smith" target="_blank" rel="noreferrer">SWE-smith 52k任务</a>，<a href="https://huggingface.co/datasets/SWE-bench/SWE-smith-trajectories" target="_blank" rel="noreferrer">26k SWE-smith-轨迹</a>，<a href="https://github.com/SWE-bench/SWE-smith-envs" target="_blank" rel="noreferrer">SWE-smith-env</a></li></ul></div><h3 id="_2504-r2e-gym-34-4分" tabindex="-1">(2504) R2E-Gym(34.4分) <a class="header-anchor" href="#_2504-r2e-gym-34-4分" aria-label="Permalink to &quot;(2504) R2E-Gym(34.4分)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">R2E-Gym 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-r2e-gym" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2504.07164?chatId=019b8c06-0a6c-7d41-b722-443e9539be96" target="_blank" rel="noreferrer">paper</a>, <a href="https://r2e-gym.github.io/" target="_blank" rel="noreferrer">r2e-gym</a>, <a href="https://huggingface.co/R2E-Gym" target="_blank" rel="noreferrer">R2E-Gym</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>自动合成SWE任务方法</code>：<code>Commit挖掘</code>+<code>测试用例生成</code>+<code>反向Issue生成</code></li><li><code>R2E-Gym 数据</code>： <code>10仓库</code>+<code>8k任务</code>+<code>3.3k蒸馏轨迹</code> ，<code>R2E-Gym Sub</code>：<code>4.5k 任务</code></li><li><code>OpenHands</code></li></ul><p><strong>模型效果(Qwen-Coder-32B + SFT)</strong></p><ul><li>SWE-Verified 达<code> 34.4分</code></li></ul><p><strong>重要结论</strong></p><ul><li><code>合成数据不输人工数据</code></li><li><code>Hybrid TTS</code> 有效果，从34.4<code>提升至51分</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li></li></ul></div><h3 id="_2412-swe-gym-19-7分" tabindex="-1">(2412) SWE-Gym(19.7分) <a class="header-anchor" href="#_2412-swe-gym-19-7分" aria-label="Permalink to &quot;(2412) SWE-Gym(19.7分)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Gym 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2412-swe-gym" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2412.21139" target="_blank" rel="noreferrer">paper</a>, <a href="https://github.com/SWE-Gym/SWE-Gym" target="_blank" rel="noreferrer">代码</a>, <a href="https://huggingface.co/SWE-Gym" target="_blank" rel="noreferrer">SWE-Gym Data</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SWE任务构建方法</code>：<code>通过脚本直接提取PR</code>，并半手动<code>构建好环境</code>(仅覆盖11仓库)</li><li><code>SWE-Gym数据集</code>：<code>2.4k任务</code>+ <code>11仓库</code></li><li><code>OpenHands</code>，<code>Moatless</code></li></ul><p><strong>模型效果(Qwen2.5-Coder-32B + SFT)</strong></p><ul><li>SWE-Verified <code>19.7分</code>，TTS-16 达32分。</li></ul><p><strong>重要结论</strong></p><ul><li><code>Best-of-16策略</code>：20.6 -&gt; <code>32分</code>，开源模型新标杆。</li></ul></div><h2 id="swe-背景" tabindex="-1">SWE 背景 <a class="header-anchor" href="#swe-背景" aria-label="Permalink to &quot;SWE 背景&quot;">​</a></h2><h3 id="swe-任务" tabindex="-1">SWE 任务 <a class="header-anchor" href="#swe-任务" aria-label="Permalink to &quot;SWE 任务&quot;">​</a></h3><h3 id="swe-挑战" tabindex="-1">SWE 挑战 <a class="header-anchor" href="#swe-挑战" aria-label="Permalink to &quot;SWE 挑战&quot;">​</a></h3><div class="custom-block warning"><div class="custom-block-title">SWE 挑战</div><p><strong>挑战</strong></p><ul><li>环境验证不足：<code>可执行环境</code> + <code>验证过的单元测试</code> + <code>代码执行套件</code>(统一执行脚本)</li><li><strong>高质量数据不足</strong>：<code>量大质低</code> + <code>质高量小</code><ul><li>SWE-Dev：数据多，但缺环境和单元测试</li><li>SWE-Gym：有环境，但仅11仓库</li></ul></li><li><strong>SWE-Scaling Law 尚不清晰</strong>：SWE数据量小，Scaling Law尚未得到验证，增加数据是否带来效果提升？</li></ul></div><p><a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#swe%E6%96%B9%E6%B3%95%E5%8F%8A%E6%8C%91%E6%88%98" target="_blank" rel="noreferrer">NEBIUS SWE挑战</a></p><div class="custom-block warning"><div class="custom-block-title">SWE 挑战</div><p><strong>SWE 存在挑战</strong></p><ul><li><code>Long-Horizon 多轮交互</code><ul><li>2阶段RL，YaRN 技术 扩展至131k</li></ul></li><li><code>反馈复杂</code>：反馈一大堆报错，可能看不懂 <ul><li>RFT 冷启动</li></ul></li><li><code>数据难以构建</code><ul><li>对策：使用ReBench做清洗，一套清洗策略</li></ul></li><li><code>奖励稀疏</code><ul><li>对策：GRPO/DAPO，Token-Level Loss</li></ul></li><li><code>评估贵且有噪声</code>：跑1次要几分钟，还学不到东西； <ul><li>对策：<code>Verified-50子集</code>、<code>去掉Noisy不稳定数据</code></li></ul></li></ul></div><h3 id="tts-方法" tabindex="-1">TTS 方法 <a class="header-anchor" href="#tts-方法" aria-label="Permalink to &quot;TTS 方法&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">TTS方法</div><p><strong>相关笔记</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#test-time-scaling-deepswe-verifier" target="_blank" rel="noreferrer">DeepSWE-TTS 笔记</a></li></ul><p><strong>环境执行验证</strong></p><ul><li>由<code>LLM生成测试用例</code>，来<code>执行验证</code>，<code>用例通过最多</code>则为<code>最优轨迹</code>。</li><li>优点：信号直接</li><li>缺点：<code>可能区分度低</code>，比如<code>测试用例都太简单</code>、<code>有bug全部都未通过</code>等。</li></ul><p><strong>免执行验证</strong></p><ul><li>不执行验证，通过<code>LLM来选择最优轨迹</code>。</li><li>缺点：<code>容易有偏见</code>，关注Agent的思考过程等，而<code>忽略了代码Patch本身</code>。</li></ul><p><strong>混合方法</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#test-time-scaling-deepswe-verifier" target="_blank" rel="noreferrer">DeepSWE-TTS 笔记</a></li></ul></div><p>R2E-Gym 混合方法：</p><img src="https://r2e-gym.github.io/assets/r2egym/overview-p2.png" style="display:block;margin:auto;" width="70%"><h3 id="scaffold-agent" tabindex="-1">Scaffold(Agent) <a class="header-anchor" href="#scaffold-agent" aria-label="Permalink to &quot;Scaffold(Agent)&quot;">​</a></h3><h4 id="agent-aci-派" tabindex="-1">Agent ACI 派 <a class="header-anchor" href="#agent-aci-派" aria-label="Permalink to &quot;Agent ACI 派&quot;">​</a></h4><div class="custom-block info"><div class="custom-block-title">AgentACI</div><p><strong>核心思想</strong></p><ul><li>核心：<code>端到端</code>，<code>多轮推理</code>。迭代plan + act + reflect等。</li></ul><p><strong>优点</strong></p><ul><li>更灵活，扩展性更好。</li></ul><p><strong>缺点</strong></p><ul><li>端到端<code>难训练</code>，<code>稳定性</code>不如Workflow</li><li><code>交互轮次长</code>，<code>上下文有压力</code>。</li><li>RL 训练不稳定 <ul><li><code>长序列信用分配</code>存在挑战：奖励稀疏</li><li>对<code>初始模型很敏感</code>，需要<code>SFT冷启动</code>。 <ul><li>如果从<code>通用模型</code>开始，可能<code>不会使用工具</code>，<code>陷入死循环</code>。</li></ul></li></ul></li></ul><p><strong>典型工作</strong></p><ul><li><p>OpenHands</p><ul><li>提供<code>编辑器</code> + <code>命令行终端</code> + <code>网页搜索</code>，agent在<code>沙箱环境</code> <code>自主迭代式完成任务</code>。</li><li>优点：上限高，能处理复杂问题，更像人。</li><li>缺点：成本高，容易陷入死循环</li></ul></li><li><p>SWE-Agent</p><ul><li>使用Agent-Computer-Interface，提供<code>编辑器</code>+<code>shell</code>+<code>测试运行器</code>给LLM。</li><li>仓库探索、写脚本复现Bug、修复Bug、测试执行、边缘case生成和测试</li></ul></li><li><p>Moatless-Tools</p></li><li><p>AutoCodeRover</p></li><li><p>SpecRover</p></li><li><p>Trae-Agent</p></li></ul></div><h4 id="workflow-派" tabindex="-1">Workflow 派 <a class="header-anchor" href="#workflow-派" aria-label="Permalink to &quot;Workflow 派&quot;">​</a></h4><div class="custom-block info"><div class="custom-block-title">Workflow 派</div><p><strong>优点</strong></p><ul><li><p><code>流程可控更稳定</code>，模块化更好</p></li><li><p><code>每一步</code>更容易使用<code>RLVR训练</code>。</p></li></ul><p><strong>缺点</strong></p><ul><li><code>探索空间</code>、<code>灵活性</code> <code>有限</code>。</li></ul><p><strong>其他Tradeoff</strong></p><ul><li><p><code>原子能力</code>可作为<code>skill priors</code>，更好的支持<code>通用Agent</code>。</p></li><li><p>定位、修复、反射、验证等。</p></li></ul><p><strong>典型工作</strong></p><ul><li><p><code>专有Pipeline</code></p><ul><li>Agentless：固定的<code>问题定位</code>-<code>Bug修复</code>-<code>执行验证</code> pipeline</li><li>Moatless：主张<code>有效上下文检索</code>才是关键。</li></ul></li><li><p>检索微调</p><ul><li>SWE-fixer：由粗到细，文件检索和编辑解耦。</li></ul></li></ul></div><h4 id="trade-off-派" tabindex="-1">Trade-off 派 <a class="header-anchor" href="#trade-off-派" aria-label="Permalink to &quot;Trade-off 派&quot;">​</a></h4><p>先Agentless训练，再适配到SWE-Agent</p><h3 id="训练流派" tabindex="-1">训练流派 <a class="header-anchor" href="#训练流派" aria-label="Permalink to &quot;训练流派&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">方法流派</div><p><strong>数据蒸馏SFT派 (主流)</strong></p><ul><li>工作：SWE-fixer,</li></ul><p><strong>RL 派 (主流)</strong></p><ul><li>不执行反馈：SWE-RL</li><li>执行反馈：主流，但成本高。</li></ul><p><strong>进化派</strong></p><ul><li>在解决问题的过程中，逐渐积累经验，</li><li>自我提升，Self-Evolution。</li><li>对抗训练。写Bug-修Bug对抗，写测试-修Bug对抗等等。</li><li>工作：SE-Agent,</li></ul></div>',77)]))}const h=o(r,[["render",t]]);export{u as __pageData,h as default};

import{_ as r,c as s,o as e,ah as o,j as l,a as t}from"./chunks/framework.CvbyeFFO.js";const y=JSON.parse('{"title":"Kimi 系列","description":"","frontmatter":{"title":"Kimi 系列","date":"2025-07-15T19:53:37.000Z","create":"2025-07-15T19:53:37.000Z","categories":["kimi"],"tags":["Kimi K2","MLA","MuonClip","QK-Clip","Agent数据合成技术","Rewards Gym","Self-Critic Rubric Reward","Kimi Researcher","上下文管理","全异步rollouts","Reinforce","端到端RL学习","工具使用数据生成方法","Kimi1.5","上下文扩展","Partial Rollouts","改进Policy","Long2short"]},"headers":[],"relativePath":"posts/llm/industry/mainllm/01-kimi-series.md","filePath":"posts/llm/industry/mainllm/01-kimi-series.md","lastUpdated":1761231250000}'),n={name:"posts/llm/industry/mainllm/01-kimi-series.md"},a={class:"custom-block danger"},d={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},g={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.65ex"},xmlns:"http://www.w3.org/2000/svg",width:"7.222ex",height:"2.195ex",role:"img",focusable:"false",viewBox:"0 -683 3192.3 970.2","aria-hidden":"true"},u={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},c={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.029ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.17ex",height:"1.005ex",role:"img",focusable:"false",viewBox:"0 -431 517 444","aria-hidden":"true"},m={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},p={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.65ex"},xmlns:"http://www.w3.org/2000/svg",width:"7.222ex",height:"2.195ex",role:"img",focusable:"false",viewBox:"0 -683 3192.3 970.2","aria-hidden":"true"},Q={class:"custom-block tip"},T={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},k={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.489ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.229ex",height:"1.486ex",role:"img",focusable:"false",viewBox:"0 -441 543 657","aria-hidden":"true"};function h(b,i,v,w,f,x){return e(),s("div",null,[i[41]||(i[41]=o('<h2 id="_2511-kimi-k2-thinking" tabindex="-1">(2511) Kimi K2-Thinking <a class="header-anchor" href="#_2511-kimi-k2-thinking" aria-label="Permalink to &quot;(2511) Kimi K2-Thinking&quot;">​</a></h2><h2 id="_2507-kimi-k2-open-agentic-intelligence" tabindex="-1">(2507) Kimi K2: Open Agentic Intelligence <a class="header-anchor" href="#_2507-kimi-k2-open-agentic-intelligence" aria-label="Permalink to &quot;(2507) Kimi K2: Open Agentic Intelligence&quot;">​</a></h2><blockquote><ul><li><a href="https://www.zhihu.com/question/1927140506573435010" target="_blank" rel="noreferrer">Kimi 发布首个万亿参数开源模型 K2 模型，哪些信息值得关注？</a></li><li><a href="https://moonshotai.github.io/Kimi-K2/" target="_blank" rel="noreferrer">Kimi-K2 Blog</a></li></ul></blockquote><div class="custom-block important"><div class="custom-block-title">摘要</div><ul><li>K2</li></ul></div><p><strong>❓问题背景</strong></p><div class="custom-block warning"><div class="custom-block-title">问题背景</div><ul><li>高质量预训练数据越来越少，需要提高学习效率 (RL自己探索)</li><li>后训练复杂：如何将预训练和知识转换为agent行为是一个挑战</li></ul></div><p><strong>📕核心方法</strong></p>',7)),l("div",a,[i[31]||(i[31]=o('<div class="custom-block-title">技术架构创新</div><p><strong>1. 模型架构</strong></p><ul><li><strong>MoE结构</strong><ul><li><code>384个</code>专家，每个token激活<code>8个</code>，激活参数<code>32B</code>，总参数<code>1T</code></li><li><code>高度稀疏</code>设计，有性能且兼顾<code>优化效率</code></li></ul></li><li><a href="https://plmsmile.github.io/posts/llm/basic/02-llm-components.html#mha-mqa-gqa" target="_blank" rel="noreferrer">Multi-Head Latent Attention </a><ul><li><strong>压缩KV</strong>来提高效率，减少计算量和带宽压力</li></ul></li><li>每层<strong>注意力头降至</strong><code>64</code><ul><li>节省<code>83%</code>FLOPS，降低推理资源消耗，更好处理长上下文</li></ul></li></ul><p><strong>2. MuonClip 优化器</strong></p>',4)),l("ul",null,[l("li",null,[i[11]||(i[11]=l("code",null,"Muon优化器 ",-1)),l("ul",null,[i[9]||(i[9]=l("li",null,[t("目的：在相同计算资源和参数的条件下，"),l("strong",null,"尽可能多"),t("的学到信息。")],-1)),i[10]||(i[10]=l("li",null,[t("优点："),l("strong",null,"token效率高"),t(" ⭐")],-1)),l("li",null,[i[8]||(i[8]=t("缺点： ")),l("ul",null,[i[7]||(i[7]=l("li",null,[l("strong",null,"训练不稳定"),t("，容易出现"),l("code",null,"注意力logits爆炸"),t("现象😓 "),l("ul",null,[l("li",null,[t("经常到1000+，导致"),l("code",null,"loss spike"),t(" (loss异常高)")])])],-1)),l("li",null,[i[2]||(i[2]=t("本质是")),i[3]||(i[3]=l("strong",null,"Query和Key",-1)),i[4]||(i[4]=t("的权")),l("mjx-container",d,[(e(),s("svg",g,i[0]||(i[0]=[o('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mstyle" fill="blue" stroke="blue"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1352.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1796.9,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g></g></g></g></g>',1)]))),i[1]||(i[1]=l("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[l("mstyle",{mathcolor:"blue"},[l("msub",null,[l("mi",null,"W"),l("mi",null,"q")]),l("mo",null,","),l("msub",null,[l("mi",null,"W"),l("mi",null,"k")])])])],-1))]),i[5]||(i[5]=l("strong",null,"增长过快",-1)),i[6]||(i[6]=t("导致的。"))])])])])]),l("li",null,[i[29]||(i[29]=l("code",null,"QK-Clip",-1)),l("ul",null,[l("li",null,[i[27]||(i[27]=t("核心 ")),l("ul",null,[l("li",null,[i[14]||(i[14]=t("实时监控")),i[15]||(i[15]=l("strong",null,"每个头的最大logit",-1)),i[16]||(i[16]=t("是否超过")),i[17]||(i[17]=l("strong",null,"阈值",-1)),l("mjx-container",u,[(e(),s("svg",c,i[12]||(i[12]=[l("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[l("g",{"data-mml-node":"math"},[l("g",{"data-mml-node":"mi"},[l("path",{"data-c":"1D70F",d:"M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z",style:{"stroke-width":"3"}})])])],-1)]))),i[13]||(i[13]=l("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[l("mi",null,"τ")])],-1))]),i[18]||(i[18]=t("(论文是100)"))]),l("li",null,[i[21]||(i[21]=l("code",null,"按比例轻量化缩小",-1)),i[22]||(i[22]=t("超过阈值注意力头的")),l("mjx-container",m,[(e(),s("svg",p,i[19]||(i[19]=[o('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mstyle" fill="blue" stroke="blue"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1352.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1796.9,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g></g></g></g></g>',1)]))),i[20]||(i[20]=l("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[l("mstyle",{mathcolor:"blue"},[l("msub",null,[l("mi",null,"W"),l("mi",null,"q")]),l("mo",null,","),l("msub",null,[l("mi",null,"W"),l("mi",null,"k")])])])],-1))]),i[23]||(i[23]=t(" 权重⭐ （")),i[24]||(i[24]=l("code",null,"query和key",-1)),i[25]||(i[25]=t("的权重矩阵）"))]),i[26]||(i[26]=l("li",null,[t("最小化"),l("strong",null,"per-head干预"),t("：只对超过的头采取，只有1小部分注意力头会爆炸💥")],-1))])]),i[28]||(i[28]=l("li",null,[t("优点：解决了"),l("code",null,"注意力logits爆炸问题")],-1))])]),i[30]||(i[30]=l("li",null,[t("整体效果🔑 "),l("ul",null,[l("li",null,[t("预训练数据"),l("code",null,"15.5T"),t(" tokens， 实现"),l("strong",null,"零loss spike"),t("，"),l("strong",null,"对收敛几乎无损"),t("(<0.1%) 👍")])])],-1))])]),i[42]||(i[42]=o('<img src="https://paper-assets.alphaxiv.org/figures/7dd1747c-0f4d-429b-a909-c58ee4a1d8f4v1/img-1.jpeg" style="display:block;margin:auto;" width="80%"><img src="https://picx.zhimg.com/80/v2-b897af5bf48345b9c76905d25264f62a_1440w.webp?source=1def8aca" style="display:block;margin:auto;" width="80%"><div class="custom-block note"><div class="custom-block-title">预训练数据改写技术</div><p><strong>背景</strong></p><ul><li>15.5T <strong>Token有限</strong>，希望提高<code>token 效率</code>，尽可能挖掘出更多信息供模型学习</li></ul><p>🍎<strong>核心思想</strong></p><ul><li>基于<code>高质量数据</code>做<code>数据合成</code><ul><li><strong>放大</strong>高质数据的<strong>价值</strong></li><li>提高模型的<strong>令牌效用</strong><code>token utility</code>， 即模型从token中<code>学到的知识量</code></li></ul></li><li>需要避免<strong>过拟合</strong>风险‼️</li></ul><p>🐱<strong>知识数据改写技术</strong></p><ul><li>背景：<code>知识密集型</code>数据简单重复训练，会导致<code>过拟合</code></li><li>步骤 <ul><li><code>多样化改写</code>prompt：风格+视角多样化</li><li><code>长文本分块改写</code>：长文本切分成带上下文的小块，逐块改写再做合并 (chunk-wise 自回归改写)</li><li><code>忠诚度校验</code>(Fidelity verification)：原文对比做<strong>质量控制</strong>，防止学到<strong>错误</strong>信息‼️</li></ul></li><li>效果 <ul><li>SimpleQA验证 <ul><li>原始数据训10次： 23.76%；1次改写训10次：27.39%；<strong>10次改写训1次</strong>：<code>28.94%</code> 💡</li></ul></li></ul></li></ul><p>😻<strong>数学数据改写技术</strong></p><ul><li>背景：为增强<code>数学推理</code>能力，对<code>数学文档</code>做改写</li><li>步骤： <ul><li>转换成<strong>学习笔记</strong>风格：<a href="https://www.alphaxiv.org/abs/2505.02881" target="_blank" rel="noreferrer">SwallowMath</a></li><li><strong>多语言翻译</strong>：其他语言翻译成英语</li></ul></li></ul></div><div class="custom-block important"><div class="custom-block-title">Agent数据合成技术(后训练-SFT)</div><p><strong>背景</strong></p><ul><li>由于<code>成本/复杂/隐私</code>等原因，agent交互<code>很难在真实世界去做scale</code></li><li>为agentic能力，构建<code>高质量合成数据</code>模拟真实交互，来教模型遵循指令、<strong>使用工具</strong>。 <ul><li><strong>合成数据有潜力</strong>：ZeroSearch/ACEBench等。</li></ul></li></ul><p><strong>核心思想</strong></p><ul><li>构建大型工具库，生成agent交互轨迹，通过拒绝采样做质量过滤，通过真实沙盒执行。</li></ul><p>🛠️<strong>工具库构建</strong></p><ul><li><p>真实工具</p><ul><li><code>3000真实MCP工具</code> (从github抓取)</li><li>缺点：<strong>分布不均衡</strong>‼️，网页开发等热门领域多，其他如机器人控制/生物等领域少。</li></ul></li><li><p>合成工具</p><ul><li><p><code>20000</code>个合成工具，每个都有清晰的<code>接口、描述和操作语义</code>。</p></li><li><p><code>层次合成策略</code>：从大领域逐渐细分到子领域做合成，<code>领域多样性非常好</code>。</p></li></ul></li></ul><p>🤖<strong>Agent和任务构建</strong></p><ul><li>Agent构建 <ul><li><code>1000+</code>agent，覆盖多个领域</li><li><strong>不同的System Prompt</strong>(合成) + <strong>不同工具组合</strong></li></ul></li><li>任务构建<code>Rubric-based </code>‼️ <ul><li>为每个Agent<strong>从简单到难</strong>构建<strong>多个</strong>任务</li><li>每个任务都<strong>有明确的评估标准</strong>⭐ <ul><li><code>怎样才算成功？应该用什么工具、顺序是什么？关键评估点是什么？</code></li></ul></li></ul></li></ul><p>📚<strong>多轮轨迹数据采集</strong></p><ul><li>采集 <ul><li>用户：LLM扮演不同风格的用户，向agent提出问题、多轮交互；</li><li>模拟环境：<code>复杂的工具模拟器</code>(类似世界模型)，<strong>具体是啥</strong>❓ <ul><li>执行工具调用，返回结果</li><li>有记忆<code>有状态</code>：工具执行后会更新状态；有助于：<code>持续影响的复杂多步交互推理</code></li><li>引入可控随机性：成功、部分失败、特殊情况。</li></ul></li></ul></li><li>过滤 <ul><li><code>LLM-as-Judge</code>：只留下满足<code>task-rubric</code>的轨迹</li></ul></li></ul><p>🗺️<strong>混合环境(模拟+真实)</strong></p><ul><li>模拟：复杂工具模拟器；模拟多样性。</li><li>真实：代码、软件等；真实性验证。</li></ul></div><img src="https://paper-assets.alphaxiv.org/figures/7dd1747c-0f4d-429b-a909-c58ee4a1d8f4v1/img-7.jpeg" style="display:block;margin:auto;" width="80%"><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/llm/agent/kimi-k2-agentic-data-synthesis.jpg" style="display:block;margin:auto;" width="80%"><div class="custom-block info"><div class="custom-block-title">强化学习</div><p><strong>背景</strong></p><ul><li>RL具有更好的token效率和泛化性。</li><li>难在如何平衡客观事实和主观偏好，在<code>可评估</code>和<code>不可评估</code>任务上<strong>进行RL学习</strong></li></ul><p><strong>核心思想</strong></p><ul><li>统一可扩展的混合奖励框架</li></ul><p><strong>可验证的Rewards Gym</strong></p><ul><li>处理具有明确对错、客观可验证的任务：数学、编程、STEMP(科学/技术)、推理等。</li><li>五大场景 <ul><li>数学、STEM、逻辑任务：多样性、难度适中。</li><li>复杂指令遵循 <ul><li>混合验证：硬规定(规则) + 软规定(AI验证) + 防作弊机制</li><li>数据生成：AI生成 + AI抬杠出题(专属模型,攻击k2短板)</li></ul></li><li>忠实性</li><li>Code</li><li>安全性</li></ul></li></ul><p><strong>自我批判的奖励(Self-Critic Rubric Reward)</strong></p><ul><li>背景：处理没有唯一正确答案、依赖主观偏好的任务，写作、对话、总结等。</li><li>核心思想：让模型学会自我评价，分为actor和critic</li><li>步骤 <ul><li>Actor：生成多个回答；Critic：依据评分准则做两两比较打分，产生偏好；Actor根据偏好调整策略。</li><li>防止Critic跑偏 <ul><li>Critic定期在可验证任务上进行校准，确保客观正确；使得主观任务能受益于客观任务。</li></ul></li></ul></li></ul><p><strong>RL 算法增强</strong></p><ul><li>预算控制：对长回答做乘法，提高推理性价比</li><li>PTX loss：在RL训练中混入高质量SFT数据，防止灾难性遗忘</li><li>温度衰减：初期：高创造探索；后期：高质量稳定输出。</li></ul></div><p><strong>✍️实验设置</strong></p><div class="custom-block tip"><div class="custom-block-title">实验配置</div></div><p><strong>🍑关键结果</strong></p><div class="custom-block caution"><div class="custom-block-title">关键结果</div></div><p><strong>⛳未来方向</strong></p><div class="custom-block note"><div class="custom-block-title">未来方向</div></div><h2 id="_2506-kimi-researcher-end-to-end-rl-training-for-emerging-agentic-capabilities" tabindex="-1">(2506) Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities <a class="header-anchor" href="#_2506-kimi-researcher-end-to-end-rl-training-for-emerging-agentic-capabilities" aria-label="Permalink to &quot;(2506) Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities&quot;">​</a></h2><blockquote><ul><li><a href="https://moonshotai.github.io/Kimi-Researcher/" target="_blank" rel="noreferrer">Kimi-Researcher</a>，<a href="https://zhuanlan.zhihu.com/p/1921119537757140195" target="_blank" rel="noreferrer">Kimi-Researcher：端到端强化学习驱动的自主智能体</a></li></ul></blockquote><div class="custom-block important"><div class="custom-block-title">摘要</div><ul><li></li></ul></div><p><strong>❓问题背景</strong></p><div class="custom-block warning"><div class="custom-block-title">问题背景</div><ul><li>当前agent开发存在问题 <ul><li>workflow方法：依赖人工设计和Prompt来，难以扩展适应动态环境。</li><li>SFT方法：成本高、泛化弱。</li></ul></li><li><strong>端到端RL的挑战</strong><ul><li><strong>适应动态环境</strong>：真实环境是变化的，<mark>agent需在变化环境保持稳定泛化</mark></li><li><strong>长序列任务</strong>：单任务可能超过70次搜索，上下文达10w token，<mark>需具备优秀记忆和长上下文能力</mark></li><li><strong>数据稀缺</strong>：<mark>agent问答高质量RL数据非常少</mark>，人工成本高，难以满足大规模训练</li><li><strong>Rollout效率慢</strong>：<mark>多轮推理和频繁工具调用，显著拖慢训练速度，成为瓶颈</mark></li></ul></li></ul></div><img src="https://pic4.zhimg.com/v2-ee4caf63ac6dddd1f3123d561cbc4deb_1440w.jpg" style="display:block;margin:auto;" width="80%"><p><strong>📕核心方法</strong></p><p>在<code>Data</code>、<code>RL算法</code>、<code>上下文管理</code>和<code>Infra</code>四个方面进行创新。</p><div class="custom-block tip"><div class="custom-block-title">训练数据生成方法</div><ul><li>目的：解决数据稀疏问题。</li><li>核心：全自动数据生成及验证pipeline，保证规模、多样性及正确性。</li><li><strong>tool中心任务</strong>： <ul><li>强调<code>必须使用工具才能解决问题</code></li><li>旨在训练agent学习 <mark>何时、有效协同使用工具</mark>。</li></ul></li><li><strong>推理密集型任务</strong>： <ul><li>数学代码：利用估计解决逻辑推理和算法问题</li><li><strong>高难度搜索</strong>：上下文约束下进行<strong>迭代式搜索</strong>、<strong>信息综合</strong>和<strong>推理</strong></li></ul></li></ul></div><img src="https://picx.zhimg.com/v2-b0cec71294ec51a7b957b0a681a42977_1440w.jpg" style="display:block;margin:auto;" width="80%">',23)),l("div",Q,[i[39]||(i[39]=l("div",{class:"custom-block-title"},"稳健RL训练",-1)),i[40]||(i[40]=l("p",null,[l("code",null,"Reinforce算法"),t("+"),l("code",null,"关键策略"),t("来保证训练稳定性")],-1)),l("ul",null,[i[36]||(i[36]=l("li",null,"严格的on-policy训练：",-1)),i[37]||(i[37]=l("li",null,"负样本控制：负样本降低模型输出概率可能导致熵崩溃，策略丢弃部分负样本，使模型能在长周期训练。",-1)),i[38]||(i[38]=l("li",null,"结果导向的奖励机制：格式奖励+正确性奖励。",-1)),l("li",null,[i[34]||(i[34]=t("效率激励：使用奖励衰减因子")),l("mjx-container",T,[(e(),s("svg",k,i[32]||(i[32]=[l("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[l("g",{"data-mml-node":"math"},[l("g",{"data-mml-node":"mi"},[l("path",{"data-c":"1D6FE",d:"M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z",style:{"stroke-width":"3"}})])])],-1)]))),i[33]||(i[33]=l("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[l("mi",null,"γ")])],-1))]),i[35]||(i[35]=t("，鼓励模型探索短且高效的路径。"))])])]),i[43]||(i[43]=o('<img src="https://pica.zhimg.com/v2-29530c032e1f26018ba54a1fa109018a_1440w.jpg" style="display:block;margin:auto;" width="80%"><div class="custom-block tip"><div class="custom-block-title">高效上下文管理</div><ul><li>背景：若无有效管理，一般10次agent迭代就可能超出上下文限制。</li><li>上下文管理机制：允许模型保留关键信息、丢弃不必要的稳定。</li><li>效果：单个rollout能扩展到50次迭代，模型迭代次数增加30%，能获取更多信息并提高性能。</li></ul></div><div class="custom-block tip"><div class="custom-block-title">Infra</div><p>解决RL训练中的<code>效率和稳定性难题</code></p><ul><li>🚀全异步rollout <ul><li>采用<code>服务器架构</code>，<strong>并行调度</strong><code>rollout</code>、<code>环境交互</code>和<code>奖励计算</code>；</li><li>消除资源等待，效率远超同步系统🐮。</li></ul></li><li>回合级部分rollout <ul><li>针对少数大量回合的长尾任务，设计部分rollout机制：<mark>超出时间放入缓冲区，后续迭代中使用新模型权重执行剩余的回合</mark>。</li><li>带来至少1.5倍rollout加速</li></ul></li><li>鲁棒的沙盒环境 <ul><li>保证隔离型、消除容器间开销，基于kubernetes混合云架构，高可用和容错性。</li></ul></li></ul></div><img src="https://pic2.zhimg.com/v2-70bbe09715a9a78dc3594cbcf02c61e1_1440w.jpg" style="display:block;margin:auto;" width="80%"><p><strong>✍️实验设置</strong></p><div class="custom-block tip"><div class="custom-block-title">实验配置</div></div><p><strong>🍑关键结果</strong></p><div class="custom-block caution"><div class="custom-block-title">关键结果</div><ul><li>Kimi-Researcher <code>通过端到端RL学习涌现出高级agent能力</code>。2个case <ul><li>🐮<strong>处理信息冲突和自我修正</strong>：多个信息源冲突时，迭代假设、自我验证来解决不一致性问题。</li><li><strong>审慎严谨的交叉验证</strong>：简单问题也很严谨，主动额外搜索和交叉验证，而非轻率回答。</li></ul></li><li>证明端到端agent-rl是一条路，涌现出复杂推理修正能力。</li></ul></div><p><strong>⛳未来方向</strong></p><div class="custom-block note"><div class="custom-block-title">未来方向</div><ul><li>更多工具和领域扩展能力</li><li>优化底层RL技术设施和算法</li></ul></div><h2 id="_2501-kimi-k1-5-多模态推理-scaling-reinforcement-learning-with-llms" tabindex="-1">(2501) Kimi k1.5(多模态推理): Scaling Reinforcement Learning with LLMs <a class="header-anchor" href="#_2501-kimi-k1-5-多模态推理-scaling-reinforcement-learning-with-llms" aria-label="Permalink to &quot;(2501) Kimi k1.5(多模态推理): Scaling Reinforcement Learning with LLMs&quot;">​</a></h2><blockquote><ul><li><a href="https://www.alphaxiv.org/overview/2501.12599v4" target="_blank" rel="noreferrer">paper</a>, <a href="https://www.zhihu.com/question/10114790245/answer/83988158333" target="_blank" rel="noreferrer">如何评价 Kimi 发布的多模态推理模型 k1.5？</a>, <a href="https://zhuanlan.zhihu.com/p/19774180151" target="_blank" rel="noreferrer">解读Kimi1.5技术报告</a>,</li><li><a href="https://zhuanlan.zhihu.com/p/19946557325" target="_blank" rel="noreferrer">一文全面揭秘Kimi 1.5最新推理模型背后的技术</a>, <a href="https://zhuanlan.zhihu.com/p/1927618790872560644" target="_blank" rel="noreferrer">Kimi 系列技术报告(K1.5+K2)解读</a></li></ul></blockquote><div class="custom-block important"><div class="custom-block-title">K1.5 多模态推理模型</div><ul><li>RL Prompt数据构建标准；通用预训练、通用SFT、LongCot SFT、RL训练这 四阶段预训练方法。</li><li>PartialRollouts长上下文扩展技术， 改进的策略优化方法，long2short方法，简洁的infra。</li></ul></div><p><strong>❓问题背景</strong></p><div class="custom-block warning"><div class="custom-block-title">问题背景</div><ul><li><strong>传统预训练方法</strong>(<code>NTP, Scaling Law</code>) ：<strong>受限于高质量预训练数据数量</strong>，难以进一步提升。⚠️</li><li><strong>RL方法</strong>：能通过<code>环境交互</code>和<code>奖励信号</code>来<strong>生成自己的训练数据，摆脱静态数据依赖</strong>。</li><li>但之前的RL工作缺乏各基准都顶级的LLM，<mark>如何设计有效且可扩展的RL仍是一个挑战</mark>。</li></ul></div><p><strong>📕核心方法</strong></p><div class="custom-block tip"><div class="custom-block-title">RL Prompt 数据构建的关键</div><ul><li>质量和多样性对RL有效性很重要，能降低reward hacking和肤浅模式过拟合的风险。</li><li>🔑<strong>三大黄金标准</strong><ul><li><strong>广泛覆盖</strong>：<mark>防止模型偏科，确保广泛适用性</mark>。如<code>STEM(科学/技术/工程/数学)</code>、<code>代码</code>和<code>通用推理</code>等。</li><li><strong>难度均衡</strong>：<mark>循序渐进，防止模型畏难或自满</mark>，<strong>避免对特定复杂问题过拟合</strong>。 覆盖<code>简单、中等、困难</code>。</li><li>⭐<strong>准确的评估(最关键)</strong>：需要<code>客观可靠的评估</code>，<mark>需要真正理解</mark>而非蒙对获得奖励，<mark>避免作弊RewardHacking</mark>。</li></ul></li><li>🐮<strong>难度均衡妙招：模型自行判断</strong><ul><li><mark>让模型自己去判断难易程度</mark>，<strong>同一个问题做10次，看它能做对几次</strong>。</li><li>🐯成功率低的是难题，高的是简单题。</li></ul></li><li>🐼<strong>Reward Hacking应对方法</strong><ul><li>定义：模型找到获得奖励的捷径，但这捷径并不是真正学会了技能。</li><li>背景：数学题猜测也可能作弊，比如1。</li><li>方法： <ul><li>💥<mark>去掉选择、判断等容易蒙对的题型，强制生成式回答</mark>。</li><li>🌟<mark>识别去掉模型易于破解的题目</mark>。<strong>不思考盲猜8次，如果都能猜对则去掉</strong>。</li></ul></li></ul></li></ul></div><p>四阶段预训练方法</p><div class="custom-block important"><div class="custom-block-title">Kimi 1.5 4阶段训练方法</div><p>🐶<strong>阶段1：预训练</strong></p><ul><li>目的：让模型掌握世界知识、语言规律、图文关联能力。</li><li>方法：在巨大高质量多模态语料库上训练，</li><li>数据规模： <ul><li>文本：覆盖英语/中文/代码/数学推理/知识等多领域。</li><li>多模态：Captin/图文混合/OCR/知识/QA等数据，让模型理解图像。</li></ul></li><li>训练阶段 <ul><li>Vision-Language预训练：vision tower在独立训练后逐渐和LLM集成</li><li><strong>Cooldown阶段</strong>：使用<mark>高质量精选和合成数据</mark>增强推理能力</li><li><strong>Long-Context激活阶段</strong>：<mark>上下文从4k逐步扩展至128k</mark></li></ul></li></ul><p>🐱<strong>阶段2：通用任务微调</strong></p><ul><li><p>目的：预训练模型学会指令遵循。</p></li><li><p><strong>数据规模</strong></p><ul><li><strong>100w文本数据</strong>(50wQA/20w代码/20w数学科学/5k创意写作/2w长文本)，</li><li><strong>100w图文数据</strong>(图表/QA/对话/编码/推理等)。</li></ul></li><li><p><strong>数据方法</strong></p><ul><li>非推理任务(问答写作等)：人工种子数据集-&gt;训练种子模型-&gt;收集提示-&gt;每个提示生成多个回答-&gt;人工答案排序-&gt;最好答案精修</li><li><strong>推理任务(数学代码等)</strong>：<mark>基于rule+奖励模型，利用拒绝采样来构建数据</mark>。</li></ul></li></ul><p>🐸 <strong>阶段3：LongCot微调</strong></p><ul><li>数据构建： <ul><li>基于<strong>RLPrompt集合</strong>利用PE工程构建一个<strong>小、高质量、针对文本和图像</strong>的的warmup<strong>数据集</strong>。</li><li>数据包含人类认知，如规划、评估、反思、探索等。</li></ul></li><li>轻量微调：使用数据集做轻量SFT。</li><li>效果：生成详尽、逻辑更连贯、推理任务效果提升等。</li></ul><p>🐻<strong>阶段4：RL强化学习</strong>(最核心内容) ⭐</p><ul><li></li></ul></div><div class="custom-block important"><div class="custom-block-title">RL 核心</div><p>🔑<strong>1、长上下文扩展(重点)</strong></p><ul><li>RL上下文扩展至128k，更长上下文能考虑更多，提升推理准确性和深度。</li><li>🐯<strong>​Partial Rollouts 解决计算成本问题</strong>👍 <ul><li>思想：<mark>把长轨迹生成分割成多个迭代步骤</mark>，避免一次性生成整个轨迹，提高训练效率和节省资源</li><li><strong>固定输出rollout tokens预算(如500)</strong>，每次只生成部分轨迹</li><li>把<strong>生成的中间轨迹和模型状态</strong>保存到<code>relay buffer</code>中</li><li>从replay buffer中<mark>读取中间轨迹继续生成新轨迹，多次迭代完成轨迹生成</mark></li><li>选择性计算loss：可以当前片段或整个轨迹的loss，具体策略看实际情况</li></ul></li></ul><img src="https://pic3.zhimg.com/v2-23515904d3f6a4c32ff79c65ac0ef19e_1440w.jpg" style="display:block;margin:auto;" width="80%"><p>🔑<strong>2、改进的策略优化(重点)</strong></p><ul><li>训练算法：<code>Online Policy Mirror Descent</code>的变体</li><li>🧠核心思想（待详细看一下）： <ul><li>每次迭代使用当前模型作为参考模型，优化相对熵正则化的策略优化问题，</li><li>通过正则化技术避免模型的推理过程偏离目标。</li><li>设计合适的奖励机制和梯度计算方法，使模型逐步优化推理路径解决复杂问题。</li></ul></li><li>改进手段：<strong>长度惩罚、有效采样策略、训练数据优化</strong>等。</li><li><strong>长度惩罚</strong>：避免过度思考且保证模型训练效率，采用渐进长度惩罚策略，缓解初期训练慢速问题、</li><li>有效采样：<mark>课程采样和优先采样策略</mark>，以利用问题难度标签和成功率来提高训练效率。</li></ul><p><strong>3、简洁的RL Infra(重点)</strong></p><ul><li>上下文足够长时，模型可以在上下文中进行隐式规划和搜索，无需依赖外部复杂组件(MCTS/价值网络等)。</li><li>迭代同步RL框架、Partial Rollout等技术。</li><li>系统通过中央主控、rollout工作人员、训练工作人员、奖励模型等组件协同工作。(见上图)</li></ul><p><strong>4、多模态训练</strong></p></div><div class="custom-block important"><div class="custom-block-title">优化和部署相关</div><p>🔑<strong>1、longt2short方法(重点)</strong></p><ul><li>目的：long转为short cot，性能接近longcot。</li><li>主要方法 <ul><li><strong>模型合并</strong>：longcot和shortcot<mark>模型权重平均</mark>，得到新模型。</li><li><strong>最短拒绝采样</strong>：longcot生成多个回答，<mark>选择最短且正确</mark>的对shortcot模型做监督微调</li><li><strong>DPO</strong>：类似最短拒绝采用，<mark>把最短且正确的答案作为正样本</mark>，其余作为负样本做DPO训练。</li><li>🐮🍺<strong>long2short RL</strong>：标准RL之后，把性能和token最平衡的模型作为基础模型，接着做long2short rl训练。使用长度惩罚，减少不必要回答。</li></ul></li><li>Long2short RL效果最好，token少、效果好。</li></ul><p><strong>2、混合部署训练和推理(infra)</strong></p><ul><li>混合部署策略：把训练和推理任务集成在一起，实现更高效资源利用和动态扩展能力</li></ul><img src="https://picx.zhimg.com/v2-5abc5c7ef491c47ba64cdda1893d6cdf_1440w.jpg" style="display:block;margin:auto;" width="80%"></div><p><strong>✍️实验设置</strong></p><div class="custom-block tip"><div class="custom-block-title">实验配置</div></div><p><strong>🍑关键结果</strong></p><div class="custom-block caution"><div class="custom-block-title">关键结果</div><ul><li>LongCot能力有惊喜：六项里有四项(AIME/Math500等)超过o1。</li><li>ShortCot也不错，和o1旗鼓相当。</li></ul></div><img src="https://picx.zhimg.com/80/v2-07917d821c10a2a8a2b3fc9a03628832_1440w.webp?source=1def8aca" style="display:block;margin:auto;" width="80%"><img src="https://picx.zhimg.com/80/v2-6207e12c67ac8974417f9d0703f5121f_1440w.webp?source=1def8aca" style="display:block;margin:auto;" width="80%"><p><strong>⛳未来方向</strong></p><div class="custom-block note"><div class="custom-block-title">未来方向</div><ul><li>接着干推理模型，可能是短期的唯一方向? <ul><li>为什么？ <ul><li>高情商：充分发挥大模型思维能力。</li><li>低情商：高质量数据用完了，scaling law暂时走不下去，得转换方向</li></ul></li></ul></li><li>提升longcot RL的效率和可扩展性：进一步优化partial rollout。</li><li>改进信用分配和减少过度思考</li><li>迭代式long2short训练：long2short和long-rl结合起来训练，在预算范围内，进一步提升模型效果。</li></ul></div>',29))])}const R=r(n,[["render",h]]);export{y as __pageData,R as default};

import{_ as l,c as e,o as a,ah as t}from"./chunks/framework.CvbyeFFO.js";const p=JSON.parse('{"title":"Gemini 系列","description":"","frontmatter":{"title":"Gemini 系列","date":"2025-12-06T11:53:07.000Z","create":"2025-12-06T11:53:07.000Z","categories":[],"tags":[]},"headers":[],"relativePath":"posts/llm/industry/mainllm/08-gemini-series.md","filePath":"posts/llm/industry/mainllm/08-gemini-series.md","lastUpdated":null}'),o={name:"posts/llm/industry/mainllm/08-gemini-series.md"};function s(n,i,r,d,c,m){return a(),e("div",null,i[0]||(i[0]=[t('<h2 id="概览" tabindex="-1">概览 <a class="header-anchor" href="#概览" aria-label="Permalink to &quot;概览&quot;">​</a></h2><div class="custom-block info"><div class="custom-block-title">信息</div><p>Google 的 PaLM–Gemini 谱系从早期的 <strong>PaLM</strong> 开始演进，</p><ul><li>PaLM 采用稠密（Dense）的仅解码器（Decoder-only）架构，利用 Pathways 系统进行扩展，并使用了 SwiGLU 和并行的注意力/前馈网络（Attention/FFN）机制。</li><li>随后，该系列转向以效率为导向的重新设计，引入了多语言预训练和 UL2 风格的去噪任务（<strong>PaLM 2</strong>）。</li><li>最终，发展为具备稀疏专家路由（Sparse Expert Routing）和内存高效长上下文注意力机制的原生多模态模型（<strong>Gemini</strong>）。</li><li>纵观几代模型，该系列通过扩展序列建模能力和集成工具使用能力，巩固了在程序合成、多语言编辑和仓库级推理方面的代码智能。</li></ul></div><h2 id="gemini-2-2-5" tabindex="-1">Gemini 2&amp;2.5 <a class="header-anchor" href="#gemini-2-2-5" aria-label="Permalink to &quot;Gemini 2&amp;2.5&quot;">​</a></h2><div class="custom-block tip"><div class="custom-block-title">提示</div><p><strong>核心思想</strong></p><ul><li>强调效率、推理和代码。</li><li>自然语言+代码数据预训练，在修复、翻译、合成上做微调。</li></ul><p><strong>Gemini 2.0 Flash</strong></p><ul><li>优化长上下文的注意力和内存，同时保留了多模态能力</li></ul><p><strong>Gemini 2.5</strong></p><ul><li>扩展上下文长度、并行能力、Agentic 能力 (ToolUse, 迭代推理等)</li></ul><p><strong>关键结果</strong></p><ul><li>在Natural2Code、Bird-SQL、LiveCodeBench、Aider Polyglot 和 SWE-Bench Verified 上取得不错结果。</li></ul></div><h2 id="gemini-1-1-5" tabindex="-1">Gemini 1&amp;1.5 <a class="header-anchor" href="#gemini-1-1-5" aria-label="Permalink to &quot;Gemini 1&amp;1.5&quot;">​</a></h2><div class="custom-block tip"><div class="custom-block-title">Gemini 1&amp;1.5 概览</div><p><strong>核心技术</strong></p><ul><li>在Pathways架构下，引入原生多模态能力(文本/语音/视觉)</li><li>Gemini1.5：Sparse MoE 架构，效率优化、百万长上下文</li></ul><p><strong>关键结果</strong></p><ul><li>Repo-Level理解、长代码推理等。Gemini1.5比Gemini1在HumanEval/Natrual2Code等Bench上效果更好。</li></ul></div><h2 id="palm-2" tabindex="-1">PaLM 2 <a class="header-anchor" href="#palm-2" aria-label="Permalink to &quot;PaLM 2&quot;">​</a></h2><div class="custom-block tip"><div class="custom-block-title">PaLM2 概览</div><p><strong>核心技术</strong></p><ul><li>通过<code>多语言预训练</code>和 <code>UL2风格的去噪目标</code>，优化了扩展性与数据之间的平衡</li></ul><p><strong>关键结果</strong></p><ul><li>在计算效率更高的尺寸下提供了更强的结果。</li><li>PaLM2-S代码变体：在<code>多语言代码</code>上训练， <ul><li>在HumanEval、MBPP、ARCADE 和 BabelCode 上效果具有竞争力</li></ul></li></ul></div><h2 id="palm" tabindex="-1">PaLM <a class="header-anchor" href="#palm" aria-label="Permalink to &quot;PaLM&quot;">​</a></h2><div class="custom-block tip"><div class="custom-block-title">PaLM 概览</div><p><strong>核心技术</strong></p><ul><li>Decoder-Only，Dense Model。</li><li>使用SwiGLU 和<code>并行注意力/FFN</code> 来提升扩展性。</li><li>在<code>自然语言</code>和<code>大量代码</code>的混合数据上进行训练。</li><li>PaLM-Coder：增强了生成、翻译、修复等能力。</li></ul></div>',10)]))}const g=l(o,[["render",s]]);export{p as __pageData,g as default};

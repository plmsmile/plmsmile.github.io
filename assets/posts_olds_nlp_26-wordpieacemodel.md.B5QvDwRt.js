import{_ as i,c as o,o as a,ah as l,j as t,a as r}from"./chunks/framework.CvbyeFFO.js";const w=JSON.parse('{"title":"Wordpiece模型","description":"","frontmatter":{"title":"Wordpiece模型","date":"2017-10-19T14:41:00.000Z","tags":["WordPiece","语音搜索","语音识别"],"categories":["论文笔记"]},"headers":[],"relativePath":"posts/olds/nlp/26-wordpieacemodel.md","filePath":"posts/olds/nlp/26-wordpieacemodel.md","lastUpdated":null}'),s={name:"posts/olds/nlp/26-wordpieacemodel.md"},n={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},d={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"2.011ex",height:"1.545ex",role:"img",focusable:"false",viewBox:"0 -683 889 683","aria-hidden":"true"},p={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Q={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"6.603ex",height:"2.452ex",role:"img",focusable:"false",viewBox:"0 -833.9 2918.6 1083.9","aria-hidden":"true"};function T(h,e,m,c,u,g){return a(),o("div",null,[e[7]||(e[7]=l('<blockquote><p>WordPiece模型，BERT也有用到。<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/37842.pdf" target="_blank" rel="noreferrer">Japanese and Korean Voice Search</a> 看了半天才发现不稳啊。</p></blockquote><h1 id="背景知识" tabindex="-1">背景知识 <a class="header-anchor" href="#背景知识" aria-label="Permalink to &quot;背景知识&quot;">​</a></h1><h2 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;摘要&quot;">​</a></h2><p>这篇文章主要讲了构建基于日语和法语的语音搜索系统遇到的困难，并且提出了一些解决的方法。主要是下面几个方面：</p><ul><li>处理无限词汇表的技术</li><li>在语言模型和词典的书面语中，完全建模并且避免系统复杂度</li><li>如何去构建词典、语言和声学模型</li></ul><p>展示了由于模糊不清，多个script语言的打分结果的困难性。这些语言语音搜索的发展，大大简化了构建一门新的语言的语音搜索系统的最初的处理过程，这些很多都成为了语言搜索国际化的默认过程。</p><h2 id="简介" tabindex="-1">简介 <a class="header-anchor" href="#简介" aria-label="Permalink to &quot;简介&quot;">​</a></h2><p>语音搜索通过手机就可以访问到互联网，这对于一些不好输入字符的语言来说，非常有用。尽管从基础技术来讲，语音识别的技术是在不同的语言之间是非常相似的，但是许多亚洲语言面临的问题，如果只是用传统的英语的方法去对待，这根本很难解决嘛。许多亚洲语言都有非常大的字符库。这让发音词典就很复杂。在解码的时候，由于很多同音异义词汇，解码也会很复杂。基本字符集里面的很多字符都会以多种形式存在，还要数字也会有多种形式，在某些情况下，这都需要适当的标准化。</p><p>很多亚洲语言句子中没有空格去分割单词。需要使用<code>segmenters</code>去产生一些<code>词单元</code>。 这些词单元会在词典和语言模型中使用，词单元之间可能需要添加或者删除空白字符。我们开发了一个纯数据驱动的sementers，可以使用任何语言，不需要修改。</p><p>还有就是如何去处理英文中的许多词汇，比如URL、数字、日期、姓名、邮件、缩写词汇、标点符号和其它特殊词汇等等。</p><h2 id="语音数据收集" tabindex="-1">语音数据收集 <a class="header-anchor" href="#语音数据收集" aria-label="Permalink to &quot;语音数据收集&quot;">​</a></h2><p>公告开放的数据集很难用作商用，有很多限制，所以自己收集数据集。通过手机，从不同的地区、年龄、方言等等，收集数据。一般是尽可能使用这些原始的数据并且建模，而不是转化为书面的数据或者有利于英语的数据。</p><h1 id="分词和词库" tabindex="-1">分词和词库 <a class="header-anchor" href="#分词和词库" aria-label="Permalink to &quot;分词和词库&quot;">​</a></h1><p>提出一种<code>WordPieceModel</code>去解决OOV(out-of-vocabulary)的问题。WordPieaceModel通过一种贪心算法，自动地、增量地从大量文本中学得单词单元（word units），一般数量是200k。算法可以，不关注语义，而去最大化训练数据语言模型的可能性，这也是解码过程中的度量标准。该算法可以有效地自动学习词库。</p><h2 id="wordpiecemodel算法步骤" tabindex="-1">WordPieceModel算法步骤 <a class="header-anchor" href="#wordpiecemodel算法步骤" aria-label="Permalink to &quot;WordPieceModel算法步骤&quot;">​</a></h2><p><strong>1 初始化词库</strong></p><p>给词库添加基本的所有的unicode字符和ascii字符。日语是22000，韩语是11000。</p><p><strong>2 建立模型</strong></p><p>基于训练数据，建立模型，使用初始化好的词库。</p><p><strong>3 生成新单元</strong></p><p>从词库中选择两个词单元组成新的词单元，加入到词库中。组成的新词要使模型的似然函数likelyhood最大。</p><p><strong>4 继续加或者停止</strong></p><p>如果达到词库数量的上限，或者似然函数增加很小，那么就停止，否则就继续2步，继续合并添加。</p><h2 id="算法优化" tabindex="-1">算法优化 <a class="header-anchor" href="#算法优化" aria-label="Permalink to &quot;算法优化&quot;">​</a></h2>',24)),t("p",null,[e[4]||(e[4]=r("你也发现了，计算所有可能的Pair这样会非常非常耗费时间。如果当前词库数量是")),t("mjx-container",n,[(a(),o("svg",d,e[0]||(e[0]=[t("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[t("g",{"data-mml-node":"math"},[t("g",{"data-mml-node":"mi"},[t("path",{"data-c":"1D43E",d:"M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z",style:{"stroke-width":"3"}})])])],-1)]))),e[1]||(e[1]=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mi",null,"K")])],-1))]),e[5]||(e[5]=r("，那么每次迭代计算的复杂度是")),t("mjx-container",p,[(a(),o("svg",Q,e[2]||(e[2]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(974,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2529.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)]))),e[3]||(e[3]=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mi",null,"O"),t("mo",{stretchy:"false"},"("),t("msup",null,[t("mi",null,"K"),t("mn",null,"2")]),t("mo",{stretchy:"false"},")")])],-1))]),e[6]||(e[6]=r(" 。有下面3个步骤可以进行优化"))]),e[8]||(e[8]=l('<ul><li>选择组合新的单元时，只测试训练数据中有的单元。</li><li>只测试有很大机会成为最好的Pair，例如high priors</li><li>把一些不会影响到彼此的group pairs组合到一起，作为一个单一的迭代过程</li><li>only modify the language model counts for the affected entries （不懂什么意思）</li></ul><p>使用这些加速算法，我们可以在一个机器上，几个小时以内，从频率加权查询列表中，构建一个200k的词库。</p><p>得到wordpiece词库之后，可以用来语言建模，做词典和解码。分割算法，构建了以基础字符开始的Pairs的逆二叉树。本身已经不需要动态规划或者其他的搜索方法。因此在计算上非常有效。分开基本的字符，基于树从上到下，会在线性时间给出一个确定的分割信息，线性时间取决于句子的长度。大约只有4%的单词具有多个发音。如果添加太多的发音会影响性能，可能是因为在训练和解码时对齐过程期间的可能数太多了</p><h2 id="继续说明" tabindex="-1">继续说明 <a class="header-anchor" href="#继续说明" aria-label="Permalink to &quot;继续说明&quot;">​</a></h2><p>一般是句子没有空格的，但是有的时候却有空格，比如韩文，搜索关键字。线上系统没有办法去把这些有空格的word pieces组合在一起。这对于常见的词汇和短查询是没有影响的，因为它们已经组合成一个完整的word unit。但是对于一些例如空格出现在不该出现的地方等不常见的查询，就很烦恼了。</p><p>在解码的时候，加空格效率更高，采用下面的技术：</p><p>1 原始语言模型数据被用来&quot;as written&quot;，表示一些有空格一些没有空格。</p><p>2 WPM模型分割LM数据时，每个单元在前面或者后面遇到一个空格，那么就添加一个空格标记。单元有4种情况：两边都有空格，左边有，右边有，两边都没有。使用下划线标记</p><p>3 基于这个新词库构建LM和词典</p><p>4 解码时，根据模型会选择一个最佳路径，之前在哪些地方放了空格或者没有。为了输出显示，需要把空格全部移除。有3种情况，移除所有空格；移除两个空格用一个空格表示；移除一个空格。</p>',10))])}const f=i(s,[["render",T]]);export{w as __pageData,f as default};

import{_ as a,c as o,o as t,a2 as r}from"./chunks/framework.CJy6NSJ1.js";const m=JSON.parse('{"title":"ELMo：Deep Contextualized Word Representations","description":"","frontmatter":{"title":"ELMo：Deep Contextualized Word Representations","date":"2018-12-11T15:55:47.000Z","tags":["ELMo","词向量","迁移学习","语言模型","LSTM"],"categories":["论文笔记"]},"headers":[],"relativePath":"posts/olds/nlp/50-elmo.md","filePath":"posts/olds/nlp/50-elmo.md","lastUpdated":null}'),n={name:"posts/olds/nlp/50-elmo.md"};function l(s,e,i,g,p,d){return t(),o("div",null,e[0]||(e[0]=[r('<blockquote><p>ACL2018 Best Paper, 性价比很高的效果提升方法。<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noreferrer">ELMo: Deep contextualized word representations</a></p></blockquote><img src="" style="display:block;margin:auto;" width="60%"><h1 id="背景" tabindex="-1">背景 <a class="header-anchor" href="#背景" aria-label="Permalink to &quot;背景&quot;">​</a></h1><h2 id="词向量的问题" tabindex="-1">词向量的问题 <a class="header-anchor" href="#词向量的问题" aria-label="Permalink to &quot;词向量的问题&quot;">​</a></h2><p><a href="https://plmsmile.github.io/2017/11/12/cs224n-notes1-word2vec/" target="_blank" rel="noreferrer">Word2vec</a>和<code>Glove</code>，可以学习到一些词汇在语义和语法上的信息。由于它是固定的，所以<strong>它无法根据上下文去表示一个词语</strong>，也无法解决一词多义的问题。</p><h2 id="相关研究" tabindex="-1">相关研究 <a class="header-anchor" href="#相关研究" aria-label="Permalink to &quot;相关研究&quot;">​</a></h2><p><strong>1. 学习subword的信息</strong></p><p><a href="https://aclweb.org/anthology/D16-1157" target="_blank" rel="noreferrer">CHARAGRAM: Embedding Words and Sentences via Character n-grams</a></p><p><strong>2. 为每种词义单独训练词向量</strong></p><p><strong>3. Context2vec</strong></p><p><a href="http://www.aclweb.org/anthology/K16-1006" target="_blank" rel="noreferrer">Learning Generic Context Embedding with Bidirectional LSTM</a></p><p><strong>4. CoVe 从机器翻译中学习词向量</strong></p><p><a href="https://arxiv.org/abs/1708.00107" target="_blank" rel="noreferrer">Learned in Translation: Contextualized Word Vectors</a></p><p><strong>5. 半监督的</strong></p><p><a href="https://arxiv.org/abs/1705.00108" target="_blank" rel="noreferrer">Semi-supervised sequence tagging with bidirectional language models</a></p><p>上述方法都得益于大规模数据集。其中<code>CoVe</code>非常火，但是受限制于双语语料，同时效果也没有<code>ELMo</code>好。</p><h1 id="elmo图解" tabindex="-1">ELMo图解 <a class="header-anchor" href="#elmo图解" aria-label="Permalink to &quot;ELMo图解&quot;">​</a></h1><h2 id="elmo的目的" tabindex="-1">ELMo的目的 <a class="header-anchor" href="#elmo的目的" aria-label="Permalink to &quot;ELMo的目的&quot;">​</a></h2><p>ELMo全称是<code>Embeddings from Language Model</code>。ELMo的特点是<strong>根据上下文来确定词向量</strong>。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/data_standard.png" style="display:block;margin:auto;" width="60%"><p>对于Glove来说，会选择<code>played</code>运动的意义。然而在ELMo中，却可以根据上下文来获得语义信息。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/elmo-know.png" style="display:block;margin:auto;" width="60%"><p>ELMo是一个<strong>2层BiLSTM的语言模型</strong>。<strong>把各个位置上的每层的输出线性组合起来</strong>便是最终的<code>ELMo 词向量</code>。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/elmo-word-embedding.png" style="display:block;margin:auto;" width="60%"><h2 id="语言模型" tabindex="-1">语言模型 <a class="header-anchor" href="#语言模型" aria-label="Permalink to &quot;语言模型&quot;">​</a></h2><p><strong>预测下一个词汇的任务</strong>，在大规模预料中进行训练，来理解语言，掌握一些语言模式。不是说能直接预测出下一个词是什么，而是正确的单词的概率远大于不会出现的单词。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/Bert-language-modeling.png" style="display:block;margin:auto;" width="50%"><h2 id="双向语言模型" tabindex="-1">双向语言模型 <a class="header-anchor" href="#双向语言模型" aria-label="Permalink to &quot;双向语言模型&quot;">​</a></h2><p>双向语言模型中，<strong>词汇对下一个词汇和前一个词汇都有感知</strong>。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/elmo-forward-backward-language-model-embedding.png" style="display:block;margin:auto;" width="60%"><h2 id="elmo词向量" tabindex="-1">ELMo词向量 <a class="header-anchor" href="#elmo词向量" aria-label="Permalink to &quot;ELMo词向量&quot;">​</a></h2><p><code>Deep Contextualized Vectors</code>由如下步骤构成：</p><ul><li>在每一层，<strong>把两个方向的隐状态拼接起来</strong>得到该层的隐状态</li><li>把<strong>初始词向量</strong>、<strong>第一层的隐状态</strong>、<strong>第二层的隐状态</strong>，<strong>线性组合</strong>起来得到最终的词向量</li></ul><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/elmo-embedding.png" style="display:block;margin:auto;" width="70%"><h2 id="elmo使用" tabindex="-1">ELMo使用 <a class="header-anchor" href="#elmo使用" aria-label="Permalink to &quot;ELMo使用&quot;">​</a></h2><p>把ELMo词向量和传统词向量拼接起来，作为新的词向量输入。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/elmo-use.png" style="display:block;margin:auto;" width="60%"><h1 id="elmo分析" tabindex="-1">ELMo分析 <a class="header-anchor" href="#elmo分析" aria-label="Permalink to &quot;ELMo分析&quot;">​</a></h1><h2 id="提升效果" tabindex="-1">提升效果 <a class="header-anchor" href="#提升效果" aria-label="Permalink to &quot;提升效果&quot;">​</a></h2><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/elmo-enhanced.png" style="display:block;margin:auto;" width="60%"><h2 id="语法和语义" tabindex="-1">语法和语义 <a class="header-anchor" href="#语法和语义" aria-label="Permalink to &quot;语法和语义&quot;">​</a></h2><p>低层偏向语法信息，高层偏向语义信息。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/semantics-syntatic.png" style="display:block;margin:auto;" width="60%"><h2 id="偏爱语法信息" tabindex="-1">偏爱语法信息 <a class="header-anchor" href="#偏爱语法信息" aria-label="Permalink to &quot;偏爱语法信息&quot;">​</a></h2><p>很多模型都<strong>偏爱低层的语法信息</strong>。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/layer-weights.png" style="display:block;margin:auto;" width="60%"><h2 id="小数据集提升" tabindex="-1">小数据集提升 <a class="header-anchor" href="#小数据集提升" aria-label="Permalink to &quot;小数据集提升&quot;">​</a></h2><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/elmo/snli-srl.png" style="display:block;margin:auto;" width="40%"><h1 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h1><ol><li>原始论文<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noreferrer">Deep contextualized word representations</a></li><li><a href="https://jalammar.github.io/illustrated-bert/" target="_blank" rel="noreferrer">illustrated-bert</a></li><li><a href="https://www.slideshare.net/shuntaroy/a-review-of-deep-contextualized-word-representations-peters-2018" target="_blank" rel="noreferrer">ELMo Slide</a></li></ol>',50)]))}const c=a(n,[["render",l]]);export{m as __pageData,c as default};

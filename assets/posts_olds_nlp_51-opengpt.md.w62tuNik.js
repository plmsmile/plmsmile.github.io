import{_ as a,c as r,o as t,a2 as o}from"./chunks/framework.CJy6NSJ1.js";const h=JSON.parse('{"title":"OpenAI GPT：Improving Language Understanding by Generative Pre-Training","description":"","frontmatter":{"title":"OpenAI GPT：Improving Language Understanding by Generative Pre-Training","date":"2018-12-13T22:44:24.000Z","tags":["transformer","语言模型","迁移学习"],"categories":["论文笔记"]},"headers":[],"relativePath":"posts/olds/nlp/51-opengpt.md","filePath":"posts/olds/nlp/51-opengpt.md","lastUpdated":null}'),n={name:"posts/olds/nlp/51-opengpt.md"};function i(l,e,s,p,g,d){return t(),r("div",null,e[0]||(e[0]=[o('<blockquote><p>使用Transformer(Decoder)预训练单向语言模型，再进行有监督数据进行特定任务finetune</p></blockquote><img src="" style="display:block;margin:auto;" width="60%"><h1 id="背景" tabindex="-1">背景 <a class="header-anchor" href="#背景" aria-label="Permalink to &quot;背景&quot;">​</a></h1><h2 id="问题提出" tabindex="-1">问题提出 <a class="header-anchor" href="#问题提出" aria-label="Permalink to &quot;问题提出&quot;">​</a></h2><p>NLP中，无标注语料很多，有标注的数据很少。</p><ul><li>很多任务不能从头开始训练（标注数据太少），需要<strong>减轻对标注数据的依赖</strong></li><li>在大规模无标注语料中预训练的语言模型可以提升很多效果</li><li>从无标注数据中学习一个<code>good representations</code>，很流行且有效果</li></ul><p>从无标注数据中学习到词级以上的意义的难点：</p><ul><li>没有一个有效的优化目标函数</li><li>对学习到的<code>representations</code>没有通用的有效的迁移方法</li></ul><h2 id="半监督学习" tabindex="-1">半监督学习 <a class="header-anchor" href="#半监督学习" aria-label="Permalink to &quot;半监督学习&quot;">​</a></h2><p>半监督是指从无监督数据中学习一些通用表示，再做轻微的有监督<code>finetune</code>到各种各样的特定任务中。</p><p><strong>1. 无监督特征表示</strong></p><p>利用大量的语料和语言模型任务，去学习到一个神经网络。作为后面模型的网络初始参数。</p><p>语言模型网络可以使用<a href="https://plmsmile.github.io/2017/10/18/rnn/" target="_blank" rel="noreferrer">LSTM</a>和<a href="https://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/" target="_blank" rel="noreferrer">Transformer</a>。<code>Transformer</code>可以解决LSTM的<code>长依赖问题</code>，具有<strong>更好的迁移能力</strong>。</p><p><strong>2. 有监督任务训练</strong></p><p>特定任务的监督数据去finetune初始的网络。一般需要根据任务类型加上输出层。</p><h2 id="相关研究" tabindex="-1">相关研究 <a class="header-anchor" href="#相关研究" aria-label="Permalink to &quot;相关研究&quot;">​</a></h2><p><strong>1. 语义研究</strong></p><p>词向量主要是迁移具有<code>词级别的信息</code>，但更需要其一些<code>词级别以上的语义信息</code>。主要有<code>phrase-level</code>和<code>sentence-level</code> embedding。</p><p><strong>2. 无监督预训练</strong></p><p>无监督预训练的目的在于为后续任务去<strong>初始化一个好的网络参数</strong>，而不需要去改变任务的目标。预训练主要是使用<code>语言模型任务</code>，Transformer比LSTM更好，更强的迁移能力和处理长依赖能力。</p><p><code>Ruder</code>大神说NLP的ImageNet时代已经来了，足以说明预训练的重要性。</p><p><strong>3. 辅助任务</strong></p><ul><li>加辅助特征(<a href="https://plmsmile.github.io/2018/12/11/50-elmo/" target="_blank" rel="noreferrer">ELMo</a>)：网络的参数需要重新学习</li><li>辅助训练目标--语言模型和特定任务一起训练：其实无监督预训练已经学习到了语言特征，无需辅助训练目标了</li></ul><h1 id="gpt模型" tabindex="-1">GPT模型 <a class="header-anchor" href="#gpt模型" aria-label="Permalink to &quot;GPT模型&quot;">​</a></h1><h2 id="预训练单向语言模型" tabindex="-1">预训练单向语言模型 <a class="header-anchor" href="#预训练单向语言模型" aria-label="Permalink to &quot;预训练单向语言模型&quot;">​</a></h2><p>采用的是<a href="https://plmsmile.github.io/2018/12/11/50-elmo/#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B" target="_blank" rel="noreferrer">单向语言模型</a>，预测下一个词语，采用的是<a href="https://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/#decoder" target="_blank" rel="noreferrer">Tansformer的Decoder</a>。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/openai-transformer-1.png" style="display:block;margin:auto;" width="60%"><p>在Decoder之外加上线性层去预测下一个单词，训练语言模型任务。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/openai-transformer-language-modeling.png" style="display:block;margin:auto;" width="60%"><h2 id="迁移到下游监督任务" tabindex="-1">迁移到下游监督任务 <a class="header-anchor" href="#迁移到下游监督任务" aria-label="Permalink to &quot;迁移到下游监督任务&quot;">​</a></h2><p>预训练的Tansformer已经具有处理语言的能力，再加上输出层，并且监督finetune，则可以达到一个不错的效果。如下</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/openai-transformer-sentence-classification.png" style="display:block;margin:auto;" width="60%"><h2 id="各种任务组织方式" tabindex="-1">各种任务组织方式 <a class="header-anchor" href="#各种任务组织方式" aria-label="Permalink to &quot;各种任务组织方式&quot;">​</a></h2><p>利用<a href="https://arxiv.org/abs/1509.06664" target="_blank" rel="noreferrer">traversal-style</a>方法，把结构化数据处理成一个序列。每个序列都有一个开始和结束符号，也有分解符号，都是随机初始化的。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/openai-input%20transformations2.png" style="display:block;margin:auto;" width="80%"><p><strong>分类任务</strong></p><p>开始符号 -- 文本 -- 结束符号</p><p><strong>文本蕴含</strong></p><p>开始 -- 前提 -- 分界 -- 假设 -- 结束</p><p><strong>相似性</strong></p><p>相似度计算与顺序无关，所以加了两个</p><p><strong>问答和常识推理</strong></p><p>文章和问题组成上下文，与每一个可能的答案作为拼接。一共有多组</p><h1 id="分析" tabindex="-1">分析 <a class="header-anchor" href="#分析" aria-label="Permalink to &quot;分析&quot;">​</a></h1><h2 id="效果" tabindex="-1">效果 <a class="header-anchor" href="#效果" aria-label="Permalink to &quot;效果&quot;">​</a></h2><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/result-1.png" style="display:block;margin:auto;" width="60%"><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/result-2.png" style="display:block;margin:auto;" width="60%"><h2 id="参数分析" tabindex="-1">参数分析 <a class="header-anchor" href="#参数分析" aria-label="Permalink to &quot;参数分析&quot;">​</a></h2><ul><li>右图：<code>zero-shot</code>上Transformer的效果是比LSTM好的</li><li>左图：可以知道Transformer层数越多效果也越好</li></ul><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/parameter-analyze.png" style="display:block;margin:auto;" width="70%"><h1 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h1><ul><li>原始论文 <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noreferrer">Improving Language Understanding by Generative Pre-Training</a></li><li><a href="https://jalammar.github.io/illustrated-bert/" target="_blank" rel="noreferrer">Illustrated-BERT</a></li><li><a href="https://github.com/huggingface/pytorch-openai-transformer-lm" target="_blank" rel="noreferrer">pytorch-openai-transformer-lm</a></li><li><a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noreferrer">openai language-unsupervised</a></li></ul>',52)]))}const m=a(n,[["render",i]]);export{h as __pageData,m as default};

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PLM&#39;s Notes</title>
  
  <subtitle>好好学习，天天笔记</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://plmsmile.github.io/"/>
  <updated>2018-12-11T09:28:44.337Z</updated>
  <id>http://plmsmile.github.io/</id>
  
  <author>
    <name>PLM</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ELMo Deep Contextualized Word Representations</title>
    <link href="http://plmsmile.github.io/2018/12/11/50-elmo/"/>
    <id>http://plmsmile.github.io/2018/12/11/50-elmo/</id>
    <published>2018-12-11T07:55:47.000Z</published>
    <updated>2018-12-11T09:28:44.337Z</updated>
    
    <content type="html"><![CDATA[<p>ACL2018 Best Paper, 带来革命性意义和效果的方法。<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">ELMo: Deep contextualized word representations</a><a id="more"></a></p><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="简介">简介</h1><h2 id="传统词向量的问题">传统词向量的问题</h2><p><code>word2vec</code>和<code>glove</code>，是一个固定的词向量，无法根据上下文去表示一个词语，也无法解决一词多义的问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ACL2018 Best Paper, 带来革命性意义和效果的方法。&lt;a href=&quot;https://arxiv.org/abs/1802.05365&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ELMo: Deep contextualized word representations&lt;/a&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="ELMo" scheme="http://plmsmile.github.io/tags/ELMo/"/>
    
      <category term="词向量" scheme="http://plmsmile.github.io/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
      <category term="迁移学习" scheme="http://plmsmile.github.io/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语言模型" scheme="http://plmsmile.github.io/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="LSTM" scheme="http://plmsmile.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>QANet</title>
    <link href="http://plmsmile.github.io/2018/08/30/49-qanet/"/>
    <id>http://plmsmile.github.io/2018/08/30/49-qanet/</id>
    <published>2018-08-30T05:47:34.000Z</published>
    <updated>2018-11-25T08:30:09.283Z</updated>
    
    <content type="html"><![CDATA[<p>常年<a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">SQuAD榜单</a>排名第一的模型。<a href="https://arxiv.org/abs/1804.09541" target="_blank" rel="noopener">QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension</a><a id="more"></a></p><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="论文模型">论文模型</h1><h2 id="概览">概览</h2><p>机器阅读任务就不说了。这个模型的主要创新点在于</p><ul><li>卷积（可分离卷积）捕捉局部信息 （并行计算，加速）</li><li>Self-Attention捕捉全局信息</li><li>数据扩增</li></ul><p>一个<code>Encoder Block</code>主要是，其中<a href="https://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/#encoder">Transformer的EncoderBlock</a>只有Attention和FFN，没有卷积。</p><ul><li>Positional Encoder</li><li><a href="https://plmsmile.github.io/2018/04/11/38-convolution/#xception">可分离卷积</a> （多个，提高内存效率和泛化性）</li><li><a href="https://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/#self-attention">Self-Attention</a></li><li>前向神经网络</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/qanet/01-qanet.png" style="display:block; margin:auto" width="60%"></p><h2 id="input-embedding">Input Embedding</h2><p><strong>词向量</strong></p><p>Glove 300维，Fix；UNK词向量可以训练</p><p><strong>字向量</strong></p><ul><li>CNN字符向量，200维，可以训练</li><li>每个单词的字符最多16个</li><li>对单词的16个字符的向量过<strong>卷积</strong>（可分离卷积）</li><li>选择所有字符中<strong>最大的向量</strong>作为单词的最终字符向量</li></ul><p><strong>拼接</strong></p><p>对词向量和字符向量拼接起来，<span class="math inline">\([x_w;x_c] \in \mathcal{R}^{d_w+d_c}\)</span>。再过两层的<code>HighwayNetwork</code>，得到最终的单词向量表示。</p><h2 id="embedding-encoder">Embedding Encoder</h2><p>每一个Encoder块是由卷积、Self-Attention、全连接层组成，一共有4个Encoder块。输入向量维数是<span class="math inline">\(d=500(200+300)\)</span>，输出是<span class="math inline">\(d=128\)</span></p><ul><li>可分离卷积：<code>kernal size=7</code>，<code>d = 128</code>。变成128维向量</li><li>Self-Attention：8头注意力，<a href="https://plmsmile.github.io/2018/03/25/33-attention-summary/#%E9%94%AE%E5%80%BC%E5%AF%B9%E6%B3%A8%E6%84%8F%E5%8A%9B">键值对注意力</a></li><li>全连接：输出也是128</li><li>QANet:层归一化+残差连接：<span class="math inline">\(f(\rm{LayerNorm}(x)) + x\)</span></li><li>Transformer 是<code>Add&amp;Norm</code>，<span class="math inline">\(\rm{LayerNorm(f(x) +x)}\)</span></li></ul><h2 id="attention-layer">Attention Layer</h2><p>Context: <span class="math inline">\(n\)</span>个单词，Question：m个单词。<span class="math inline">\(C \in \mathcal{R}^{n\times d}\)</span>，<span class="math inline">\(Q \in \mathcal{R}^{m \times d}\)</span></p><p><strong>关联性矩阵</strong></p><p>采用的是BiDAF的计算策略： <span class="math display">\[S = f(q, c) = W_0 [q, c, q \odot c] \in \mathcal{R} ^{n \times m}\]</span> DCN： <span class="math inline">\(S = C \cdot Q^T \in \mathcal{R}^{n \times m}\)</span></p><p><strong>Context2Query Attention</strong></p><p>C2Q的attention weights，对行做softmax <span class="math display">\[A^Q = \rm{softmax}(S) \in \mathcal{R}^{n \times m}\]</span> C2Q <strong>Attention</strong>（Context） <span class="math display">\[S^C = A^Q \cdot Q \in \mathcal{R} ^{n \times d}\]</span> <strong>Query2Context Attention</strong></p><p>Q2C Attention weights，对列做Softmax <span class="math display">\[A^C = \rm{softmax}(S^T) \in \mathcal{R}^{m \times n}\]</span> Q2C Attention（Query） <span class="math display">\[S^Q = A^C \cdot C  \in \mathcal{R}^{m \times d}\]</span> Context的<strong>Coattention</strong>，参考自<a href="https://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/#coattention%E6%B7%B1%E5%B1%82%E7%90%86%E8%A7%A3">DCN的Coattention</a> <span class="math display">\[C^C = A^Q \cdot S^Q \in \mathcal{R}^{n \times d}\]</span> 最终得到两个对Context的编码</p><ul><li>普通Attention：<span class="math inline">\(A = S^C \in \mathcal{R}^{n \times d}\)</span></li><li>Coattention：<span class="math inline">\(B = C^C \in \mathcal{R}^{n \times d}\)</span></li></ul><h2 id="model-encoder">Model Encoder</h2><p>输入是3个关于Context的矩阵信息：</p><ul><li><strong>原始Context</strong>：<span class="math inline">\(C \in \mathcal{R}^{n\times d}\)</span></li><li>Context的<strong>Attention</strong>： <span class="math inline">\(A \in \mathcal{R}^{n\times d}\)</span></li><li>Context的<strong>Coattention</strong>：<span class="math inline">\(B \in \mathcal{R}^{n \times d}\)</span></li></ul><p>每个单词的编码信息为上面三个矩阵的一个拼接： <span class="math display">\[f(w) = [c, a, c \odot a, c \odot b]\]</span> 一个有7个Encoder-Block，每个Encoder-Block：2个卷积层、Self-Attention、FFN。其它参数和Embedding Encoder一样。</p><p>一共有3个Model-Encoder，共享所有参数。输出依次为<span class="math inline">\(M_0, M_1, M_2\)</span></p><h2 id="output-layer">Output Layer</h2><p>这一层是和特定任务相关的。输出各个位置作为开始和结束位置的概率： <span class="math display">\[p^1 = \rm{softmax}(W_1[M_0; M_1]), \quad p^2 = \rm{softmax}(W_1[M_0; M_2])\]</span> 目标函数 <span class="math display">\[L(\theta) = -\frac{1}{N} \sum_{i}^N [\log(p^1_{y_i^1}) + \log(p^2_{y_i^2})]\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;常年&lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SQuAD榜单&lt;/a&gt;排名第一的模型。&lt;a href=&quot;https://arxiv.org/abs/1804.09541&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension&lt;/a&gt;
    
    </summary>
    
      <category term="机器阅读理解" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    
    
      <category term="Self-Attention" scheme="http://plmsmile.github.io/tags/Self-Attention/"/>
    
      <category term="可分离卷积" scheme="http://plmsmile.github.io/tags/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/"/>
    <id>http://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/</id>
    <published>2018-08-29T07:41:32.000Z</published>
    <updated>2018-11-25T08:30:09.233Z</updated>
    
    <content type="html"><![CDATA[<p>大名鼎鼎的Transformer，<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a><a id="more"></a></p><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="transformer概览">Transformer概览</h1><h2 id="论文结构">论文结构</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/02-multi-head-attention.png" style="display:block; margin:auto" width="60%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/01-transformer.png" style="display:block; margin:auto" width="40%"></p><h2 id="总览结构">总览结构</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/30-transformer_resideual_layer_norm_3.png" style="display:block; margin:auto" width="80%"></p><h1 id="图解总览">图解总览</h1><p>其实也是一个<code>Encoder-Decoder</code>的翻译模型。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/03-the_transformer_3.png" style="display:block; margin:auto" width="70%"></p><p>由一个Encoders和一个Decoders组成。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/04-The_transformer_encoders_decoders.png" style="display:block; margin:auto" width="70%"></p><p>Encoders由多个Encoder块组成。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/05-The_transformer_encoder_decoder_stack.png" style="display:block; margin:auto" width="70%"></p><h1 id="encoder">Encoder</h1><h2 id="总体结构">总体结构</h2><p><strong>1个Encoder由Self-Attention和FFN组成</strong></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/06-Transformer_encoder.png" style="display:block; margin:auto" width="70%"></p><p><strong>一个Encoder的结果再给到下一个Encoder</strong></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/10-encoder_with_tensors_2.png" style="display:block; margin:auto" width="70%"></p><p><strong>Encoder-Decoder</strong></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/07-Transformer_decoder.png" style="display:block; margin:auto" width="70%"></p><h2 id="编码实例">编码实例</h2><p>对一个句子进行编码</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/08-embeddings.png" style="display:block; margin:auto" width="60%"></p><p><code>Self-Attention</code>会对每一个单词进行编码，得到对应的向量。<span class="math inline">\(\mathbf{x_1}, \mathbf{x_2}, \mathbf{x_3} \to \mathbf{z_1}, \mathbf{z_2}, \mathbf{z_3}\)</span>，再给到FFN，会得到一个Encoder的结果<span class="math inline">\(\mathbf{r_1}, \mathbf{r_2}, \mathbf{r_3}\)</span>， 再继续给到下一个Encoder</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/09-encoder_with_tensors.png" style="display:block; margin:auto" width="70%"></p><h1 id="attention">Attention</h1><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/10-encoder_with_tensors_2.png" style="display:block; margin:auto" width="70%"></p><h2 id="self-attention">Self-Attention</h2><p><strong>1. 乘以3个矩阵生成3个向量：Query、Key、Value</strong></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/12-transformer_self_attention_vectors.png" style="display:block; margin:auto" width="70%"></p><p><strong>2. 计算与每个位置的score</strong></p><p>编码一个单词时，会计算它与句子中其他单词的得分。会得到每个单词对于当前单词的关注程度。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/13-transformer_self_attention_score.png" style="display:block; margin:auto" width="70%"></p><p><strong>3. 归一化和softmax得到每个概率</strong></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/14-self-attention_softmax.png" style="display:block; margin:auto" width="70%"></p><p><strong>4. 依概率结合每个单词的向量</strong></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/15-self-attention-output.png" style="display:block; margin:auto" width="70%"></p><p><strong>Attention图示</strong></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/11-transformer_self-attention_visualization.png" style="display:block; margin:auto" width="60%"></p><h2 id="矩阵形式">矩阵形式</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/16-self-attention-matrix-calculation.png" style="display:block; margin:auto" width="60%"></p><p><strong>其实就是一个注意力矩阵公式</strong></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/17-self-attention-matrix-calculation-2.png" style="display:block; margin:auto" width="60%"></p><h2 id="多头注意力">多头注意力</h2><p>其实就是多个KV注意力。从两个方面提升了<code>Attention Layer</code>的优点</p><ul><li>让模型能够关注到句子中的各个不同位置</li><li>Attention Layer可以有多个不同的表示子空间<code>representation subspaces</code></li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/18-transformer_attention_heads_qkv.png" style="display:block; margin:auto" width="70%"></p><p><strong>多头注意力矩阵形式</strong></p><p>经过多头注意力映射，会生成多个注意力<span class="math inline">\(Z_0, Z_1, \cdots, Z_7\)</span>。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/19-transformer_attention_heads_z.png" style="display:block; margin:auto" width="60%"></p><p>把这些注意力头拼接起来，再乘以一个大矩阵，最终融合得到一个信息矩阵。会给到FFN进行计算。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/20-transformer_attention_heads_weight_matrix_o.png" style="display:block; margin:auto" width="70%"></p><h2 id="注意力总结">注意力总结</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/21-transformer_multi-headed_self-attention-recap.png" style="display:block; margin:auto" width="80%"></p><h2 id="attention图示">Attention图示</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/22-transformer_self-attention_visualization_2.png" style="display:block; margin:auto" width="40%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/23-transformer_self-attention_visualization_3.png" style="display:block; margin:auto" width="40%"></p><h1 id="前向神经网络">前向神经网络</h1><p><code>Position-wise Feed-Forward Network</code>，会对每个位置过两个线性层，其中使用<a href="https://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/#relu">ReLU</a>作为激活函数。 <span class="math display">\[\rm{FFN}(x) = \rm{Linear}(\rm{ReLU}(\rm{Linear}(x))) = \rm{max}(0, xW_1 + b_1)W_2 + b_2\]</span></p><h1 id="位置编码">位置编码</h1><p><strong>词向量+位置信息=带位置信息的词向量</strong> <span class="math display">\[\rm{PE}(pos, 2i) = \sin (\rm{pos} / 10000^{\frac{2i}{d}})\]</span></p><p><span class="math display">\[\rm{PE}(pos, \rm{2i+1}) = \cos (\rm{pos} / 10000^{\frac{2i}{d}})\]</span></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/24-transformer_positional_encoding_vectors.png" style="display:block; margin:auto" width="70%"></p><p><strong>示例</strong></p><p>再把sin和cos的两个值拼接起来，就得到如下图所示。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/26-transformer_positional_encoding_example.png" style="display:block; margin:auto" width="70%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/27-transformer_positional_encoding_large_example.png" style="display:block; margin:auto" width="60%"></p><h1 id="encoder-block">Encoder-Block</h1><h2 id="残差连接">残差连接</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/28-transformer_resideual_layer_norm.png" style="display:block; margin:auto" width="50%"></p><h2 id="层归一化">层归一化</h2><p><a href="https://plmsmile.github.io/2018/03/30/35-nerual-network-optim/#%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96">层归一化</a></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/29-transformer_resideual_layer_norm_2.png" style="display:block; margin:auto" width="50%"></p><h1 id="总览">总览</h1><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/30-transformer_resideual_layer_norm_3.png" style="display:block; margin:auto" width="80%"></p><h1 id="decoder">Decoder</h1><h2 id="单步">单步</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/31-transformer_decoding_1.gif" style="display:block; margin:auto" width="60%"></p><h2 id="多步">多步</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/32-transformer_decoding_2.gif" style="display:block; margin:auto" width="60%"></p><h2 id="linear-softmax">Linear-Softmax</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/33-transformer_decoder_output_softmax.png" style="display:block; margin:auto" width="50%"></p><h1 id="模型样例">模型样例</h1><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/34-vocabulary.png" style="display:block; margin:auto" width="60%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/35-one-hot-vocabulary-example.png" style="display:block; margin:auto" width="60%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/36-output_target_probability_distributions.png" style="display:block; margin:auto" width="60%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/transformer/37-output_trained_model_probability_distributions.png" style="display:block; margin:auto" width="60%"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大名鼎鼎的Transformer，&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Attention Is All You Need&lt;/a&gt;
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Self-Attention" scheme="http://plmsmile.github.io/tags/Self-Attention/"/>
    
      <category term="机器翻译" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
      <category term="Transformer" scheme="http://plmsmile.github.io/tags/Transformer/"/>
    
      <category term="多头注意力" scheme="http://plmsmile.github.io/tags/%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
      <category term="残差连接" scheme="http://plmsmile.github.io/tags/%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5/"/>
    
      <category term="层归一化" scheme="http://plmsmile.github.io/tags/%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    
      <category term="位置编码" scheme="http://plmsmile.github.io/tags/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>Bidirectional Attention Flow</title>
    <link href="http://plmsmile.github.io/2018/05/22/47-bidaf/"/>
    <id>http://plmsmile.github.io/2018/05/22/47-bidaf/</id>
    <published>2018-05-22T12:01:43.000Z</published>
    <updated>2018-11-25T08:30:09.193Z</updated>
    
    <content type="html"><![CDATA[<p>阅读理解中很经典很有名的BiDAF模型。<a id="more"></a></p><h1 id="模型">模型</h1><h2 id="问题定义">问题定义</h2><p><span class="math display">\[P = (w^P_1, w^P_2, \cdots, w^P_m)\]</span></p><p><span class="math display">\[Q = (w^Q_1,w^Q_2, \cdots, w^Q_n)\]</span></p><p><span class="math display">\[A = (w^A_1, w^A_2, \cdots, w^A_k)\]</span></p><p><span class="math display">\[p_{start} \quad p_{end}\]</span></p><ul><li>望燕</li><li>枝俏</li><li>索寒</li><li>盘峰</li><li>漫枫</li><li>照月、焕月、（<del>揽月</del>）</li><li>后阳</li><li>（蚍蜉 ）撼树</li><li>永诀</li><li>（龙岩 ）上杭</li><li>黔驴</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;阅读理解中很经典很有名的BiDAF模型。
    
    </summary>
    
      <category term="机器阅读理解" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    
    
      <category term="机器阅读理解" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    
      <category term="BiDAF" scheme="http://plmsmile.github.io/tags/BiDAF/"/>
    
  </entry>
  
  <entry>
    <title>R-Net (Gated Self-Matching Networks)</title>
    <link href="http://plmsmile.github.io/2018/05/15/46-rnet-selfmatch/"/>
    <id>http://plmsmile.github.io/2018/05/15/46-rnet-selfmatch/</id>
    <published>2018-05-15T02:46:49.000Z</published>
    <updated>2018-11-25T08:30:09.149Z</updated>
    
    <content type="html"><![CDATA[<p>微软亚研院和北大的阅读理解模型R-Net。<a id="more"></a></p><blockquote><ol style="list-style-type: decimal"><li>Gated Attention-based RNN 来获得question-aware passage representation，即编码P</li><li>Self-matching Attention来修正编码P，即P与自己做match，有效从全文中编码信息</li><li>Pointer Network预测开始和结束位置</li></ol></blockquote><p>论文地址：</p><ul><li><a href="http://www.aclweb.org/anthology/P17-1018" target="_blank" rel="noopener">Gated Self-Matching Networks for Reading Comprehension and Question Answering</a></li><li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf" target="_blank" rel="noopener">R-NET: MACHINE READING COMPREHENSION WITH SELF-MATCHING NETWORKS</a></li></ul><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="introduction">Introduction</h1><h2 id="经典模型">经典模型</h2><p><strong>1. Match-LSTM</strong></p><p><a href="https://plmsmile.github.io/2018/05/09/45-match-lstm/">Match-LSTM and Answer Pointer笔记</a></p><p><strong>2. Dynamic Coatteion Network</strong></p><p><a href="https://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/">DCN笔记</a>。<a href="https://plmsmile.github.io/2018/03/14/31-co-attention-vqa/">Coattention</a>同时处理P和Q，动态迭代预测答案的位置。</p><p><strong>3. Bi-Directional Attention Flow Network</strong></p><h2 id="本文模型概要">本文模型概要</h2><p><strong>1. BiRNN 分别编码P和Q</strong></p><p>分别编码Question和Passage</p><p><strong>2. gated matching layer 编码Q-Aware的Passage</strong></p><p><code>Gated Attention-based RNN</code>。在<a href="https://plmsmile.github.io/2018/05/09/45-match-lstm/#match-lstm">Match-LSTM</a>上添加了<strong>门机制</strong>。</p><ul><li>段落有多个部分，根据与Q的相关程度，分配重要性权值</li><li>忽略不重要的，强调重要的部分</li></ul><p><strong>3. self-matching layer</strong></p><p>再次从整个Passage中提取信息。它的<code>缺点</code>：</p><ul><li>RNN只能存储少部分上下文内容</li><li>一个候选答案不知道其他部分的线索</li></ul><p>解决方法：<strong>对P做self-match</strong>。使用Gated Attention-based RNN对P和P自己做match。</p><p><strong>4. pointer-network</strong></p><h1 id="模型">模型</h1><p>BiRNN，GARNN（P+Q），GARNN-Selfmatch（P+P），Pointer Network</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/papers/rnet.png" style="display:block; margin:auto" width="80%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/papers/gan-selfmatch.png" style="display:block; margin:auto" width="80%"></p><h2 id="birnn编码q和p">BiRNN编码Q和P</h2><p><span class="math inline">\(Q=\{w_t^Q\}_{t=1}^m\)</span>，<span class="math inline">\(P=\{w_t^P\}_{t=1}^n\)</span>。 P是n个单词，Q是m个单词。</p><p><strong>词向量和字符向量</strong></p><p><code>词向量</code>：<span class="math inline">\(\{e_t^Q\}_{t=1}^m\)</span>、<span class="math inline">\(\{e_t^P\}_{t=1}^n\)</span></p><p><code>字符向量</code>：<span class="math inline">\(\{c_t^Q\}_{t=1}^m\)</span>、<span class="math inline">\(\{c_t^P\}_{t=1}^n\)</span></p><p>字符向量，使用RNN，用每个单词的最后时刻的隐状态，作为字符向量。有助于处理OOV词汇。</p><p><strong>编码Question和Passage</strong> <span class="math display">\[\mathbf u_t^Q = \rm{BiRNN }(u_{t-1}^Q, [e_t^Q, c_t^Q])\]</span></p><p><span class="math display">\[\mathbf u_t^P = \rm{BiRNN }(u_{t-1}^P, [e_t^P, c_t^P])\]</span></p><h2 id="gated-attention-based-rnn">Gated Attention-based RNN</h2><p>要基于U（<span class="math inline">\(U^Q\)</span>）去编码P（<span class="math inline">\(U^P\)</span>） ，得到<strong>Question-Aware的Passage编码</strong>，<span class="math inline">\(V^P\)</span>。</p><p><strong>1. Attention RNN</strong></p><p><span class="math inline">\(p_t\)</span>与<span class="math inline">\(q_j\)</span>两个单词的<strong>相关性函数</strong>（能量函数） <span class="math display">\[s_j^t = v^T \tanh (W_u^Q\mathbf u_j^Q + W_u^P \mathbf u_t^P + W_v^P \mathbf v_{t-1}^P), \quad j = 1, \cdots, m\]</span> <span class="math inline">\(p_t\)</span>与所有Q单词的<strong>注意力权值</strong><span class="math inline">\(\mathbf \alpha^t\)</span> <code>doc 2 query attention</code> <span class="math display">\[\alpha_{j}^t = \rm{softmax}(s_j^t)\]</span> <span class="math inline">\(p_t\)</span>基于<span class="math inline">\(\mathbf \alpha^t\)</span><strong>对<span class="math inline">\(Q\)</span>的信息汇总</strong>（注意力）<code>attention pooling vector of the whole question</code> <span class="math display">\[\mathbf c_t = \sum_{i=1}^m \alpha_i^t \mathbf u_i^Q\]</span> 实际上：<span class="math inline">\(\mathbf c_t = \rm{attn}(U^Q, [\mathbf u_t^P, \mathbf v_{t-1}^P])​\)</span>。</p><p><strong>注意力$c_t $</strong>和<strong>上一时刻隐状态 <span class="math inline">\(\mathbf v_{t-1}^P\)</span></strong>，输入RNN，<strong>计算当前的信息</strong> <span class="math display">\[\mathbf v_t^P = \rm{RNN}(\mathbf v_{t-1}^P, \mathbf c_t)\]</span> 每个<span class="math inline">\(\mathbf v_t^P\)</span>动态地合并了来自整个Q的匹配信息。</p><p><strong>2. Match RNN</strong></p><p><a href="https://plmsmile.github.io/2018/05/09/45-match-lstm/#match-lstm">Match-LSTM</a>。在输入RNN计算时，把当前<span class="math inline">\(\mathbf u_t^P\)</span>也输入进去，带上Passage的信息。输入是<strong><span class="math inline">\(\rm{input}=[\mathbf u_t^P, \mathbf c_t]\)</span></strong>。 <span class="math display">\[\mathbf v_t^P = \rm{RNN}(\mathbf v_{t-1}^P, [\mathbf u_t^P, \mathbf c_t])\]</span> <strong>3. Gated Attention-based RNN</strong></p><p>用门机制去控制每个<span class="math inline">\(p_t\)</span>的重要程度。 <span class="math display">\[g_t = \rm{sigmoid}(W_g \cdot [\mathbf u_t^P, \mathbf c_t])\]</span></p><p><span class="math display">\[[\mathbf u_t^P, \mathbf c_t]^* = g_t \odot [\mathbf u_t^P, \mathbf c_t]\]</span></p><p><span class="math display">\[\mathbf v_t^P = \rm{RNN}(\mathbf v_{t-1}^P, [\mathbf u_t^P, \mathbf c_t]^*)\]</span></p><p>GARNN的<strong>门机制</strong></p><ul><li>与GRU和LSTM不同</li><li>门机制是基于<strong>当前<span class="math inline">\(p_t\)</span></strong>和它的对应的Q的<strong>注意力向量<span class="math inline">\(\mathbf c_t\)</span></strong>（包含当前<span class="math inline">\(p_t\)</span>和Q的关系）</li><li><code>模拟</code>了阅读理解中，<strong>只有<span class="math inline">\(P\)</span>的一部分才与问题相关</strong>的特点</li></ul><p>最终得到了<code>question-aware passage representation</code> ：<span class="math inline">\(\{\mathbf v_t^P\}_{t=1}^n\)</span>。它的缺点如下：</p><ul><li>对Passage的上下文感知太少</li><li>候选答案对它窗口之外的线索未知</li><li>Question和Passage在词法、句法上有区别</li></ul><h2 id="self-matching-attention">Self-Matching Attention</h2><p>为了充分利用Passage的上下文信息。<span class="math inline">\(\{\mathbf v_t^P\}_{t=1}^n\)</span></p><p><strong>对P做self-match</strong>。使用Gated Attention-based RNN对P和P自己做match。</p><p>注意力计算 <span class="math display">\[s_j^t = v^T \tanh (W_v^P \mathbf v_j^P + W_v^{\bar P} \mathbf v_t^P), \quad j = 1, \cdots, n\]</span></p><p><span class="math display">\[\alpha_{j}^t = \rm{softmax}(s_j^t)\]</span></p><p><span class="math display">\[\mathbf c_t = \sum_{i=1}^n \alpha_i^t \mathbf v_i^P\]</span></p><p>RNN计算 <span class="math display">\[\mathbf h_t^P = \rm{BiRNN}(\mathbf h_{t-1}^P, [\mathbf v_t^P, \mathbf c_t]^*)\]</span> Self-Matching根据当前p单词、Q，从整个Passage中提取信息。<strong>最终得到Passage的表达<span class="math inline">\(H^P\)</span></strong>。</p><h2 id="output-layer">Output Layer</h2><p>其实就是个<a href="https://plmsmile.github.io/2018/05/09/45-match-lstm/#answer-pointer%E5%B1%82">Pointer Network的边界模型</a>，预测起始位置<span class="math inline">\(p^1\)</span>和结束位置<span class="math inline">\(p^2\)</span>。用RNN计算两次。</p><p><strong>1. 基于Q计算初始隐状态</strong></p><p>初始<code>hidden state</code>是Question的<code>attention-pooling vector</code> <span class="math display">\[\mathbf h_{t-1}^Q = \mathbf r^Q \]</span> 基于Q的编码和一组参数<span class="math inline">\(V_r^Q\)</span>，利用注意力机制计算<span class="math inline">\(\mathbf r^Q\)</span> <span class="math display">\[\mathbf r^Q = \rm{attn}(U^Q, V_r^Q)\]</span></p><p><span class="math display">\[s_j = \mathbf v^T \tanh(W_u^Q \mathbf u_j^Q + W_v^Q V_r^Q), \quad j = 1, \cdots, m\]</span></p><p><span class="math display">\[\alpha_i = \rm{softmax}(s_i) = \frac{\exp(s_i)}{\sum_{j=1}^m \exp(s_j)}\]</span></p><p><span class="math display">\[\mathbf r^Q = \sum_{i=1}^m \alpha_i  \mathbf u_i^Q\]</span></p><p><strong>2. RNN计算开始位置和结束位置</strong></p><p>计算t时刻的<code>attention-pooling passage</code> （<strong>注意力<span class="math inline">\(\mathbf c_t\)</span></strong>） <span class="math display">\[s_j^t = \mathbf v^T \tanh(W_h^P\mathbf h_j^P + W_h^a \mathbf h_{t-1}^a)\]</span></p><p><span class="math display">\[\alpha_i^t = \rm{softmax}(s_j^t)\]</span></p><p><span class="math display">\[\mathbf c_t = \sum_{i=1}^n \alpha_i^t \mathbf h_i^P\]</span></p><p>RNN前向计算 <span class="math display">\[\mathbf h_t^a = \rm{RNN} (\mathbf h_{t-1}^a, \mathbf c_t)\]</span> 基于注意力权值去选择位置 <span class="math display">\[p^t = \arg \max_{i}(a_i^t)\]</span></p><h1 id="实验">实验</h1><h2 id="实现细节">实现细节</h2><p><strong>数据集</strong></p><p>训练集80%，验证集10%，测试10%</p><p><strong>分词</strong></p><p>斯坦福的CoreNLP中的tokenizer</p><p><strong>词向量</strong></p><p>预训练好的Glove Vectors。训练中保持不变。</p><p><strong>字符向量</strong></p><p>单层的双向GRU，末尾隐状态作为该单词的字符向量</p><p><strong>BiRNN编码Question和Passage</strong></p><p>3层的双向GRU</p><p><strong>Hidden Size大小</strong></p><p>所有都是75</p><p><strong>Dropout</strong></p><p>每层之间的DropOut比例是0.2</p><p><strong>优化器</strong></p><p><a href="https://plmsmile.github.io/2018/03/30/35-nerual-network-optim/#%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%80%92%E5%87%8F">AdaDelta</a>。初始学习率为1，衰减率<span class="math inline">\(\beta = 0.95\)</span>，<span class="math inline">\(\epsilon = 1e^{-6}\)</span></p><h2 id="效果">效果</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/papers/results.png" style="display:block; margin:auto" width="80%"></p><h2 id="对比分析">对比分析</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/papers/gating-match.png" style="display:block; margin:auto" width="60%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/papers/question-passage-analysis.png" style="display:block; margin:auto" width="80%"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;微软亚研院和北大的阅读理解模型R-Net。
    
    </summary>
    
      <category term="机器阅读理解" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器阅读" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB/"/>
    
      <category term="Gated Attention RNN" scheme="http://plmsmile.github.io/tags/Gated-Attention-RNN/"/>
    
      <category term="Self-Matching" scheme="http://plmsmile.github.io/tags/Self-Matching/"/>
    
      <category term="RNet" scheme="http://plmsmile.github.io/tags/RNet/"/>
    
      <category term="Pointer Network" scheme="http://plmsmile.github.io/tags/Pointer-Network/"/>
    
  </entry>
  
  <entry>
    <title>Match-LSTM and Answer Pointer</title>
    <link href="http://plmsmile.github.io/2018/05/09/45-match-lstm/"/>
    <id>http://plmsmile.github.io/2018/05/09/45-match-lstm/</id>
    <published>2018-05-09T06:20:56.000Z</published>
    <updated>2018-11-25T08:30:09.115Z</updated>
    
    <content type="html"><![CDATA[<p>最初的Match-LSTM阅读理解模型。<a id="more"></a></p><p>论文地址：<a href="https://arxiv.org/abs/1608.07905" target="_blank" rel="noopener">Machine Comprehension Using Match-LSTM and Answer Pointer</a></p><h1 id="背景">背景</h1><h2 id="阅读理解任务">阅读理解任务</h2><p>后面会详细补充。</p><p>传统解决问答的方法：语言分析、特征工程等，具体包括句法分析、命名实体识别、问题分类、语义分析等。</p><h2 id="squad数据集">Squad数据集</h2><ul><li>答案是文章中出现的任意长度片段</li><li>Wiki文章为主</li><li>众包人工标注产生</li><li>每个问题3人标注，降低了人工标注误差</li><li>数量较大：500多篇文章，2万多个段落，10万个问题</li><li>鼓励用自己的语言提问</li></ul><h2 id="match-lstm">Match-LSTM</h2><p><strong>1. 文本蕴含任务</strong></p><p>一个前提集合P，一个假设H。去预测P里是否能蕴含出H。</p><p><strong>2. Match-LSTM</strong></p><p>有K个前提<span class="math inline">\(\{P_1, \cdots, P_K\}\)</span>，1个假设<span class="math inline">\(H\)</span>。假设的长度为m。遍历假设的每一个词汇<span class="math inline">\(h_i\)</span></p><ul><li><p>在<span class="math inline">\(h_i\)</span>处，利用注意力机制，综合K个前提，得到一个向量<span class="math inline">\(p_i\)</span></p></li><li><p>聚合匹配<span class="math inline">\([h_i, p_i]\)</span>一起，给到LSTM</p></li></ul><p>其实类似于Attention-Based NMT的解码过程。</p><h2 id="pointer-net">Pointer-Net</h2><p>从一个输入序列中，选择一个位置作为输出。</p><ul><li>序列模型：选择多个位置，就组成一个序列</li><li>边界模型：选择开始和结束位置，中间的片段是答案</li></ul><h1 id="模型">模型</h1><p>段落<span class="math inline">\(P\)</span>有m个单词，问题<span class="math inline">\(Q\)</span>有n个单词。</p><h2 id="lstm编码层">LSTM编码层</h2><p>单向LSTM编码 <span class="math display">\[H^p = \rm{LSTM}(P), \quad H^q = \rm{LSTM}(Q)\]</span> 取每一时刻的隐状态，得到对文章和问题的编码。<span class="math inline">\(H^p \in \mathbb R^{m \times h}, H^q \in \mathbb R^{n \times h}\)</span>。<span class="math inline">\(h\)</span>是编码的维度。</p><h2 id="match-lstm层">Match-LSTM层</h2><p>这一层实际上是一个LSTM，<strong>输入依次是P中的各个单词<span class="math inline">\(p_i\)</span></strong>。每一时刻，利用注意力机制计算相对应的Q的编码。</p><p><strong>问题--前提，段落--假设，看问题蕴含P的哪些部分</strong>。</p><p>先计算<code>注意力权值</code> <span class="math display">\[\overrightarrow{ G_i} = \tanh (W^qH^q + (W^p\mathbf h_i^p + W^r \overrightarrow{\mathbf h_{i-1}^r} + \mathbf b^p) \otimes \mathbf e_Q)\]</span></p><p><span class="math display">\[\overrightarrow{ \mathbf \alpha_i} = \rm{softmax}(\mathbf w^T  \overrightarrow{ G_i} + b \otimes \mathbf e_Q)\]</span></p><p>利用注意力机制，计算所有Q基于当前<span class="math inline">\(p_i\)</span>的<code>注意力</code>，把<strong>注意力和<span class="math inline">\(\mathbf h_i^p\)</span>拼接起来</strong> <span class="math display">\[\overrightarrow {\mathbf z_i} = [\mathbf h_i^p, \underbrace{H^q \overrightarrow{ \mathbf \alpha_i}}_{\color{blue}{\rm{attention}}}]\]</span> 把match后的结果，输入到LSTM， <span class="math display">\[\overrightarrow {\mathbf h_i^r} = \rm{LSTM}(\overrightarrow {\mathbf z_i}, \overrightarrow {\mathbf h_{i-1}^r})\]</span> 定义从右向左，得到<span class="math inline">\(\overleftarrow {\mathbf h_i^r}\)</span>。最终，拼接两个方向的向量，得到 <span class="math display">\[H^r = [\overrightarrow{H^r}, \overleftarrow{H^r}] \quad \in \mathbb R^{m \times 2h}\]</span></p><h2 id="answer-pointer层">Answer-Pointer层</h2><p>输入Match-LSTM层对Passage的编码结果<span class="math inline">\(H^r\)</span>，输出一个序列。</p><p><strong>序列模型</strong></p><p>不断生成一个序列<span class="math inline">\(\mathbf a = (a_1, a_2, \cdots)\)</span>，表示P中的位置。</p><p>在P的末尾设置一个停止标记，如果选择它，则停止迭代。新的<span class="math inline">\(\bar H^r \in \mathbb R^{(m+1) \times 2h}\)</span></p><p>1、计算<strong>注意力权值</strong><span class="math inline">\(\mathbf \beta_k\)</span>，<span class="math inline">\(\beta_{k,j}\)</span>表示，选<span class="math inline">\(p_j\)</span>作为<span class="math inline">\(a_k\)</span>的概率 <span class="math display">\[F_k = \tanh(V \bar H^r + (W^a \mathbf h_{k-1}^a + \mathbf b^a) \otimes \mathbf e_{(m+1)})\]</span></p><p><span class="math display">\[\mathbf \beta_k = \rm{softmax}(\mathbf v^TF_k + \mathbf c \otimes \mathbf e_{(m+1)})\]</span></p><p>2、使用<strong>注意力机制</strong>得到<strong>当前时刻需要的<span class="math inline">\(H^r\)</span>的信息</strong>，结合<strong>上一时刻的隐状态</strong>，输入到LSTM中 <span class="math display">\[\mathbf h_k^a = \overrightarrow{\rm{LSTM}} ( \underbrace{\bar H^r \mathbf \beta_k^T}_{\color{blue}{\rm{attention}}}, \mathbf h_{k-1}^r)\]</span> 答案的概率计算如下： <span class="math display">\[p(\mathbf a \mid H^r) = \prod_{k} p(a_k \mid a_1, \cdots, a_{k-1}, H^r)\]</span></p><p><span class="math display">\[p(a_k = j \mid a_1, \cdots, a_{k-1}, H^r) = \beta_{k,j}\]</span></p><p>目标函数： <span class="math display">\[- \sum_{n=1}^N \log p(\mathbf a_n \mid P_n, Q_n)\]</span> <strong>边界模型</strong></p><p>不用预测完整的序列，只<strong>预测开始和结束位置</strong>就可以了。 <span class="math display">\[p(\mathbf a \mid H^r) = p(a_s \mid H^r) \cdot p(a_e \mid a_s, H^r)\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最初的Match-LSTM阅读理解模型。
    
    </summary>
    
      <category term="机器阅读理解" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器阅读" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB/"/>
    
      <category term="Match-LSTM" scheme="http://plmsmile.github.io/tags/Match-LSTM/"/>
    
      <category term="Pointer Net" scheme="http://plmsmile.github.io/tags/Pointer-Net/"/>
    
  </entry>
  
  <entry>
    <title>强化学习在NLP中的应用</title>
    <link href="http://plmsmile.github.io/2018/05/03/44-reinforce-nlp/"/>
    <id>http://plmsmile.github.io/2018/05/03/44-reinforce-nlp/</id>
    <published>2018-05-03T06:00:55.000Z</published>
    <updated>2018-11-25T08:30:09.059Z</updated>
    
    <content type="html"><![CDATA[<p>强化学习在自然语言处理中的应用。<a id="more"></a></p><p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="阿里小蜜的任务型问答">阿里小蜜的任务型问答</h1><p>小蜜包含<code>QA问答</code>、<code>开放域聊天</code>、<code>任务型对话</code>。</p><h2 id="任务型对话">任务型对话</h2><blockquote><p>1、TaskBot：由任务驱动的多轮对话，每一轮去读取用户的slot信息，直到槽填满，全部ok</p><p>2、Action Policy：强化学习去管理多轮对话，小蜜每一轮给出一个动作，询问用户或者完成订单</p><p>3、Belief Tracker：深度学习去提取slot信息，LSTM-CRF标注</p></blockquote><p><strong>1. TaskBot</strong></p><p>任务型对话是指<strong>由任务驱动的多轮对话</strong>。在对话中帮助用户完成某个任务，比如订机票、订酒店等。</p><ul><li><code>传统</code>：用<a href="https://plmsmile.github.io/2018/05/02/43-intent-detection-slot-filling/">slot filling</a>来做，但需要大量人工模板、规则和训练语料</li><li><code>小蜜</code>：基于<strong>强化学习</strong>和<strong>Neural Belief Tracker</strong>的<strong>端到端</strong>可训练的TaskBot方案</li></ul><p>在每轮对话中，都需要抽取用户当前给的<strong>slot状态</strong>（<strong>任务需要的组件信息</strong>）。不断地去填满所有的slot，最后去下单。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/alime/vslots.png" style="display:block; margin:auto" width="60%"></p><p><strong>2. Action Policy - 强化学习</strong></p><p>系统如何给用户合适的回复：接着询问用户、出订单。使用<a href="https://plmsmile.github.io/2018/04/01/37-reinforce-learning/">强化学习</a>去<strong>管理这个多轮对话</strong>。各个定义如下：</p><ul><li>智能体：小蜜（系统）</li><li><strong>策略</strong>：小蜜给用户的回答，反问哪个slot、出订单</li><li>环境：用户</li><li><strong>状态</strong>：用户回答中提取出的slot状态（信息）</li><li><strong>反馈</strong>：继续聊天、退出、下单</li></ul><p><strong>3. Belief Tracker - 深度学习</strong></p><p>Belief Tracker用来提取用户的slot状态，实际是一个序列标注问题。使用<code>LSTM-CRF</code>进行标注。传统是<a href="%5Bslot%20filling%5D(https://plmsmile.github.io/2018/05/02/43-intent-detection-slot-filling/)">slot filling</a></p><h2 id="系统结构">系统结构</h2><p>系统分为下面三层。</p><ul><li><code>数据预处理层</code> ： 分词、实体抽取等。</li><li><code>端到端的对话管理层</code> ：强化学习</li><li><code>任务生成层</code></li></ul><p>强化学习包括：</p><ul><li><strong>Intent Network</strong> ：处理用户输入</li><li><strong>Neural Belief Tracker</strong> ：记录读取slot信息</li><li><strong>Policy Network</strong> ：决定小蜜的回答：反问哪个slot 或 出订单。</li></ul><h2 id="intent-network">Intent Network</h2><p><a href="https://plmsmile.github.io/2018/03/31/36-alime-chat/#%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB">阿里小蜜意图分类</a>。使用CNN学一个<code>sentence embedding</code>来表示用户的意图。后面给到Policy Network。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/alime/cnn.png" style="display:block; margin:auto" width="70%"></p><h2 id="belief-tracker">Belief Tracker</h2><p>使用BiLSTM-CRF来进行标记句子，提取出slot信息。</p><table><thead><tr class="header"><th align="center">句子</th><th align="center">first</th><th align="center">class</th><th align="center">fares</th><th align="center">from</th><th align="center">Boston</th><th align="center">to</th><th align="center">Denver</th></tr></thead><tbody><tr class="odd"><td align="center"><strong>Slots</strong></td><td align="center">B-机舱类别</td><td align="center">I-机舱类别</td><td align="center">O</td><td align="center">O</td><td align="center">B-出发地</td><td align="center">O</td><td align="center">B-目的地</td></tr></tbody></table><h2 id="policy-network">Policy Network</h2><p>四个关键：episode、reward、state、action。</p><p><strong>1. 一轮交互的定义</strong></p><ul><li>episode开始：识别出用户意图为<code>购买机票</code></li><li>episode结束：用户<code>成功购买机票</code> 或 <code>退出会话</code></li></ul><p><strong>2. 反馈</strong></p><p>获取用户的反馈非常关键。</p><ul><li>收集线上用户的反馈，如用户下单、退出等行为</li><li>使用预训练环境</li></ul><p>预训练环境的两部分反馈</p><ul><li>Action Policy ：<a href="https://plmsmile.github.io/2018/04/22/41-strategy-learning/#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6">策略梯度</a> 更新模型。正反馈<span class="math inline">\(r=1\)</span>，负反馈<span class="math inline">\(r=-1\)</span></li><li>Belief Tracker：仅使用正反馈作为正例，出现错误由小二标出正确的slots</li></ul><p><strong>3. 状态</strong></p><p>当前slot：Intent Network得到的Sentence Embedding，再过Belief Tracker得到的slot信息。</p><p>使用当前slot+历史slot，过线性层，softmax，到各个Action。</p><p><strong>4. 动作</strong></p><p>订机票，Action是离散的。主要是：<strong>对各个Slot的反问</strong>和<strong>下单</strong>。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/alime/pretrained-alime.png" style="display:block; margin:auto" width="70%"></p><h2 id="整体模型">整体模型</h2><p>符号定义</p><ul><li><span class="math inline">\(q_i\)</span> ：当前用户的问题</li><li><span class="math inline">\(a_{i-1}\)</span> ：上一轮问题的答案</li><li><span class="math inline">\(S_i\)</span> ：历史slot信息</li></ul><p><span class="math display">\[\begin{align}&amp; O_i = \rm{IntentNet}(q_i)  \\ &amp; C_i = \rm{BeliefTracker}(q_i, a_{i-1}) \\&amp; X_i = O_i \oplus C_i \oplus S_{i-1} \\&amp; H_i = \rm{Linear} (X_i) \\&amp; P(\cdot) = \rm{Softmax}(H_i)\end{align}\]</span></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/alime/TaskBot.png" style="display:block; margin:auto" width="70%"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;强化学习在自然语言处理中的应用。
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="强化学习" scheme="http://plmsmile.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TaskBot" scheme="http://plmsmile.github.io/tags/TaskBot/"/>
    
  </entry>
  
  <entry>
    <title>意图识别和槽填充</title>
    <link href="http://plmsmile.github.io/2018/05/02/43-intent-detection-slot-filling/"/>
    <id>http://plmsmile.github.io/2018/05/02/43-intent-detection-slot-filling/</id>
    <published>2018-05-02T06:11:38.000Z</published>
    <updated>2018-11-25T08:30:08.983Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1609.01454" target="_blank" rel="noopener">Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling</a> <a id="more"></a></p><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="背景">背景</h1><h2 id="语义理解的两个方面">语义理解的两个方面</h2><p><strong>1. 语义理解的两个主要方面</strong></p><p>在对话系统中，<code>Spoken language understanding</code>（语言理解）很重要。主要是下面两个关键点：</p><ul><li>理解说话人的意图 -- <strong>意图检测</strong>（Intent Detection）</li><li>从句子中提取语义成分 -- <strong>槽填充</strong>（Slot Filling）</li></ul><p><strong>2. 意图检测</strong></p><p>意图检测是一个语义句子的<code>分类问题</code>。可以用SVM、DNN来进行分类。</p><p><strong>3. 槽填充</strong></p><p>槽填充是要读取句子中的一些语义成分，是一个<code>序列标注问题</code>。可以用MEMMs来做。</p><p><strong>4. 处理</strong></p><p>传统一般是用两个模型去分别处理意图检测和槽填充，现在可以使用一个模型<a href="https://plmsmile.github.io/2017/10/10/attention-model/#encoder-decoder">Encoder-Decoder</a>去同时解决这两个问题。</p><p><strong>5. 对齐和注意力</strong></p><p>序列标注具有明确的对齐信息。</p><p>输入n，输出n，相同长度。输入和输出每一个位置严格<strong>对齐</strong>。Alignment-based RNN。</p><p>输入n，输出m，不同长度，本身不具有对齐信息。需要<a href="https://plmsmile.github.io/2017/10/10/attention-model/#attention-model">注意力机制</a>来进行对齐。Attention-based Encoder-Decoder。</p><h2 id="槽填充">槽填充</h2><p><strong>1. 问题</strong></p><p><strong>槽填充</strong>是一个序列标注问题，具有明确的对齐信息。</p><table><thead><tr class="header"><th align="center">句子</th><th align="center">first</th><th align="center">class</th><th align="center">fares</th><th align="center">from</th><th align="center">Boston</th><th align="center">to</th><th align="center">Denver</th></tr></thead><tbody><tr class="odd"><td align="center"><strong>Slots</strong></td><td align="center">B-机舱类别</td><td align="center">I-机舱类别</td><td align="center">O</td><td align="center">O</td><td align="center">B-出发地</td><td align="center">O</td><td align="center">B-目的地</td></tr></tbody></table><p><strong>意图</strong>：订机票。</p><p>本质上是学得一个<strong>映射函数</strong><span class="math inline">\(\cal {X \to Y}\)</span>。训练样本：<span class="math inline">\(\{ (\mathbf x^{(n)}, \mathbf y^{(n)}), n=1,\cdots, N \}\)</span>。</p><p><strong>2. RNN 槽填充</strong></p><p>符号定义</p><ul><li><span class="math inline">\(\mathbf x\)</span> ：输入序列</li><li><span class="math inline">\(\mathbf y\)</span> ：输出序列</li><li><span class="math inline">\(y_t\)</span> ：第t个单词的slot lable</li></ul><p>预测<span class="math inline">\(y_t\)</span>，需要<span class="math inline">\(\mathbf x\)</span>和<span class="math inline">\(y_{t-1}\)</span>。</p><p><strong>训练</strong>是找到一个最大的使概率似然最大的参数<span class="math inline">\(\theta\)</span> ： <span class="math display">\[\arg \max_{\theta} \prod P(y_t \mid y_{t-1}, \mathbf x; \theta)\]</span> <strong>预测</strong>是找到最大概率的标记序列<span class="math inline">\(\mathbf y\)</span> <span class="math display">\[\mathbf {\hat y} = \arg \max_{\mathbf y} P(\mathbf y \mid \mathbf x)\]</span> <strong>3. RNN Encoder-Decoder 槽填充</strong></p><p>序列标注有明确的对齐信息，所以先没有使用注意力机制。把<span class="math inline">\(\mathbf x\)</span>编码为语义向量<span class="math inline">\(\mathbf c\)</span>： <span class="math display">\[P(\mathbf y) = \prod_{t=1}^T P(y_t \mid y_{t-1}, \mathbf c)\]</span> Seq2Seq可以处理不同长度的映射信息，这时没有明确的对齐信息。但是可以使用注意力机制来进行<code>软对齐Soft Alignment</code>。</p><h1 id="两种方法">两种方法</h1><h2 id="seq2seq方法">Seq2Seq方法</h2><p>Encoder-Decoder with Aligned Inputs</p><p><strong>1. 编码</strong></p><p>使用双向RNN对输入序列进行编码，<span class="math inline">\(\mathbf {h_i} = [fh_i, bh_i]\)</span>。</p><p><strong>2. 意图识别</strong></p><p>最后时刻的隐状态<span class="math inline">\(\mathbf {h_T}\)</span>携带了整个句子的信息，使用它进行意图分类。</p><p><strong>3. 槽填充</strong></p><p>用单向RNN作为Decoder。初始<span class="math inline">\(\mathbf s_0= \mathbf h_T\)</span>。有3种方式：</p><ol style="list-style-type: lower-alpha"><li><p>只有注意力输入</p></li><li><p>只有对齐输入</p></li><li><p>有注意力和对齐两个输入</p></li></ol><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/papers/en-decoder-slot-filling.png" style="display:block; margin:auto" width="60%"></p><p><strong>4. 带注意力和对齐输入的RNN槽填充计算方式</strong> <span class="math display">\[s_0 = h_T\]</span> 计算注意力的上下文<span class="math inline">\(\mathbf c_i\)</span> <span class="math display">\[\alpha_{ij} = \rm{softmax}(e_{ij})\]</span></p><p><span class="math display">\[e_{ij} = g(\mathbf s_{i-1}, \mathbf h_k)\]</span></p><p><span class="math display">\[\mathbf c_i = \sum_{j=1}^T\alpha_{ij} \mathbf h_j\]</span></p><p>计算新的状态 <span class="math display">\[s_i = f(\mathbf s_{i-1}, y_{i-1}, \mathbf h_i, \mathbf c_i)\]</span></p><h2 id="基于注意力的rnn">基于注意力的RNN</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/papers/atten-rnn-slot-filling.png" style="display:block; margin:auto" width="60%"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.01454&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling&lt;/a&gt;
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="注意力" scheme="http://plmsmile.github.io/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
      <category term="意图检测" scheme="http://plmsmile.github.io/tags/%E6%84%8F%E5%9B%BE%E6%A3%80%E6%B5%8B/"/>
    
      <category term="槽填充" scheme="http://plmsmile.github.io/tags/%E6%A7%BD%E5%A1%AB%E5%85%85/"/>
    
      <category term="RNN" scheme="http://plmsmile.github.io/tags/RNN/"/>
    
      <category term="对齐" scheme="http://plmsmile.github.io/tags/%E5%AF%B9%E9%BD%90/"/>
    
  </entry>
  
  <entry>
    <title>强化学习算法小结</title>
    <link href="http://plmsmile.github.io/2018/04/24/42-reinforce-conclusion-simple/"/>
    <id>http://plmsmile.github.io/2018/04/24/42-reinforce-conclusion-simple/</id>
    <published>2018-04-24T08:28:41.000Z</published>
    <updated>2018-11-25T08:30:08.939Z</updated>
    
    <content type="html"><![CDATA[<p>强化学习算法的简单总结，主要包括基于值函数/策略函数的学习方法、Actor-Critic算法。<a id="more"></a></p><p>强化学习笔记： <a href="https://plmsmile.github.io/2018/04/01/37-reinforce-learning/">强化学习基础</a> 、<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/">基于值函数的学习方法</a>、<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/">基于值函数的学习方法</a></p><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="强化学习的目标">强化学习的目标</h1><p>强化学习的目标是<strong>学习到一个策略<span class="math inline">\(\pi_{\theta}(a\mid s)\)</span></strong>，来<strong>最大化这个策略的期望回报</strong>。<strong>希望智能体能够获得更多的回报</strong>。本质上是策略搜索。 <span class="math display">\[J(\theta) = E_{\tau \sim p_{\theta}(\tau)} [\sum_{t=0}^{T-1}\gamma ^tr_{t+1}]\]</span></p><p><span class="math display">\[J(\theta) = \int p_{\theta}(\tau) \cdot G(\tau) \, {\rm d}\tau\]</span></p><h1 id="基于值函数的学习方法">基于值函数的学习方法</h1><h2 id="策略迭代">策略迭代</h2><p>已知模型。利用<strong>贝尔曼方程</strong>（<code>算均值</code>）迭代计算出<span class="math inline">\(V(s)\)</span>，再算出<span class="math inline">\(Q(s,a)\)</span>。选择最好的动作<span class="math inline">\(a\)</span>去优化策略<span class="math inline">\(\pi(s)\)</span>。 <span class="math display">\[\forall s, \quad V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)] \]</span></p><p><span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[\forall s, \qquad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span></p><h2 id="值迭代">值迭代</h2><p>已知模型。利用<strong>贝尔曼最优方程</strong>迭代算出<span class="math inline">\(V(s)\)</span>，再算出<span class="math inline">\(Q(s,a)\)</span>。选择最好的动作<span class="math inline">\(a\)</span>去优化策略<span class="math inline">\(\pi(s)\)</span>。 <span class="math display">\[\forall s \in S, \quad V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span></p><p><span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[\forall s, \quad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span></p><h2 id="蒙特卡罗">蒙特卡罗</h2><p>未知模型。从<span class="math inline">\((s,a)\)</span><strong>随机游走，采集N个样本</strong>。使用<strong>所有轨迹回报平均值近似估计<span class="math inline">\(Q(s,a)\)</span></strong> ，再去改进策略。重复，直至收敛。 <span class="math display">\[Q^\pi(s, a)  \approx \hat Q^\pi(s, a) = \frac{1}{N} \sum_{n=1}^NG(\tau^{(n)})\]</span></p><h2 id="时序差分算法">时序差分算法</h2><p>无需知道完整轨迹就能对策略进行评估。</p><p>时序差分学习=动态规划-贝尔曼估计<span class="math inline">\(G(\tau)\)</span> + 蒙特卡罗采样-增量计算<span class="math inline">\(Q(s,a)\)</span></p><p>贝尔曼估计轨迹总回报<span class="math inline">\(G(\tau)\)</span> <span class="math display">\[G(\tau) \leftarrow r(s, a, s^\prime) + \gamma \cdot Q(s^\prime, a^\prime)\]</span> 增量计算<span class="math inline">\(Q(s,a)\)</span> <span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (\underbrace{ r+ \gamma \cdot Q(s^\prime, a^\prime)}_{\color{blue}{实际值}} - \underbrace{Q(s, a)}_{\color{blue}{预期值}})\]</span></p><h2 id="sarsa">SARSA</h2><p>同策略的时序差分算法，是Q学习的改进。</p><p>1、当前状态<span class="math inline">\(s\)</span>，当前动作<span class="math inline">\(a\)</span> （初始时选择<span class="math inline">\(a=\pi^\epsilon(s)\)</span>，后续是更新得到的）</p><p>2、<strong>执行动作</strong><span class="math inline">\(a\)</span>，得到<strong>新状态</strong><span class="math inline">\(s^\prime\)</span>，得到<strong>奖励</strong><span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>4、<strong>依概率选择新动作</strong><span class="math inline">\(a = \pi^\epsilon(s^\prime)\)</span>，<strong>新状态新动作的值函数</strong>：<span class="math inline">\(Q(s^\prime, a^\prime)\)</span></p><p>5、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 6、更新状态和动作：<span class="math inline">\(s = s^\prime, a = a^\prime\)</span></p><h2 id="q学习">Q学习</h2><p>1、当前状态<span class="math inline">\(s\)</span>，选择当前动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>、得到新状态<span class="math inline">\(s^\prime\)</span>和奖励 <span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>3、<strong>不依概率选择新动作</strong>，而是<strong>直接选择最大的值函数<span class="math inline">\(\max_\limits{a^\prime}Q(s^\prime, a^\prime)\)</span></strong></p><p>4、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot \max_{a^\prime} Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 5、更新状态：<span class="math inline">\(s = s^\prime\)</span></p><h2 id="q网络">Q网络</h2><p>使用神经网络<span class="math inline">\(Q_{\phi}(\mathbf{s,a})\)</span>去近似值函数<span class="math inline">\(Q(s,a)\)</span>。两个问题：实际目标值不稳定；样本之间具有强相关性。 <span class="math display">\[L(s, a, s^\prime; \phi) = \left(\underbrace{r + \gamma \cdot \max_{a^\prime} Q_\phi(\mathbf s^\prime, \mathbf a^\prime)}_{\color{blue}{实际目标值}} - \underbrace{Q_\phi(\mathbf s, \mathbf a)}_{\color{blue}{\text{网络值}}}\right)^2\]</span></p><h2 id="dqn">DQN</h2><p>深度Q网络：</p><ul><li><strong>目标网络冻结</strong>-<strong>稳定目标值</strong>。<span class="math inline">\(Q_{\phi}(\mathbf{s,a})\)</span>训练网络，<span class="math inline">\(Q_{\hat \phi}(\mathbf{s,a})\)</span>目标值网络。定期更新参数<span class="math inline">\(\hat \phi \leftarrow \phi\)</span></li><li><strong>经验池的经验回放</strong>-<strong>去除样本相关性</strong>- 每次采集一条数据放入经验池，再从经验池取数据进行训练。</li></ul><p><strong>生成新数据加入经验池</strong></p><p>1、状态<span class="math inline">\(s\)</span>， 选择动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>， 得到<span class="math inline">\(r\)</span>和<span class="math inline">\(s^\prime\)</span></p><p>3、<span class="math inline">\((s,a, r, s^\prime)\)</span> 加入经验池<span class="math inline">\(\cal D\)</span></p><p><strong>采经验池中采样一条数据计算</strong></p><p>1、从<span class="math inline">\(\cal D\)</span>中采样一条数据，<span class="math inline">\((ss,aa, rr, ss^\prime)\)</span>。 （<strong>去除样本相关性</strong>）</p><p>2、<strong>计算实际目标值</strong><span class="math inline">\(Q_{\hat \psi}(\mathbf{ss, aa})\)</span>。 （<strong>解决目标值不稳定的问题</strong>） <span class="math display">\[Q_{\hat \psi}(\mathbf{ss, aa}) =\begin{cases}&amp; rr, &amp; ss^\prime 为终态 \\&amp; rr + \gamma \cdot \max_\limits{\mathbf a^\prime}Q_{\hat \phi}(\mathbf {ss^\prime}, \mathbf {a^\prime}), &amp;其它\end{cases}\]</span> 3、<code>损失函数</code>如下，<strong>梯度下降法去训练Q网络</strong> <span class="math display">\[J(\phi)= \left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - y \right)^2=\left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - Q_{\hat \psi}(\mathbf{ss, aa}) \right)^2\]</span> <strong>状态前进</strong></p><p><span class="math inline">\(s \leftarrow s^\prime\)</span></p><p><strong>更新目标Q网络的参数</strong></p><p>每隔C步更新：<span class="math inline">\(\hat \phi \leftarrow \phi\)</span></p><h1 id="基于策略函数的学习方法">基于策略函数的学习方法</h1><p><code>策略搜索</code>本质上是一个<code>优化问题</code>，无需值函数可以直接优化策略。参数化的策略可以处理连续状态和动作。</p><p><strong>策略梯度</strong> ：是一种基于梯度的强化学习方法。</p><p><strong>策略连续可微假设</strong>：假设<span class="math inline">\(\pi_{\theta}(a \mid s)\)</span>是一个关于<span class="math inline">\(\theta\)</span>的连续可微函数。</p><p>最大化策略的期望回报 <span class="math display">\[J(\theta) = \int p_{\theta}(\tau) \cdot G(\tau) \, {\rm d}\tau\]</span></p><h2 id="策略梯度">策略梯度</h2><p><span class="math display">\[\frac{\partial J(\theta)}{\partial \theta}  \triangleq E_{\tau \sim  p_{\theta}(\tau)} \left[ \color{blue}{\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau)} \cdot G(\tau)\right]\]</span></p><p><span class="math display">\[\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau) =  \sum_{t=0}^{T-1} \frac{\partial}{\partial \theta}  \color{blue}{ \log\pi_{\theta}(a_t \mid s_t)}\]</span></p><p><span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; =  E_{\tau \sim p_{\theta}(\tau)}\left[ \sum_{t=0}^{T-1} \left(\frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t) \cdot \gamma^t G(\tau_{t:T})\right)\right] \end{align}\]</span></p><h2 id="reinforce算法">REINFORCE算法</h2><p>期望用采样的方式来近似，随机采样N个轨迹。 <span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; \approx  \frac{1}{N}\sum_{n=1}^   N\left[ \sum_{t=0}^{T-1} \left(\frac{\partial}{\partial \theta}  \log\pi_{\theta}(a^{(n)}_t \mid s^{(n)}_t) \cdot \gamma^t G(\tau^{(n)}_{t:T})\right)\right] \end{align}\]</span> 1、根据<span class="math inline">\(\pi_\theta(a\mid s)\)</span><strong>生成一条完整的轨迹</strong> ：<span class="math inline">\(\tau = s_0, a_0, s_1,a_1, \cdots, s_{T-1}, a_{T-1}, s_{T}\)</span></p><p>2、<strong>在每一时刻更新参数</strong> (0~T)</p><p>先计算<strong>当前时刻的回报<span class="math inline">\(G(\tau_{t:T})\)</span></strong>，再更新参数： <span class="math display">\[\theta \leftarrow \theta + \alpha \cdot \gamma^tG(\tau_{t:T}) \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\]</span></p><p>缺点：</p><ul><li>需要完整的轨迹</li><li>不同轨迹之间的策略梯度方差大，导致训练不稳定</li></ul><h2 id="带基准函数的reinforce算法">带基准函数的REINFORCE算法</h2><p>每个时刻<span class="math inline">\(t\)</span>的策略梯度 <span class="math display">\[\frac{\partial J_t(\theta)}{\partial \theta} =  E_{s_t,a_t}\left[\alpha \cdot \gamma^tG(\tau_{t:T}) \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\right]\]</span> <strong>基准函数</strong></p><ul><li>为了减小策略梯度的方差</li><li>引入与<span class="math inline">\(a_t\)</span>无关的基准函数<span class="math inline">\(b(s_t) = V(s_t)\)</span></li><li>越相关方差越小，所以选择值函数</li></ul><p>每一时刻的策略梯度为： <span class="math display">\[\frac{\partial \hat J_t(\theta)}{\partial \theta} =  E_{s_t,a_t}\left[\alpha \cdot \gamma^t \left( \color{blue} {G(\tau_{t:T})  - b(s_t)}\right)\cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\right]\]</span> 1、根据策略<span class="math inline">\(\pi_\theta(a\mid s)\)</span><strong>生成一条完整轨迹</strong> ：<span class="math inline">\(\tau = s_0, a_0, s_1,a_1, \cdots, s_{T-1}, a_{T-1}, s_{T}\)</span></p><p>2、在每一时刻更新参数</p><p>计算<strong>当前时刻的轨迹回报<span class="math inline">\(G(\tau_{t:T})\)</span></strong> ，再利用<code>基准函数(值函数)</code>进行修正，得到<span class="math inline">\(\delta\)</span> <span class="math display">\[\delta \leftarrow  G(\tau_{t:T}) - V_{\phi} (s_t)\]</span> 更新<strong>值函数<span class="math inline">\(V_\phi(s)\)</span>的参数<span class="math inline">\(\phi\)</span></strong> <span class="math display">\[\phi \leftarrow  \phi + \beta \cdot \delta \cdot \frac{\partial}{ \partial \phi} V_{\phi}(s_t)\]</span> 更新<strong>策略函数<span class="math inline">\(\pi_\theta(a \mid s)\)</span>的参数<span class="math inline">\(\theta\)</span></strong> <span class="math display">\[\theta \leftarrow  \theta + \alpha \cdot \gamma^t\delta \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\]</span> <code>缺点</code>： 需要根据策略采集一条完整的轨迹。</p><h1 id="actor-critic算法">Actor-Critic算法</h1><p><code>演员-评论家</code>算法结合了<code>策略梯度</code>和<code>时序差分</code>算法。不需要一条完整的轨迹，可以单步更新参数，无需等到回合结束才进行更新。</p><p><strong>演员</strong></p><p>根据<span class="math inline">\(s\)</span>和策略<span class="math inline">\(\pi_\theta(a\mid s)\)</span>，执行动作<span class="math inline">\(a\)</span>，环境变为<span class="math inline">\(s^\prime\)</span>，得到奖励<span class="math inline">\(r\)</span></p><p><strong>评论员</strong></p><p>根据<code>真实奖励</code><span class="math inline">\(r\)</span>和<code>之前的标准</code>，<strong>打分（估计回报）</strong>：<span class="math inline">\(r + \gamma V_\phi(s^\prime)\)</span> ，<strong>再调整自己的打分标准<span class="math inline">\(\phi\)</span></strong>。<span class="math inline">\(\min_{\phi} \left(\hat G(\tau_{t:T}) - V_{\phi}(s_{t}) \right)^2\)</span></p><p>使评分更加接近环境的真实回报。</p><p><strong>演员</strong></p><p>演员<strong>根据评论的打分</strong>，<strong>调整自己的策略<span class="math inline">\(\pi_\theta\)</span></strong>，争取下次做得更好。<span class="math inline">\(\theta \leftarrow \theta + \alpha \cdot \gamma^t \left( G(\tau_{t:T}) - V_{\phi} (s_t)\right) \cdot \frac{\partial}{\partial \theta} \log\pi_{\theta}(a_t \mid s_t)\)</span></p><p><strong>1. 执行策略，生成样本</strong> <span class="math display">\[s, a, r, s^\prime\]</span> <strong>2. 估计回报，生成<span class="math inline">\(\delta\)</span></strong> <span class="math display">\[G(s) = r + \gamma V_\phi(s^\prime), \quad \delta  =  G(s) - V_{\phi}(s)\]</span> <strong>3. 更新值函数和策略</strong> <span class="math display">\[\phi  \leftarrow  \phi + \beta \cdot \delta \cdot \frac{\partial V_{\phi}(s)}{\partial \phi}\]</span></p><p><span class="math display">\[\theta  \leftarrow  \theta + \alpha \cdot \lambda \delta \cdot \frac{\partial }{\partial \theta} \log \pi_\theta(a\mid s)\]</span></p><p><strong>4. 更新折扣率和状态</strong> <span class="math display">\[\lambda \leftarrow \lambda \cdot \gamma, \quad s \leftarrow s^\prime\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;强化学习算法的简单总结，主要包括基于值函数/策略函数的学习方法、Actor-Critic算法。
    
    </summary>
    
      <category term="强化学习" scheme="http://plmsmile.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="策略迭代" scheme="http://plmsmile.github.io/tags/%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3/"/>
    
      <category term="值迭代" scheme="http://plmsmile.github.io/tags/%E5%80%BC%E8%BF%AD%E4%BB%A3/"/>
    
      <category term="蒙特卡罗" scheme="http://plmsmile.github.io/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97/"/>
    
      <category term="时序差分" scheme="http://plmsmile.github.io/tags/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/"/>
    
      <category term="SARSA" scheme="http://plmsmile.github.io/tags/SARSA/"/>
    
      <category term="Q学习" scheme="http://plmsmile.github.io/tags/Q%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Q网络" scheme="http://plmsmile.github.io/tags/Q%E7%BD%91%E7%BB%9C/"/>
    
      <category term="DQN" scheme="http://plmsmile.github.io/tags/DQN/"/>
    
      <category term="策略梯度" scheme="http://plmsmile.github.io/tags/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>基于策略函数的学习方法</title>
    <link href="http://plmsmile.github.io/2018/04/22/41-strategy-learning/"/>
    <id>http://plmsmile.github.io/2018/04/22/41-strategy-learning/</id>
    <published>2018-04-22T05:44:08.000Z</published>
    <updated>2018-11-25T08:30:08.899Z</updated>
    
    <content type="html"><![CDATA[<p>基于策略函数的学习方法和Actor-Critc算法。<a id="more"></a></p><p><a href="https://plmsmile.github.io/2018/04/01/37-reinforce-learning/">强化学习基础</a> 、<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/">基于值函数的学习方法</a></p><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="基于策略函数的学习方法">基于策略函数的学习方法</h1><h2 id="强化学习目标">强化学习目标</h2><p>强化学习的目标是<strong>学习到一个策略<span class="math inline">\(\pi_{\theta}(a\mid s)\)</span></strong>，来<strong>最大化这个策略的期望回报</strong>。<strong>希望智能体能够获得更多的回报</strong>。本质上是策略搜索。 <span class="math display">\[J(\theta) = E_{\tau \sim p_{\theta}(\tau)} [\sum_{t=0}^{T-1}\gamma ^tr_{t+1}]\]</span></p><p><span class="math display">\[J(\theta) = \int p_{\theta}(\tau) \cdot G(\tau) \, {\rm d}\tau\]</span></p><p><code>策略搜索</code>本质上是一个<code>优化问题</code>，无需值函数可以直接优化策略。参数化的策略可以处理连续状态和动作。</p><ul><li><strong>基于梯度的优化</strong></li><li>无梯度的优化</li></ul><h2 id="策略梯度">策略梯度</h2><p><strong>1. 思想和假设</strong></p><p><strong>策略梯度</strong> ：是一种基于梯度的强化学习方法。</p><p><strong>策略连续可微假设</strong>：假设<span class="math inline">\(\pi_{\theta}(a \mid s)\)</span>是一个关于<span class="math inline">\(\theta\)</span>的连续可微函数。</p><p><strong>2. 优化目标</strong></p><p>最大化策略的期望回报。</p><p><span class="math display">\[J(\theta) = \int p_{\theta}(\tau) \cdot G(\tau) \, {\rm d}\tau\]</span></p><p><strong>3. 策略梯度推导</strong></p><p>采用梯度上升法来优化参数<span class="math inline">\(\theta\)</span>来使得<span class="math inline">\(J(\theta)\)</span>最大。</p><p><strong>策略梯度<span class="math inline">\(\frac{\partial J(\theta)}{\partial \theta}\)</span></strong>的推导如下：</p><p>1、参数<span class="math inline">\(\theta\)</span>的优化方向是总回报<span class="math inline">\(G(\tau)\)</span>大的轨迹<span class="math inline">\(\tau\)</span>，其概率<span class="math inline">\(p_\theta(\tau)\)</span>也就越大。 <span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; = \frac{\partial} {\partial \theta}  \int p_{\theta}(\tau) \cdot G(\tau) \, {\rm d}\tau = \int \left(\frac{\partial}{\partial \theta}  p_{\theta}(\tau)\right) \cdot G(\tau) \, {\rm d}\tau \\&amp; =  \int \color{blue}{p_{\theta}(\tau)} \cdot \left(\color{blue}{\frac{1} {p_{\theta}(\tau)}}\frac{\partial}{\partial \theta}  p_{\theta}(\tau)\right) \cdot G(\tau) \, {\rm d}\tau \\&amp; = \int p_{\theta}(\tau) \cdot \left( \color{blue}{ \frac{\partial}{\partial \theta} \log  p_{\theta}(\tau) }\right) \cdot G(\tau) \, {\rm d}\tau \\&amp; \triangleq E_{\tau \sim  p_{\theta}(\tau)} \left[ \color{blue}{\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau)} \cdot G(\tau)\right]\end{align}\]</span> 2、梯度只和策略相关，轨迹的梯度 == 各个时刻的梯度的求和 <span class="math display">\[\begin{align}\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau)&amp; = \frac{\partial}{\partial \theta} \log \left( p(s_0) \cdot \prod_{t=0}^{T-1} \underbrace {\pi_{\theta}(a_t \mid s_t) }_{\color{blue}{执行动作}} \underbrace{p(s_{t+1} \mid s_t,a_t)}_{\color{blue}{环境改变}}\right) \\&amp; = \frac{\partial}{\partial \theta} \log  \left( \log p(s_0) + \color{blue}{\sum_{t=0}^{T-1} \log\pi_{\theta}(a_t \mid s_t) }+  \sum_{t=0}^{T-1} \log p(s_{t+1} \mid s_t,a_t)  \right) \\&amp; = \sum_{t=0}^{T-1} \frac{\partial}{\partial \theta}  \color{blue}{ \log\pi_{\theta}(a_t \mid s_t)}\end{align}\]</span> 3、策略梯度 == 轨迹的梯度*轨迹的回报 的期望 <span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; = E_{\tau \sim p_{\theta}(\tau)}\left[ \left(\sum_{t=0}^{T-1} \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t) \right)\cdot G(\tau)\right] \\&amp; =  E_{\tau \sim p_{\theta}(\tau)}\left[ \left(\sum_{t=0}^{T-1} \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t) \right)\cdot \left( \color{blue}{G(\tau_{1:k-1}) + \gamma^k \cdot G(\tau_{k:T})}\right)\right] \\&amp; =  E_{\tau \sim p_{\theta}(\tau)}\left[ \sum_{t=0}^{T-1} \left(\frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t) \cdot \gamma^t G(\tau_{t:T})\right)\right] \end{align}\]</span> 其中<span class="math inline">\(G(\tau_{t:T})​\)</span>是从时刻<span class="math inline">\(t​\)</span>作为起始时刻收到的总回报 <span class="math display">\[G(\tau_{t:T}) =  \sum_{i=t}^{T-1} \gamma ^{i-t} r_{i+1}\]</span> <strong>4. 总结</strong></p><p><span class="math display">\[\frac{\partial J(\theta)}{\partial \theta}  \triangleq E_{\tau \sim  p_{\theta}(\tau)} \left[ \color{blue}{\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau)} \cdot G(\tau)\right]\]</span></p><p><span class="math display">\[\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau) =  \sum_{t=0}^{T-1} \frac{\partial}{\partial \theta}  \color{blue}{ \log\pi_{\theta}(a_t \mid s_t)}\]</span></p><p><span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; =  E_{\tau \sim p_{\theta}(\tau)}\left[ \sum_{t=0}^{T-1} \left(\frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t) \cdot \gamma^t G(\tau_{t:T})\right)\right] \end{align}\]</span></p><h2 id="reinforce算法">REINFORCE算法</h2><p><strong>期望</strong>可以通过<strong>采样的方法来近似</strong>。对于当前策略<span class="math inline">\(\pi_\theta\)</span>，可以<strong>随机游走采集N个轨迹</strong>。<span class="math inline">\(\tau^{(1)},\cdots, \tau^{(N)}\)</span> <span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; \approx  \frac{1}{N}\sum_{n=1}^   N\left[ \sum_{t=0}^{T-1} \left(\frac{\partial}{\partial \theta}  \log\pi_{\theta}(a^{(n)}_t \mid s^{(n)}_t) \cdot \gamma^t G(\tau^{(n)}_{t:T})\right)\right] \end{align}\]</span> 可微分策略函数<span class="math inline">\(\pi_\theta(a\mid s)​\)</span>，折扣率<span class="math inline">\(\gamma​\)</span>，学习率<span class="math inline">\(\alpha​\)</span></p><p>初始化参数<span class="math inline">\(\theta\)</span>， 训练，直到<span class="math inline">\(\theta\)</span>收敛</p><p>1、<strong>根据<span class="math inline">\(\pi_\theta(a\mid s)\)</span>生成一条轨迹</strong> <span class="math display">\[\tau = s_0, a_0, s_1,a_1, \cdots, s_{T-1}, a_{T-1}, s_{T}\]</span> 2、<strong>在每一时刻更新参数</strong> (0~T)</p><ul><li>计算<span class="math inline">\(G(\tau_{t:T})\)</span></li><li><span class="math inline">\(\theta \leftarrow \theta + \alpha \cdot \gamma^tG(\tau_{t:T}) \cdot \frac{\partial}{\partial \theta} \log\pi_{\theta}(a_t \mid s_t)\)</span></li></ul><p>REINFORCE算法<code>缺点</code>：<strong>不同路径之间的方差很大，导致训练不稳定</strong>；需要根据一个策略<strong>采集一条完整的轨迹</strong>。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/reinforce.png" style="display:block; margin:auto" width="80%"></p><h2 id="带基准线的reinforce算法">带基准线的REINFORCE算法</h2><blockquote><p>值函数作为基准函数，去减小策略梯度的方差</p></blockquote><p>由于不同轨迹之间的方差很大，导致训练不稳定，使用基准函数去减小策略梯度的方差。</p><p><strong>1. 减小方差的办法</strong></p><p>目标：估计函数<span class="math inline">\(f\)</span>的期望，同时要减小<span class="math inline">\(f\)</span>的方差。</p><p>方法</p><ul><li>引入已知期望的函数<span class="math inline">\(g\)</span></li><li><strong><span class="math inline">\(\hat f = f - \alpha(g - E[g])\)</span></strong></li><li>推导可知： <span class="math inline">\(E[f] = E[\hat f]\)</span></li><li>用<span class="math inline">\(g\)</span>去减小<span class="math inline">\(f\)</span>的方差， <span class="math inline">\(D(f)=Var(f)\)</span></li></ul><p><span class="math display">\[D(\hat f) = D(f) - 2\alpha \cdot \rm{Cov}(f,g) + \alpha^2 \cdot D(g)\]</span></p><p><span class="math display">\[令\frac{\partial D(\hat f)}{\partial \alpha} = 0 \quad \to \quad \frac{D(\hat f)}{D(f)} = 1 - \rho^2(f, g)\]</span></p><p>所以<strong>，<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>的相关性越高，<span class="math inline">\(D(\hat f)\)</span>越小</strong>。</p><p><strong>2. 带基准线的REINFORCE算法核心</strong></p><p>每个时刻<span class="math inline">\(t\)</span>的策略梯度 <span class="math display">\[\frac{\partial J_t(\theta)}{\partial \theta} =  E_{s_t,a_t}\left[\alpha \cdot \gamma^tG(\tau_{t:T}) \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\right]\]</span> 为了<strong>减小策略梯度的方差</strong>，引入一个和<span class="math inline">\(a_t\)</span>无关的<strong>基准函数<span class="math inline">\(b(s_t)\)</span></strong>，策略梯度为： <span class="math display">\[\frac{\partial \hat J_t(\theta)}{\partial \theta} =  E_{s_t,a_t}\left[\alpha \cdot \gamma^t \left( \color{blue} {G(\tau_{t:T})  - b(s_t)}\right)\cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\right]\]</span> 因为<span class="math inline">\(b(s_t)\)</span>与<span class="math inline">\(a_t\)</span>无关，可以证明得到：（使用积分求平均，<span class="math inline">\(\int_{a_t} \pi_{\theta}(a_t \mid s_t) \,{\rm d} a_t= 1\)</span>） <span class="math display">\[E_{a_t}\left[b(s_t)\cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\right] =  \frac{\partial}{\partial \theta} (b(s_t) \cdot 1) = 0\]</span> 所以得到：<span class="math inline">\(\frac{\partial J_t(\theta)}{\partial \theta} = \frac{\partial \hat J_t(\theta)}{\partial \theta}​\)</span></p><p><strong>4. 基准函数的选择</strong></p><p>为了减小策略梯度的方差，希望<strong><span class="math inline">\(b(s_t)\)</span>与<span class="math inline">\(G(\tau_{t:T})\)</span> 越相关越好</strong>，所以选择<strong><span class="math inline">\(b(s_t) = V(s_t)\)</span></strong>（<a href="https://plmsmile.github.io/2018/04/01/37-reinforce-learning/#%E5%80%BC%E5%87%BD%E6%95%B0">值函数</a>）。</p><p>1、可学习的函数<span class="math inline">\(V_{\phi}(s_t)\)</span>来近似值函数，类似于<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/#%E6%B7%B1%E5%BA%A6q%E7%BD%91%E7%BB%9C">Q网络</a> <span class="math display">\[\phi^* = \arg \min_{\phi} \left(V(s_t) - V_{\phi}(s_t)\right)^2\]</span> 2、<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/#%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E9%87%87%E6%A0%B7%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95">蒙塔卡罗方法</a>进行估计值函数 也就是<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/#%E6%80%BB%E4%BD%93%E6%80%9D%E6%83%B3-1">增量计算</a>Q(s,a)嘛。</p><p><strong>5. 带基准线的REINFORCE算法步骤</strong></p><p>输入</p><ul><li>状态空间和动作空间，<span class="math inline">\(\cal {S,A}\)</span></li><li>可微分的策略函数<span class="math inline">\(\pi_\theta(a \mid s)\)</span></li><li>可微分的状态值函数<span class="math inline">\(V_{\phi}(s_t)\)</span></li><li>折扣率<span class="math inline">\(\gamma\)</span>，学习率<span class="math inline">\(\alpha, \beta\)</span></li></ul><p>随机初始化参数<span class="math inline">\(\theta, \phi\)</span></p><p>不断训练，直到<span class="math inline">\(\theta\)</span>收敛</p><p>1、根据策略<span class="math inline">\(\pi_\theta(a\mid s)\)</span><strong>生成一条完整轨迹</strong> ：<span class="math inline">\(\tau = s_0, a_0, s_1,a_1, \cdots, s_{T-1}, a_{T-1}, s_{T}\)</span></p><p>2、在每一时刻更新参数</p><p>计算<strong>当前时刻的轨迹回报<span class="math inline">\(G(\tau_{t:T})\)</span></strong> ，再利用<code>基准函数(值函数)</code>进行修正，得到<span class="math inline">\(\delta\)</span> <span class="math display">\[\delta \leftarrow  G(\tau_{t:T}) - V_{\phi} (s_t)\]</span> 更新<strong>值函数<span class="math inline">\(V_\phi(s)\)</span>的参数<span class="math inline">\(\phi\)</span></strong> <span class="math display">\[\phi \leftarrow  \phi + \beta \cdot \delta \cdot \frac{\partial}{ \partial \phi} V_{\phi}(s_t)\]</span> 更新<strong>策略函数<span class="math inline">\(\pi_\theta(a \mid s)\)</span>的参数<span class="math inline">\(\theta\)</span></strong> <span class="math display">\[\theta \leftarrow  \theta + \alpha \cdot \gamma^t\delta \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\]</span> <code>缺点</code>： 需要根据策略采集一条完整的轨迹。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/reinforce-base.png" style="display:block; margin:auto" width="80%"></p><h1 id="actor-critic算法">Actor-Critic算法</h1><h2 id="思想">思想</h2><p><code>演员-评论员算法</code>是一种结合<code>策略梯度</code>和<code>时序差分学习</code>的强化学习方法。</p><p>开始，演员随机表演，评论员随机打分；不断学习，评论员评分越来越准，演员的动作越来越好。</p><ul><li>演员：<strong>策略函数<span class="math inline">\(\pi_\theta(s,a)\)</span></strong>，学习一个策略来得到尽可能高的回报</li><li>评论员：<strong>值函数<span class="math inline">\(V_\phi(s)\)</span></strong>，评估当前策略函数（演员）的好坏</li></ul><p><strong>演员</strong></p><p>根据<span class="math inline">\(s\)</span>和策略<span class="math inline">\(\pi_\theta(a\mid s)\)</span>，执行动作<span class="math inline">\(a\)</span>，环境变为<span class="math inline">\(s^\prime\)</span>，得到奖励<span class="math inline">\(r\)</span></p><p><strong>评论员</strong></p><p>根据<code>真实奖励</code><span class="math inline">\(r\)</span>和<code>之前的标准</code>，<strong>打分（估计回报）</strong>：<span class="math inline">\(r + \gamma V_\phi(s^\prime)\)</span> ，<strong>再调整自己的打分标准<span class="math inline">\(\phi\)</span></strong>。<span class="math inline">\(\min_{\phi} \left(\hat G(\tau_{t:T}) - V_{\phi}(s_{t}) \right)^2\)</span></p><p>使评分更加接近环境的真实回报。</p><p><strong>演员</strong></p><p>演员根据评论的打分，调整自己的策略<span class="math inline">\(\pi_\theta\)</span>，争取下次做得更好。</p><p><code>优点</code>：可以单步更新参数，不需要等到回合结束才进行更新。</p><h2 id="值函数的三个功能">值函数的三个功能</h2><p><strong>1. 估计轨迹真实回报（打分）</strong> <span class="math display">\[\hat G(\tau_{t:T}) = r_{t+1} + \gamma V_{\phi}(s_{t+1})\]</span> <strong>2. 更新值函数参数<span class="math inline">\(\phi\)</span> （调整打分标准）</strong> <span class="math display">\[\min_{\phi} \left(\hat G(\tau_{t:T}) - V_{\phi}(s_{t}) \right)^2\]</span> <strong>3. 更新策略参数<span class="math inline">\(\theta\)</span>时，作为基函数来减少策略梯度的方差（调整策略）</strong> <span class="math display">\[\theta \leftarrow  \theta + \alpha \cdot \gamma^t \left( G(\tau_{t:T}) - V_{\phi} (s_t)\right) \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\]</span></p><h2 id="算法实现步骤">算法实现步骤</h2><p><strong>输入</strong></p><ul><li>状态空间和动作空间，<span class="math inline">\(\cal {S,A}\)</span></li><li>可微分的策略函数<span class="math inline">\(\pi_\theta(a \mid s)\)</span></li><li>可微分的状态值函数<span class="math inline">\(V_{\phi}(s_t)\)</span></li><li>折扣率<span class="math inline">\(\gamma\)</span>，学习率<span class="math inline">\(\alpha &gt;0, \beta&gt;0\)</span></li></ul><p><strong>随机初始化参数<span class="math inline">\(\theta, \phi\)</span></strong></p><p><strong>迭代直到<span class="math inline">\(\theta\)</span>收敛，初始状态<span class="math inline">\(s\)</span>, <span class="math inline">\(\lambda=1\)</span></strong></p><p>从s开始，直到<span class="math inline">\(s\)</span>为终止状态</p><p>1、状态s，选择动作<span class="math inline">\(a = \pi_\theta(a\mid s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>，得到即时奖励<span class="math inline">\(r\)</span>和新状态<span class="math inline">\(s^\prime\)</span></p><p>3、利用值函数作为基函数计算<span class="math inline">\(\delta\)</span>，<span class="math inline">\(\delta \leftarrow r + \gamma V_{\phi}(s^\prime) - V_{\phi}(s)\)</span></p><p>4、更新值函数：<span class="math inline">\(\phi \leftarrow \phi + \beta \cdot \delta \cdot \frac{\partial V_{\phi}(s)}{\partial \phi}\)</span></p><p>5、更新策略函数：<span class="math inline">\(\theta \leftarrow \theta + \alpha \cdot \lambda \delta \cdot \frac{\partial }{\partial \theta} \log \pi_\theta(a\mid s)\)</span></p><p>6、更新折扣率和状态：<span class="math inline">\(\lambda \leftarrow \lambda \cdot \gamma, \quad s \leftarrow s^\prime\)</span></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/actor-critic.png" style="display:block; margin:auto" width="80%"></p><h1 id="强化学习算法总结">强化学习算法总结</h1><h2 id="方法总览">方法总览</h2><p><strong>1. 通用步骤</strong></p><ol style="list-style-type: decimal"><li>执行策略，生成样本</li><li>估计回报</li><li>更新策略</li></ol><p><strong>2. 值函数与策略函数的比较</strong></p><p><code>值函数的方法</code></p><p>策略更新，导致值函数的改变比较大，对收敛性有一定的影响</p><p><code>策略函数的方法</code></p><p>策略更新，<strong>更加平稳</strong>。</p><p>缺点：策略函数的解空间很大，难以进行充分采样，导致方差较大，容易陷入局部最优解。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/reinforce-all.png" style="display:block; margin:auto" width="75%"></p><h2 id="四个典型方法">四个典型方法</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/four-compare.png" style="display:block; margin:auto" width="70%"></p><h2 id="与监督学习的区别">与监督学习的区别</h2><table><thead><tr class="header"><th align="center"></th><th align="center">强化学习</th><th align="center">监督学习</th></tr></thead><tbody><tr class="odd"><td align="center">样本</td><td align="center">与环境进行交互产生样本，进行试错学习</td><td align="center">人工收集并标注</td></tr><tr class="even"><td align="center">反馈</td><td align="center">只有奖励，并且是延迟的</td><td align="center">需要明确的指导信息（每个状态对应一个动作）</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;基于策略函数的学习方法和Actor-Critc算法。
    
    </summary>
    
      <category term="强化学习" scheme="http://plmsmile.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="策略梯度" scheme="http://plmsmile.github.io/tags/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/"/>
    
      <category term="策略函数" scheme="http://plmsmile.github.io/tags/%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0/"/>
    
      <category term="REINFORCE" scheme="http://plmsmile.github.io/tags/REINFORCE/"/>
    
      <category term="基准函数" scheme="http://plmsmile.github.io/tags/%E5%9F%BA%E5%87%86%E5%87%BD%E6%95%B0/"/>
    
      <category term="Actot-Critic" scheme="http://plmsmile.github.io/tags/Actot-Critic/"/>
    
      <category term="强化学习算法总结" scheme="http://plmsmile.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>基于值函数的学习</title>
    <link href="http://plmsmile.github.io/2018/04/21/40-value-learning/"/>
    <id>http://plmsmile.github.io/2018/04/21/40-value-learning/</id>
    <published>2018-04-21T05:14:36.000Z</published>
    <updated>2018-11-25T08:30:08.851Z</updated>
    
    <content type="html"><![CDATA[<p>基于值函数的学习方法：贝尔曼方程，动态规划、蒙特卡洛、时续差分、Q网络。<a id="more"></a></p><p><img src="" style="display:block; margin:auto" width="70%"></p><blockquote><p>强化学习基于值函数的学习方法。最重要要是SARSA、Q学习、DQN。但是这些都依赖于前面的动态规划和蒙特卡罗方法。</p></blockquote><p><a href="https://plmsmile.github.io/2018/04/01/37-reinforce-learning/">强化学习基础笔记</a></p><h1 id="贝尔曼和贝尔曼最优方程">贝尔曼和贝尔曼最优方程</h1><blockquote><ol style="list-style-type: decimal"><li><span class="math inline">\(V(s)\)</span>函数和<span class="math inline">\(Q(s,a)\)</span>函数</li><li>贝尔曼方程（选择所有可能的均值）</li><li>贝尔曼最优方程（直接选择最大值）</li></ol></blockquote><h2 id="v函数与q函数">V函数与Q函数</h2><p>V函数：以<strong>s为初始状态</strong>，执行策略<span class="math inline">\(\pi\)</span>得到的<code>期望回报</code>（所有轨迹回报的均值） <span class="math display">\[V^\pi(s) = E_{\tau \sim p(\tau)} [\sum_{t=0}^{T-1}r_{t+1} \mid \tau_{s_0} = s]\]</span> Q函数：以<strong>s为初始状态，执行动作a</strong>，执行策略<span class="math inline">\(\pi\)</span>得到的期望回报 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> 利用V函数去计算Q函数 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><h2 id="贝尔曼方程">贝尔曼方程</h2><p><span class="math inline">\(V(s)\)</span>的贝尔曼方程，选择<strong>所有a的期望回报</strong>， 也是<strong>Q函数的均值</strong>，<span class="math inline">\(V(s)=E_a[Q(s, a)]\)</span> <span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}[Q^\pi(s, a)]\]</span></p><p><span class="math inline">\(Q(s,a)\)</span>函数的贝尔曼方程 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma E_{a\prime \sim \pi(a\prime \mid s\prime)}[Q^\pi(s\prime, a\prime)]]\]</span></p><h2 id="贝尔曼最优方程">贝尔曼最优方程</h2><p><span class="math inline">\(V(s)\)</span>函数的贝尔曼最优方程，实际上是<strong>直接选择所有a中的最大回报</strong> ： <span class="math display">\[V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span> <span class="math inline">\(Q(s,a)\)</span>函数的贝尔曼最优方程 <span class="math display">\[Q^*(s, a) =  E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma \max_\limits{a\prime}Q^*(s^\prime, a^\prime)]\]</span></p><h1 id="值函数的学习方法">值函数的学习方法</h1><blockquote><ol style="list-style-type: decimal"><li>穷举所有策略选择最好策略（没用）</li><li>迭代优化策略，<strong>根据策略的值函数去优化策略</strong> （重点）</li><li>动态规划（已知状态转移概率<span class="math inline">\(p(s^\prime \mid s, a)\)</span>和奖励函数<span class="math inline">\(r(s, a, s^\prime)\)</span>）</li><li>蒙特卡罗方法（不知模型），先采一些样本，再优化</li><li>时序差分学习算法：SARAS和Q学习</li><li>深度Q网络</li></ol></blockquote><p>值函数（<span class="math inline">\(V^\pi(s)\)</span>、<span class="math inline">\(Q^\pi(s, a)\)</span> ）用来对策略<span class="math inline">\(\pi(a \mid s)\)</span>进行评估。</p><p><strong>1. 穷举策略法</strong></p><p>如果策略有限，可以对所有策略进行评估，选出最优策略 <span class="math display">\[\forall s, \qquad \pi^* = \arg \max_\limits{\pi} V^\pi(s)\]</span> 策略空间<span class="math inline">\(\vert \mathcal A\vert^{\vert \mathcal S\vert}\)</span>非常大，根本无法搜索。</p><p><strong>2. 迭代法优化策略</strong></p><p>步骤如下，直到收敛</p><ol style="list-style-type: decimal"><li>随机初始化一个策略</li><li><strong>计算该策略的值函数</strong>：<code>动态规划</code>， <code>蒙特卡罗</code></li><li><strong>根据值函数来设置新的策略</strong></li></ol><p>比如</p><p>给一个初始策略<span class="math inline">\(\pi(a\mid s)\)</span>， 根据<span class="math inline">\(Q^\pi(s, a)\)</span>去不断迭代去优化，得到新的策略函数<span class="math inline">\(\pi^\prime (a\mid s)\)</span>（<strong>确定性策略</strong>），直到收敛。 <span class="math display">\[\pi^\prime (a\mid s) = \begin {cases}&amp; 1  &amp; a  = \arg \max_\limits{\hat a}Q^\pi(s, \hat a) \\ &amp; 0 &amp;  \text{others}\\\end {cases}\]</span> <strong>新策略的值函数</strong>会不断变大： <span class="math display">\[Q^{\pi^\prime}(s, \hat a) \ge Q^\pi(s, \hat a)\]</span></p><h1 id="动态规划算法">动态规划算法</h1><h2 id="总体思想">总体思想</h2><blockquote><ol style="list-style-type: decimal"><li>思想：已知模型，通过贝尔曼方程来<strong>迭代计算值函数</strong>，通过<strong>值函数去优化策略</strong>为固定策略</li><li>两种方法：策略迭代-贝尔曼方程（所有可能的均值），值迭代-贝尔曼最优方程（直接）</li><li>缺点：要求模型已知，效率太低</li></ol></blockquote><p>基于模型的强化学习，叫做<code>模型相关的强化学习</code>，或有模型的强化学习。</p><p><strong>1. 动态规划思想</strong></p><ul><li><strong>已知模型</strong>：状态转移概率<span class="math inline">\(p(s \prime \mid s, a)\)</span> 和奖励<span class="math inline">\(r(s, a, s\prime)\)</span></li><li>可以通过<code>贝尔曼方程</code>或<code>贝尔曼最优方程</code>来<strong>迭代计算值函数<span class="math inline">\(V(s)\)</span> </strong>，再<strong>通过<span class="math inline">\(V(s)\)</span>去计算<span class="math inline">\(Q(s,a)\)</span></strong></li><li><strong>通过值函数来优化策略</strong>，一般为优化为固定策略<span class="math inline">\(\pi(s)=a\)</span></li></ul><p><strong>2. 两种方法</strong></p><ul><li>策略迭代算法 ： <code>贝尔曼方程</code>更新值函数，算出所有值函数，计算均值</li><li>值迭代算法：<code>贝尔曼最优方程</code>更新值函数，<strong>直接优化计算最大值</strong></li></ul><p><strong>3. 缺点</strong></p><ul><li>要求模型已知：<span class="math inline">\(p(s \prime \mid s, a)\)</span>、 <span class="math inline">\(r(s, a, s\prime)\)</span></li><li>效率太低：可以通过神经网络来近似计算值函数</li></ul><h2 id="策略迭代算法">策略迭代算法</h2><blockquote><ol style="list-style-type: decimal"><li>已知模型：状态转移概率<span class="math inline">\(p(s^\prime \mid s, a)\)</span>和奖励函数<span class="math inline">\(r(s, a, s^\prime)\)</span></li><li>使用<code>贝尔曼方程</code>迭代计算<span class="math inline">\(V^{\pi}(s)\)</span>，<strong>求均值</strong>， <span class="math inline">\(V^{\pi}(s) = E_{a}E_{s^\prime}[r(s,a,s^\prime)+ \gamma V^\pi(s\prime)]\)</span></li><li>利用<span class="math inline">\(V^{\pi}(s)\)</span>去计算<span class="math inline">\(Q^{\pi}(s,a)\)</span>。<strong>求均值</strong>，<span class="math inline">\(Q^\pi(s, a) = E_{s\prime} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\)</span></li><li>根据<span class="math inline">\(Q^\pi(s, a)\)</span>更新策略<span class="math inline">\(\pi(s)=a\)</span>，选择最好的动作a，<strong>更新为固定策略</strong></li></ol></blockquote><p><strong>1. 初始化策略</strong> <span class="math display">\[\forall s, \forall a, \quad \pi(a \mid s) = \frac{1}{\vert \cal A\vert}\]</span> <strong>2. 使用贝尔曼方程迭代计算值函数<span class="math inline">\(V^\pi(s)\)</span></strong> <span class="math display">\[\forall s, \quad V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> <strong>3. 利用值函数<span class="math inline">\(V^\pi(s)\)</span>计算<span class="math inline">\(Q^\pi(s, a)\)</span></strong> <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> <strong>4. 根据<span class="math inline">\(Q^\pi(s, a)\)</span>更新策略，选择最好的动作a，更新为固定策略</strong> <span class="math display">\[\forall s, \qquad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span> <img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/strategy-iter.png" style="display:block; margin:auto" width="70%"></p><h2 id="值迭代算法">值迭代算法</h2><blockquote><ol style="list-style-type: decimal"><li>最优值函数：最优策略对应的值函数</li><li>贝尔曼最优方程：<span class="math inline">\(V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\)</span> ，直接选择最大的a</li><li>值迭代算法：最优方程更新值函数<span class="math inline">\(V(s)\)</span>， 计算<span class="math inline">\(Q(s,a)\)</span>， 更新策略<span class="math inline">\(\pi(s)=a\)</span></li></ol></blockquote><p><strong>1. 最优值函数</strong></p><p><strong>最优策略<span class="math inline">\(\pi^*\)</span>对应的值函数</strong>就是<code>最优值函数</code> <span class="math display">\[V^*(s) = \max_\limits{a} Q^*(s, a)\]</span> <strong>2. 贝尔曼最优方程</strong> <span class="math display">\[V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span></p><p><span class="math display">\[Q^*(s, a) =  E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma \max_\limits{a\prime}Q^*(s^\prime, a^\prime)]\]</span></p><p><strong>3. 值迭代算法</strong></p><p>值迭代算法：使用<strong>贝尔曼最优方程去更新值函数</strong>，收敛时的值函数就是最优值函数，对应的策略也是最优的策略。</p><p>1、 <strong>初始化值函数</strong> <span class="math display">\[\forall s \in S, \quad V(s) = 0\]</span> 2、 <strong>使用贝尔曼最优方程更新<span class="math inline">\(V(s)\)</span>，直到收敛</strong> <span class="math display">\[\forall s \in S, \quad V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span> 3、 <strong>计算<span class="math inline">\(Q(s,a)\)</span></strong> <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> 4、 <strong>更新策略<span class="math inline">\(\pi(s)=a\)</span></strong> <span class="math display">\[\forall s, \quad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span> 5、 <strong>输出策略<span class="math inline">\(\pi\)</span></strong></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/value-iter.png" style="display:block; margin:auto" width="70%"></p><h1 id="蒙特卡罗采样学习方法">蒙特卡罗采样学习方法</h1><blockquote><ol style="list-style-type: decimal"><li>蒙特卡罗方法：随机游走采集样本，估计 <span class="math inline">\(Q^\pi(s, a) \approx \frac{1}{N} \sum_{n=1}^NG(\tau^{(n)})\)</span>，根据Q去改进策略</li><li><span class="math inline">\(\epsilon 贪心法\)</span>: 依概率选择<span class="math inline">\(\pi^\epsilon (s) = \pi(s)\)</span></li><li>同策略和异策略：采样与改进策略相同为同策略</li><li>需要拿到完整的轨迹才能对策略评估更新模型，效率较低</li></ol></blockquote><p><code>模型无关的强化学习</code>也称为无模型的强化学习。蒙特卡罗方法：</p><blockquote><p>在不知<span class="math inline">\(p(s \prime \mid s, a)\)</span>、 <span class="math inline">\(r(s, a, s\prime)\)</span> 的情况下， 需要智能体和环境进行交互，并且收集一些样本。然后根据这些样本去求解马尔可夫决策过程最优策略</p></blockquote><p>Q函数<span class="math inline">\(Q^\pi(s, a)\)</span>，初始状态为<span class="math inline">\(s\)</span>， 执行动作<span class="math inline">\(a\)</span>后，策略<span class="math inline">\(\pi\)</span>能得到的期望总回报。<br><span class="math display">\[Q^\pi(s, a) = E_{\tau \sim p(\tau)} [G(\tau) \mid \tau_{s_0} = s, \tau_{a_0} = a]\]</span> 模型未知，Q函数可以通过采样来计算。</p><p><strong>1. 蒙特卡罗方法</strong></p><p>1、从状态<span class="math inline">\(s\)</span>、 动作<span class="math inline">\(a\)</span>开始<strong>随机游走</strong>探索环境， <strong>采集N个样本</strong>（N个轨迹）</p><p>2、得到N个轨迹<span class="math inline">\(\tau^{(1)}, \cdots, \tau^{(N)}\)</span>，得到它们的总回报<span class="math inline">\(G(\tau^{(1)}), \cdots, G(\tau^{(N)})\)</span></p><p>3、利用<strong>轨迹的总回报去估计出<span class="math inline">\(Q^\pi(s, a)\)</span></strong> 。 <span class="math inline">\(Q^\pi(s, a) \approx \hat Q^\pi(s, a) = \frac{1}{N} \sum_{n=1}^NG(\tau^{(n)})\)</span></p><p>4、基于<span class="math inline">\(Q^\pi(s, a)\)</span> 去<strong>改进策略</strong>， <strong><span class="math inline">\(\epsilon\)</span>贪心法</strong></p><p>5、在新的策略下，再去采集样本、去估计Q，再去改进策略，直到收敛</p><p><strong>2. 利用和探索</strong></p><p>如果采用<code>确定性策略</code> :</p><ul><li>则每次试验得到的轨迹是一样的</li><li>只能计算出<span class="math inline">\(Q^\pi(s, \pi(s))\)</span> ，无法计算出<span class="math inline">\(Q^\pi(s, a\prime)\)</span>，即无法计算出其它的<span class="math inline">\(a\prime\)</span>的Q函数</li><li>只对当前环境进行了利用，而没有探索</li><li>而试验的轨迹应该覆盖所有的状态和动作，以找到更好的策略</li></ul><p>采用<strong><span class="math inline">\(\epsilon\)</span>贪心法</strong> <span class="math display">\[\pi^\epsilon(s) =\begin {cases}&amp; \pi(s)  &amp; \text{依概率 } 1-\epsilon \\ &amp; a\prime \quad\text{(随机选择)} &amp;  \text{依概率 } \epsilon\\\end {cases}\]</span> <strong>3. 同策略和异策略</strong></p><ul><li>同策略：采样策略<span class="math inline">\(\pi^\epsilon(s)\)</span>， 改进策略<span class="math inline">\(\pi^\epsilon(s)\)</span>， 相同</li><li>异策略：采样策略<span class="math inline">\(\pi^\epsilon(s)\)</span>， 改进策略<span class="math inline">\(\pi(s)\)</span>， 不同。可以使用重要性采样、重要性权重来优化<span class="math inline">\(\pi\)</span></li></ul><h1 id="时序差分学习算法">时序差分学习算法</h1><h2 id="总体思想-1">总体思想</h2><blockquote><ol style="list-style-type: decimal"><li>无需知道完整轨迹就能对策略进行评估。时序差分学习=蒙特卡罗+动态规划</li><li>贝尔曼<strong>估计轨迹的回报</strong>。<span class="math inline">\(G(\tau_{0:T}^{(N)}) = r(s, a, s^\prime) + \gamma \cdot \hat Q^\pi_{N-1}(s^\prime, a^ \prime)\)</span></li><li><strong>增量计算<span class="math inline">\(\hat Q_N^\pi(s,a)\)</span></strong>。 <span class="math inline">\(\hat Q_N^\pi(s,a) = \hat Q_{N-1}^\pi(s,a) + \alpha \cdot \left(G(\tau ^{(N)}) - \hat Q_{N-1}^\pi(s,a) \right)\)</span></li></ol></blockquote><p>蒙特卡罗方法需要拿到完整的轨迹，才能对策略进行评估。</p><p>时序差分学习（temporal-difference learning）结合了动态规划和蒙特卡罗方法。</p><p><strong>1. 增量计算<span class="math inline">\(\hat Q_N^\pi(s,a)\)</span></strong></p><p>蒙特卡罗方法：从状态<span class="math inline">\(s\)</span>，动作<span class="math inline">\(a\)</span>开始，随机游走，采样N个样本</p><p><span class="math inline">\(G(\tau ^{(N)})\)</span> ：第N次试验的总回报</p><p><span class="math inline">\(\hat Q_N^\pi(s,a)\)</span> ：第N次试验后值函数的平均值，推导如下： <span class="math display">\[\begin{align}\hat Q_N^\pi(s,a) &amp; = \frac{1}{N} \sum_{i=1}^NG(\tau ^{(i)}) \\&amp; = \frac{1}{N} \left(G(\tau ^{(N)}) + \color{blue}{\sum_{i=1}^{N-1}G(\tau ^{(i)})} \right) \\&amp; = \frac{1}{N} \left(G(\tau ^{(N)}) + \color{blue}{(N-1) \hat Q_{N-1}^\pi(s,a)} \right) \\&amp; = \hat Q_{N-1}^\pi(s,a) +  \frac{1}{N} \left(G(\tau ^{(N)}) - \hat Q_{N-1}^\pi(s,a)\right)\end{align}\]</span> <strong>值函数</strong><span class="math inline">\(\hat Q_{N}^\pi (s, a)\)</span> ： <strong>第N次后的平均</strong> = <strong>N-1次后的平均</strong> + <strong>一个增量</strong>， <span class="math inline">\(\alpha\)</span>是一个较小的权值。 <span class="math display">\[\hat Q_N^\pi(s,a)  = \hat Q_{N-1}^\pi(s,a) + \alpha \cdot \left(G(\tau ^{(N)}) - \hat Q_{N-1}^\pi(s,a) \right)\]</span> <strong>增量</strong> ：实际回报与估计回报直接的误差。 <span class="math display">\[\delta = G(\tau ^{(N)}) - \hat Q_{N-1}^\pi(s,a)\]</span> <strong>2. 贝尔曼方程估计<span class="math inline">\(G(\tau ^{(N)})\)</span></strong></p><p>从<span class="math inline">\(s, a\)</span>开始，采样下一步的状态和动作<span class="math inline">\((s^\prime, a^\prime)\)</span> ，得到奖励<span class="math inline">\(r(s, a, s^\prime)\)</span>。</p><p><strong>无需得到完整的轨迹去计算总回报</strong>，使用贝尔曼方程去估计第N次试验后面<span class="math inline">\((s^\prime, a^\prime)\)</span>的总回报。</p><p>使用N-1次实验后的<span class="math inline">\(\hat Q^\pi_{N-1}(s^\prime, a^\prime)\)</span>，去估计第N次试验中后续<span class="math inline">\((s^\prime, a^\prime)\)</span>的总回报 <span class="math inline">\(G(\tau_{1:T}^{(N)} \mid \tau_{s_1} = s^\prime, \tau_{a_1} = a^\prime)\)</span>。 <span class="math display">\[\begin{align}G(\tau_{0:T}^{(N)}) &amp; =r(s, a, s^\prime) + \gamma \cdot \color{blue}{G(\tau_{1:T}^{(N)} \mid \tau_{s_1} = s^\prime, \tau_{a_1} = a^\prime)}\\&amp; \approx r(s, a, s^\prime) + \gamma \cdot \color{blue}{\hat Q^\pi_{N-1}(s^\prime, a^ \prime)} \end{align}\]</span> <strong>3. 两种算法</strong></p><ul><li>SARSA：同策略。采样下一个动作：<span class="math inline">\(a^\prime = \pi^\epsilon (s^\prime)\)</span>，值函数更新<span class="math inline">\(Q(s^\prime, a^\prime)\)</span>，更新的Q是关于策略<span class="math inline">\(\pi^\epsilon\)</span>的</li><li>Q学习算法：直接选择最大的值函数<span class="math inline">\(\max_\limits{a^\prime}Q(s^\prime, a^\prime)\)</span>更新，更新的Q是关于策略<span class="math inline">\(\pi\)</span>的。</li></ul><p><strong>4. 蒙特卡罗方法和时序差分方法比较</strong></p><ul><li>蒙特卡罗方法：需要完整路径才能知道总回报，不依赖马尔可夫性质</li><li>时序差分学习：只需要一步就能知道总回报，依赖于马尔可夫性质</li></ul><p><strong>5. 总结</strong></p><p>贝尔曼估计总回报（马尔可夫性，动态规划） <span class="math display">\[G(\tau) \leftarrow r(s, a, s^\prime) + \gamma \cdot Q(s^\prime, a^\prime)\]</span> 增量更新值函数（蒙特卡罗） <span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (G(\tau) - Q(s, a))\]</span></p><p><span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (\underbrace{ r+ \gamma \cdot Q(s^\prime, a^\prime)}_{\color{blue}{实际值}} - \underbrace{Q(s, a)}_{\color{blue}{预期值}})\]</span></p><p><span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (r+ \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a)\]</span></p><h2 id="sarsa算法">SARSA算法</h2><blockquote><ol style="list-style-type: decimal"><li>当前<span class="math inline">\(s, a\)</span>， 奖励<span class="math inline">\(r(s, a, s^\prime)\)</span>， 新的<span class="math inline">\(s^\prime, a^\prime\)</span>， 优化<span class="math inline">\(Q(s,a)\)</span></li><li>贝尔曼<strong>估计实际奖励<span class="math inline">\(G(\tau)\)</span></strong>：<span class="math inline">\(r+ \gamma \cdot Q(s^\prime, a^\prime)\)</span></li><li><strong>增量计算Q</strong>：<span class="math inline">\(Q(s, a) \leftarrow Q(s, a) + \alpha \cdot (r+ \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a))\)</span></li><li><strong>更新策略<span class="math inline">\(\pi(s)\)</span></strong> ：<span class="math inline">\(\pi(s) = \arg \max_\limits{a \in \cal A} Q(s, a)\)</span></li><li>SARAS：优化所有<span class="math inline">\(Q(s,a)\)</span>直到收敛。对每一个<span class="math inline">\(s,a\)</span>，每一步状态转移，计算Q，直到s为终止状态</li></ol></blockquote><p>SARAS<code>State Action Reward State Action</code>算法，是一种同策略的时序差分学习算法。</p><p><strong>1. 思想目的</strong></p><p>要算出<span class="math inline">\(\hat Q_N^\pi(s,a)\)</span>，只需知道下面三项：</p><ul><li>当前状态和动作<span class="math inline">\((s, a)\)</span></li><li>得到的奖励<span class="math inline">\(r(s, a, s^\prime)\)</span></li><li>下一步的状态和动作<span class="math inline">\((s^\prime, a^\prime)\)</span></li></ul><p>不断优化Q函数，减少实际值和预期值的差距。</p><p><strong>2. 核心计算</strong></p><p>结合增量计算<span class="math inline">\(\hat Q_N^\pi(s,a)\)</span> ， 贝尔曼方程估计<span class="math inline">\(G(\tau ^{(N)})\)</span> <span class="math display">\[\hat Q_N^\pi(s,a)  = \hat Q_{N-1}^\pi(s,a) + \alpha \cdot \left(G(\tau ^{(N)}) - \hat Q_{N-1}^\pi(s,a) \right)\]</span></p><p><span class="math display">\[G(\tau_{0:T}^{(N)}) = r(s, a, s^\prime) + \gamma \cdot \hat Q^\pi_{N-1}(s^\prime, a^ \prime)\]</span></p><p>得到： <span class="math display">\[\hat Q_N^\pi(s,a) = (1-\alpha)\cdot \hat Q_{N-1}^\pi(s,a) + \alpha \cdot \left( r(s, a, s^\prime) + \gamma \cdot \hat Q^\pi_{N-1}(s^\prime, a^ \prime)\right)\]</span> <strong>简单点</strong>： <span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (G(\tau) - Q(s, a))\]</span></p><p><span class="math display">\[G(\tau) \leftarrow r(s, a, s^\prime) + \gamma \cdot Q(s^\prime, a^\prime)\]</span></p><p><span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (\underbrace{ r+ \gamma \cdot Q(s^\prime, a^\prime)}_{\color{blue}{实际值}} - \underbrace{Q(s, a)}_{\color{blue}{预期值}})\]</span></p><p><strong>3. SARSA算法步骤</strong></p><p>输入：状态空间<span class="math inline">\(\cal S\)</span>， 动作空间<span class="math inline">\(\cal A\)</span>，折扣率<span class="math inline">\(\gamma\)</span>， 学习率<span class="math inline">\(\alpha\)</span></p><p>输出：策略<span class="math inline">\(\pi(s)\)</span></p><p>1 初始化</p><p>随机初始化<span class="math inline">\(Q(s,a)\)</span>，平均初始化策略<span class="math inline">\(\pi(s)\)</span> <span class="math display">\[\forall s, \forall a, \quad \pi(a \mid s) = \frac{1}{\vert \cal A\vert}\]</span> 2 计算所有<span class="math inline">\(Q(s,a)\)</span>， 直到全部收敛</p><p>选择初始状态<span class="math inline">\(s\)</span>，和动作<span class="math inline">\(a=\pi^\epsilon(s)\)</span>。从<span class="math inline">\((s, a)\)</span>开始向后执行，直到<span class="math inline">\(s\)</span>为终止状态</p><p><strong>a. 执行动作，得到新状态和新动作</strong></p><ul><li>当前状态<span class="math inline">\(s\)</span>，动作<span class="math inline">\(a\)</span></li><li>执行动作<span class="math inline">\(a\)</span>：得到奖励<span class="math inline">\(r\)</span>和新状态<span class="math inline">\(s^\prime\)</span></li><li>选择新动作：<span class="math inline">\(a^\prime=\pi^\epsilon(s^\prime)\)</span></li></ul><p><strong>b. 增量计算 <span class="math inline">\(Q(s, a)\)</span></strong> <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> <strong>c. 更新策略<span class="math inline">\(\pi(s)\)</span></strong> <span class="math display">\[\pi(s) = \arg \max_\limits{a \in \cal A} Q(s, a)\]</span> <strong>d. 状态前进</strong> <span class="math display">\[s \leftarrow s^\prime, \quad a \leftarrow a^\prime\]</span> <img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/sarsa.png" style="display:block; margin:auto" width="80%"></p><h2 id="q学习算法">Q学习算法</h2><blockquote><ol style="list-style-type: decimal"><li>SARAS：<span class="math inline">\(s,a \to r, s^\prime\)</span>， 选择新状态<span class="math inline">\(a^\prime = \pi^\epsilon(s^\prime)\)</span>，值函数<span class="math inline">\(Q(s^\prime, a^\prime)\)</span></li><li>Q：<span class="math inline">\(s\)</span>，选择当前动作<span class="math inline">\(a= \pi^\epsilon(s)\)</span>，<span class="math inline">\(\to r, s^\prime\)</span>，直接选择最大的值函数<span class="math inline">\(\max_\limits{a^\prime}Q(s^\prime, a^\prime)\)</span></li></ol></blockquote><p>SARAS是Q学习算法的一种改进。</p><p><strong>1. SARAS</strong></p><p>1、当前状态<span class="math inline">\(s\)</span>，当前动作<span class="math inline">\(a\)</span> （初始时选择<span class="math inline">\(a=\pi^\epsilon(s)\)</span>，后续是更新得到的）</p><p>2、<strong>执行动作</strong><span class="math inline">\(a\)</span>，得到<strong>新状态</strong><span class="math inline">\(s^\prime\)</span>，得到<strong>奖励</strong><span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>4、<strong>依概率选择新动作</strong><span class="math inline">\(a = \pi^\epsilon(s^\prime)\)</span>，<strong>新状态新动作的值函数</strong>：<span class="math inline">\(Q(s^\prime, a^\prime)\)</span></p><p>5、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 6、更新状态和动作：<span class="math inline">\(s = s^\prime, a = a^\prime\)</span></p><p><strong>2. Q学习</strong></p><p>1、当前状态<span class="math inline">\(s\)</span>，选择当前动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>、得到新状态<span class="math inline">\(s^\prime\)</span>和奖励 <span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>3、<strong>不依概率选择新动作</strong>，而是<strong>直接选择最大的值函数<span class="math inline">\(\max_\limits{a^\prime}Q(s^\prime, a^\prime)\)</span></strong></p><p>4、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot \max_{a^\prime} Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 5、更新状态：<span class="math inline">\(s = s^\prime\)</span></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/q-learning.png" style="display:block; margin:auto" width="80%"></p><h1 id="深度q网络">深度Q网络</h1><h2 id="q网络">Q网络</h2><p><strong>1. Q网络</strong></p><p><span class="math inline">\(\mathbf s, \mathbf a\)</span> 是状态动作<span class="math inline">\(s,a\)</span>的向量表达。 用函数<span class="math inline">\(Q_\phi(\mathbf s, \mathbf a) \approx Q^\pi (s, a)\)</span> 。</p><p>参数为<span class="math inline">\(\phi\)</span>的神经网络<span class="math inline">\(Q_\phi(\mathbf s, \mathbf a)\)</span>，则称为<code>Q网络</code>。输入两个向量，输出为1个实数。 <span class="math display">\[Q_\phi(\mathbf s) = \begin{bmatrix}Q_\phi(\mathbf s, \mathbf a_1) \\\vdots \\Q_\phi(\mathbf s, \mathbf a_m) \\\end{bmatrix}\approx\begin{bmatrix}Q^\pi(s,  a_1) \\\vdots \\Q^\pi(s,  a_1) \\\end{bmatrix}\]</span> <strong>2. 两种逼近</strong></p><p>学习一组参数<span class="math inline">\(\phi\)</span>使得<span class="math inline">\(Q_\phi(\mathbf s, \mathbf a)\)</span>逼近值函数<span class="math inline">\(Q^\pi(s, a)\)</span>。</p><p>蒙特卡罗方法：<span class="math inline">\(\hat Q^\pi(s, a) = \frac{1}{N} \sum_{n=1}^NG(\tau^{(n)})\)</span>，总回报的平均</p><p>时序差分方法：<span class="math inline">\(E[r + \gamma Q_\phi(\mathbf s^\prime, \mathbf a^\prime)]\)</span></p><p><strong>3. Q学习的目标函数</strong></p><p>以Q学习为例，采用随机梯度下降来优化，目标函数（减小差距）如下： <span class="math display">\[L(s, a, s^\prime; \phi) = \left(\underbrace{r + \gamma \cdot \max_{a^\prime} Q_\phi(\mathbf s^\prime, \mathbf a^\prime)}_{\color{blue}{实际目标值}} - \underbrace{Q_\phi(\mathbf s, \mathbf a)}_{\color{blue}{\text{网络值}}}\right)^2\]</span> 一般，标记是一个标量，不包含参数；不依赖于网络参数，与网络独立。 <span class="math display">\[J(\theta) = \frac{1}{2m} \sum_{i=1}^m\left (\underbrace{y^{(i)}}_{\color{blue}{\text{实际目标值}}}-\underbrace{f_\theta(\mathbf x^{(i)}) }_{\color{blue}{网络值}}\right)^2\]</span> 两个问题：</p><ul><li><strong>实际目标值不稳定</strong>。参数学习的目标依赖于参数本身。<strong>label本身也包含参数</strong></li><li>样本之间有很强的相关性</li></ul><h2 id="deep-q-network">Deep Q Network</h2><p>深度Q网络（deep Q-networks, DQN）</p><ul><li><strong>目标网络冻结</strong>。在一个时间段，固定目标值中的参数</li><li><strong>经验回放</strong>。构建<strong>经验池</strong>来去除数据相关性</li><li><code>经验池</code>。最近的经历组成的数据集</li></ul><h2 id="带经验回放的dqn算法">带经验回放的DQN算法</h2><p>带经验回放的深度Q网络。</p><p><strong>1. 初始化经验池、Q网络参数、目标Q网络参数</strong></p><ul><li>经验池： <span class="math inline">\(\cal D\)</span>，容量为N</li><li>Q网络参数：<span class="math inline">\(\phi\)</span></li><li>目标Q网络的参数：<span class="math inline">\(\hat \phi = \phi\)</span></li></ul><p><strong>2. 要让<span class="math inline">\(\forall s, \forall a, \; Q_\phi(\mathbf s,\mathbf a)\)</span>都收敛</strong></p><p>每一次初始化起始状态为<span class="math inline">\(s\)</span>， 遍历直到<span class="math inline">\(s\)</span>为最终态</p><p><strong>3. 每一时刻</strong></p><p><strong>生成新数据加入经验池</strong></p><p>1、状态<span class="math inline">\(s\)</span>， 选择动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>， 得到<span class="math inline">\(r\)</span>和<span class="math inline">\(s^\prime\)</span></p><p>3、<span class="math inline">\((s,a, r, s^\prime)\)</span> 加入经验池<span class="math inline">\(\cal D\)</span></p><p><strong>采经验池中采样一条数据计算</strong></p><p>1、从<span class="math inline">\(\cal D\)</span>中采样一条数据，<span class="math inline">\((ss,aa, rr, ss^\prime)\)</span>。 （<strong>去除样本相关性</strong>）</p><p>2、<strong>计算实际目标值</strong><span class="math inline">\(Q_{\hat \psi}(\mathbf{ss, aa})\)</span>。 （<strong>解决目标值不稳定的问题</strong>） <span class="math display">\[Q_{\hat \psi}(\mathbf{ss, aa}) =\begin{cases}&amp; rr, &amp; ss^\prime 为终态 \\&amp; rr + \gamma \cdot \max_\limits{\mathbf a^\prime}Q_{\hat \phi}(\mathbf {ss^\prime}, \mathbf {a^\prime}), &amp;其它\end{cases}\]</span> 3、损失函数如下，梯度下降法去训练Q网络 <span class="math display">\[J(\phi)= \left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - y \right)^2=\left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - Q_{\hat \psi}(\mathbf{ss, aa}) \right)^2\]</span> <strong>状态前进</strong></p><p><span class="math inline">\(s \leftarrow s^\prime\)</span></p><p><strong>更新目标Q网络的参数</strong></p><p>每隔C步更新：<span class="math inline">\(\hat \phi \leftarrow \phi\)</span></p><p><strong>4. DQN算法中经验池的优点</strong></p><p>1、去除样本相关性，避免陷入局部最优</p><p>经验池中抽取样本代替当前样本进行训练，打破了与相邻训练样本的相关性，避免陷入局部最优。</p><p>2、经验回放类似于监督学习</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/deep-qnet.png" style="display:block; margin:auto" width="70%">s</p><h1 id="总结">总结</h1><h2 id="策略迭代">策略迭代</h2><p>已知模型。利用<strong>贝尔曼方程</strong>（<code>算均值</code>）迭代计算出<span class="math inline">\(V(s)\)</span>，再算出<span class="math inline">\(Q(s,a)\)</span>。选择最好的动作<span class="math inline">\(a\)</span>去优化策略<span class="math inline">\(\pi(s)\)</span>。 <span class="math display">\[\forall s, \quad V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)] \]</span></p><p><span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[\forall s, \qquad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span></p><h2 id="值迭代">值迭代</h2><p>已知模型。利用<strong>贝尔曼最优方程</strong>迭代算出<span class="math inline">\(V(s)\)</span>，再算出<span class="math inline">\(Q(s,a)\)</span>。选择最好的动作<span class="math inline">\(a\)</span>去优化策略<span class="math inline">\(\pi(s)\)</span>。 <span class="math display">\[\forall s \in S, \quad V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span></p><p><span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[\forall s, \quad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span></p><h2 id="蒙特卡罗">蒙特卡罗</h2><p>未知模型。从<span class="math inline">\((s,a)\)</span><strong>随机游走，采集N个样本</strong>。使用<strong>所有轨迹回报平均值近似估计<span class="math inline">\(Q(s,a)\)</span></strong> ，再去改进策略。重复，直至收敛。 <span class="math display">\[Q^\pi(s, a)  \approx \hat Q^\pi(s, a) = \frac{1}{N} \sum_{n=1}^NG(\tau^{(n)})\]</span></p><h2 id="时序差分算法">时序差分算法</h2><p>无需知道完整轨迹就能对策略进行评估。</p><p>时序差分学习=动态规划-贝尔曼估计<span class="math inline">\(G(\tau)\)</span> + 蒙特卡罗采样-增量计算<span class="math inline">\(Q(s,a)\)</span></p><p>贝尔曼估计轨迹总回报<span class="math inline">\(G(\tau)\)</span> <span class="math display">\[G(\tau) \leftarrow r(s, a, s^\prime) + \gamma \cdot Q(s^\prime, a^\prime)\]</span> 增量计算<span class="math inline">\(Q(s,a)\)</span> <span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (\underbrace{ r+ \gamma \cdot Q(s^\prime, a^\prime)}_{\color{blue}{实际值}} - \underbrace{Q(s, a)}_{\color{blue}{预期值}})\]</span></p><h2 id="sarsa">SARSA</h2><p>同策略的时序差分算法，是Q学习的改进。</p><p>1、当前状态<span class="math inline">\(s\)</span>，当前动作<span class="math inline">\(a\)</span> （初始时选择<span class="math inline">\(a=\pi^\epsilon(s)\)</span>，后续是更新得到的）</p><p>2、<strong>执行动作</strong><span class="math inline">\(a\)</span>，得到<strong>新状态</strong><span class="math inline">\(s^\prime\)</span>，得到<strong>奖励</strong><span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>4、<strong>依概率选择新动作</strong><span class="math inline">\(a = \pi^\epsilon(s^\prime)\)</span>，<strong>新状态新动作的值函数</strong>：<span class="math inline">\(Q(s^\prime, a^\prime)\)</span></p><p>5、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 6、更新状态和动作：<span class="math inline">\(s = s^\prime, a = a^\prime\)</span></p><h2 id="q学习">Q学习</h2><p>1、当前状态<span class="math inline">\(s\)</span>，选择当前动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>、得到新状态<span class="math inline">\(s^\prime\)</span>和奖励 <span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>3、<strong>不依概率选择新动作</strong>，而是<strong>直接选择最大的值函数<span class="math inline">\(\max_\limits{a^\prime}Q(s^\prime, a^\prime)\)</span></strong></p><p>4、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot \max_{a^\prime} Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 5、更新状态：<span class="math inline">\(s = s^\prime\)</span></p><h2 id="q网络-1">Q网络</h2><p>使用神经网络<span class="math inline">\(Q_{\phi}(\mathbf{s,a})\)</span>去近似值函数<span class="math inline">\(Q(s,a)\)</span>。两个问题：实际目标值不稳定；样本之间具有强相关性。 <span class="math display">\[L(s, a, s^\prime; \phi) = \left(\underbrace{r + \gamma \cdot \max_{a^\prime} Q_\phi(\mathbf s^\prime, \mathbf a^\prime)}_{\color{blue}{实际目标值}} - \underbrace{Q_\phi(\mathbf s, \mathbf a)}_{\color{blue}{\text{网络值}}}\right)^2\]</span></p><h2 id="dqn">DQN</h2><p>深度Q网络：</p><ul><li><strong>目标网络冻结</strong>-<strong>稳定目标值</strong>。<span class="math inline">\(Q_{\phi}(\mathbf{s,a})\)</span>训练网络，<span class="math inline">\(Q_{\hat \phi}(\mathbf{s,a})\)</span>目标值网络。定期更新参数<span class="math inline">\(\hat \phi \leftarrow \phi\)</span></li><li><strong>经验池的经验回放</strong>-<strong>去除样本相关性</strong>- 每次采集一条数据放入经验池，再从经验池取数据进行训练。</li></ul><p><strong>生成新数据加入经验池</strong></p><p>1、状态<span class="math inline">\(s\)</span>， 选择动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>， 得到<span class="math inline">\(r\)</span>和<span class="math inline">\(s^\prime\)</span></p><p>3、<span class="math inline">\((s,a, r, s^\prime)\)</span> 加入经验池<span class="math inline">\(\cal D\)</span></p><p><strong>采经验池中采样一条数据计算</strong></p><p>1、从<span class="math inline">\(\cal D\)</span>中采样一条数据，<span class="math inline">\((ss,aa, rr, ss^\prime)\)</span>。 （<strong>去除样本相关性</strong>）</p><p>2、<strong>计算实际目标值</strong><span class="math inline">\(Q_{\hat \psi}(\mathbf{ss, aa})\)</span>。 （<strong>解决目标值不稳定的问题</strong>） <span class="math display">\[Q_{\hat \psi}(\mathbf{ss, aa}) =\begin{cases}&amp; rr, &amp; ss^\prime 为终态 \\&amp; rr + \gamma \cdot \max_\limits{\mathbf a^\prime}Q_{\hat \phi}(\mathbf {ss^\prime}, \mathbf {a^\prime}), &amp;其它\end{cases}\]</span> 3、<code>损失函数</code>如下，<strong>梯度下降法去训练Q网络</strong> <span class="math display">\[J(\phi)= \left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - y \right)^2=\left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - Q_{\hat \psi}(\mathbf{ss, aa}) \right)^2\]</span> <strong>状态前进</strong></p><p><span class="math inline">\(s \leftarrow s^\prime\)</span></p><p><strong>更新目标Q网络的参数</strong></p><p>每隔C步更新：<span class="math inline">\(\hat \phi \leftarrow \phi\)</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;基于值函数的学习方法：贝尔曼方程，动态规划、蒙特卡洛、时续差分、Q网络。
    
    </summary>
    
      <category term="强化学习" scheme="http://plmsmile.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="值函数" scheme="http://plmsmile.github.io/tags/%E5%80%BC%E5%87%BD%E6%95%B0/"/>
    
      <category term="策略迭代" scheme="http://plmsmile.github.io/tags/%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3/"/>
    
      <category term="值迭代" scheme="http://plmsmile.github.io/tags/%E5%80%BC%E8%BF%AD%E4%BB%A3/"/>
    
      <category term="蒙特卡罗" scheme="http://plmsmile.github.io/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97/"/>
    
      <category term="时序差分" scheme="http://plmsmile.github.io/tags/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/"/>
    
      <category term="SARSA" scheme="http://plmsmile.github.io/tags/SARSA/"/>
    
      <category term="Q学习" scheme="http://plmsmile.github.io/tags/Q%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Q网络" scheme="http://plmsmile.github.io/tags/Q%E7%BD%91%E7%BB%9C/"/>
    
      <category term="DQN" scheme="http://plmsmile.github.io/tags/DQN/"/>
    
      <category term="动态规划" scheme="http://plmsmile.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>阅读理解模型总结</title>
    <link href="http://plmsmile.github.io/2018/04/12/39-squard-models/"/>
    <id>http://plmsmile.github.io/2018/04/12/39-squard-models/</id>
    <published>2018-04-12T05:54:12.000Z</published>
    <updated>2018-11-25T08:30:08.812Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="qanet">QANet</h1><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/cnn_self_attention.png" style="display:block; margin:auto" width="100%"></p><h1 id="bidaf">BiDAF</h1><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/papers/bidaf.png" style="display:block; margin:auto" width="100%"></p><h1 id="aoa">AoA</h1><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/papers/aoa.png" style="display:block; margin:auto" width="100%"></p><h1 id="dcn">DCN</h1><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/deep-residual-coattention-encoder.png" style="display:block; margin:auto" width="90%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcn-decoder.png" style="display:block; margin:auto" width="100%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/hmn.png" style="display:block; margin:auto" width="60%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;70%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;qanet&quot;&gt;QANet&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://plm-images.oss-cn-hongkon
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="QANet" scheme="http://plmsmile.github.io/tags/QANet/"/>
    
      <category term="机器阅读理解" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络总结</title>
    <link href="http://plmsmile.github.io/2018/04/11/38-convolution/"/>
    <id>http://plmsmile.github.io/2018/04/11/38-convolution/</id>
    <published>2018-04-11T07:42:36.000Z</published>
    <updated>2018-11-25T08:30:08.755Z</updated>
    
    <content type="html"><![CDATA[<p>卷积基本概念和常见的卷积神经网络<a id="more"></a></p><p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="卷积神经网络">卷积神经网络</h1><p>全连接网络的两个问题：</p><ul><li>参数太多：训练效率低、容易过拟合</li><li>局部不变形特征：全连接很难提取出图片的不变性特征</li></ul><h2 id="三个特性">三个特性</h2><p><strong>1. 局部性</strong></p><p>图片特征只在局部。图片特征决定图片类别，这些图片特征在一些局部的区域中。</p><p>局部连接。</p><p><strong>2. 相同性</strong></p><p>用同样的检测模式去检测不同图片的相同特征。只是这些特征出现在图片的不同位置。</p><p>参数共享。</p><p><strong>3. 不变性</strong></p><p>对于一张大图片，进行下采样，图片的性质基本保持不变。</p><p>下采样保持不变性。</p><h2 id="卷积">卷积</h2><blockquote><ol style="list-style-type: decimal"><li>一维卷积：卷积核、步长、首位0填充</li><li>三种卷积：窄卷积、宽卷积、等长卷积</li><li>二维卷积</li></ol></blockquote><p><strong>1. 一维卷积</strong></p><ul><li>卷积核：参数<span class="math inline">\([1, 0, -1]\)</span>就是一个<code>卷积核</code> 或 <code>滤波器</code></li><li>步长：卷积核滑动的间隔</li><li>零填充：在输入向量两端进行补零</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/ondim-conv-2.jpeg" style="display:block; margin:auto" width="70%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/onedim-conv-0.png" style="display:block; margin:auto" width="50%"> <strong>2. 三种卷积</strong></p><p>输入n，卷积大小m，步长s，输入神经元各两端填补p个0</p><ul><li>窄卷积：<code>s=1</code>，不补0，输出长度为<code>n-m+1</code></li><li>宽卷积：<code>s=1</code>，两端补0，<span class="math inline">\(p=m-1\)</span>， 输出长度为<code>n+m-1</code></li><li>等长卷积：<code>s=1</code>，两端补0，<span class="math inline">\(p=\frac{m-1}{2}\)</span>， 输出长度为<code>n</code></li></ul><p>一般卷积默认为窄卷积。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/onedim-conv-1.png" style="display:block; margin:auto" width="50%"></p><p><strong>3. 二维卷积</strong></p><p>输入一张图片（假设深度为1），<span class="math inline">\(X \in \mathbb R^{M \times N}\)</span>， 卷积核<span class="math inline">\(W \in \mathbb R ^{m \times n}\)</span> ，则卷积（互相关代替）结果为： <span class="math display">\[y_{ij} = \sum_{u=1}^m \sum_{v=1}^n w_{uv} x_{i+u-1, j+v-1}\]</span> <img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/twodim-conv-0.png" style="display:block; margin:auto" width="70%"></p><p>一个卷积核提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。</p><p>卷积后的结果称为<code>特征映射（feature map）</code>。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/twodim-conv-2.png" style="display:block; margin:auto" width="50%"></p><h2 id="卷积层">卷积层</h2><blockquote><ol style="list-style-type: decimal"><li>一个卷积核</li></ol><p><span class="math inline">\(W_p \in \mathbb R ^{m \times n \times D}\)</span>， 对D个通道做卷积，结果相加求和，过激活函数，得到一个特征图<span class="math inline">\(Y^p \in\mathbb R^{M^\prime \times N^\prime}\)</span></p><ol start="2" style="list-style-type: decimal"><li>多个卷积核：得到P个特征图</li></ol></blockquote><p>输入图片(feature map)是<span class="math inline">\(X \in \mathbb R^{M \times N \times D}\)</span>，深度是D</p><p><strong>1. 一个卷积核</strong></p><ul><li>用1个卷积核<span class="math inline">\(W_p \in \mathbb R ^{m \times n \times D}\)</span>（实际上是D个<span class="math inline">\(\mathbb R^{m\times n}\)</span>）去卷积这张图片（所有深度）</li><li>对各个深度的卷积结果进行<strong>相加求和</strong>，再<strong>加上偏置</strong></li><li><strong>过激活函数</strong>，输出最终的FM，是<span class="math inline">\(Y^p\)</span></li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/conv.png" style="display:block; margin:auto" width="100%"></p><p><strong>2. 多个卷积核</strong></p><p>多个卷积核可以提取出多种不同的特征。输入图片是<span class="math inline">\(X \in \mathbb R^{M \times N \times D}\)</span>，</p><ul><li>有P个不同的卷积核<span class="math inline">\(W_p \in \mathbb R ^{m \times n \times D}\)</span>， 实际上是四维的[m, n, D, P]，后两维是<code>in_channel</code>、<code>out_channel</code></li><li>输出P个特征图<span class="math inline">\(\mathbb R^{M^\prime \times N^\prime \times P}\)</span></li><li>对每一个卷积核<span class="math inline">\(W \in \mathbb R ^{m \times n \times D}\)</span>，对D个深度<span class="math inline">\(\mathbb R ^{m \times n}\)</span>分别做卷积，对D个卷积结果进行求和相加，经过激活函数，得到一个特征图 <span class="math inline">\(Y^p \in\mathbb R^{M^\prime \times N^\prime}\)</span></li><li>一共需要<span class="math inline">\(P \times D \times (m \times n) + P\)</span>个参数</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/conv_layer.gif" style="display:block; margin:auto" width="80%"></p><h2 id="卷积代替全连接">卷积代替全连接</h2><blockquote><ol style="list-style-type: decimal"><li>局部连接：卷积核只与输入的一个局部做连接，计算出FM中的一个值，局部性</li><li>权值共享：同一个卷积核与图片的各个位置进行连接，权值是一样的，提取出同样的特征</li></ol></blockquote><p><strong>1. 局部连接</strong></p><ul><li>卷积层的神经元只与输入数据的一个局部区域做连接</li><li>因为图片的局部性，图片的特征在局部</li><li>FM中的每一个值，只与输入的局部相关。而不是与所有的相关</li></ul><p><strong>2. 权值共享</strong></p><ul><li>一个卷积核会分多次对输入数据的各个部分做卷积操作</li><li>对每个部分的连接参数实际上是相同的，因为是同一个卷积核</li><li>因为图片的相同性，同样的卷积核可以检测出相同的特征，只是特征在不同的位置</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/full-conv.png" style="display:block; margin:auto" width="60%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/threedim-conv.png" style="display:block; margin:auto" width="60%"></p><h2 id="汇聚层">汇聚层</h2><blockquote><ol style="list-style-type: decimal"><li>卷积层的不足：FM的维数很高</li><li>汇聚层的作用：选择特征、降低特征数量、减少参数数量、避免过拟合</li><li>两种汇聚方式：最大和平均。</li></ol></blockquote><p><strong>1. 卷积层的不足</strong></p><ul><li>减少网络连接数量</li><li>但是<strong>FM中的神经元个数依然很多</strong></li><li>如果直接接分类器全连接，则维数会很高，<strong>容易过拟合</strong></li></ul><p><strong>2. 汇聚层的作用</strong></p><p><code>汇聚层</code>(pooling layer)，也作子采样层(subsampling layer)。作用是：</p><ul><li>进行特征选择</li><li>降低特征数量</li><li>进而减少参数数量、避免过拟合</li><li>拥有更大感受野，大图片缩小，保持<code>不变性</code></li></ul><p><strong>3. 两种汇聚方式</strong></p><ul><li>最大汇聚：一个区域内所有神经元的最大值</li><li>平均汇聚：一个区域内所有神经元的平均值</li></ul><p>过大采样区会急剧减少神经元的数量，造成过多的信息损失！</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/max-pool.png" style="display:block; margin:auto" width="90%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/pool.jpeg" style="display:block; margin:auto" width="40%"></p><h2 id="典型的卷积网络结构">典型的卷积网络结构</h2><p>由多个卷积块组成，一个卷积块：</p><ul><li>连续2~5个卷积层，ReLU激活函数</li><li>0~1个汇聚层</li></ul><p>目前，趋向于使用更小的卷积核，比如<span class="math inline">\(1\times 1, 3 \times 3\)</span>。汇聚层的比例也逐渐降低，趋向于全卷积网络。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/conv-nn.png" style="display:block; margin:auto" width="100%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/convnet.jpeg" style="display:block; margin:auto" width="70%"></p><h1 id="常见卷积网络">常见卷积网络</h1><h2 id="lenet">LeNet</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/LeNet.png" style="display:block; margin:auto" width="70%"></p><h2 id="alex-net">Alex Net</h2><p>使用ReLU作为非线性激活函数、Dropout防止过拟合、数据增强提高模型准确率。</p><p><strong>AlexNet分组卷积</strong></p><ul><li>对所有通道进行分组，进行分组卷积，执行<strong>标准卷积操作</strong></li><li>在最后时刻才使用两个全连接融合通道的信息</li><li>降低了模型的泛化能力</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/AlexNet.png" style="display:block; margin:auto" width="80%"></p><h2 id="inception-net">Inception Net</h2><p>如何选择卷积核大小非常关键：</p><ul><li><strong>一个卷积层同时使用多种尺寸的卷积核</strong></li><li>先过<span class="math inline">\(1\times 1\)</span>卷积减少卷积层参数量</li></ul><p>Inception Net由多个Inception模块堆叠而成。一个Inception同时使用<span class="math inline">\(1\times 1\)</span>、<span class="math inline">\(3\times 3\)</span>、<span class="math inline">\(5\times 5\)</span> 的卷积，如下：</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/inception.png" style="display:block; margin:auto" width="70%"></p><p><span class="math inline">\(3\times 3\)</span> 、<span class="math inline">\(5\times 5\)</span> 卷积前，先进行<span class="math inline">\(1\times 1\)</span>卷积的作用：</p><ul><li>减少输入数据的深度</li><li>减少各个深度的冗余信息，先进行一次特征抽取</li></ul><p>后续还有各种各样的Inception Net，最终演变成Xception Net。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/depth-wise-conv.jpg" style="display:block; margin:auto" width="60%"></p><p>Inception Net的极限就是，对每个channel做一个单独的卷积。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/depth-wise-conv2.jpg" style="display:block; margin:auto" width="60%"></p><h2 id="res-net">Res Net</h2><p>越深的网络可以用ResNet来训练。<a href="https://zhuanlan.zhihu.com/p/28124810?group_id=883267168542789632" target="_blank" rel="noopener">ResNet可以很深的原因</a></p><p><code>残差连接</code>通过<strong>给非线性的卷积层增加直连边</strong>的方式</p><ul><li>来<strong>提高信息的传播效率</strong></li><li>可以减小梯度消失问题</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/resnet.png" style="display:block; margin:auto" width="70%"></p><h2 id="xception">Xception</h2><p>卷积需要同时考虑所有通道吗？</p><p>输入图片(feature map)是<span class="math inline">\(X \in \mathbb R^{M \times N \times D}\)</span>，深度是D</p><p><strong>1. 传统卷积核会同时考虑所有通道</strong></p><ul><li>传统<strong>1个卷积核</strong>会对所有channel的FM做同样的卷积</li><li>得到D个卷积结果</li><li>再<strong>对D个卷积结果进行相加求过激活函数</strong>得到<strong>一个FM</strong></li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/conv-stantard.jpg" style="display:block; margin:auto" width="70%"></p><p><strong>2. 深度可分离卷积核</strong></p><p><code>Depth Separable Convolution</code></p><p>输入数据有D个FM，输出P个FM。<code>深度可分离卷积(DepthWise Convolution)</code> 如下：</p><ul><li>对<span class="math inline">\(X\)</span>的每个channel，分别做一个单独的卷积，得到D个新的FM</li><li>对D个新的FM，做<span class="math inline">\(1\times 1\)</span>的传统卷积(<code>PointWise Convolution</code>)，<span class="math inline">\(P \times D \times (1 \times 1)\)</span></li><li>最终输出P个FM （通道数变换）</li></ul><p>卷积操作不一定需要同时考虑通道和区域。<code>可分离卷积</code>。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/xception-2.jpg" style="display:block; margin:auto" width="70%"></p><p><strong>3. 可分离卷积参数大大减小</strong></p><p>输入通道<span class="math inline">\(D=3\)</span>，输出通道<span class="math inline">\(P=256\)</span>，卷积核大小为<span class="math inline">\(3 \times 3\)</span></p><ul><li>传统卷积参数：<span class="math inline">\(256 \times 3 \times (3 \times 3) = 6912\)</span></li><li>DepthWise卷积参数：<span class="math inline">\(3 \times 3 \times 3 +256 \times 3 \times (1 \times 1) =795\)</span>， 降低九分之一</li></ul><p>同时，效果更好。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/depthwise-conv-compare.jpg" style="display:block; margin:auto" width="60%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/xception.jpg" style="display:block; margin:auto" width="70%"></p><h2 id="shuffle-net">Shuffle Net</h2><p><strong>1. AlexNet分组卷积</strong></p><ul><li>对所有通道进行分组，进行分组卷积，执行<strong>标准卷积操作</strong></li><li>在最后时刻才使用全连接融合通道的信息</li><li>降低了模型的泛化能力</li></ul><p><strong>2. ShuffleNet 分组卷积</strong></p><p><code>ShuffleNet</code> = <strong>分组卷积</strong>（通道分组）+ <strong>深度可分离卷积</strong>（Depthwise+PointWise）</p><p>对通道进行分组卷积时</p><ul><li>每一个组执行深度可分离卷积，而不是标准传统卷积</li><li>每一次层叠分组卷积时，都进行channel shuffle</li><li>实际上每个组各取一个也能实现shuffle</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/Reading_Note_20170720_ShuffleNet_0.png" style="display:block; margin:auto" width="100%"></p><h2 id="senet">SENet</h2><p>Inception、ShuffleNet等网络中，<strong>对所有通道产生的特征</strong>都是<strong>不分权重直接相加求和</strong>的。</p><p>为什么所有通道的特征对模型的作用是相等的呢？</p><p><img src="" style="display:block; margin:auto" width="70%"></p><h2 id="总结">总结</h2><p>参考自<a href="https://zhuanlan.zhihu.com/p/28749411" target="_blank" rel="noopener">知乎卷积网络中十大拍案叫绝的操作</a></p><p><strong>1. 卷积核</strong></p><ol style="list-style-type: decimal"><li>大卷积核用多个小卷积核代替</li><li>单一尺寸卷积核用多尺寸卷积核代替</li><li>固定形状卷积核趋于用可变形卷积核</li><li>使用<span class="math inline">\(1\times 1\)</span>卷积核</li></ol><p><strong>2. 卷积层通道</strong></p><ol style="list-style-type: decimal"><li>标准卷积使用深度可分离卷积代替</li><li>使用分组卷积</li><li>分组卷积前使用channel shuffle</li><li>通道加权计算</li></ol><p><strong>3. 卷积层连接</strong></p><ol style="list-style-type: decimal"><li>使用skip connection，让模型更深</li><li>densely connection，使每一层都融合其它层的特征输出</li></ol><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/cnn/conv-compare.jpg" style="display:block; margin:auto" width="70%"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;卷积基本概念和常见的卷积神经网络
    
    </summary>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="可分离卷积" scheme="http://plmsmile.github.io/tags/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="卷积" scheme="http://plmsmile.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="1*1卷积" scheme="http://plmsmile.github.io/tags/1-1%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="分组卷积" scheme="http://plmsmile.github.io/tags/%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="LeNet" scheme="http://plmsmile.github.io/tags/LeNet/"/>
    
      <category term="AlexNet" scheme="http://plmsmile.github.io/tags/AlexNet/"/>
    
      <category term="InceptionNet" scheme="http://plmsmile.github.io/tags/InceptionNet/"/>
    
      <category term="ResNet" scheme="http://plmsmile.github.io/tags/ResNet/"/>
    
      <category term="XceptionNet" scheme="http://plmsmile.github.io/tags/XceptionNet/"/>
    
      <category term="ShuffleNet" scheme="http://plmsmile.github.io/tags/ShuffleNet/"/>
    
  </entry>
  
  <entry>
    <title>强化学习</title>
    <link href="http://plmsmile.github.io/2018/04/01/37-reinforce-learning/"/>
    <id>http://plmsmile.github.io/2018/04/01/37-reinforce-learning/</id>
    <published>2018-04-01T01:30:47.000Z</published>
    <updated>2018-11-25T08:30:08.716Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>强化学习的基础知识。基本要素、轨迹、值函数、V函数和Q函数、贝尔曼方程。<a id="more"></a></p></blockquote><p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="强化学习定义">强化学习定义</h1><h2 id="概览">概览</h2><p>强化学习是指一个<strong>智能体</strong>从与<strong>环境</strong>的<strong>交互</strong>中不断学习去完成特定的目标。</p><p>强化学习不需要给出正确策略作为监督信息，只需要给出<code>策略的（延迟）回报</code>，并通过<strong>调整策略</strong>来取得最大化的期望回报。</p><blockquote><ol style="list-style-type: decimal"><li>智能体、环境</li><li>环境状态<span class="math inline">\(s\)</span>，智能体的动作<span class="math inline">\(a\)</span>，智能体的策略<span class="math inline">\(\pi(a\mid s)\)</span>，状态转移概率<span class="math inline">\(p(s_{t+1}\mid s_t, a_t)\)</span>，即使奖励<span class="math inline">\(r(s, a, s \prime)\)</span></li></ol></blockquote><h2 id="智能体和环境">智能体和环境</h2><p><strong>智能体</strong></p><ul><li>感知环境的状态和反馈的奖励，进行学习和决策</li><li><code>决策</code> ：根据 -- 环境状态 -- 做出不同的动作</li><li><code>学习</code>： 根据 -- 反馈奖励 -- 调整策略</li></ul><p><strong>环境</strong></p><ul><li>智能体外部的所有事物</li><li>收到 -- 智能体的动作 -- 改变状态</li><li>给 -- 智能体 -- 反馈奖励</li></ul><h2 id="个基本要素">5个基本要素</h2><p><strong>状态<span class="math inline">\(s\)</span></strong></p><p>环境的状态，状态空间<span class="math inline">\(\mathcal S\)</span>， 离散/连续</p><p><strong>动作<span class="math inline">\(a\)</span></strong></p><p>智能体的行为，动作空间<span class="math inline">\(\mathcal A\)</span>， 离散/连续</p><p><strong>策略<span class="math inline">\(\pi(a\mid s)\)</span></strong></p><p>智能体 根据 -- 环境状态s -- 决定下一步的动作a 的函数</p><p><strong>状态转移概率<span class="math inline">\(p(s \prime \mid s, a)\)</span></strong></p><p>根据 -- 当前状态<span class="math inline">\(s\)</span>和智能体的动作<span class="math inline">\(a\)</span> -- 环境状态变为<span class="math inline">\(s \prime\)</span>的概率</p><p><strong>即时奖励<span class="math inline">\(r(s, a, s\prime)\)</span></strong></p><p>环境给智能体的奖励，标量函数。根据 -- 环境当前状态 、智能体执行的动作、环境新状态</p><h2 id="智能体的策略">智能体的策略</h2><p><span class="math inline">\(\pi(a \mid s)\)</span> 智能体根据环境状态决定下一步的动作。分为确定性策略和<code>随机性策略</code>。 <span class="math display">\[\pi(a\mid s) \triangleq p(a\mid s) , \quad \quad \sum_{a \in \mathcal A} \pi(a\mid s) = 1\]</span> <img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/agent-env-interact.png" style="display:block; margin:auto" width="50%"></p><h1 id="马尔科夫决策过程">马尔科夫决策过程</h1><blockquote><ol style="list-style-type: decimal"><li>马尔可夫过程，<span class="math inline">\(p(s_{t+1}\mid s_t)\)</span></li><li>马尔可夫决策过程，<span class="math inline">\(p(s_{t+1} \mid s_t, a_t)\)</span></li><li>轨迹，给初始状态，智能体与环境的一次交互过程</li></ol></blockquote><h2 id="马尔可夫过程">马尔可夫过程</h2><p>状态序列<span class="math inline">\(s_0, s_1, \cdots, s_t\)</span>具有马尔可夫性，<span class="math inline">\(s_{t+1}\)</span>只依赖于<span class="math inline">\(s_t\)</span> <span class="math display">\[p(s_{t+1}\mid s_t, \cdots, s_0) = p(s_{t+1} \mid s_t)\]</span> ## 马尔可夫决策过程</p><p><span class="math inline">\(s_{t+1}\)</span>依赖于<span class="math inline">\(s_t\)</span>和<span class="math inline">\(a_t\)</span>， 即<strong>环境新状态</strong>依赖于<strong>当前状态</strong>和<strong>当前智能体的动作</strong>。 <span class="math display">\[p(s_{t+1} \mid s_t, a_t, \cdots, s_0, a_0) = p(s_{t+1}\mid s_t, a_t)\]</span> <strong>智能体与环境的交互</strong>是一个<code>马尔可夫决策过程</code>。</p><h2 id="轨迹">轨迹</h2><p>给定策略<span class="math inline">\(\pi(a\mid s)\)</span>， <strong>轨迹</strong>是智能体与环境的<strong>一次交互过程</strong>，是一个马尔可夫决策过程，如下： <span class="math display">\[\tau = s_0, a_0, s_1, r_1, \cdots, s_{T-1}, a_{T-1}, s_{T}, r_{T}\]</span> 其中<span class="math inline">\(r_t = r(s_{t-1}, a_{t-1}, s_t)\)</span>是时刻<span class="math inline">\(t\)</span>的即时奖励。</p><p>轨迹的概率</p><ul><li>初始状态</li><li>所有时刻概率的乘积</li><li><strong>智能体执行动作，环境更新状态</strong></li></ul><p><span class="math display">\[p(\tau) = p(s_0) \prod_{t=0}^{T-1}\pi(a_t \mid s_t) p(s_{t+1} \mid s_t, a_t)\]</span></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/rl/markov-decision-process.png" style="display:block; margin:auto" width="70%"></p><h1 id="目标函数">目标函数</h1><blockquote><ol style="list-style-type: decimal"><li>一个轨迹的总回报。<span class="math inline">\(G(\tau) = \sum_{t=0}^{T-1} r_{t+1}\)</span></li><li>一个策略的期望回报。<span class="math inline">\(E_{\tau \sim p(\tau)} [G(\tau)]\)</span>。 所有轨迹的回报的期望</li><li>强化学习的目标。学一个策略<span class="math inline">\(\pi_{\theta}(a \mid s)\)</span>， 最大化这个策略的期望回报</li></ol></blockquote><h2 id="轨迹的总回报">轨迹的总回报</h2><p><strong>1. 某一时刻的奖励</strong></p><p><span class="math inline">\(r_t = r(s_{t-1}, a_{t-1}, s_t)\)</span>是<span class="math inline">\(t\)</span>时刻， 环境给智能体的<strong>奖励</strong>。</p><p>给定策略<span class="math inline">\(\pi(a\mid s)\)</span>， 智能体与环境<code>一次交互过程</code>(回合，试验)为轨迹<span class="math inline">\(\tau\)</span></p><p><strong>2. 一条轨迹的总回报</strong></p><p>总回报是一条轨迹所有时刻的累积奖励和。 <span class="math display">\[G(\tau) = \sum_{t=0}^{T-1} r(s_{t-1}, a_{t-1}, s_t) =  \sum_{t=0}^{T-1} r_{t+1}\]</span> <strong>3. 一条轨迹的折扣回报</strong></p><p>折扣回报引入<code>折扣率</code>，<strong>降低远期回报的权重</strong>（T无限大时）。 <span class="math display">\[G(\tau) = \sum_{t=0}^{T-1} \gamma^t r_{t+1}, \quad \quad \gamma \in [0, 1]\]</span> <code>折扣率</code><span class="math inline">\(\gamma\)</span></p><ul><li><span class="math inline">\(\gamma \sim 0\)</span>， 在意短期回报</li><li><span class="math inline">\(\gamma \sim 1\)</span>， 在意长期回报</li></ul><h2 id="策略的期望回报">策略的期望回报</h2><p>给一个策略<span class="math inline">\(\pi(a\mid s)\)</span>， <strong>有多个轨迹</strong>。</p><p>一个策略的<code>期望回报</code>：该策略下<strong>所有轨迹总回报的期望值</strong>。 <span class="math display">\[E_{\tau \sim p(\tau)} [G(\tau)] =E_{\tau \sim p(\tau)} [\sum_{t=0}^{T-1}r_{t+1}]\]</span></p><h2 id="强化学习的目标">强化学习的目标</h2><p>强化学习的目标是<strong>学习到一个策略<span class="math inline">\(\pi_{\theta}(a\mid s)​\)</span></strong>，来<strong>最大化这个策略的期望回报</strong>。<strong>希望智能体能够获得更多的回报</strong>。 <span class="math display">\[J(\theta) = E_{\tau \sim p_{\theta}(\tau)} [\sum_{t=0}^{T-1}\gamma ^tr_{t+1}]\]</span></p><h1 id="值函数">值函数</h1><blockquote><ol style="list-style-type: decimal"><li>状态值函数。<span class="math inline">\(V^\pi(s)\)</span>, 初始状态为s，执行策略<span class="math inline">\(\pi\)</span>得到的期望回报。</li><li>贝尔曼方程迭代计算值函数</li><li>状态-动作值函数。<span class="math inline">\(Q^\pi(s, a)\)</span>， 初始状态为s，进行动作a，执行策略<span class="math inline">\(\pi\)</span>得到的期望回报</li><li>V函数与Q函数的关系。<span class="math inline">\(V^\pi(s) = E_{a \sim \pi(a \mid s)}[Q^\pi(s, a)]\)</span></li><li>值函数的作用。评估策略<span class="math inline">\(\pi(a \mid s)\)</span>， 对好的动作a（<span class="math inline">\(Q^\pi(s, a)\)</span>大 ），增大其概率<span class="math inline">\(\pi(a \mid s)\)</span></li></ol></blockquote><h2 id="状态值函数">状态值函数</h2><p>状态值函数<span class="math inline">\(V^\pi(s)\)</span>是初始状态为<span class="math inline">\(s\)</span>，执行策略<span class="math inline">\(\pi\)</span>得到的期望回报。（因为有多个轨迹，每个轨迹的初始状态都是<span class="math inline">\(\tau_{s_0} = s\)</span>） <span class="math display">\[V^\pi(s) = E_{\tau \sim p(\tau)} [\sum_{t=0}^{T-1}r_{t+1} \mid \tau_{s_0} = s]\]</span></p><h2 id="贝尔曼方程计算值函数">贝尔曼方程计算值函数</h2><p><strong>当前状态的值函数</strong>，可以<strong>通过下个状态的值函数</strong>进行<strong>递推计算</strong>。</p><p>核心：<strong><span class="math inline">\(V^\pi(s) \sim r(s, a, s \prime) + V^\pi(s\prime)\)</span></strong>。 有动态规划的意思</p><ul><li>关键在于状态转移：<span class="math inline">\(s \sim s\prime\)</span></li><li><strong>选动作</strong>、<strong>选新状态</strong> ： <span class="math inline">\(s \sim a\)</span>， <span class="math inline">\(s, a \sim s\prime\)</span></li><li>策略<span class="math inline">\(\pi(a\mid s)\)</span> 和状态转移概率<span class="math inline">\(p(s\prime \mid s, a)\)</span></li><li>对这两层可能性的所有值函数，求期望即可</li></ul><p>给定<code>策略</code><span class="math inline">\(\pi(a\mid s)\)</span>、<code>状态转移概率</code><span class="math inline">\(p(s\prime \mid s, a)\)</span>、<code>奖励</code><span class="math inline">\(r(s, a, s\prime)\)</span>， <strong>迭代计算值函数</strong>： <span class="math display">\[V^\pi(s) = E[ r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><strong>V函数的贝尔曼方程</strong> <span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><h2 id="状态-动作值函数">状态-动作值函数</h2><p>状态-动作值函数是 <strong>初始状态为<span class="math inline">\(s\)</span></strong>并<strong>进行动作<span class="math inline">\(a\)</span></strong>， <strong>执行策略<span class="math inline">\(\pi\)</span></strong>得到的<code>期望总回报</code>。 也称为<strong>Q函数</strong>。 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> <strong>Q函数的贝尔曼方程</strong> <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma E_{a\prime \sim \pi(a\prime \mid s\prime)}[Q^\pi(s\prime, a\prime)]]\]</span> ## V函数与Q函数</p><ul><li><p><span class="math inline">\(V(s)\)</span>函数要 先确定动作<span class="math inline">\(s \sim a\)</span>， 再确定新状态<span class="math inline">\(s, a \sim s\prime\)</span></p></li><li><p><span class="math inline">\(Q(s,a)\)</span>函数是确定动作a后的V函数</p></li></ul><p>V函数是所有动作a的Q函数的期望 <span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}[Q^\pi(s, a)]\]</span> ## 值函数的作用</p><p><code>值函数</code>用<strong>来对策略<span class="math inline">\(\pi(a\mid s)\)</span>进行评估</strong>。</p><p>如果在状态s，有一个动作a使得<span class="math inline">\(Q^\pi(s, a) &gt; V^\pi(s)\)</span></p><ul><li>s状态，执行动作a 比 s状态 所有动作的期望，都要好。<strong>状态a高于所有状态的平均值</strong></li><li>说明执行动作a比当前策略<span class="math inline">\(\pi(a \mid s)\)</span>好</li><li><strong>调整参数使<span class="math inline">\(\pi(a \mid s)\)</span>的概率增加</strong></li></ul><h1 id="贝尔曼和贝尔曼最优方程">贝尔曼和贝尔曼最优方程</h1><blockquote><ol style="list-style-type: decimal"><li><span class="math inline">\(V(s)\)</span>函数和<span class="math inline">\(Q(s,a)\)</span>函数</li><li>贝尔曼方程（选择所有可能的均值）</li><li>贝尔曼最优方程（直接选择最大值）</li></ol></blockquote><h2 id="v函数与q函数">V函数与Q函数</h2><p>V函数：以<strong>s为初始状态</strong>，执行策略<span class="math inline">\(\pi\)</span>得到的<code>期望回报</code>（所有轨迹回报的均值） <span class="math display">\[V^\pi(s) = E_{\tau \sim p(\tau)} [\sum_{t=0}^{T-1}r_{t+1} \mid \tau_{s_0} = s]\]</span> Q函数：以<strong>s为初始状态，执行动作a</strong>，执行策略<span class="math inline">\(\pi\)</span>得到的期望回报 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> 利用V函数去计算Q函数 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><h2 id="贝尔曼方程">贝尔曼方程</h2><p><span class="math inline">\(V(s)\)</span>的贝尔曼方程，选择<strong>所有a的期望回报</strong>， 也是<strong>Q函数的均值</strong>，<span class="math inline">\(V(s)=E_a[Q(s, a)]\)</span> <span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}[Q^\pi(s, a)]\]</span></p><p><span class="math inline">\(Q(s,a)\)</span>函数的贝尔曼方程 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma E_{a\prime \sim \pi(a\prime \mid s\prime)}[Q^\pi(s\prime, a\prime)]]\]</span></p><h2 id="贝尔曼最优方程">贝尔曼最优方程</h2><p><span class="math inline">\(V(s)\)</span>函数的贝尔曼最优方程，实际上是<strong>直接选择所有a中的最大回报</strong> ： <span class="math display">\[V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span> <span class="math inline">\(Q(s,a)\)</span>函数的贝尔曼最优方程 <span class="math display">\[Q^*(s, a) =  E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma \max_\limits{a\prime}Q^*(s^\prime, a^\prime)]\]</span></p><h1 id="深度强化学习">深度强化学习</h1><p>有些任务的状态和动作非常多，并且是连续的。普通方法很难去计算。</p><p>可以使用更复杂的函数（深度神经网络）使智能体来感知更复杂的环境状态，建立更复杂的策略。</p><p>深度强化学习</p><ul><li><code>强化学习</code> -- <strong>定义问题和优化目标</strong></li><li><code>深度学习</code> -- 解决<strong>状态表示</strong>、<strong>策略表示</strong>等问题</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;强化学习的基础知识。基本要素、轨迹、值函数、V函数和Q函数、贝尔曼方程。
    
    </summary>
    
      <category term="强化学习" scheme="http://plmsmile.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能体" scheme="http://plmsmile.github.io/tags/%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    
      <category term="环境" scheme="http://plmsmile.github.io/tags/%E7%8E%AF%E5%A2%83/"/>
    
      <category term="值函数" scheme="http://plmsmile.github.io/tags/%E5%80%BC%E5%87%BD%E6%95%B0/"/>
    
      <category term="贝尔曼方程" scheme="http://plmsmile.github.io/tags/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/"/>
    
      <category term="V函数" scheme="http://plmsmile.github.io/tags/V%E5%87%BD%E6%95%B0/"/>
    
      <category term="Q函数" scheme="http://plmsmile.github.io/tags/Q%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>阿里小蜜论文</title>
    <link href="http://plmsmile.github.io/2018/03/31/36-alime-chat/"/>
    <id>http://plmsmile.github.io/2018/03/31/36-alime-chat/</id>
    <published>2018-03-31T06:23:20.000Z</published>
    <updated>2018-11-25T08:30:08.677Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://www.aclweb.org/anthology/P17-2079" target="_blank" rel="noopener">AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine</a> <a id="more"></a></p><p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="alime-chat">AliMe Chat</h1><h2 id="概览">概览</h2><p><strong>1. IR Model</strong></p><p>Information Retrieval。有一个QA对知识库，给一个问题，选择最相似的问题Pair，得出答案。</p><p>缺点：很难处理那些不在QA知识库里面的<code>Long tail</code>问句</p><p><strong>2. Generation Model</strong></p><p><code>(Seq2Seq)</code> ：基于Question生成一个回答</p><p>缺点：会产生一些不连贯或者没意义的回答</p><p><strong>3. 小蜜的混合模型</strong></p><p>集成了IR和生成式模型。</p><ol style="list-style-type: decimal"><li>收到一个句子</li><li><strong>IR模型</strong>：从QA知识库中选择一些答案作为候选答案</li><li><strong>打分模型</strong>：利用<code>Attentive Seq2Seq</code>对候选答案进行打分</li><li>最高得分大于<code>阈值</code>，直接输出该得分</li><li><strong>生成式模型</strong>：否则，利用生成式模型生成一个回答</li></ol><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/alime/alimechat.png" style="display:block; margin:auto" width="80%"></p><h2 id="qa知识库">QA知识库</h2><p>从用户和员工的对话数据中，提取一些问题和答案。也会把几个问题连在一起。最终共9164834个QA对。</p><h2 id="ir模型">IR模型</h2><p><strong>1. IR步骤</strong></p><ol style="list-style-type: decimal"><li>建立索引：每个单词 -- 多个问题</li><li>收到一个问句计算它的词集：分词 -- 去掉停用词 -- 扩展同义词</li><li>利用词集和索引去找到若干个QA对</li><li>利用BM25算法，去计算问句和所有候选QA对里问题的相似度</li><li>选择最相似的QA对</li></ol><p><strong>2. BM25算法</strong></p><p>BM25通常用来搜索相关性评分。</p><p>一个<code>query</code>和一个<code>d</code> 。把query分割成<span class="math inline">\(K\)</span>个<strong>语素<span class="math inline">\(q_i\)</span></strong>（中文是分词） <span class="math display">\[\rm{score}(q, d) = \sum_{i}^K w_i \cdot r(q_i, d)\]</span> <span class="math inline">\(w_i​\)</span>是判断一个词与一个文档的相关性权重。这里使用<code>IDF</code>来计算。</p><ul><li><span class="math inline">\(N\)</span>是总文档数，<span class="math inline">\(N(q_i)\)</span>为包含词<span class="math inline">\(q_i\)</span>的文档数</li><li><span class="math inline">\(f_i\)</span> 为<span class="math inline">\(q_i\)</span>在d中的出现频率，<span class="math inline">\(g_i\)</span>为<span class="math inline">\(q_i\)</span>在<span class="math inline">\(q\)</span>中的出现频率<br></li><li><span class="math inline">\(d_l\)</span> 为<span class="math inline">\(d\)</span>的长度，<span class="math inline">\(\rm{avg}(d_l)\)</span>是所有文档的长度<br></li><li><span class="math inline">\(k_1, k_2, b\)</span> 为调节因子，一般<span class="math inline">\(k_1=2,b=0.75\)</span></li></ul><p><span class="math display">\[w_i = \rm{IDF}(q_i) = \log \frac{N - N(q_i) + 0.5}{N(q_i) + 0.5}\]</span></p><p><span class="math display">\[r(q_i, d) = \frac{f_i \cdot (k_1 + 1)} {f_i + K} \cdot \frac{g_i \cdot (k_2 +1 )}{g_i + k_2}\]</span></p><p><span class="math display">\[K = k_1 \cdot (1 - b + b \cdot \frac{d_l}{\rm{avg}(d_l)})\]</span></p><p>特别地，一般<span class="math inline">\(q_i\)</span>只在<span class="math inline">\(q\)</span>中出现一次，即<span class="math inline">\(g_i = 1\)</span>， 则 <span class="math display">\[r(q_i, d) = \frac{f_i \cdot (k_1 + 1)} {f_i + K}\]</span> 调节因子</p><ul><li>K ：相同<span class="math inline">\(f_i\)</span>的情况下，文档越长，相似性越低</li><li>b：越大，提高长文档与<span class="math inline">\(q_i\)</span>的相似性</li></ul><h2 id="生成式模型">生成式模型</h2><p><strong>1. Attentive Seq2Seq</strong></p><ol style="list-style-type: decimal"><li>输入问句的语义信息： <span class="math inline">\((h_1, h_2, \cdots, h_m)\)</span></li><li>上一时刻的单词和隐状态：<span class="math inline">\(y_{i-1},s_{i-1}\)</span></li><li>计算注意力分布：<span class="math inline">\(\alpha_{ij} = a(s_{i-1}, h_j)\)</span></li><li>语义信息：<span class="math inline">\(c_i = \sum_{j=1}^m \alpha_{ij}h_j\)</span></li><li>预测当前单词：<span class="math inline">\(p(y_i=w_i \mid \theta_i) = p(y_i=w_i \mid y_1, \cdots, y_{i-1}, c_i) = f(y_{i-1}, s_{i-1}, c_i)\)</span></li></ol><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/alime/attentive-seq2seq.png" style="display:block; margin:auto" width="70%"></p><p><strong>2. 数据padding</strong></p><p>利用Tensorflow的<code>Bucket Mechanism</code>组织。选择(5,5),(5,10),(10,15),(20,30),(45,60)。</p><p><strong>3. Softmax</strong></p><p>训练时，softmax词表使用目标词汇+512个随机词汇。</p><p><strong>4. BeamSearch 解码</strong></p><p>每个时刻选择top-k(k=10)</p><h2 id="打分模型">打分模型</h2><p>打分模型，对所有候选答案计算一个得分，然后选择得分最高的答案。</p><p>生成式模型，在解码时会计算各个单词的概率。打分模型和生成式模型使用同一个模型。</p><p>打分模型，计算候选回答中每个单词在Decoder时出现的概率。再求平均值作为该回答的得分。 <span class="math display">\[s^{\text{avg}(p)} = \frac{1}{n} \sum_{i=1}^n p(y_i = w_i \mid \theta_i)\]</span></p><h2 id="评价方法">评价方法</h2><p>5个评价规则：</p><ul><li>语法正确</li><li>意义相关</li><li>标准的表达</li><li>上下文无关 context independent</li><li>not overly generalized</li></ul><p>答案的三个级别：</p><ul><li>2 ：适合。满足所有规则</li><li>1： 一般。满足前三项，不满足后面两项其中一项</li><li>0：不适合</li></ul><p>top-1概率 <span class="math display">\[P_{\rm{top}_1} = \frac{N_{合适} + N_{一般}}{N_{所有}}\]</span></p><h1 id="阿里小蜜助手">阿里小蜜助手</h1><p><a href="https://arxiv.org/abs/1801.05032" target="_blank" rel="noopener">小蜜助手</a></p><p>小蜜主要包括：助手(Task)服务、客户服务、聊天服务。支持声音、文本输入，支持多轮对话。</p><h2 id="系统概览">系统概览</h2><p><strong>1. 系统概览</strong></p><ol style="list-style-type: decimal"><li>输入层：接收多个终端和多种形式的输入</li><li>意图分类层：<code>Rules Parser</code> 直接解析意图，失败则通过 <code>Intention Classifier</code> 解析意图</li><li>处理问题组件层：语义解析、知识图引擎、信息提取引擎、Slot Filling引擎、聊天引擎</li><li>知识库：QA对，知识图。</li></ol><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/alime/ali-assist.png" style="display:block; margin:auto" width="70%"></p><p><strong>2. 问题的信息流</strong></p><p>收到一个问句后</p><p>1 使用<code>Business Rules Parser</code> （trie-based）去解析q，如果匹配到一个模式</p><ol style="list-style-type: decimal"><li>q是一个任务型的问题（助手服务）：给<code>Slot Filling Engine</code>（槽填充） 直接给答案</li><li>q是一个促销活动：给到<code>Sales Promotion</code> ，回答准备好的答案</li><li>q是请求人工：则先询问客户有什么问题</li></ol><p>2 没有匹配到一个模式，给到<code>意图分类器</code>去识别意图，也就是识别出意图的场景（比如退货、退款、人工等）</p><p>3 如果场景是要转人工，则直接转人工</p><p>4 否则，q给到语义解析器，去识别是否包含语义标签（知识图谱中的实体）</p><ol style="list-style-type: decimal"><li>如果识别出语义标签，则通过知识图谱去找答案，如果找到直接输出</li><li>如果知识图谱没有答案</li><li>如果有上下文，结合上下文和q再去解析语义，再给到语义解析器解析语义标签</li><li>如果没有上下文，则判断是否要询问用户</li><li>如果要询问，则通过模板去询问用户</li><li>如果不用询问，则通过<code>IR</code>去提取信息，如果有答案，则输出；如果没有，则转人工</li></ol><p>5 如果不包含语义标签</p><ol style="list-style-type: decimal"><li>如果要聊天，则通过聊天引擎去产生结果</li><li>否则，通过词模板去输出结果</li></ol><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/alime/alime-assist-flow.png" style="display:block; margin:auto" width="70%"></p><h2 id="意图分类">意图分类</h2><p>对一个问句结合上下文（前面的文件）去识别出它的意图。有3个大范围：</p><ul><li>助手。我要订机票</li><li>信息咨询、解决方案。怎么找回密码</li><li>聊天。我不开心</li></ul><p>每一个大的范围都会进行商业细化。比如助手服务会包含订机票、手机充值。</p><p>意图分类由<strong>商业规则解析器</strong>和<strong>意图分类器</strong>组成。前者解析失败，才会执行后者</p><ul><li>规则解析器：一颗很大的trie树，写了很多的规则</li><li>意图分类器：CNN</li></ul><p>CNN，使用<code>fast-text</code>训练的词向量，fine tuned in CNN</p><ul><li>输入1：问题q</li><li>输入2：问题q和上下文(之前的问题)的语义标签</li></ul><p>CNN的好处</p><ul><li>也可以捕获上下文信息（前一个和后一个），足够好、够用的结果就行</li><li>CNN快啊，QPS=200，Query per Second</li><li>多个卷积池化层或者RNN能实现一个更好的结果，但是扩展性不好？为什么？</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/alime/cnn.png" style="display:block; margin:auto" width="70%"></p><h1 id="taskbot">TaskBot</h1><p>主要是以强化学习为中心的端到端的对话管理，由下面三个部分组成：</p><h2 id="intent-network">Intent network</h2><blockquote><p>处理用户的输入</p></blockquote><p>使用单层CNN对用户的问句进行编码，得到用户的意图语义向量</p><h2 id="neural-belief-tracker">Neural belief tracker</h2><blockquote><p>提取记录用户的slot信息</p></blockquote><ul><li>使用BiLSTM-CRF来提取用户每次输入的slot信息</li><li>根据上一轮系统的回答和当前用户的问句生成当前的Context信息，给到后面的Policy Network</li><li>优点是：BiLSTM可以挖掘出当前词的Context信息，而CRF能有效地对标记序列进行建模</li></ul><h2 id="policy-networker">Policy networker</h2><blockquote><p>决定系统的操作，继续反问用户或者直接产生相应的实际操作</p></blockquote><p>这也是强化学习的核心点，主要包含Episode，Reward，State和Action四个部分。 <strong>Episode</strong> 在某个场景下，识别出用户该场景的意图，则认为一个Episode开始；执行目的操作或者退出，则认为Episode结束 <strong>Reward</strong> 收集线上用户的反馈，并根据正负给出相应的Reward。特别注意要使用预训练的环境 <strong>State</strong> 结合当前新的Slot状态（Context）、历史的Slot信息和用户的当前问句信息，使用线性层+Softmax直接算出各个Actions的概率 <strong>Action</strong> Action就是系统可以给用户的一些反馈操作，比如继续询问用户、执行一个真实的操作等等。</p><p>该Taskbot的瓶颈主要是难以确定用户退出的原因，从而很难给出一些确定的惩罚。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P17-2079&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine&lt;/a&gt;
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="chatbot" scheme="http://plmsmile.github.io/tags/chatbot/"/>
    
      <category term="qa" scheme="http://plmsmile.github.io/tags/qa/"/>
    
      <category term="IR" scheme="http://plmsmile.github.io/tags/IR/"/>
    
      <category term="BM25" scheme="http://plmsmile.github.io/tags/BM25/"/>
    
      <category term="seq2seq" scheme="http://plmsmile.github.io/tags/seq2seq/"/>
    
      <category term="CNN" scheme="http://plmsmile.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>网络优化</title>
    <link href="http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/"/>
    <id>http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/</id>
    <published>2018-03-30T05:54:34.000Z</published>
    <updated>2018-11-25T08:30:08.636Z</updated>
    
    <content type="html"><![CDATA[<p>参数初始化、数据预处理、逐层归一化、各种优化方法、超参数优化。<a id="more"></a></p><blockquote><p>任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法</p></blockquote><p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="神经网络的问题">神经网络的问题</h1><p>神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过<code>优化</code>和<code>正则化</code>来提升网络。</p><h2 id="优化问题">优化问题</h2><p><strong>优化问题的难点</strong></p><ul><li>网络是一个<strong>非凸函数</strong>，深层网络的<strong>梯度消失</strong>问题，很难优化</li><li>网络<strong>结构多样性</strong>，很难找到通用优化方法</li><li>参数多、数据大，<strong>训练效率低</strong></li><li>参数多，存在<strong>高维变量的非凸优化</strong></li></ul><p>低维空间非凸优化：存在局部最优点，难在初始化参数和逃离局部最优点</p><p>高维空间非凸优化：难在如何逃离<code>鞍点</code>。 鞍点是梯度为0，但一些维度是最高点，另一些维度是最低点。</p><p>梯度下降法<strong>很难逃离鞍点</strong>。</p><p><strong>梯度下降法面临的问题</strong></p><ul><li>如何初始化参数</li><li>预处理数据</li><li>如何选择合适的学习率，避免陷入局部最优</li></ul><h2 id="泛化问题">泛化问题</h2><p>神经网络拟合能力很强，容易过拟合。<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/">解决过拟合的5个方法</a></p><h1 id="参数初始化">参数初始化</h1><p><a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96">我之前的参数初始化笔记</a></p><h2 id="对称权重问题">对称权重问题</h2><p><strong>全0产生的对称权重问题</strong></p><p><strong>参数千万不能全0初始化</strong>。如果全0初始化，会导致隐层神经元激活值都相同，导致深层神经元没有区分性。这就是<code>对称权重</code>现象。</p><p>通俗点：</p><ul><li>每个神经元输出相同 -- BP时梯度也相同 -- 参数更新也相同</li><li>神经元之间就<strong>失去了不对称性的源头</strong></li></ul><p>应该对每个参数<code>随机初始化</code>，打破这个对称权重现象，<strong>使得不同神经元之间区分性更好</strong>。</p><p><strong>参数区间的选择</strong></p><p><code>参数太小</code>时</p><p>使得<code>Sigmoid</code>激活函数<strong>丢失非线性的能力</strong>。在0附近近似线性，多层神经网络的优势也不存在。</p><p><code>参数太大</code>时</p><p><code>Sigmoid</code>的输入会变得很大，<strong>输出接近1</strong>。<strong>梯度直接等于0</strong>。</p><p>选择一个<strong>合适的初始化区间非常重要</strong>。如果，一个神经元输入连接很多，那么每个输入连接上的权值就应该小一些。</p><h2 id="高斯分布初始化">高斯分布初始化</h2><p>高斯分布也就是正态分布。</p><p>初始化一个深度网络，比较好的方案是<strong>保持每个神经元输入的方差</strong>为一个<code>常量</code>。</p><p>如果神经元输入是<span class="math inline">\(n_{in}\)</span>， 输出是<span class="math inline">\(n_{out}\)</span>， 则按照<span class="math inline">\(N(0, \sqrt{\frac {2}{n_{in} + n_{out}}})\)</span> 来初始化参数。</p><h2 id="均匀分布初始化">均匀分布初始化</h2><p>在<span class="math inline">\([-r, r]\)</span>区间内，采用均匀分布来初始化参数</p><h2 id="xavier均匀分布初始化">Xavier均匀分布初始化</h2><p>会自动计算超参数<span class="math inline">\(r\)</span>， 来对参数进行<span class="math inline">\([-r, r]\)</span>均匀分布初始化。</p><p>设<span class="math inline">\(n^{l}\)</span>为第<span class="math inline">\(l\)</span> 层神经元个数， <span class="math inline">\(n^{l-1}\)</span> 是第<span class="math inline">\(l-1\)</span>层神经元个数。</p><ul><li><code>logsitic</code>激活函数 ：<span class="math inline">\(r = \sqrt{\frac{6}{n^{l-1} + n^l}}\)</span></li><li><code>tanh</code>激活函数： <span class="math inline">\(r = 4 \sqrt{\frac{6}{n^{l-1} + n^l}}\)</span></li></ul><p><span class="math inline">\(l\)</span>层的一个神经元<span class="math inline">\(z^l\)</span>，收到<span class="math inline">\(l-1\)</span>层的<span class="math inline">\(n^{l-1}\)</span>个神经元的输出<span class="math inline">\(a_i^{l-1}\)</span>, <span class="math inline">\(i \in [1, n^{(l-1)}]\)</span>。 <span class="math display">\[z^l = \sum_{i=1}^n w_i^l a_i^{l-1}\]</span> 为了避免初始化参数使得激活值变得饱和，尽量使<span class="math inline">\(z^l\)</span>处于线性区间，即<strong>神经元的输出</strong><span class="math inline">\(a^l = f(z^l) \approx z^l\)</span>。</p><p>假设<span class="math inline">\(w_i^l\)</span>和<span class="math inline">\(a_i^{l-1}\)</span>相互独立，均值均为0，则a的均值为 <span class="math display">\[E[a^l] = E[\sum_{i=1}^n w_i^l a_i^{l-1}] = \sum_{i=1}^d E[\mathbf w_i] E[a_i^{l-1}] = 0\]</span> <span class="math inline">\(a^l\)</span>的方差 <span class="math display">\[\mathrm{Var} [a^l] = n^{l-1} \cdot \mathrm{Var} [w_i^l] \cdot \mathrm{Var} [a^{l-1}_i]\]</span> 输入信号经过该神经元后，被放大或缩小了<span class="math inline">\(n^{l-1} \cdot \mathrm{Var} [w_i^l]\)</span>倍。</p><p>为了使输入信号经过多层网络后，不被过分放大或过分缩小，应该使<span class="math inline">\(n^{l-1} \cdot \mathrm{Var} [w_i^l]=1\)</span>。</p><p>综合前向和后向，使<strong>信号在前向和反向传播中都不被放大或缩小</strong>，综合设置方差： <span class="math display">\[\mathrm{Var} [w_i^l] = \frac{2} {n^{l-1} + n^l}\]</span></p><h1 id="数据预处理">数据预处理</h1><h2 id="为什么要归一化">为什么要归一化</h2><p>每一维的特征的来源和度量单位不同，导致特征分布不同。</p><p><strong>未归一化数据的3个坏处</strong></p><ol style="list-style-type: decimal"><li>样本之间的欧式距离度量不准。取值范围大的特征会占主导作用。类似于<a href="https://plmsmile.github.io/2018/03/05/29-desicion-tree/#%E7%86%B5%E5%92%8C%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A">信息增益和信息增益比</a></li><li>降低神经网络的训练效率</li><li>降低梯度下降法的搜索效率</li></ol><p><strong>未归一化对梯度下降的影响</strong></p><ul><li>取值范围不同：大多数位置的梯度方向不是最优的，要多次迭代才能收敛</li><li>取值范围相同：大部分位置的梯度方向近似于最优搜索方向，每一步都指向最小值，训练效率大大提高</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/data_standard.png" style="display:block; margin:auto" width="70%"></p><p><strong>归一化要做的事情</strong></p><ol style="list-style-type: decimal"><li>各个维度特征归一化到同一个取值区间</li><li>消除不同特征的相关性</li></ol><h2 id="标准归一化">标准归一化</h2><p>实际上是由<code>中心化</code>和<code>标准化</code>结合的。 把<strong>数据归一化到标准正态分布</strong>。<span class="math inline">\(X \sim N(0, 1^2)\)</span></p><p>计算均值和方差 <span class="math display">\[\mu = \frac{1}{N} \sum_{i=1}^n x^{(i)} \\\sigma^2 =  \frac{1}{N} \sum_{i=1}^n(x^{(i)} - \mu)^2\]</span> 归一化数据，减均值除以标准差。如果<span class="math inline">\(\sigma = 0\)</span>， 说明特征没有区分性，应该直接删掉。 <span class="math display">\[\hat x^{(i)} = \frac {x^{(i)} - \mu}{ \sigma }\]</span> <img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/data-process.jpg" style="display:block; margin:auto" width="70%"></p><h2 id="缩放归一化">缩放归一化</h2><p>把数据归一化到<span class="math inline">\([0, 1]\)</span> 或者<span class="math inline">\([-1, 1]\)</span> 直接。 <span class="math display">\[x^{(i)} = \frac {x^{(i)} - \min(x)}{\max(x) - \min (x)}\]</span></p><h2 id="白化">白化</h2><p><code>白化</code>用来降低输入数据特征之间的冗余性。白化主要使用PCA来去掉特征之间的相关性。<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#%E7%99%BD%E5%8C%96">我的白化笔记</a></p><p>处理后的数据</p><ul><li>特征之间相关性较低</li><li>所有特征具有相同的方差</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/image/nlp/cs224n/notes3/data-pca-process.jpg" style="display:block; margin:auto" width="70%"></p><p><strong>白化的缺点</strong></p><p>可能会夸大数据中的噪声。所有维度都拉到了相同的数值范围。可能有一些差异性小、但大多数是噪声的维度。可以使用平滑来解决。</p><h1 id="逐层归一化">逐层归一化</h1><h2 id="原因">原因</h2><p>深层神经网络，中间层的输入是上一层的输出。每次SGD参数更新，都会导致<strong>每一层的输入分布发生改变</strong>。</p><p>像高楼，低楼层发生较小偏移，就会导致高楼层发生较大偏移。</p><p>如果<strong>某个层的输入发生改变</strong>，其<strong>参数就需要重新学习</strong>，这也是<code>内部协变量偏移</code>问题。</p><p>在训练过程中，要使得每一层的输入分布保持一致。简单点，对每一个神经层进行归一化。</p><ul><li>批量归一化</li><li>层归一化</li><li>其它方法</li></ul><h2 id="批量归一化">批量归一化</h2><p>针对<strong>每一个维度</strong>，对<strong>每个batch的数据</strong>进行<strong>归一化+缩放平移</strong>。</p><p>批量归一化<code>Batch Normalization</code> ，<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#batch-normalization">我的BN详细笔记</a>。 对每一层（<strong>单个神经元</strong>）的输入进行归一化 <span class="math display">\[\begin{align}&amp; \mu = \frac{1}{m} \sum_{i=1}^m x_i &amp;  \text{求均值} \\&amp; \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 &amp; \text{求方差} \\&amp; \hat x = \frac{x - E(x)} {\sqrt{\sigma^2 + \epsilon}} &amp;  \text{标准归一化} \\&amp; y =  \gamma \hat x+ \beta &amp; \text{缩放和平移} \end{align}\]</span> <code>缩放参数</code><span class="math inline">\(\gamma\)</span> ，和<code>平移参数</code> <span class="math inline">\(\beta\)</span> 的作用</p><ul><li>强行归一化会破坏刚学习到的特征。用这两个变量去还原应该学习到的数据分布</li><li>归一化会聚集在0处，会减弱神经网络的非线性性质。缩放和平移解决这个问题</li></ul><p>注意：</p><ul><li>BN是对中间层的<strong>单个神经元</strong>进行归一化</li><li>要求<strong>批量样本数量不能太小</strong>，否则难以计算单个神经元的统计信息</li><li>如果层的净输入的分布是<strong>动态变化</strong>的，则<strong>无法使用批量归一化</strong>。如循环神经网络</li></ul><h2 id="层归一化">层归一化</h2><p>对每个样本，对所有维度做一个归一化，即对<strong>同层的所有神经元</strong>的输入做归一化。</p><ul><li><code>层归一化</code>是<strong>对一个中间层的所有神经元进行归一化</strong></li><li>批量归一化是对一个中间层的单个神经元进行归一化</li></ul><p>设第<span class="math inline">\(l\)</span>层的净输入为<span class="math inline">\(\mathbf z^{(l)}\)</span>， 求<strong>第<span class="math inline">\(l\)</span>层所有输入</strong>的<code>均值</code>和<code>方差</code> <span class="math display">\[\begin{align}&amp; \mu^{(l)} = \frac{1}{n^l} \sum_{i=1}^{n^l} z_i^{(l)} &amp;  \text{第l层输入的均值} \\&amp; \sigma^{(l)^2} = \frac{1}{n^l} \sum_{i=1}^{n^l} (z_i^{(l)} - \mu^{(l)})^2 &amp; \text{第l层输入的方差} \\ \end{align}\]</span> <code>层归一化</code> 如下，其中<span class="math inline">\(\gamma, \beta\)</span>是缩放和平移的参数向量，与<span class="math inline">\(\mathbf z^{(l)}\)</span>维数相同 <span class="math display">\[\hat {\mathbf z}^{(l)} = \rm{LayerNorm}_{\gamma, \beta} (\mathbf z^{(l)}) = \frac {\mathbf z^{(l) - \mu^{(l)}}}{\sqrt{\sigma ^{(l)^2} + \epsilon}} \cdot \gamma + \beta\]</span> <strong>层归一化的RNN</strong> <span class="math display">\[\mathbf z_t = U\mathbf h_{t-1} + W \mathbf x_t \\\mathbf h_t = f (\rm{LN}_{\gamma, \beta}(\mathbf z_t)))\]</span> RNN的净输入一般会随着时间慢慢变大或变小，导致梯度爆炸或消失。</p><p>层归一化的RNN可以有效缓解梯度消失和梯度爆炸。</p><h2 id="批归和层归对比">批归和层归对比</h2><p>思想类似，都是<code>标准归一化</code> + <code>缩放和平移</code>。</p><ul><li>批量归一化：针对每一个维度，对batch的所有数据做归一化</li><li>层归一化：针对每一个样本，对所有维度做归一化。可以用在RNN上，减小梯度消失和梯度爆炸。</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/layer-batch-norm.jpg" style="display:block; margin:auto" width="70%"></p><h2 id="其它归一化">其它归一化</h2><p><strong>权重归一化</strong></p><p>对神经网络的连接权重进行归一化。</p><p><strong>局部相应归一化</strong></p><p>对同层的神经元进行归一化。但是局部响应归一化，用在激活函数之后，对邻近的神经元进行局部归一化。</p><h1 id="梯度下降法的改进">梯度下降法的改进</h1><h2 id="梯度下降法">梯度下降法</h2><p><code>Mini-Batch</code>梯度下降法。设<span class="math inline">\(f(\mathbf x ^{(i)}, \theta)\)</span> 是神经网络。</p><p>在第<span class="math inline">\(t\)</span>次迭代(epoch)时，选取<span class="math inline">\(m\)</span>个训练样本<span class="math inline">\(\{\mathbf x^{(i)}, y^{(i)} \}_{i=1}^m\)</span>。 计算梯度<span class="math inline">\(\mathbf g_t\)</span> <span class="math display">\[\mathbf g_t = \frac{1}{m} \sum_{i \in I_t} \frac {\partial J(y^{(i)}, f(\mathbf x ^{(i)}, \theta))}{\partial \theta} + \lambda \|\theta\| ^2   \]</span> 更新参数，其中学习率<span class="math inline">\(\alpha \ge 0\)</span> ： <span class="math display">\[\theta_t = \theta_{t-1} - \alpha \mathbf g_t \]</span></p><p><span class="math display">\[\theta_t = \theta_{t-1}+ \Delta \theta_t\]</span></p><p><strong>1. BGD</strong></p><p>Batch Gradient Descent</p><p><code>意义</code>：每一轮选择所有整个数据集去计算梯度更新参数</p><p><code>优点</code></p><ul><li>凸函数，可以保证收敛到全局最优点；非凸函数，保证收敛到局部最优点</li></ul><p><code>缺点</code></p><ul><li>批量梯度下降非常慢。因为在整个数据集上计算</li><li>训练次数多时，耗费内存</li><li>不允许在线更新模型，例如更新实例</li></ul><p><strong>2. SGD</strong></p><p>Stochastic Gradient Descent</p><p><code>意义</code>：每轮值选择一条数据去计算梯度更新参数</p><p><code>优点</code></p><ul><li>算法收敛快（BGD每轮会计算很多相似样本的梯度，冗余的）</li><li>可以在线更新</li><li>有一定几率跳出比较差的局部最优而到达更好的局部最优或者全局最优</li></ul><p><code>缺点</code></p><ul><li>容易收敛到局部最优，并且容易困在鞍点</li></ul><p><strong>3. Mini-BGD</strong></p><p>Mini-Batch Gradient Descent</p><p><code>意义</code>： 每次迭代只计算一个mini-batch的梯度去更新参数</p><p>优点</p><ul><li>计算效率高，收敛较为稳定</li></ul><p><code>缺点</code></p><ul><li>更新方向依赖于当前batch算出的梯度，不稳定</li></ul><p><strong>4. 梯度下降法的难点</strong></p><ol style="list-style-type: decimal"><li>学习率<span class="math inline">\(\alpha\)</span>难以选择。太小，导致收敛缓慢；太大，造成较大波动妨碍收敛</li><li>学习率一直相同是不合理的。出现频率低的特征，大学习率；出现频率小的特征，小学习率</li><li>按迭代次数和loss阈值在训练时去调整学习率。然而次数和阈值难以设定，无法适应所有数据</li><li>很难逃离鞍点。梯度为0，一些特征是最高点（上升），一些特征是最低点（下降）</li><li>更新方向依赖于当前batch算出的梯度，不稳定</li></ol><p>主要通过<strong>学习率递减</strong>和<strong>动量法</strong>来优化梯度下降法。</p><p>可以看出</p><ul><li>SGD，整体下降，但局部会来回震荡</li><li>MBGD，一个batch来说，batch越大，下降越快，越平滑</li><li>MBGD，整体来说，batch越小，下降越明显</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/sgd_batch.png" style="display:block; margin:auto" width="70%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/sgd-whole.png" style="display:block; margin:auto" width="71%"></p><h2 id="学习率递减">学习率递减</h2><p><strong>0 指数加权平均</strong></p><p>求10天的平均温度，可以直接利用平均数求，每天的权值是一样的，且要保存所有的数值才能计算。 <span class="math display">\[v_{avg} = \frac {v_1 + \cdots + v_{100}}{100}\]</span> 设<span class="math inline">\(v_t\)</span>是到第t天的平均温度，<span class="math inline">\(\theta_t\)</span>是第t天的真实温度，<span class="math inline">\(\beta=0.9\)</span>是衰减系数。</p><p>则有<code>指数加权平均</code>：<br><span class="math display">\[v_t = \beta * v_{t-1} + (1-\beta) \theta_t\]</span></p><p><span class="math display">\[v_{100} = 0.1 \cdot \theta_{100} + 0.1(0.9)^1 \cdot \theta_{99} + 0.1 (0.9)^2 \cdot \theta_{98} + 0.1(0.9)^3 \cdot \theta_{97} + \ldots\]</span></p><p>离当前越近，权值越大。越远，权值越小（指数递减），也有一定权值。</p><p><strong>1. 按迭代次数递减</strong></p><p>设置<span class="math inline">\(\beta = 0.96\)</span>为衰减率</p><p><code>反时衰减</code> <span class="math display">\[\alpha_t = \alpha_0 \cdot \frac {1} {1 + \beta \times t}\]</span> <code>指数衰减</code> : <span class="math display">\[\alpha_t = \alpha_0 \cdot \beta^t\]</span> <code>自然指数衰减</code> <span class="math display">\[\alpha_t = \alpha_0 \cdot e^{-\beta \cdot t}\]</span> <strong>2. AdaGrad</strong></p><p><code>Adaptive Gradient</code></p><p><code>意义</code>：每次迭代时，根据历史梯度累积量来减小学习率，减小梯度。<strong>梯度平方的累计值</strong>来减小梯度</p><p>初始学习率<span class="math inline">\(\alpha_0\)</span>不变，实际学习率减小。<span class="math inline">\(\alpha = \frac {\alpha_0} {\sqrt {G_t + \epsilon}}\)</span> <span class="math display">\[G_t = \sum_{i=1}^t g_i^2\]</span></p><p><span class="math display">\[\Delta \theta_t = - \frac {\alpha_0}{\sqrt {G_t + \epsilon}} \cdot g_t\]</span></p><p><code>优点</code></p><ul><li>累积梯度<span class="math inline">\(G_t\)</span>的<span class="math inline">\(\frac{1}{\sqrt{G_t + \epsilon}}\)</span>实际上构成了一个约束项<br></li><li>前期<span class="math inline">\(G_t\)</span>较小， 约束值大，能够放大梯度</li><li>后期<span class="math inline">\(G_t\)</span>较大， 约束值小，能够约束梯度</li><li>适合处理稀疏梯度</li></ul><p><code>缺点</code></p><ul><li>经过一些迭代，学习率会变非常小，参数难以更新。过早停止训练</li><li>依赖于人工设置的全局学习率<span class="math inline">\(\alpha_0\)</span></li><li><span class="math inline">\(\alpha_0\)</span>设置过大，约束项大，则对梯度的调节太大</li></ul><p><strong>3. RMSprop</strong></p><p>意义：计算<strong>梯度<span class="math inline">\(\mathbf g_t\)</span>平方</strong>的<code>指数递减移动平均</code>， 即<strong>梯度平方的平均值</strong>来减小梯度 <span class="math display">\[G_t = \beta G_{t-1} + (1-\beta) \cdot \mathbf g_t^2\]</span></p><p><span class="math display">\[\Delta \theta_t = - \frac {\alpha_0}{\sqrt {G_t + \epsilon}} \cdot \mathbf g_t\]</span></p><p><code>优点</code></p><ul><li>解决了AdaGrad学习率一直递减过早停止训练的问题，学习率可大可小</li><li>训练初中期，加速效果不错，很快；训练后期，反复在局部最小值抖动</li><li><strong>适合处理非平稳目标</strong>，对于RNN效果很好</li></ul><p><code>缺点</code></p><ul><li>依然依赖于全局学习率<span class="math inline">\(\alpha_0\)</span></li></ul><p><strong>4. AdaDelta</strong></p><p><code>意义</code> 不初始化学习率。计算<strong>梯度更新差平方</strong>的<code>指数衰减移动平均</code>来作为分子学习率， <span class="math display">\[G_t = \beta G_{t-1} + (1-\beta) \cdot \mathbf g_t^2\]</span></p><p><span class="math display">\[\Delta X_{t-1}^2 = \beta \Delta X_{t-2}^2 + (1-\beta) \Delta \theta_{t-1}^2\]</span></p><p><span class="math display">\[\Delta \theta_t = - \frac { \sqrt {\Delta X_{t-1}^2 + \epsilon}}{\sqrt {G_t + \epsilon}} \cdot \mathbf g_t\]</span></p><p><code>优点</code></p><ul><li>初始学习率<span class="math inline">\(\alpha_0\)</span>改成了动态计算的<span class="math inline">\(\sqrt {\Delta X_{t-1}^2 + \epsilon}\)</span> ，一定程度上平抑了学习率的波动。</li></ul><h2 id="动量法">动量法</h2><p>结合<strong>前面更新的方向</strong>和<strong>当前batch的方向</strong>，来更新参数。</p><p>解决了MBGD的不稳定性，增加了<code>稳定性</code>。可以<code>加速</code>或者<code>减速</code>。</p><p><strong>1. 普通动量法</strong></p><p>设<span class="math inline">\(\rho = 0.9\)</span>为动量因子，计算<strong>负梯度</strong>的<code>移动加权平均</code> <span class="math display">\[\Delta \theta_t = \rho \cdot \Delta \theta_{t-1} - \alpha \cdot \mathbf g_t\]</span></p><p>当前梯度与最近时刻的梯度方向：</p><ul><li>前后<strong>梯度方向一致</strong>：参数更新幅度变大，<strong>会加速</strong></li><li>前后<strong>梯度方向不一致</strong>：参数更新幅度变小，<strong>会减速</strong></li></ul><p>优点：</p><ul><li>迭代初期，梯度方向一致，动量法加速，更快到达最优点</li><li>迭代后期，梯度方向不一致，在收敛值附近震荡，动量法会减速，增加稳定性</li></ul><p>当前梯度叠加上上次的梯度，可以近似地看成二阶梯度。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/momentumjpg.jpg" style="display:block; margin:auto" width="70%"></p><h2 id="adam">Adam</h2><p><code>Adaptive Momentum Estimation</code> = <code>RMSProp</code> + <code>Momentum</code>， 即<strong>自适应学习率+稳定性</strong>（动量法）。</p><p>意义：计算梯度<span class="math inline">\(\mathbf g_t\)</span>的指数权值递减移动平均(<code>动量</code>)，计算梯度平方<span class="math inline">\(\mathbf g_t^2\)</span>的指数权值递减移动平均(<code>自适应alpha</code>)</p><p>设<span class="math inline">\(\beta_1 = 0.9\)</span>， <span class="math inline">\(\beta_2 = 0.99\)</span> 为衰减率 <span class="math display">\[M_t = \beta_1M_{t-1} + (1-\beta_1) \mathbf g_t \quad \quad \sim E(\mathbf g_t)\]</span></p><p><span class="math display">\[G_t = \beta_2 G_{t-1} + (1-\beta_2) \mathbf g_t^2 \quad \quad \sim E(\mathbf g_t^2)\]</span></p><p><span class="math display">\[\hat M_t = \frac {M_t}{1 - \beta_1^t}, \quad \hat G_t = \frac{G_t}{1 - \beta_2^t} \quad \quad \text{初始化偏差修正}\]</span></p><p><span class="math display">\[\Delta \theta_t = - \frac {\alpha_0}{\sqrt{\hat G_t + \epsilon}} \hat M_t \]</span></p><p><code>优点</code></p><ul><li>有RMSprop的处理<strong>非稳态</strong>目标的优点，有Adagrad处理<strong>稀疏梯度</strong>的优点</li><li>对内存需求比较小，高效地计算</li><li>为不同的参数计算不同的自适应学习率</li><li>适用于大多数的非凸优化</li><li>超参数好解释，只需极少量的调参</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/optimizers.png" style="display:block; margin:auto" width="80%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/optimizer-1.gif" style="display:block; margin:auto" width="70%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/dl/optimizer-2.gif" style="display:block; margin:auto" width="70%"></p><h2 id="梯度截断">梯度截断</h2><p>一般按模截断，如果<span class="math inline">\(\|\mathbf g_t\|^2 &gt; b\)</span>， 则 <span class="math display">\[\mathbf g_t = \frac{b}{\|\mathbf g_t\|} \mathbf g_t\]</span></p><h1 id="超参数优化">超参数优化</h1><h2 id="优化内容和难点">优化内容和难点</h2><p><strong>优化内容</strong></p><ul><li>网络结构：神经元之间连接关系、层数、每层的神经元数量、激活函数类型等</li><li>优化参数：优化方法、学习率、小批量样本数量</li><li>正则化系数</li></ul><p><strong>优化难点</strong></p><ul><li>参数优化是组合优化问题，没有梯度下降法来优化，没有通用的有效的方法</li><li>评估一组超参数配置的实际代价非常高</li></ul><p><strong>配置说明</strong></p><ul><li>有<span class="math inline">\(K\)</span>个超参数， 每个超参数配置表示为1个向量<span class="math inline">\(\mathbf x \in X\)</span></li><li><span class="math inline">\(f(\mathbf x)\)</span> 是衡量超参数配置<span class="math inline">\(\mathbf x\)</span>效果的函数</li><li><span class="math inline">\(f(\mathbf x)\)</span>不是<span class="math inline">\(\mathbf x\)</span>的连续函数，<span class="math inline">\(\mathbf x\)</span>也不同。 无法使用梯度下降等优化方法</li></ul><h2 id="超参数设置-搜索">超参数设置-搜索</h2><p>超参数设置：人工搜索、网格搜索、随机搜索。</p><p>缺点：没有利用到不同超参数组合之间的相关性，搜索方式都比较低效。</p><p><strong>1. 网格搜索</strong></p><p>对于<span class="math inline">\(K\)</span>个超参数，第<span class="math inline">\(k\)</span>个参数有<span class="math inline">\(m_k\)</span>种取值。总共的配置数量： <span class="math display">\[N = m_1 \times m_2 \times \cdots \times m_K\]</span> 如果超参数是连续的，可以根据经验选择一些经验值，比如学习率 <span class="math display">\[\alpha \in \{0.01, 0.1, 0.5, 1.0\}\]</span> 对这些超参数的不同组合，分别训练一个模型，测试在开发集上的性能。选取一组性能最好的配置。</p><p><strong>2. 随机搜索</strong></p><p>有的超参数对模型影响力有限（正则化），有的超参数对模型性能影响比较大。网格搜索会遍历所有的可能性。</p><p>随机搜索：对超参数进行随机组合，选择一个性能最好的配置。</p><p>优点：比网格搜索好，更容易实现，更有效。</p><h2 id="贝叶斯优化">贝叶斯优化</h2><p>根据当前已经试验的超参数组合，来预测下一个可能带来的最大收益的组合。</p><p>贝叶斯优化过程：根据已有的N组试验结果来建立高斯过程，计算<span class="math inline">\(f(\mathbf x)\)</span>的后验分布。</p><h2 id="动态资源分配">动态资源分配</h2><p>在早期阶段，估计出一组配置的效果会比较差，则中止这组配置的评估。把更多的资源留给其他配置。</p><p>这是多臂赌博机的泛化问题：最优赌博机。在给定有限次数的情况下，玩赌博机，找到收益最大的臂。</p><h2 id="神经架构搜索">神经架构搜索</h2><p>通过神经网络来自动实现网络架构的设计。</p><ul><li>变长字符串 -- 描述神经网络的架构</li><li>控制器 -- 生成另一个子网络的架构描述</li><li>控制器 -- RNN来实现</li><li>控制器训练 -- 强化学习来完成</li><li>奖励信号 -- 生成的子网络在开发集上的准确率</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参数初始化、数据预处理、逐层归一化、各种优化方法、超参数优化。
    
    </summary>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络" scheme="http://plmsmile.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="数据预处理" scheme="http://plmsmile.github.io/tags/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="归一化" scheme="http://plmsmile.github.io/tags/%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    
      <category term="优化方法" scheme="http://plmsmile.github.io/tags/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    
      <category term="Adam" scheme="http://plmsmile.github.io/tags/Adam/"/>
    
      <category term="动量法" scheme="http://plmsmile.github.io/tags/%E5%8A%A8%E9%87%8F%E6%B3%95/"/>
    
      <category term="学习率" scheme="http://plmsmile.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>各种注意力总结</title>
    <link href="http://plmsmile.github.io/2018/03/25/33-attention-summary/"/>
    <id>http://plmsmile.github.io/2018/03/25/33-attention-summary/</id>
    <published>2018-03-25T06:14:12.000Z</published>
    <updated>2018-12-13T07:52:43.916Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>一切都应该尽可能简单，但不能过于简单。</p></blockquote><blockquote><p>本文主要是总结：注意力机制、注意力机制的变体、论文中常见的注意力</p></blockquote><a id="more"></a><p><img src="" style="display:block; margin:auto" width="80%"></p><h1 id="注意力机制系统介绍">注意力机制系统介绍</h1><h2 id="问题背景">问题背景</h2><p><strong>计算能力不足</strong></p><p>神经网络有很强的能力。但是对于复杂任务，需要大量的输入信息和复杂的计算流程。计算机的计算能力是神经网络的一个瓶颈。</p><p><strong>减少计算复杂度</strong></p><p>常见的：局部连接、权值共享、汇聚操作。</p><p>但仍然需要：尽量<code>少增加模型复杂度</code>（参数），来<code>提高模型的表达能力</code>。</p><p><strong>简单文本分类可以使用单向量表达文本</strong></p><p>只需要一些关键信息即可，所以一个向量足以表达一篇文章，可以用来分类。</p><p><strong>阅读理解需要所有的语义</strong></p><p>文章比较长时，<strong>一个RNN很难反应出文章的所有语义信息</strong>。</p><p>对于阅读理解任务来说，编码时并不知道会遇到什么问题。这些问题可能会涉及到文章的所有信息点，如果丢失任意一个信息就可能导致无法正确回答问题。</p><p><strong>网络容量与参数成正比</strong></p><p>神经网络中可以存储的信息称为<code>网络容量</code>。 存储的多，参数也就越多，网络也就越复杂。 LSTM就是一个存储和计算单元。</p><p><strong>注意力和记忆力解决信息过载问题</strong></p><p>输入的信息太多(<code>信息过载问题</code>)，但不能同时处理这些信息。只能选择重要的信息进行计算，同时用额外空间进行信息存储。</p><ul><li><code>信息选择</code>：聚焦式自上而下地选择重要信息，过滤掉无关的信息。<strong>注意力机制</strong></li><li><code>外部记忆</code> ： 优化神经网络的记忆结构，使用额外的外部记忆，来提高网络的存储信息的容量。 <strong>记忆力机制 </strong></li></ul><p>比如，一篇文章，一个问题。答案只与几个句子相关。所以只需把相关的片段挑选出来交给后续的神经网络来处理，而不需要把所有的文章内容都给到神经网络。</p><h2 id="注意力">注意力</h2><p>注意力机制<code>Attention Mechanism</code> 是<code>解决信息过载</code>的一种资源分配方案，把<strong>计算资源分配给更重要的任务</strong>。</p><blockquote><p>注意力：人脑可以有意或无意地从大量的输入信息中，选择小部分有用信息来重点处理，并忽略其它信息</p></blockquote><p><strong>聚焦式注意力</strong></p><p>自上而下<code>有意识</code>的注意力。有预定目的、依赖任务、<code>主动有意识</code>的<code>聚焦于某一对象</code>的<code>注意力</code>。</p><p>一般注意力值聚焦式注意力。聚焦式注意力会根据环境、情景或任务的不同而选择不同的信息。</p><p><strong>显著性注意力</strong></p><p>自下而上<code>无意识</code>的注意力。由外界刺激驱动的注意力，无需主动干预，也和任务无关。如<code>Max Pooling</code>和<code>Gating</code>。</p><p><strong>鸡尾酒效应</strong></p><p>鸡尾酒效应可以理解这两种注意力。 在吵闹的酒会上</p><ul><li>噪音很多，依然可以听到朋友谈话的内容</li><li>没有关注背景声音，但是突然有人叫自己（重要信息），依然会马上注意到</li></ul><h2 id="普通注意力机制">普通注意力机制</h2><p>把目前的最大汇聚<code>Max Pooling</code>和门控<code>Gating</code> 近似地看做自下而上的基于显著性的注意力机制。</p><p>为了节省资源，选择重要的信息给到后续的神经网络进行计算，而不需要把所有的内容都给到后面的神经网络。</p><p><strong>输入N个信息</strong></p><p><span class="math inline">\(X_{1:N} = [\mathbf{x}_1, \cdots, \mathbf{x}_N]\)</span>， 问题<span class="math inline">\(\mathbf{q}\)</span>。 要从<span class="math inline">\(X\)</span>中选择一些和任务相关的信息输入给神经网络。</p><p><strong>计算注意力分布</strong></p><p><span class="math inline">\(\alpha_i\)</span> : 选择第<span class="math inline">\(i\)</span>个信息的概率，也称为<code>注意力分布</code> ，<span class="math inline">\(z\)</span>表示被选择信息的索引位置 <span class="math display">\[\alpha_i = p(z = i \mid X, \mathbf{q}) = \rm{softmax}\left(s(\mathbf{x}_i, \mathbf{q})\right) = \frac{\exp\left(s(\mathbf{x}_i, \mathbf{q})\right)}{\sum_{j=1}^N \exp\left(s(\mathbf{x}_j, \mathbf{q})\right)}\]</span></p><p><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#global-attention">NMT里面三种score打分函数</a> : <span class="math display">\[\color{blue}{\rm{score}(h_t, \bar h_s)} = \begin{cases}h_t^T \bar h_s &amp; \text{dot} \\h_t^T W_a \bar h_s  &amp; \text{general} \\v_a^T \tanh (W_a [h_t; \bar h_s]) &amp; \text{concat} \\\end{cases}\]</span> <code>加性模型</code> <span class="math display">\[s(\mathbf{x}_i, \mathbf{q}) = v^T\rm{tanh} (W\mathbf{x}_i + U\mathbf{q})\]</span> <code>点击模型</code> <span class="math display">\[s(\mathbf{x}_i, \mathbf{q}) = \mathbf{x}_i^T \mathbf{q}\]</span> <strong>计算注意力</strong></p><p><code>Soft Attention</code> 是对所有的信息进行加权求和。<code>Hard Attention</code>是选择最大信息的那一个。</p><p>使用软性注意力选择机制，对输入信息编码为，实际上也是一个期望。 <span class="math display">\[\rm{attn} (X, \mathbf q) = \sum_{i=1}^N \alpha_i \mathbf x_i = E_{z\sim p(z\mid X, \mathbf{q})} [X]\]</span> <img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/attention.png" style="display:block; margin:auto" width="80%"></p><h2 id="应用与优点">应用与优点</h2><p>传统机器翻译Encoder-Decoder的缺点：</p><ul><li>编码向量容量瓶颈问题：所有信息都需要保存在编码向量中</li><li>长距离依赖问题：长距离信息传递时，信息会丢失</li></ul><p><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/">注意力机制和PyTorch实现机器翻译</a></p><p>注意力机制直接从源语言信息中选择相关的信息作为辅助，有下面几个好处：</p><ul><li>解码过程中每一步都直接访问源语言所有位置上的信息。无需让所有信息都通过编码向量进行传递。</li><li>缩短了信息的传递距离。源语言的信息可以直接传递到解码过程中的每一步</li></ul><p>图像描述生成</p><h1 id="注意力机制变体">注意力机制变体</h1><h2 id="多头注意力">多头注意力</h2><p><code>Multi-head Attention</code>利用多个查询<span class="math inline">\(\mathbf{q}_{1:M}={\mathbf{q}_1, \cdots, \mathbf{q}_M}\)</span>来并行地从输入信息中选取多个信息。每个注意力关注输入信息的不同部分。比如<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/transformer.png" style="display:block; margin:auto" width="60%"> <img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/multi-head-attention.png" style="display:block; margin:auto" width="80%"></p><h2 id="硬性注意力">硬性注意力</h2><p>硬性注意力是<code>只关注到一个位置上</code>。</p><ul><li>选取最高概率的输入信息</li><li>在注意力分布上随机采样</li></ul><p>缺点：loss与注意力分布之间的函数关系不可导，无法使用反向传播训练。一般使用软性注意力。</p><p>需要：硬性注意力需要强化学习来进行训练。</p><h2 id="键值对注意力">键值对注意力</h2><p>输入信息：键值对<code>(Key, Value)</code>。 Key用来计算注意力分布<span class="math inline">\(\alpha_i\)</span>，值用来生成选择的信息。 <span class="math display">\[\rm{attn} (\mathbf{(K, V)}, \mathbf q) = \sum_{i=1}^N \alpha_i \mathbf v_i  =  \sum_{i=1}^N\frac{\exp\left(s(\mathbf{k}_i, \mathbf{q})\right)}{\sum_{j=1}^N \exp\left(s(\mathbf{k}_j, \mathbf{q})\right)} \mathbf v_i\]</span> <img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/attention.png" style="display:block; margin:auto" width="80%"></p><h2 id="结构化注意力">结构化注意力</h2><p>普通注意力是在输入信息上的一个多项分布，是一个扁平结构。</p><p>如果输入信息，本身就有<strong>层次化</strong>的结构，词、句子、段落、篇章等不同粒度的层次。这时用<code>层次化的注意力</code>来进行更好的信息选择。</p><p>也可以使用一种图模型，来构建更加复杂的结构化注意力分布。</p><h2 id="指针网络">指针网络</h2><p>前面的都是计算注意力对信息进行筛选：计算注意力分布，利用分布对信息进行加权平均。</p><p>指针网络<code>pointer network</code>是一种序列到序列的模型，用来指出相关信息的位置。也就是只做第一步。</p><p>输入： <span class="math inline">\(X_{1:n} = [\mathbf{x}_1, \cdots, \mathbf{x}_n]\)</span></p><p>输出：<span class="math inline">\(c_{1:m} = c_1, c_2, \cdots, c_m, \; c_i \in [1,n]\)</span>， 输出是序列的下标。如输入123，输出312</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/pointer-net.png" style="display:block; margin:auto" width="80%"></p><p>条件概率 <span class="math display">\[p(c_{1:m} \mid  \mathbf x_{1:n}) = \prod_{i=1}^m p(c_i \mid c_{1:i-1}, \mathbf x_{1:n})\approx \prod_{i=1}^m p(c_i \mid \mathbf x_{c_1}, \cdots,  \mathbf x_{c_{i-1}}\mathbf x_{1:n})\]</span></p><p><span class="math display">\[p(c_i \mid c_{1:i-1}, \mathbf x_{1:n}) = \rm{softmax}(s_{i,j})\]</span></p><p>第i步时，每个输入向量的得分（未归一化的注意力分布）： <span class="math display">\[s_{i,j} = v^T\rm{tanh} (W\mathbf{x}_j + U\mathbf{e}_i)\]</span> 其中向量<span class="math inline">\(\mathbf e_i\)</span>是第i个时刻，RNN对<span class="math inline">\(\mathbf x_{c_1}, \cdots, \mathbf x_{c_{i-1}}\mathbf x_{1:n}\)</span> 的编码。</p><h1 id="各种注意力计算模型">各种注意力计算模型</h1><h2 id="注意力的本质">注意力的本质</h2><p>有<span class="math inline">\(k\)</span>个<span class="math inline">\(d\)</span>维的特征向量<span class="math inline">\(\mathbf h_i \;(i \in [1,k])\)</span>，想要整合这k个特征向量的信息。得到一个向量<span class="math inline">\(\mathbf h^*\)</span>，一般也是d维。</p><ul><li>简单粗暴：对k个向量求平均。当然不合理啦。</li><li>加权平均：<span class="math inline">\(\mathbf h^* = \sum_{i=1}^k \alpha_i \mathbf h_i\)</span> 。<code>合理</code></li></ul><p>所以最重要的就是<strong>合理地求出<span class="math inline">\(\alpha_i\)</span></strong>，根据<code>所关心的对象</code><span class="math inline">\(\mathbf q\)</span>(可能是自身)去计算注意力分布</p><ul><li>针对每个<span class="math inline">\(\mathbf h_i\)</span>， 计算出一个<strong>得分</strong>，<span class="math inline">\(s_i\)</span>。 <span class="math inline">\(h_i\)</span>与<span class="math inline">\(q\)</span>越相关，得分越高。</li><li><span class="math inline">\(\alpha_i = \rm{softmax}(s_i)\)</span></li></ul><p><span class="math display">\[s_i = \rm{score}(\mathbf h_i, \mathbf q)\]</span></p><p>打分函数的计算：(<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#global-attention">NMT里面三种score打分函数</a> )</p><ul><li><code>Local-based Attention</code> ，没有外部的关注对象，自己关注自己。</li><li><code>General Attention</code>， 有外部的关注对象，直接乘积，全连接层。</li><li><code>Concatenation-based Attention</code>， 有关注的对象，先concat或相加再过连接层。</li></ul><h2 id="local-based">Local-based</h2><p>没有外部的信息，每个向量的得分<strong>与自己相关，与外部无关</strong>。</p><p>比如：<code>Where is the football?</code> ，<code>where</code>和<code>football</code>在句子中起总结性作用。Attention只与句子中的每个词有关。</p><p>一个句子，有多个词，多个向量。通过自己计算注意力分布，再对这些词的注意力进行加权求和，则可以得到这个句子的最终表达。 <span class="math display">\[s_i  = f(\mathbf h_i) = \rm{a}(W^T \mathbf h_i + b)\]</span></p><p><span class="math display">\[\mathbf h^* = \sum_{i=1}^n s_i \cdot \mathbf h_i\]</span></p><p>a是<a href="https://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">激活函数</a>。 <code>sigmoid</code>, <code>tanh</code>, <code>relu</code>, <code>maxout</code>， <code>y=x</code>（无激活函数）。</p><p><strong>1 一个得分简单求法</strong></p><p><a href="https://openreview.net/pdf?id=SkyQWDcex" target="_blank" rel="noopener">A Context-Aware Attention Network For Interactive Question Answering</a></p><p>利用自己计算注意力分布 <span class="math display">\[\gamma_j = \rm{softmax} (\mathbf v^T \mathbf g_j^q)\]</span> 利用新的注意力分布去计算最终的attention向量 <span class="math display">\[\mathbf u = W_{ch} \sum_{j=1}^N \gamma_j \mathbf g_j^q + \mathbf b_c ^q\]</span> <strong>2 两个得分合并为一个得分</strong></p><p><a href="http://wnzhang.net/papers/dadm-kdd.pdf" target="_blank" rel="noopener">Dynamic Attention Deep Model for Article Recommendation by Learning Human Editors’ Demonstration</a></p><p>计算两个得分 <span class="math display">\[\lambda _{m_t}^M = w_{m_t}^M \cdot \mathbf o + b_{m_t}^M , \quad \lambda_t ^T = w_t^T \cdot \mathbf o + b_t^T\]</span> 权值合并，求注意力分布： <span class="math display">\[p_t = \rm{softmax} (\alpha \lambda _{m_t}^M + (1-\alpha) \lambda_t ^T)\]</span> <strong>3 论文图片</strong></p><p>CAN for QA</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/CAN-1.png" style="display:block; margin:auto" width="80%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/CAN.png" style="display:block; margin:auto" width="80%"></p><p>Dynamic Attention</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dynamic-attention-1.png" style="display:block; margin:auto" width="80%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dynamic-attention-2.png" style="display:block; margin:auto" width="80%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dynamic-attention-3.png" style="display:block; margin:auto" width="80%"></p><h2 id="general-attention">General Attention</h2><p>有外部的信息，<span class="math inline">\(\mathbf h_i\)</span> 与 <span class="math inline">\(\mathbf q\)</span>进行乘积得分。 <a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#global-attention">机器翻译的应用</a> <span class="math display">\[\rm{score}(\mathbf h_i, \mathbf q) = \mathbf h_i^T W\mathbf q\]</span></p><h2 id="concatenation-based">Concatenation-based</h2><p>要关注的外部对象是<span class="math inline">\(\mathbf h_t\)</span>， 可以随时间变化，也可以一直不变(question)。 <span class="math display">\[s_i = f(\mathbf h_i, \mathbf h_t) = \mathbf v^T \rm a(W_1 \mathbf h_i + W_2 \mathbf h_t + b)\]</span> <strong>1 多个元素组成Attention</strong></p><p><a href="https://www.comp.nus.edu.sg/~xiangnan/papers/sigir17-AttentiveCF.pdf" target="_blank" rel="noopener">Attentive Collaborative Filtering Multimedia Recommendation with Item- and Component-Level Attention_sigir17</a></p><p>Item-Level Attention。可以看到需要加什么Attention，直接向公式里面一加就可以了。 <span class="math display">\[a(i, l) = w_1^T \phi(W_{1u} \mathbf u_i + W_{1v} \mathbf v_l + W_{1p} \mathbf p_l + W_{1x} \mathbf {\bar x}_l + \mathbf b_1) + \mathbf c_1\]</span></p><p><span class="math display">\[\alpha(i, l) = \frac {\exp (a(i, l))}{\sum_{n \in R(i)} \exp (a(i, n))}\]</span></p><h2 id="多层attention">多层Attention</h2><p>有<span class="math inline">\(m\)</span>个句子，每个句子有<span class="math inline">\(k\)</span>个词语。</p><p><code>Word-level Attention</code></p><p>每个句子，有k个词语，每个词语一个词向量，使用<code>Local-based Attention</code> ， 可以得到这个句子的向量表达<span class="math inline">\(\mathbf s_i\)</span>。</p><p><code>Sentence-level Attention</code></p><p>有<span class="math inline">\(m\)</span>个句子，每个句子是一个句子向量<span class="math inline">\(\mathbf s_i\)</span>。 可以再次Attention，得到文档的向量表达<span class="math inline">\(\mathbf d\)</span>， 也可以得到每个句子的权值<span class="math inline">\(\alpha_i\)</span>。</p><p>得到这些信息之后，再具体问题具体分析。</p><p><strong>1. 文章摘要生成</strong></p><p><a href="https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/ren-leveraging-2017.pdf" target="_blank" rel="noopener">Leveraging Contextual Sentence Relations for Extractive Summarization Using a Neural Attention Model_SIGIR2017</a></p><p>输入一篇文档，输出它的摘要。</p><ul><li>第一层：<code>Local-based Attention</code>， 生成每个句子的vector</li><li>第二层：当前句子作为中心，2n+1个句子。输入RNN（不明白）。将中心句子作为attention，来编码上下文。通过上下文对中心句子进行打分。作为该句子对整个文本的重要性</li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/ap-bi-cnn-sentence-modeling.png" style="display:block; margin:auto" width="100%"></p><h2 id="can的实时问答">CAN的实时问答</h2><p><a href="https://openreview.net/pdf?id=SkyQWDcex" target="_blank" rel="noopener">A Context-Aware Attention Network For Interactive Question Answering</a></p><p>第一层Attention</p><p>对句子过GRU，每一时刻的output作为词的编码。再使用Local-Attention对这些词，得到<strong>问句的表达</strong><span class="math inline">\(\mathbf u\)</span>。</p><p>第二层Attention</p><p>由于上下文有多个句子。</p><p>首先，对一个句子进行过GRU，得到每一时刻单词的语义信息<span class="math inline">\(\alpha^t\)</span>， 然后利用Concat-Attention对这些单词计算，得到这句话的语义信息<span class="math inline">\(\mathbf y_t\)</span>。</p><p>再把当前句子的语义信息给到句子的GRU</p><p>第三次Attention</p><p>经过GRU，得到<strong>每个句子的表达</strong><span class="math inline">\(\mathbf s_t\)</span>。 再使用Concat-Attention来得到<strong>每个句子的注意力分配</strong><span class="math inline">\(\mathbf \beta_t\)</span>, 然后加权求和得到 <strong>整个Context的表达</strong><span class="math inline">\(\mathbf m\)</span>。</p><p>输出</p><p>结合<span class="math inline">\(\mathbf {m, u}\)</span>通过GRU去生成答案</p><ul><li><code>Period Symbol</code> ：是正确答案，直接输出</li><li><code>Question Mask</code>： 输出是一个问题，要继续问用户相应的信息</li></ul><p>用户重新给了反馈之后，对所有词汇信息使用<code>simple attention mechanism</code>， 即平均加权，所有的贡献都是一样的。得到反馈的向量表达<span class="math inline">\(\mathbf f\)</span>。</p><p>使用新的反馈向量和原始的问句向量，结合，重新生成新的context的语义表达<span class="math inline">\(\mathbf m\)</span>。 最终得到新的<span class="math inline">\(\mathbf {m, u}\)</span> 去重新回答。 <span class="math display">\[\mathbf r = \tanh (W_{rf}f + \mathbf b_r^{(f)})\]</span></p><p><span class="math display">\[\beta_t = \rm{softmax}(\mathbf u^T \mathbf s_t + \mathbf r^T \mathbf s_t)\]</span></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/CAN.png" style="display:block; margin:auto" width="100%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/ntm.png" style="display:block; margin:auto" width="80%"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;一切都应该尽可能简单，但不能过于简单。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;本文主要是总结：注意力机制、注意力机制的变体、论文中常见的注意力&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Attention" scheme="http://plmsmile.github.io/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>Dynamic Coattention Network (Plus)</title>
    <link href="http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/"/>
    <id>http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/</id>
    <published>2018-03-15T00:33:16.000Z</published>
    <updated>2018-12-13T08:08:07.295Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>先放四张图，分别是DCN的Encoder、Decoder，DCN+的Encoder和Objective。后面再详细总结</p></blockquote><a id="more"></a><p><a href="https://arxiv.org/abs/1611.01604" target="_blank" rel="noopener">Dynamic Coattention Networks For Question Answering</a></p><p><a href="https://arxiv.org/pdf/1711.00106" target="_blank" rel="noopener">DCN+: Mixed Objective and Deep Residual Coattention for Question Answering</a></p><p><img src="" style="display:block; margin:auto" width="80%"></p><h1 id="dcn">DCN</h1><h2 id="coattention-encoder">Coattention Encoder</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcn.png" style="display:block; margin:auto" width="100%"></p><h2 id="dynamic-pointing-decoder">Dynamic Pointing Decoder</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcn-decoder.png" style="display:block; margin:auto" width="100%"></p><h2 id="hmn">HMN</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/hmn.png" style="display:block; margin:auto" width="60%"></p><h1 id="dcn-1">DCN+</h1><h2 id="dcn的问题">DCN的问题</h2><p><strong>loss没有判断真正的意义</strong></p><p>DCN使用传统交叉熵去优化<code>optimization</code>，只考虑答案字符串的匹配程度。但是实际上人的评判<code>evaluation</code>却是看回答的意义。如果只考虑span，则有下面两个问题：</p><ul><li>精确答案：没影响</li><li>但是对正确答案周围重叠的单词，却可能认为是错误的。</li></ul><p>句子：<code>Some believe that the Golden State Warriors team of 2017 is one of the greatest teams in NBA history</code></p><p>问题：<code>which team is considered to be one of the greatest teams in NBA history</code></p><p>正确答案：<code>the Golden State Warriors team of 2017</code></p><p>其实<code>Warriors</code>也是正确答案， 但是传统交叉熵却认为它还不如<code>history</code>。</p><p>DCN没有建立起<code>Optimization</code>和 <code>evaluation</code>的联系。 这也是Word Overlap。</p><p><strong>单层coattention表达力不强</strong></p><h2 id="dcn的优化点">DCN+的优化点</h2><p><strong>Mixed Loss</strong></p><p>交叉熵+自我批评学习（强化学习）。Word真正<strong>意义相似</strong>才会给一个好的<code>reward</code>。</p><ul><li>强化学习会鼓励意义相近的词语，而dis不相近的词语</li><li>交叉熵让强化学习朝着正确的轨迹发展</li></ul><p><strong>Deep Residual Coattention Encoder</strong></p><p>多层表达能力更强，详细看下面的优点。</p><h2 id="deep-residual-encoder">Deep Residual Encoder</h2><p><strong>优点</strong></p><p>两个别人得出的重要结论：</p><ul><li><code>stacked self-attention</code> 可以加速信号传递</li><li>减少信号传递路径，可以增加长依赖</li></ul><p>比DCN的两个优化点：</p><ul><li><code>coattention with self-attention</code>和多层<code>coattention</code> 。可以对输入有<code>richer representations</code></li><li>对每层的<code>coattention outputs</code>进行残差连接。缩短了信息传递路径。</li></ul><h2 id="coattention深层理解">Coattention深层理解</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/deep-residual-coattention-encoder.png" style="display:block; margin:auto" width="90%"></p><blockquote><p>当时理解了很久都不懂，后来一个下午，一直看，结合机器翻译实现和实际例子矩阵计算，终于理解了Attention、Coattention。</p></blockquote><p>参考了我的下面三篇笔记。</p><ul><li><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#pytorch%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91">注意力机制在机器翻译中的思想和代码实现总结</a></li><li><a href="https://plmsmile.github.io/2017/10/10/attention-model/">图文介绍Attention</a></li><li><a href="https://plmsmile.github.io/2018/03/14/31-co-attention-vqa/#co-attention">Coattention的两种形式</a></li></ul><p><strong>单个Coattention层计算</strong></p><p>经过双向RNN后，得到两个语义编码：文档<span class="math inline">\(E_0^D \in \mathbb R^{m\times e}\)</span>， 问题编码<span class="math inline">\(E^Q_0 \in \mathbb R ^{n \times h}\)</span> 。 <span class="math display">\[E_1^D = \rm{biGRU_1}(E_0^D) \quad\in \mathbb R^{m \times h}\]</span></p><p><span class="math display">\[E_1^Q = \tanh(\rm{W \; \rm{biGRU_1(Q_E)+b)}} \quad \in \mathbb R^{n \times h}\]</span></p><p>计算<code>关联得分矩阵</code>A <span class="math display">\[A = E_1^D (E_1^Q)^T \in \mathbb R^{m \times n}\]</span></p><p><span class="math display">\[\begin{bmatrix} 0 &amp; 0 \\ 2 &amp; 3 \\0 &amp; 2 \\ 1 &amp; 1 \\3 &amp; 3 \\\end{bmatrix}_{5 \times 2}\cdot \begin{bmatrix} 1&amp; 3 \\ 1 &amp; 1 \\1&amp; 3 \\ \end{bmatrix}_{3 \times 2}^T=\begin{bmatrix} 0&amp; 0 &amp;0 \\ 11&amp; 5 &amp;11 \\ 6&amp; 2 &amp;6 \\ 4&amp; 2 &amp;4 \\ 12&amp; 6 &amp;12 \\ \end{bmatrix}_{5\times 3}\]</span></p><p>做<code>行Softmax</code>，得到Q对D的权值分配概率<span class="math inline">\(A^Q\)</span>， <code>attention_weights</code></p><ul><li>每一行是一个文档单词w</li><li>元素值是所有问句单词对当前文档单词w的注意力分配权值</li><li>元素值是每个问句单词的权值概率</li></ul><p><span class="math display">\[\begin{bmatrix} 0.3333 &amp; 0.3333 &amp; 0.3333 \\ 0.4994  &amp;0.0012 &amp; 0.4994 \\ 0.4955  &amp;0.0091  &amp;0.4955 \\ 0.4683  &amp;0.0634  &amp;0.4683\\ 0.4994  &amp;0.0012  &amp;0.4994 \\ \end{bmatrix}_{5\times 3}\]</span></p><p>计算D的summary， <span class="math inline">\(S^D = A^Q \cdot Q\)</span> <span class="math display">\[S^D = A^Q \cdot Q\]</span></p><ul><li>D所需要的新的语义，参考机器翻译的<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#%E8%AE%A1%E7%AE%97%E6%96%B0%E7%9A%84%E8%AF%AD%E4%B9%89">新语义理解</a></li><li><span class="math inline">\(A^Q\)</span>的每一行去乘以Q的每一列去表达单词w</li><li>用Q去表达D，每个<span class="math inline">\(D_w\)</span>都是<strong>Q的所有单词对w的线性表达</strong>，权值就是<span class="math inline">\(A^Q\)</span><br></li><li>所以<span class="math inline">\(S^D\)</span>也是D的<code>summary</code>， 也称作D需要<code>context</code></li></ul><p>同理，对<code>列做softmax</code>， 得到D对Q的权值分配概率<span class="math inline">\(A^D\)</span>， 得到Q的<code>summary</code>， <span class="math inline">\(S^Q = A^D \cdot D\)</span></p><p>这时，借鉴<a href="https://plmsmile.github.io/2018/03/14/31-co-attention-vqa/#alternating-co-attention">alternation-coattention思想</a> 去计算对D的<code>Coattention context</code><span class="math inline">\(C^D\)</span> ： <span class="math display">\[C^D = S^Q \cdot A^Q\]</span> 实际上，<span class="math inline">\(C^D\)</span>与<span class="math inline">\(S^D\)</span>类似，都是<code>Summary</code>， 都是<code>context</code>。 只是<span class="math inline">\(C^D\)</span>使用的是新的<span class="math inline">\(S^Q\)</span>， 而不是<span class="math inline">\(E^Q_1\)</span>。</p><h2 id="coattention-encoder总结">Coattention Encoder总结</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/deep-residual-coattention-encoder.png" style="display:block; margin:auto" width="90%"></p><p>使用两层<code>coattention</code>， 最后再残差连接，经过LSTM输出。</p><p><strong>第一层</strong> <span class="math display">\[E_1^D = \rm{biGRU_1}(E_0^D) \quad\in \mathbb R^{m \times h} \\E_1^Q = \tanh(\rm{W \cdot \rm{biGRU_1(E_0^Q)+b)}} \quad \in \mathbb R^{n \times h}\]</span></p><p><span class="math display">\[\rm{coattn_1} (E_1^D, E_1^Q) =  S_1^D, S_1^Q, C_1^Q \\\]</span></p><p><strong>第二层</strong> <span class="math display">\[E_2^D = \rm{biGRU_2}(E_1^D) \quad\in \mathbb R^{m \times h} \\E_2^Q = \tanh (W \cdot \rm{biGRU_2}(E_1^Q) + b) \quad\in \mathbb R^{m \times h}\]</span></p><p><span class="math display">\[\rm{coattn_2} (E_2^D, E_2^Q) =  S_2^D, S_2^Q, C_2^Q \\\]</span></p><p><strong>残差连接所有的D</strong> <span class="math display">\[c = \rm {concat}((E_1^D, E_2^D, S_1^D, S_2^D, C_1^D, C_2^D)\]</span> <strong>LSTM编码输出，得到Encoder的输出</strong> <span class="math display">\[U = \rm{biGRU}(c) \quad \in \mathbb R^{m \times 2h}\]</span></p><h2 id="mixed-objective">Mixed Objective</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dcnplus-loss.png" style="display:block; margin:auto" width="100%"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;先放四张图，分别是DCN的Encoder、Decoder，DCN+的Encoder和Objective。后面再详细总结&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="DCN" scheme="http://plmsmile.github.io/tags/DCN/"/>
    
      <category term="coattention" scheme="http://plmsmile.github.io/tags/coattention/"/>
    
      <category term="QA" scheme="http://plmsmile.github.io/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>协同注意力简介</title>
    <link href="http://plmsmile.github.io/2018/03/14/31-co-attention-vqa/"/>
    <id>http://plmsmile.github.io/2018/03/14/31-co-attention-vqa/</id>
    <published>2018-03-14T08:56:27.000Z</published>
    <updated>2018-12-13T07:53:38.618Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>只是记录一下Co-Attention，后续再补上本篇论文的全部笔记吧。</p></blockquote><a id="more"></a><p>论文：<a href="https://arxiv.org/abs/1606.00061" target="_blank" rel="noopener">Hierarchical Question-Image Co-Attention for Visual Question Answering</a></p><p>我的相关笔记：<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/">Attention-based NMT阅读笔记</a>和<a href="https://plmsmile.github.io/2017/10/10/attention-model/#encoder-decoder">NLP中的Attention笔记</a></p><h1 id="co-attention">Co-Attention</h1><p>这里以VQA里面的两个例子记录一下Co-Attention。即图片和问题。</p><h2 id="注意力和协同注意力">注意力和协同注意力</h2><p><strong>注意力</strong></p><p><code>注意力机制</code>就像人<strong>带着问题去阅读</strong>， 先看问题，再去文本中有目标地阅读寻找答案。</p><p>机器阅读则是结合问题和文本的信息，生成一个关于文本段落各部分的<code>注意力权重</code>，再<strong>对文本信息进行加权</strong>。</p><p>注意力机制可以帮助我们更好地去捕捉段落中和问题相关的信息。</p><p><strong>协同注意力</strong></p><p><code>协同注意力</code>是一种<strong>双向的注意力</strong>， 再利用注意力去生成文本和问句的注意力。</p><ul><li>给文本生成注意力权值</li><li>给问句生成注意力权值</li></ul><p>协同注意力分为两种方式：</p><ul><li><strong>Parallel Co-Attention</strong> : 两种数据源A和B，先结合得到C，再基于结合信息C对A和B分别生成对应的Attention。<code>同时生成注意力</code></li><li><strong>Alternating Co-Attention</strong>： 先基于A产生B的attention，得到新的B；再基于新B去产生A的attention。两次<code>交替生成注意力</code></li></ul><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/coattention.png" style="display:block; margin:auto" width="100%"></p><h2 id="parallel-co-attention">Parallel Co-Attention</h2><p>图片特征：<span class="math inline">\(V \in \mathbb {R}^{d\times N}\)</span> ，问句特征：<span class="math inline">\(Q \in \mathbb R^{d \times T}\)</span> 。</p><p><code>同时</code>生成图片和问题的注意力。</p><p>先计算<code>关联矩阵</code>： <span class="math display">\[C = \rm{tanh}(Q^T W_b V) \in \mathbb R^{T \times N}\]</span> 计算<code>注意力权值</code> <span class="math inline">\(a^v\)</span>和<span class="math inline">\(a^q\)</span></p><p>方法1：直接选择最大值。<span class="math inline">\(a^v_n = \max \limits_i(C_{i, n})\)</span> ，<span class="math inline">\(a_t^q = \max \limits_i (C_{t, j})\)</span></p><p>方法2：把关联矩阵当做特征给到网络中，进行计算注意力权值，再进行<code>softmax</code>。<strong>更好</strong> <span class="math display">\[H^v = \rm{tanh} (W_vV + (W_qQ)C), \quad \quad H^q = \rm{tanh} (W_qQ + (W_vV)C^T)\]</span></p><p><span class="math display">\[a^v = \rm{softmax}(w_{hv}^TH^v), \quad \quad a^q = \rm{softmax}(w^T_{hq}H^q)\]</span></p><p>利用注意力和原特征向量去计算<code>新的特征向量</code> <span class="math display">\[\mathbf {\hat v} = \sum_{n=1}^N a^v_n \mathbf v_n, \quad \quad \mathbf { \hat q} = \sum_{t=1}^Tq_t^q \mathbf q_t\]</span></p><h2 id="alternating-co-attention">Alternating Co-Attention</h2><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/coattention.png" style="display:block; margin:auto" width="100%"></p><p><code>交替</code>生成图片和问题的注意力。</p><ul><li>把问题归纳成一个单独向量<span class="math inline">\(\mathbf {q}\)</span></li><li>基于<span class="math inline">\(\mathbf q\)</span> 去和图片特征<span class="math inline">\(V\)</span>去生成图像特征<span class="math inline">\(\mathbf {\hat v}\)</span></li><li>基于<span class="math inline">\(\mathbf v\)</span>和问题特征<span class="math inline">\(Q\)</span>去生成问题特征<span class="math inline">\(\mathbf {\hat q}\)</span></li></ul><p>具体地，给一个<span class="math inline">\(X\)</span>和<code>attention guidance</code><span class="math inline">\(\mathbf g\)</span> ，通过<span class="math inline">\(\mathbf {\hat x} = f(X, \mathbf g)\)</span>去得到特征向量<span class="math inline">\(\mathbf {\hat x}\)</span> <span class="math display">\[H = \rm {tanh} (W_x X+ (W_g \mathbf g) \mathbb 1^T)\]</span> <span class="math inline">\(\mathbf a ^x\)</span> 是特征<span class="math inline">\(X\)</span>的<code>注意力权值</code> ： <span class="math display">\[\mathbf a^x = \rm(softmax)(w^T_{hx} H)\]</span> 新的<strong>注意力向量</strong> (<code>attended image (or question) vector)</code> : <span class="math display">\[\mathbf {\hat x} = \sum a_i^x \mathbf x_i\]</span> 对应本例子如下：</p><ul><li><span class="math inline">\(X = Q, \; g = 0 \to \mathbf q\)</span></li><li><span class="math inline">\(X = V, \; g = \mathbf q \to \mathbf {\hat v}\)</span></li><li><span class="math inline">\(X = Q, \; g = \mathbf {\hat v} \to \mathbf {\hat q}\)</span></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;只是记录一下Co-Attention，后续再补上本篇论文的全部笔记吧。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="注意力" scheme="http://plmsmile.github.io/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
      <category term="VQA" scheme="http://plmsmile.github.io/tags/VQA/"/>
    
  </entry>
  
  <entry>
    <title>使用Dynamic Memory Network实现一个简单QA</title>
    <link href="http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/"/>
    <id>http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/</id>
    <published>2018-03-13T08:07:29.000Z</published>
    <updated>2018-12-13T08:06:29.591Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文概要：介绍DMN的基本原理，使用PyTorch进行实现一个简单QA</p></blockquote><a id="more"></a><p><img src="" style="display:block; margin:auto" width="50%"></p><p>论文：<a href="https://arxiv.org/abs/1506.07285" target="_blank" rel="noopener">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a></p><h1 id="模型简介">模型简介</h1><h2 id="概要说明">概要说明</h2><p>许多NLP问题都可以看做一个Question-Answer问题。<code>Dynamic Memory Network</code> 由4部分组成。</p><p><strong>输入模块</strong></p><p>对输入的句子<code>facts</code>(先<code>embedding</code>)使用<a href="https://plmsmile.github.io/2017/10/18/rnn/#gru">GRU</a>进行编码，得到<code>encoded_facts</code>，给到后面的<code>情景记忆模块</code>。</p><p><strong>问题模块</strong></p><p>对输入的问题<code>question</code>使用<code>GRU</code>进行编码，得到<code>encoded_question</code>， 给到后面的<code>情景记忆模块</code> 和<code>回答模块</code> 。</p><p><strong>情景记忆模块</strong></p><p><code>Episodic Memory Module</code>由<code>memory</code>和<code>attention</code>组成。</p><ul><li>attention：会选择更重要的<code>facts</code></li><li>memory：根据<code>question</code>、<code>facts</code>和 <code>旧memory</code>来生成<code>新momery</code> 。初始：<code>memory=encoded_question</code></li></ul><p>会在<code>facts</code>上迭代多次去计算<code>memory</code>。 每一次迭代会提取出新的信息。</p><p>输出最终的<code>momery</code>， 给到<code>回答模块</code>。</p><p><strong>回答模块</strong></p><p><code>memory</code> + <code>question</code>， 在<code>GRUCell</code>上迭代<code>原本的回答长度</code>次， 得到最终的预测结果。</p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dmn-simple.png" style="display:block; margin:auto" width="50%"></p><p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/nlp/papers/dmn-detail.png" style="display:block; margin:auto" width="90%"></p><h2 id="输入模块">输入模块</h2><p><strong>输入</strong></p><ul><li>一个句子，有<span class="math inline">\(T_I\)</span>个单词</li><li><span class="math inline">\(T_I\)</span>个句子，则把这些句子合并成一个大句子。在每个句子的末尾添加一个<code>句子结束标记&lt;/s&gt;</code>。如上图蓝色的部分</li></ul><p><strong>GRU计算隐状态</strong></p><p>句子过RNN时，对于每一时刻<span class="math inline">\(t\)</span>的单词<span class="math inline">\(w_t\)</span> ，有<span class="math inline">\(h_t\)</span> : <span class="math display">\[h_t = \rm{RNN}(w_t, h_{t-1})\]</span> <strong>输出</strong></p><p>使用RNN的<code>h = hidden states</code> 作为<code>输入句子的向量表达</code>，也就是<code>encoded_facts</code></p><ul><li>一个句子，输出所有时刻的<span class="math inline">\(h_t\)</span></li><li>多个句子，输出每个句子<code>结束标记&lt;/s&gt;</code>时刻的<span class="math inline">\(h_t\)</span>。</li></ul><h2 id="问题模块">问题模块</h2><p><strong>输入</strong></p><p>输入一个句子<code>question</code>，有<span class="math inline">\(T_Q\)</span>个单词。</p><p><strong>GRU计算隐状态</strong> <span class="math display">\[q_t = \rm{RNN}(w_t^Q, q_{t-1})\]</span> <strong>输出Q编码</strong></p><p><code>最后时刻的隐状态</code><span class="math inline">\(q_{T_Q}\)</span>作为句子的编码。</p><hr><h2 id="情景记忆模块">情景记忆模块</h2><p><strong>总体思路</strong></p><p>记忆模块收到两个编码表达：<code>encoded_facts</code>和<code>encoded_question</code> ， 也就是<span class="math inline">\(h\)</span>和<span class="math inline">\(q\)</span>。</p><p>模块会生成一个记忆<code>memory</code>，初始时<code>memory = encoded_question</code></p><p>记忆模块在<code>encoded_facts</code>上反复迭代多轮，每一轮去提取新的信息<code>episode</code>， 更新<code>memory</code></p><ul><li>遍历所有<code>facts</code>， 对于每一个的<code>fact</code>， 不停地更新当前轮的信息<code>e</code></li><li>计算新的信息：<span class="math inline">\(e_{new}=\rm{RNN}(fact, e)\)</span> ，使用当前fact和当前信息</li><li>计算新信息的保留比例注意门<span class="math inline">\(g\)</span></li><li><code>更新信息</code>：<span class="math inline">\(e = g * e_{new} + (1-g) * e\)</span></li><li><p>计算保留比例g：结合当前<code>fact</code> 、<code>memory</code>、 <code>question</code> 去生成多个特征，再过一个<code>两层前向网络G</code>得到一个比例数值</p></li><li><p><code>更新memory</code> ，<span class="math inline">\(m^i = \rm{GRU}(e, m^{i-1})\)</span></p></li></ul><p><strong>特征函数与前向网络</strong></p><p>保留比例门<code>g</code>充当着<code>attention</code>的作用 。</p><p>特征函数<span class="math inline">\(z(c, m, q)\)</span>， 其中c就是当前的<code>fact</code> ，（论文里面是9个特征）： <span class="math display">\[z(c, m, q) = [c \circ q,  c \circ m, \vert c-q\vert, \vert c-m\vert]\]</span> 前向网络<span class="math inline">\(g=G(c, m ,q)\)</span> ： <span class="math display">\[t = \rm{tanh}(W^1z(c, m, q) + b^1)  \\g = G(c, m, q) = \sigma(W^2 t + b^2)\]</span> <strong>e更新</strong></p><p>在每个fact遍历中，e会结合fact和旧e去生成新的信息<span class="math inline">\(e_{new}\)</span>，再结合旧<span class="math inline">\(e\)</span>和新<span class="math inline">\(e_{new}\)</span> 去生成最终的<span class="math inline">\(e^i\)</span> ： <span class="math display">\[e_{new}=\rm{RNN}(fact, e)\]</span></p><p><span class="math display">\[e = g * e_{new} + (1-g) * e\]</span></p><p><strong>记忆更新</strong></p><p>每一轮迭代后，结合旧记忆和当前轮的信息e去更新记忆： <span class="math display">\[m^i = \rm{GRU}(e, m^{i-1})\]</span> <strong>迭代停止条件</strong></p><ul><li>设置最大迭代次数<span class="math inline">\(T_M\)</span></li><li>在输入里面追加停止迭代信号，如果注意门选择它，则停止。</li></ul><h2 id="回答模块">回答模块</h2><p>回答模块结合memory和question，来生成对问题的答案。也是通过GRU来生成答案的。</p><p>设<code>a</code> 是<code>answer_gru</code>的hidden state，初始<span class="math inline">\(a_0= m^{T_M}\)</span> <span class="math display">\[y_t = \rm{softmax}(W^a a_t) \\a_t = \rm{GRU} ([y_{t-1}, q], a_{t-1})\]</span> 使用<code>交叉熵</code>去计算loss，进行优化。</p><h1 id="实现细节">实现细节</h1><p><a href="https://github.com/plmsmile/NLP-Demos/tree/master/question-answer-DMN" target="_blank" rel="noopener">我的github源代码</a> ，实现参考自<a href="https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/10.Dynamic-Memory-Network-for-Question-Answering.ipynb" target="_blank" rel="noopener">DSKSD的代码</a> 。</p><h2 id="数据处理">数据处理</h2><p><strong>原始数据</strong></p><p>使用过的数据是facebook的<a href="http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz" target="_blank" rel="noopener">bAbi Tasks Data 1-20</a>里面的 <code>en-10k</code>下的<code>qa5_three-arg-relations_train.txt</code> 和test数据。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> Bill travelled to the office.</span><br><span class="line"><span class="number">2</span> Bill picked up the football there.</span><br><span class="line"><span class="number">3</span> Bill went to the bedroom.</span><br><span class="line"><span class="number">4</span> Bill gave the football to Fred.</span><br><span class="line">5 What did Bill give to Fred?   football        4</span><br><span class="line"><span class="number">6</span> Fred handed the football to Bill.</span><br><span class="line"><span class="number">7</span> Jeff went back to the office.</span><br><span class="line">8 Who received the football?    Bill    6</span><br><span class="line"><span class="number">9</span> Bill travelled to the office.</span><br><span class="line"><span class="number">10</span> Bill got the milk there.</span><br><span class="line">11 Who received the football?   Bill    6</span><br><span class="line"><span class="number">12</span> Fred travelled to the garden.</span><br><span class="line"><span class="number">13</span> Fred went to the hallway.</span><br><span class="line"><span class="number">14</span> Bill journeyed to the bedroom.</span><br><span class="line"><span class="number">15</span> Jeff moved to the hallway.</span><br><span class="line"><span class="number">16</span> Jeff journeyed to the bathroom.</span><br><span class="line"><span class="number">17</span> Bill journeyed to the office.</span><br><span class="line"><span class="number">18</span> Fred travelled to the bathroom.</span><br><span class="line"><span class="number">19</span> Mary journeyed to the kitchen.</span><br><span class="line"><span class="number">20</span> Jeff took the apple there.</span><br><span class="line"><span class="number">21</span> Jeff gave the apple to Fred.</span><br><span class="line">22 Who did Jeff give the apple to?      Fred    21</span><br><span class="line"><span class="number">23</span> Bill went back to the bathroom.</span><br><span class="line"><span class="number">24</span> Bill left the milk.</span><br><span class="line">25 Who received the apple?      Fred    21</span><br><span class="line"><span class="number">1</span> Mary travelled to the garden.</span><br><span class="line"><span class="number">2</span> Mary journeyed to the kitchen.</span><br><span class="line"><span class="number">3</span> Bill went back to the office.</span><br><span class="line"><span class="number">4</span> Bill journeyed to the hallway.</span><br><span class="line"><span class="number">5</span> Jeff went back to the bedroom.</span><br><span class="line"><span class="number">6</span> Fred moved to the hallway.</span><br><span class="line"><span class="number">7</span> Bill moved to the bathroom.</span><br><span class="line"><span class="number">8</span> Jeff went back to the garden.</span><br><span class="line"><span class="number">9</span> Jeff went back to the kitchen.</span><br><span class="line"><span class="number">10</span> Fred went back to the garden.</span><br><span class="line"><span class="number">11</span> Mary got the football there.</span><br><span class="line"><span class="number">12</span> Mary handed the football to Jeff.</span><br><span class="line">13 What did Mary give to Jeff?  football        12</span><br></pre></td></tr></table></figure><p>比如1-25是一个大的情景</p><ul><li>没有问号的都是陈述句，是情景数据<code>fact</code>。只有<code>.</code>号， 都是简单句</li><li>带问号的：是问句，带有答案和答案所在句子。使用<code>tab</code>分割</li></ul><p><strong>加载原始数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_raw_data</span><span class="params">(file_path, seq_end=<span class="string">'&lt;/s&gt;'</span>)</span>:</span></span><br><span class="line">    <span class="string">''' 从文件中读取文本数据，并整合成[facts, question, answer]一条一条的可用数据，原始word形式</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file_path -- 数据文件</span></span><br><span class="line"><span class="string">        seq_end -- 句子结束标记</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        data -- list，元素是[facts, question, answer]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    source_data = open(file_path).readlines()</span><br><span class="line">    <span class="keyword">print</span> (file_path, <span class="string">":"</span>, len(source_data), <span class="string">"lines"</span>)</span><br><span class="line">    <span class="comment"># 去掉换行符号</span></span><br><span class="line">    source_data = [line[:<span class="number">-1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> source_data]</span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> source_data:</span><br><span class="line">        index = line.split(<span class="string">' '</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> index == <span class="string">'1'</span>:</span><br><span class="line">            <span class="comment"># 一个新的QA开始</span></span><br><span class="line">            facts = []</span><br><span class="line">            <span class="comment">#qa = []</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'?'</span> <span class="keyword">in</span> line:</span><br><span class="line">            <span class="comment"># 当前QA的一个问句</span></span><br><span class="line">            <span class="comment"># 问题 答案 答案所在句子的编号 \t分隔</span></span><br><span class="line">            tmp = line.split(<span class="string">'\t'</span>)</span><br><span class="line">            question = tmp[<span class="number">0</span>].strip().replace(<span class="string">'?'</span>, <span class="string">''</span>).split(<span class="string">' '</span>)[<span class="number">1</span>:] + [<span class="string">'?'</span>]</span><br><span class="line">            answer = tmp[<span class="number">1</span>].split() + [seq_end]</span><br><span class="line">            facts_for_q = deepcopy(facts)</span><br><span class="line">            data.append([facts_for_q, question, answer])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 普通的事件描述，简单句，只有.和空格</span></span><br><span class="line">            sentence = line.replace(<span class="string">'.'</span>, <span class="string">''</span>).split(<span class="string">' '</span>)[<span class="number">1</span>:] + [seq_end]</span><br><span class="line">            facts.append(sentence)</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p><strong>把数据转成id格式</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triple_word2id</span><span class="params">(triple_word_data, th)</span>:</span></span><br><span class="line">    <span class="string">'''把文字转成id</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        triple_word_data -- [(facts, q, a)] word形式</span></span><br><span class="line"><span class="string">        th -- textheler</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        triple_id_data -- [(facts, q, a)]index形式</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 把各个word转成数字id</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> triple_word_data:</span><br><span class="line">        <span class="comment"># 处理facts句子</span></span><br><span class="line">        <span class="keyword">for</span> i, fact <span class="keyword">in</span> enumerate(t[<span class="number">0</span>]):</span><br><span class="line">            t[<span class="number">0</span>][i] = th.sentence2indices(fact)</span><br><span class="line">        <span class="comment"># 问题与答案</span></span><br><span class="line">        t[<span class="number">1</span>] = th.sentence2indices(t[<span class="number">1</span>])</span><br><span class="line">        t[<span class="number">2</span>] = th.sentence2indices(t[<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> triple_word_data</span><br></pre></td></tr></table></figure><p><strong>根据batch_size取数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_loader</span><span class="params">(data, batch_size=<span class="number">1</span>, shuffle=False)</span>:</span></span><br><span class="line">    <span class="string">''' 以batch的格式返回数据</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data -- list格式的data</span></span><br><span class="line"><span class="string">        batch_size -- </span></span><br><span class="line"><span class="string">        shuffle -- 每一个epoch开始的时候，对数据进行shuffle</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        数据遍历的iterator</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.shuffle(data)</span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    end = batch_size</span><br><span class="line">    <span class="keyword">while</span> (start &lt; len(data)):</span><br><span class="line">        batch = data[start:end]</span><br><span class="line">        start, end = end, end + batch_size</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br><span class="line">    <span class="keyword">if</span> end &gt;= len(data) <span class="keyword">and</span> start &lt; len(data):</span><br><span class="line">        batch = data[start:]</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br></pre></td></tr></table></figure><p><strong>对每一个batch进行padding</strong></p><p>这部分有点复杂。要求问题、答案、fact的长度一致，每个问题的fact的数量也要一样。</p><blockquote><p>其实和模型也有关，模型写的有点坑，就是每条数据的所有fact应该连接在一起成为一个大的fact送进GRU里，在每个fact后面加上结束标记。但是我这却分开了，分成了多个标记好的fact，也怪当时没有仔细看好论文，这个也是参考别人的实现。循环也导致训练贼慢，但是现在忙着找实习，就先不改了。后面好好写DMNPLUS吧。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_batch_data</span><span class="params">(raw_batch_data, th)</span>:</span></span><br><span class="line">    <span class="string">''' 对数据进行padding，问题、答案、fact长度分别一致，同时每条数据的fact的数量一致。输入到网络的时候要用</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        raw_batch_data -- [[facts, q, a]]，都是以list wordid表示</span></span><br><span class="line"><span class="string">        th -- TextHelper</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        all_facts -- [b, nfact, flen]，pad后的facts，Variable</span></span><br><span class="line"><span class="string">        all_facts_mask -- [b, nfact, flen]，facts的mask，Variable</span></span><br><span class="line"><span class="string">        questions -- [b, qlen]，pad后的questions，Variable</span></span><br><span class="line"><span class="string">        questions_mask -- [b, qlen]，questions的mask，Variable</span></span><br><span class="line"><span class="string">        answers -- [b, alen]，pad后的answers，Variable</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    all_facts, questions, answers = [list(i) <span class="keyword">for</span> i <span class="keyword">in</span> zip(*raw_batch_data)]</span><br><span class="line">    batch_size = len(raw_batch_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 计算各种长度。一个QA的facts数量，fact、Q、A句子的最大长度</span></span><br><span class="line">    n_fact = max([len(facts) <span class="keyword">for</span> facts <span class="keyword">in</span> all_facts])</span><br><span class="line">    flen = max([len(f) <span class="keyword">for</span> f <span class="keyword">in</span> flatten(all_facts)])</span><br><span class="line">    qlen = max([len(q) <span class="keyword">for</span> q <span class="keyword">in</span> questions])</span><br><span class="line">    alen = max([len(a) <span class="keyword">for</span> a <span class="keyword">in</span> answers])</span><br><span class="line">    padid = th.word2index(th.pad)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 对数据进行padding</span></span><br><span class="line">    all_facts_mask = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        <span class="comment"># 2.1 pad fact</span></span><br><span class="line">        facts = all_facts[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(facts)):</span><br><span class="line">            t = flen - len(facts[j])</span><br><span class="line">            <span class="keyword">if</span> t &gt; <span class="number">0</span>:</span><br><span class="line">                all_facts[i][j] = facts[j] + [padid] * t</span><br><span class="line">        <span class="comment"># fact数量pad</span></span><br><span class="line">        <span class="keyword">while</span> (len(facts) &lt; n_fact):</span><br><span class="line">            all_facts[i].append([padid] * flen)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算facts内容是否是填充给的，填充为1，不填充为0</span></span><br><span class="line">        mask = [tuple(map(<span class="keyword">lambda</span> v: v == padid, fact)) <span class="keyword">for</span> fact <span class="keyword">in</span> all_facts[i]]</span><br><span class="line">        all_facts_mask.append(mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2.2 pad question</span></span><br><span class="line">        q = questions[i]</span><br><span class="line">        <span class="keyword">if</span> len(q) &lt; qlen:</span><br><span class="line">            questions[i] = q + [padid] * (qlen - len(q))</span><br><span class="line">        <span class="comment"># 2.3 pad answer</span></span><br><span class="line">        a = answers[i]</span><br><span class="line">        <span class="keyword">if</span> len(a) &lt; alen:</span><br><span class="line">            answers[i] = a + [padid] * (alen - len(a))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 把list数据转成Variable</span></span><br><span class="line">    all_facts = get_variable(torch.LongTensor(all_facts))</span><br><span class="line">    all_facts_mask = get_variable(torch.ByteTensor(all_facts_mask))</span><br><span class="line">    answers = get_variable(torch.LongTensor(answers))</span><br><span class="line">    questions = torch.LongTensor(questions)</span><br><span class="line">    questions_mask = [(tuple(map(<span class="keyword">lambda</span> v: v == padid, q))) <span class="keyword">for</span> q <span class="keyword">in</span> questions]</span><br><span class="line">    questions_mask = torch.ByteTensor(questions_mask)</span><br><span class="line">    questions, questions_mask = get_variable(questions), get_variable(questions_mask)</span><br><span class="line">    <span class="keyword">return</span> all_facts, all_facts_mask, questions, questions_mask, answers</span><br></pre></td></tr></table></figure><h2 id="模型">模型</h2><p><strong>模型定义</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DMN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, hidden_size, padding_idx, seqbegin_id, dropout_p=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            vocab_size -- 词汇表大小</span></span><br><span class="line"><span class="string">            embed_size -- 词嵌入维数</span></span><br><span class="line"><span class="string">            hidden_size -- GRU的输出维数</span></span><br><span class="line"><span class="string">            padding_idx -- pad标记的wordid</span></span><br><span class="line"><span class="string">            seqbegin_id -- 句子起始的wordid</span></span><br><span class="line"><span class="string">            dropout_p -- dropout比率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(DMN, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.seqbegin_id = seqbegin_id</span><br><span class="line">        </span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)</span><br><span class="line">        self.input_gru = nn.GRU(embed_size, hidden_size, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        self.question_gru = nn.GRU(embed_size, hidden_size, batch_first=<span class="keyword">True</span>)    </span><br><span class="line">        self.gate = nn.Sequential(</span><br><span class="line">                        nn.Linear(hidden_size * <span class="number">4</span>, hidden_size),</span><br><span class="line">                        nn.Tanh(),</span><br><span class="line">                        nn.Linear(hidden_size, <span class="number">1</span>),</span><br><span class="line">                        nn.Sigmoid()</span><br><span class="line">                    )</span><br><span class="line">        self.attention_grucell = nn.GRUCell(hidden_size, hidden_size)</span><br><span class="line">        self.memory_grucell = nn.GRUCell(hidden_size, hidden_size)</span><br><span class="line">        self.answer_grucell = nn.GRUCell(hidden_size * <span class="number">2</span>, hidden_size)</span><br><span class="line">        self.answer_fc = nn.Linear(hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        </span><br><span class="line">        self.init_weight()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="string">'''GRU的初始hidden。单层单向'''</span></span><br><span class="line">        hidden = torch.zeros(<span class="number">1</span>, batch_size, self.hidden_size)</span><br><span class="line">        hidden = get_variable(hidden)</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weight</span><span class="params">(self)</span>:</span></span><br><span class="line">        nn.init.xavier_uniform(self.embed.state_dict()[<span class="string">'weight'</span>])</span><br><span class="line">        components = [self.input_gru, self.question_gru, self.gate, self.attention_grucell,</span><br><span class="line">                     self.memory_grucell, self.answer_grucell]</span><br><span class="line">        <span class="keyword">for</span> component <span class="keyword">in</span> components:</span><br><span class="line">            <span class="keyword">for</span> name, param <span class="keyword">in</span> component.state_dict().items():</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">                    nn.init.xavier_normal(param)</span><br><span class="line">        nn.init.xavier_uniform(self.answer_fc.state_dict()[<span class="string">'weight'</span>])</span><br><span class="line">        self.answer_fc.bias.data.fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><strong>前向计算参数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, allfacts, allfacts_mask, questions, questions_mask, alen, n_episode=<span class="number">3</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            allfacts -- [b, n_fact, flen]，输入的多个句子</span></span><br><span class="line"><span class="string">            allfacts_mask -- [b, n_fact, flen]，mask=1表示是pad的，否则不是</span></span><br><span class="line"><span class="string">            questions -- [b, qlen]，问题</span></span><br><span class="line"><span class="string">            questions_mask -- [b, qlen]，mask=1：pad</span></span><br><span class="line"><span class="string">            alen -- Answer len</span></span><br><span class="line"><span class="string">            seqbegin_id -- 句子开始标记的wordid</span></span><br><span class="line"><span class="string">            n_episodes -- </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            preds -- [b * alen,  vocab_size]，预测的句子。b*alen合在一起方便后面算交叉熵</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 0. 计算常用的信息，batch_size，一条数据nfact条句子，每个fact长度为flen，每个问题长度为qlen</span></span><br><span class="line">        bsize = allfacts.size(<span class="number">0</span>)</span><br><span class="line">        nfact = allfacts.size(<span class="number">1</span>)</span><br><span class="line">        flen = allfacts.size(<span class="number">2</span>)</span><br><span class="line">        qlen = questions.size(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>输入模块</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 输入模块，用RNN编码输入的句子</span></span><br><span class="line"><span class="comment"># TODO 两层循环，待优化</span></span><br><span class="line">encoded_facts = []</span><br><span class="line"><span class="comment"># 对每一条数据，计算facts编码</span></span><br><span class="line"><span class="keyword">for</span> facts, facts_mask <span class="keyword">in</span> zip(allfacts, allfacts_mask):</span><br><span class="line">    facts_embeds = self.embed(facts)</span><br><span class="line">    facts.embeds = self.dropout(facts_embeds)</span><br><span class="line">    hidden = self.init_hidden(nfact)</span><br><span class="line">    <span class="comment"># 1.1 把输入(多条句子)给到GRU</span></span><br><span class="line">    <span class="comment"># b=nf, [nf, flen, h], [1, nf, h]</span></span><br><span class="line">    outputs, hidden = self.input_gru(facts_embeds, hidden)</span><br><span class="line">    <span class="comment"># 1.2 每条句子真正结束时(real_len)对应的输出，作为该句子的hidden。GRU：ouput=hidden</span></span><br><span class="line">    real_hiddens = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, o <span class="keyword">in</span> enumerate(outputs):</span><br><span class="line">        real_len = facts_mask[i].data.tolist().count(<span class="number">0</span>)</span><br><span class="line">        real_hiddens.append(o[real_len - <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 1.3 把所有单个fact连接起来，unsqueeze(0)是为了后面的所有batch的cat</span></span><br><span class="line">        hiddens = torch.cat(real_hiddens).view(nfact, <span class="number">-1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        encoded_facts.append(hiddens)</span><br><span class="line">        <span class="comment"># [b, nfact, h]</span></span><br><span class="line">        encoded_facts = torch.cat(encoded_facts)</span><br></pre></td></tr></table></figure><p><strong>问句模块</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. 问题模块，对问题使用RNN编码</span></span><br><span class="line">questions_embeds = self.embed(questions)</span><br><span class="line">questions_embeds = self.dropout(questions_embeds)</span><br><span class="line">hidden = self.init_hidden(bsize)</span><br><span class="line"><span class="comment"># [b, qlen, h], [1, b, h]</span></span><br><span class="line">outputs, hidden = self.question_gru(questions_embeds, hidden)</span><br><span class="line">real_questions = []</span><br><span class="line"><span class="keyword">for</span> i, o <span class="keyword">in</span> enumerate(outputs):</span><br><span class="line">    real_len = questions_mask[i].data.tolist().count(<span class="number">0</span>)</span><br><span class="line">    real_questions.append(o[real_len - <span class="number">1</span>])</span><br><span class="line">    encoded_questions = torch.cat(real_questions).view(bsize, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Memory模块</span></span><br><span class="line">memory = encoded_questions</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_episode):</span><br><span class="line">    <span class="comment"># e</span></span><br><span class="line">    e = self.init_hidden(bsize).squeeze(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># [nfact, b, h]</span></span><br><span class="line">    encoded_facts_t = encoded_facts.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 根据memory, episode，计算每一时刻的e。最终的e和memory来计算新的memory</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(nfact):</span><br><span class="line">        <span class="comment"># [b, h]</span></span><br><span class="line">        bfact = encoded_facts_t[t]</span><br><span class="line">        <span class="comment"># TODO 计算4个特征，论文是9个</span></span><br><span class="line">        f1 = bfact * encoded_questions</span><br><span class="line">        f2 = bfact * memory</span><br><span class="line">        f3 = torch.abs(bfact - encoded_questions)</span><br><span class="line">        f4 = torch.abs(bfact - memory)</span><br><span class="line">        z = torch.cat([f1, f2, f3, f4], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [b, 1] 对每个fact的注意力</span></span><br><span class="line">        gt = self.gate(z)</span><br><span class="line">        e = gt * self.attention_grucell(bfact, e) + (<span class="number">1</span> - gt) * e</span><br><span class="line">        <span class="comment"># 每一轮的e和旧memory计算新的memory</span></span><br><span class="line">        memory = self.memory_grucell(e, memory)</span><br></pre></td></tr></table></figure><p><strong>回答模块</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. Answer模块</span></span><br><span class="line"><span class="comment"># [b, h]</span></span><br><span class="line">answer_hidden = memory</span><br><span class="line">begin_tokens = get_variable(torch.LongTensor([self.seqbegin_id]*bsize))</span><br><span class="line"><span class="comment"># [b, h]</span></span><br><span class="line">last_word = self.embed(begin_tokens)</span><br><span class="line">preds = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(alen):</span><br><span class="line">    inputs = torch.cat([last_word, encoded_questions], dim=<span class="number">1</span>)</span><br><span class="line">    answer_hidden = self.answer_grucell(inputs, answer_hidden)</span><br><span class="line">    <span class="comment"># to vocab_size</span></span><br><span class="line">    probs = self.answer_fc(answer_hidden)</span><br><span class="line">    <span class="comment"># [b, v]</span></span><br><span class="line">    probs = F.log_softmax(probs.float())</span><br><span class="line">    _, indics = torch.max(probs, <span class="number">1</span>)</span><br><span class="line">    last_word = self.embed(indics)</span><br><span class="line">    <span class="comment"># for cross entropy</span></span><br><span class="line">    preds.append(probs.view(bsize, <span class="number">1</span>, <span class="number">-1</span>))</span><br><span class="line">    <span class="comment">#print (preds[0].data.shape)</span></span><br><span class="line">    preds = torch.cat(preds, dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> preds.view(bsize * alen, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="配置信息">配置信息</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">'''配置文件'''</span></span><br><span class="line">    <span class="comment"># 数据信息</span></span><br><span class="line">    train_file = <span class="string">"./datasets/tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_train.txt"</span></span><br><span class="line">    test_file = <span class="string">"./datasets/tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_test.txt"</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 一些特殊符号</span></span><br><span class="line">    seq_end = <span class="string">'&lt;/s&gt;'</span></span><br><span class="line">    seq_begin = <span class="string">'&lt;s&gt;'</span></span><br><span class="line">    pad = <span class="string">'&lt;pad&gt;'</span></span><br><span class="line">    unk = <span class="string">'&lt;unk&gt;'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># DataLoader信息</span></span><br><span class="line">    batch_size = <span class="number">128</span></span><br><span class="line">    shuffle = <span class="keyword">False</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    num_workers = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># model</span></span><br><span class="line">    embed_size = <span class="number">64</span></span><br><span class="line">    hidden_size = <span class="number">64</span></span><br><span class="line">    <span class="comment"># 对inputs推理的轮数</span></span><br><span class="line">    n_episode = <span class="number">3</span></span><br><span class="line">    dropout_p = <span class="number">0.1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    max_epoch = <span class="number">500</span></span><br><span class="line">    learning_rate = <span class="number">0.001</span></span><br><span class="line">    min_loss = <span class="number">0.01</span></span><br><span class="line">    print_every_epoch = <span class="number">5</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># cuda信息</span></span><br><span class="line">    use_cuda = <span class="keyword">True</span></span><br><span class="line">    device_id = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># model_path</span></span><br><span class="line">    model_path = <span class="string">"./models/DMN.pkl"</span></span><br></pre></td></tr></table></figure><h2 id="训练">训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(opt, th, train_data)</span>:</span></span><br><span class="line">    <span class="string">''' 训练</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        opt -- 配置信息</span></span><br><span class="line"><span class="string">        th -- TextHelper实例</span></span><br><span class="line"><span class="string">        train_data -- 训练数据，[[facts, question, answer]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 加载原始数据</span></span><br><span class="line">    seqbegin_id = th.word2index(th.seq_begin)</span><br><span class="line">    </span><br><span class="line">    model = DMN(th.vocab_size, opt.embed_size, opt.hidden_size, seqbegin_id, th.word2index(th.pad))</span><br><span class="line">    <span class="keyword">if</span> opt.use_cuda:</span><br><span class="line">        model = model.cuda(opt.device_id)</span><br><span class="line">    </span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr = opt.learning_rate)</span><br><span class="line">    loss_func = nn.CrossEntropyLoss(ignore_index=th.word2index(th.pad))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(opt.max_epoch):</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> batch_data <span class="keyword">in</span> get_data_loader(train_data, opt.batch_size, opt.shuffle):</span><br><span class="line">            <span class="comment"># batch内的数据进行pad，转成Variable</span></span><br><span class="line">            allfacts, allfacts_mask, questions, questions_mask, answers = \</span><br><span class="line">                    pad_batch_data(batch_data, th)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 前向</span></span><br><span class="line">            preds = model(allfacts, allfacts_mask, questions, questions_mask, </span><br><span class="line">                          answers.size(<span class="number">1</span>), opt.n_episode)</span><br><span class="line">            <span class="comment"># loss</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss = loss_func(preds, answers.view(<span class="number">-1</span>))</span><br><span class="line">            losses.append(loss.data.tolist()[<span class="number">0</span>])</span><br><span class="line">            <span class="comment"># 反向</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        avg_loss = np.mean(losses)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> avg_loss &lt;= opt.min_loss <span class="keyword">or</span> e % opt.print_every_epoch == <span class="number">0</span> <span class="keyword">or</span> e == opt.max_epoch - <span class="number">1</span>:    </span><br><span class="line">            info = <span class="string">"e=&#123;&#125;, loss=&#123;&#125;"</span>.format(e, avg_loss)</span><br><span class="line">            losses = []</span><br><span class="line">            <span class="keyword">print</span> (info)</span><br><span class="line">            <span class="keyword">if</span> e == opt.max_epoch - <span class="number">1</span> <span class="keyword">and</span> avg_loss &gt; opt.min_loss:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"epoch finish, loss &gt; min_loss"</span>)</span><br><span class="line">                torch.save(model, opt.model_path)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">elif</span> avg_loss &lt;= opt.min_loss:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Early stop"</span>)</span><br><span class="line">                torch.save(model, opt.model_path)</span><br><span class="line">                <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h2 id="预测和效果">预测和效果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_test_accuracy</span><span class="params">(model, test_data, th, n_episode=DefaultConfig.n_episode)</span>:</span></span><br><span class="line">    <span class="string">'''测试，测试数据'''</span></span><br><span class="line">    batch_size = <span class="number">1</span></span><br><span class="line">    model.eval()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> get_data_loader(test_data, batch_size, <span class="keyword">False</span>):</span><br><span class="line">        facts, facts_mask, question, question_mask, answer = pad_batch_data(item, th)</span><br><span class="line">        preds = model(facts, facts_mask, question, question_mask, answer.size(<span class="number">1</span>), n_episode)</span><br><span class="line">        <span class="comment">#print (answer.data.shape, preds.data.shape)</span></span><br><span class="line">        preds = preds.max(<span class="number">1</span>)[<span class="number">1</span>].data.tolist()</span><br><span class="line">        answer = answer.view(<span class="number">-1</span>).data.tolist()</span><br><span class="line">        <span class="keyword">if</span> preds == answer:</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"acccuracy = "</span>, correct / len(test_data)) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one_data</span><span class="params">(model, item, th, n_episode=DefaultConfig.n_episode)</span>:</span></span><br><span class="line">    <span class="string">''' 测试一条数据</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model -- DMN模型</span></span><br><span class="line"><span class="string">        item -- [facts, question, answer]</span></span><br><span class="line"><span class="string">        th -- TextHelper</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># batch_size = 1</span></span><br><span class="line">    model.eval()</span><br><span class="line">    item = [item]</span><br><span class="line">    facts, facts_mask, question, question_mask, answer = pad_batch_data(item, th)</span><br><span class="line">    preds = model(facts, facts_mask, question, question_mask, answer.size(<span class="number">1</span>), n_episode)</span><br><span class="line">    </span><br><span class="line">    item = item[<span class="number">0</span>]</span><br><span class="line">    preds = preds.max(<span class="number">1</span>)[<span class="number">1</span>].data.tolist()</span><br><span class="line">    fact = item[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    facts = [th.indices2sentence(fact) <span class="keyword">for</span> fact <span class="keyword">in</span> item[<span class="number">0</span>]]</span><br><span class="line">    facts = [<span class="string">" "</span>.join(fact) <span class="keyword">for</span> fact <span class="keyword">in</span> facts]</span><br><span class="line">    q = <span class="string">" "</span>.join(th.indices2sentence(item[<span class="number">1</span>]))</span><br><span class="line">    a = <span class="string">" "</span>.join(th.indices2sentence(item[<span class="number">2</span>]))</span><br><span class="line">    preds = <span class="string">" "</span>.join(th.indices2sentence(preds))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Facts:"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"\n"</span>.join(facts))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Question:"</span>, q)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Answer:"</span>, a)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Predict:"</span>, preds)</span><br><span class="line">    <span class="keyword">print</span> ()</span><br></pre></td></tr></table></figure><p>在本数据集上效果较好，但是数据量小、句子简单，还没有在别的数据集上面进行测试。等忙完了测试一下。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文概要：介绍DMN的基本原理，使用PyTorch进行实现一个简单QA&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="QA" scheme="http://plmsmile.github.io/tags/QA/"/>
    
      <category term="DMN" scheme="http://plmsmile.github.io/tags/DMN/"/>
    
  </entry>
  
</feed>

[{"title":"机器阅读理解概述","date":"2019-03-07T14:35:45.000Z","path":"2019/03/07/53-rc-brief/","text":"阅读理解的历史发展；数学定义；阅读理解和问答系统的区别；数据集和模型的发展 发展动机 传统NLP任务 1. 词性分析 part-of-speech tagging ：判断词性 2. 命名实体识别 named entity recognition 识别实体 3. 句法依存 sytactic parsing 找到词间关系、语法结构信息 4. 指代消解 coreference resolution 阅读理解动机 让机器理解人类语言是AI领域长期存在的问题 阅读理解能综合评估各项NLP任务，是一个综合性任务 阅读理解探索更加深层次的理解 回答问题是检测机器是否读懂文章最好的办法 历史发展 早期系统 1. QUALM系统 Lehnert，1977年 2. 早期数据集 Hirschman，1999年 小学文章，3年级-6年级 60篇 - 60篇：dev - test 只需要返回包含正确答案的句子即可 who what when where why 3. Deep Read系统 Hirschman，1999年 rule-based bag-of-words，基于规则的词袋模型 浅层语言处理：词干提取、语义类识别、指代消解 4. QUARC系统 Riloff and Thelen，2000年 rule-based 基于词汇和语义对应 还有3和4的结合（Charniak，2000年），准确率在30%-40%左右。 机器学习 三元组 （文章，问题，答案） 两个数据集 MCTest 四选一 660篇科幻小说 ProcessBank 二分类 585问题，200个段落 生物类型文章 需要理解实体关系和事件 规则方法 不使用训练数据集 1. 启发式的滑动窗口方法 计算word overlap、distance information 2. 文本蕴含方法 用现有的文本蕴含系统，把(问题，答案)对转化为一个statement。 3. max-margin 学习框架 使用很多语言特征： 句法依存 semantic frames 指代消解 discourse relation 词向量 机器学习 深度学习 任务定义 RC和QA的关系 数据集和模型","tags":[{"name":"机器阅读","slug":"机器阅读","permalink":"http://plmsmile.github.io/tags/机器阅读/"}]},{"title":"BERT 详解","date":"2018-12-15T04:07:30.000Z","path":"2018/12/15/52-bert/","text":"目前最强NLP模型，横扫NLP11项任务。利用Transformer去预训练双向语言模型和NSP任务，再finetune达到了各种任务的最优效果。 背景 简介 在NLP中，利用语言模型去做预训练，能显著提升许多任务的效果。 句子级别任务：自然语言推理等。学习多个句子之间的关系 词级别任务 ：NER、QA、SQuAD等 两种迁移方式 基于特征：ELMo 基于finetune：OpenAI GPT 存在的问题 单向语言模型和两个方向语言模型简单拼接，这两个方法的能力都小于双向语言模型。 单向语言模型对于： sentence-level任务：次优方案。 token-level任务：效果很差。比如SQuAD，非常需要融合两个方向的信息 相关研究 1. 基于特征的方法 词向量 sentence 、paragraph 向量 从语言模型中获得具有上下文语义的向量--ELMo 2. 基于Finetune的方法 预训练一个语言模型，在下游任务只需要简单finetune，只需要从新训练很少的参数。 OpenAI GPT 3. 基于监督数据的迁移学习 从一些大规模是数据集中进行迁移学习。比如 从自然语言推理中迁移 从机器翻译中迁移学习词向量：Learned in Translation: Contextualized Word Vectors BERT模型 模型架构 1. 总体架构 利用Transformer的Encoder去训练双向语言模型BERT，再在BERT后面接上特定任务的分类器。 使用方法示例： 2. 输入与输出 3. 两种规模 \\(\\rm{BERT}_{BASE}\\) ：\\(\\rm{L=12, H=768, A=12}\\)。总参数为110M。和GPT一样 \\(\\rm{BERT}_{LARGE}\\) ：\\(\\rm{L=24, H=1024, A=16}\\)。总参数为340M。最优模型 单个位置的输入 每个位置输入三个部分相加而成： wordpiece-token向量 位置向量：512个。训练 段向量：sentence A B两个向量。训练 一些符号： CLS：special classification embedding，用于分类的向量，会聚集所有的分类信息 SEP：输入是QA或2个句子时，需添加SEP标记以示区别 \\(E_A\\)和\\(E_B\\)：输入是QA或2个句子时，标记的sentence向量。如只有一个句子，则是sentence A向量 特定任务的BERT 单句子分类：CLS+句子。利用CLS进行分类 多句子分类：CLS+句子A+SEP+句子B。利用CLS分类 SQuAD：CLS+问题+SEP+文章。利用所有文章单词的输出做计算start和end NER：CLS+句子。利用句子单词做标记 预训练语言模型 我们都知道单向语言模型的能力很差，单独训练两个方向的语言模型再把结果拼接起来也不好。那么怎么才能训练一个真正的双向语言模型呢？如何让一个单词is conditioned on both left and right context呢？答案就是Masked Language Model Masked LM 在进行WordPiece之后，随机掩盖一些（15%）词汇，再去预测这些词汇。 但是有2个缺点 缺点1 大量mask标记，造成预训练和finetune时候的差距，因为finetune没有mask 80%：替换为mask 10%：随机替换为其它词汇 10%：保留原来的词汇。这部分正确的保留，保证了语言能力。 由于Transformer不知道要预测哪个词语，所以它会强制学习到所有单词的上下文表达。 缺点2 收敛很慢，但是效果好 比单向语言模型收敛较慢。 预训练NSP任务 对于像QA、NLI等需要理解多个句子之间关系的下游任务，只靠语言模型是不够的。还需要提前学习到句子之间的关系。 Next Sentence Prediction NSP-Next Sentence Prediction，是一个二分类任务。输入是A和B两个句子，标记是IsNext或NotNext，用来判断B是否是A后面的句子。这样，就能从大规模预料中学习到一些句间关系。 模型最终能达到97%-98%的准确率，对QA和NLI都很有效果。 预训练细节 数据组成 语料是下面两个库，合计33亿词汇。采用文档级别的语料，有利于学习长依赖序列。 BooksCorpus：8亿个词。(800M) 英文维基百科：25亿个词。(2,500M) 从语料库中随机选择2个片段(较长)作为两个AB句子，构成一条输入数据： 0.5概率A-B两个句子连续，0.5概率随机选择B A使用A embedding，B使用B embedding A和B总长度最大为512 tokens WordPiece Tokenization后再mask掉15%的词汇。 训练参数 batch_size：256。每条数据长度：512 100万步，40个epoch。语料合计33亿词汇 Adam ：\\(\\beta_1=0.9, \\beta_2=0.999\\) L2权值衰减为0.01。所有层的dropout为0.1 学习率的warmup的step为10000 GELU激活函数 训练loss：LM和NSP的loss加起来 BERT base 16个TPU，Large 64个TPU，训练4天 Finetune细节 两种不同类型的任务所需要的向量，详情见特定任务的BERT sentence-level：一般只拿CLS位置的向量，过线性层再softmax即可得到分类结果 token-level：SQuAD或NER，取对应位置的向量，过线性层再softmax得到相应的结果 Finetune时超参数基本一致，但有一些是与特定任务相关的。下面是比较好的选择 Batch size：16, 32 学习率：\\(5*10^{-5}\\)，\\(3*10^{-5}\\)，\\(2*10^{-5}\\) epoch ：3,4 BERT与GPT比较 BERT和OpenAI GPT都是使用Transformer进行预训练语言模型， 再进行finetune达到不错的效果。区别如下： 项目 BERT GPT 训练数据 BooksCorpus(8亿)+维基百科(25亿) BooksCorpus(8亿) 任务 双向语言模型(MLM)+NSP+Finetune 单向语言模型+Finetune Transformer Encoder Decoder 符号 CLS, SEP, sentence A-B Start, SEP, 结束(CLS) 符号使用 预训练、finetune过程 finetune过程 Batch数据 1M步，128000个词汇 1M步，32000个词汇 finetune学习率 与特定任务相关 与预训练一样，\\(5*10^{-5}\\) 实验结果 9项GLUE任务 General Language Understanding Evaluation 包含了很多自然语言理解的任务。 1. MNLI Multi-Genre Natural Language Inference是一个众包大规模的文本蕴含任务。 给2个句子，判断第二个句子与第一个句子之间的关系。蕴含、矛盾、中立的 2. QQP Quora Question Pairs 给2个问题，判断是否语义相同 3. QNLI Question Natural Language Inference 是一个二分类任务，由SQuAD数据变成。 给1个(问题，句子)对，判断句子是否包含正确答案 4. SST-2 Stanford Sentiment Treebank，二分类任务，从电影评论中提取。 给1个评论句子，判断情感 5. CoLA The Corpus of Linguistic Acceptablity，二分类任务，判断一个英语句子是否符合语法的 给1个英语句子，判断是否符合语法 6. STS-B The Semantic Textual Similarity Benchmark，多分类任务，判断两个句子的相似性，0-5。由新闻标题和其他组成 给2个句子，看相似性 7. MRPC Microsoft Research Paraphrase Corpus，2分类任务，判断两个句子是否语义相等，由网上新闻组成。05年的，3600条训练数据。 给1个句子对，判断2个句子语义是否相同 8. RTE Recognizing Textual Entailment，二分类任务，类似于MNLI，但是只是蕴含或者不蕴含。训练数据更少 9. WNLI Winograd NLI一个小数据集的NLI。据说官网评测有问题。所以评测后面的评测没有加入这个 GLUE评测结果 对于sentence-level的分类任务，只用CLS位置的输出向量来进行分类。 SQuAD-v1.1 SQuAD属于token-level的任务，不是用CLS位置，而是用所有的文章位置的向量去计算开始和结束位置。 Finetune了3轮，学习率为\\(5*10^{-5}\\)，batchsize为32。取得了最好的效果 NER SWAG The Situations With Adversarial Generations 是一个常识性推理数据集，是一个四分类问题。给一个背景，选择一个最有可能会发生的情景。 Finetune了3轮，学习率为\\(2*10^{-5}\\)，batchsize=16 效果分析 任务影响 No NSP：不加NSP任务；LTR 单向语言模型。 NSP：对SQuAD、QNLI、MNLI有影响，下降1个点左右 双向LM：如果是LTR，SQuAD、MRPC都严重下降10个点。因为token-level的任务，更需要右边上下文的信息 模型大小 BERT的base和large的参数分别为：110M和340M。 一般来说，对于大型任务，扩大模型会提升效果。但是，BERT得到充分预训练以后，扩大模型，也会对小数据集有提升！这是第一个证明这个结论的任务。 训练步数 确实需要训练100万步。12800词/batch MLM收敛到最优慢，但是效果好。在早期就早早超过单向语言模型了 BERT使用方法 基于finetune 基于特征 Finetune稍微麻烦一些。可以使用BERT预训练好的contextualized embeddings，每一层都有。 其中效果最好的方法，就是拼接最后四层的向量。 参考和思考 参考 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Illustrated-bert 思考 今年很流行 无监督预训练和有监督finetune，那么有什么演变历程吗？ BERT的成功会是因为更多的数据吗？ BERT很通用太重量级，8个GPU据说要训练一年。普通人难以训练。 BERT是一个通用预训练模型，那么我们还要继续研究特定任务的架构吗？","tags":[{"name":"语言模型","slug":"语言模型","permalink":"http://plmsmile.github.io/tags/语言模型/"},{"name":"迁移学习","slug":"迁移学习","permalink":"http://plmsmile.github.io/tags/迁移学习/"},{"name":"ELMo","slug":"ELMo","permalink":"http://plmsmile.github.io/tags/ELMo/"},{"name":"Transformer","slug":"Transformer","permalink":"http://plmsmile.github.io/tags/Transformer/"},{"name":"MLM","slug":"MLM","permalink":"http://plmsmile.github.io/tags/MLM/"},{"name":"NSP","slug":"NSP","permalink":"http://plmsmile.github.io/tags/NSP/"},{"name":"BERT","slug":"BERT","permalink":"http://plmsmile.github.io/tags/BERT/"},{"name":"GPT","slug":"GPT","permalink":"http://plmsmile.github.io/tags/GPT/"}]},{"title":"OpenAI GPT：Improving Language Understanding by Generative Pre-Training","date":"2018-12-13T14:44:24.000Z","path":"2018/12/13/51-opengpt/","text":"使用Transformer(Decoder)预训练单向语言模型，再进行有监督数据进行特定任务finetune 背景 问题提出 NLP中，无标注语料很多，有标注的数据很少。 很多任务不能从头开始训练（标注数据太少），需要减轻对标注数据的依赖 在大规模无标注语料中预训练的语言模型可以提升很多效果 从无标注数据中学习一个good representations，很流行且有效果 从无标注数据中学习到词级以上的意义的难点： 没有一个有效的优化目标函数 对学习到的representations没有通用的有效的迁移方法 半监督学习 半监督是指从无监督数据中学习一些通用表示，再做轻微的有监督finetune到各种各样的特定任务中。 1. 无监督特征表示 利用大量的语料和语言模型任务，去学习到一个神经网络。作为后面模型的网络初始参数。 语言模型网络可以使用LSTM和Transformer。Transformer可以解决LSTM的长依赖问题，具有更好的迁移能力。 2. 有监督任务训练 特定任务的监督数据去finetune初始的网络。一般需要根据任务类型加上输出层。 相关研究 1. 语义研究 词向量主要是迁移具有词级别的信息，但更需要其一些词级别以上的语义信息。主要有phrase-level和sentence-level embedding。 2. 无监督预训练 无监督预训练的目的在于为后续任务去初始化一个好的网络参数，而不需要去改变任务的目标。预训练主要是使用语言模型任务，Transformer比LSTM更好，更强的迁移能力和处理长依赖能力。 Ruder大神说NLP的ImageNet时代已经来了，足以说明预训练的重要性。 3. 辅助任务 加辅助特征(ELMo)：网络的参数需要重新学习 辅助训练目标--语言模型和特定任务一起训练：其实无监督预训练已经学习到了语言特征，无需辅助训练目标了 GPT模型 预训练单向语言模型 采用的是单向语言模型，预测下一个词语，采用的是Tansformer的Decoder。 在Decoder之外加上线性层去预测下一个单词，训练语言模型任务。 迁移到下游监督任务 预训练的Tansformer已经具有处理语言的能力，再加上输出层，并且监督finetune，则可以达到一个不错的效果。如下 各种任务组织方式 利用traversal-style方法，把结构化数据处理成一个序列。每个序列都有一个开始和结束符号，也有分解符号，都是随机初始化的。 分类任务 开始符号 -- 文本 -- 结束符号 文本蕴含 开始 -- 前提 -- 分界 -- 假设 -- 结束 相似性 相似度计算与顺序无关，所以加了两个 问答和常识推理 文章和问题组成上下文，与每一个可能的答案作为拼接。一共有多组 分析 效果 参数分析 右图：zero-shot上Transformer的效果是比LSTM好的 左图：可以知道Transformer层数越多效果也越好 参考 原始论文 Improving Language Understanding by Generative Pre-Training Illustrated-BERT pytorch-openai-transformer-lm openai language-unsupervised","tags":[{"name":"transformer","slug":"transformer","permalink":"http://plmsmile.github.io/tags/transformer/"},{"name":"语言模型","slug":"语言模型","permalink":"http://plmsmile.github.io/tags/语言模型/"},{"name":"迁移学习","slug":"迁移学习","permalink":"http://plmsmile.github.io/tags/迁移学习/"}]},{"title":"ELMo：Deep Contextualized Word Representations","date":"2018-12-11T07:55:47.000Z","path":"2018/12/11/50-elmo/","text":"ACL2018 Best Paper, 性价比很高的效果提升方法。ELMo: Deep contextualized word representations 背景 词向量的问题 Word2vec和Glove，可以学习到一些词汇在语义和语法上的信息。由于它是固定的，所以它无法根据上下文去表示一个词语，也无法解决一词多义的问题。 相关研究 1. 学习subword的信息 CHARAGRAM: Embedding Words and Sentences via Character n-grams 2. 为每种词义单独训练词向量 3. Context2vec Learning Generic Context Embedding with Bidirectional LSTM 4. CoVe 从机器翻译中学习词向量 Learned in Translation: Contextualized Word Vectors 5. 半监督的 Semi-supervised sequence tagging with bidirectional language models 上述方法都得益于大规模数据集。其中CoVe非常火，但是受限制于双语语料，同时效果也没有ELMo好。 ELMo图解 ELMo的目的 ELMo全称是Embeddings from Language Model。ELMo的特点是根据上下文来确定词向量。 对于Glove来说，会选择played运动的意义。然而在ELMo中，却可以根据上下文来获得语义信息。 ELMo是一个2层BiLSTM的语言模型。把各个位置上的每层的输出线性组合起来便是最终的ELMo 词向量。 语言模型 预测下一个词汇的任务，在大规模预料中进行训练，来理解语言，掌握一些语言模式。不是说能直接预测出下一个词是什么，而是正确的单词的概率远大于不会出现的单词。 双向语言模型 双向语言模型中，词汇对下一个词汇和前一个词汇都有感知。 ELMo词向量 Deep Contextualized Vectors由如下步骤构成： 在每一层，把两个方向的隐状态拼接起来得到该层的隐状态 把初始词向量、第一层的隐状态、第二层的隐状态，线性组合起来得到最终的词向量 ELMo使用 把ELMo词向量和传统词向量拼接起来，作为新的词向量输入。 ELMo分析 提升效果 语法和语义 低层偏向语法信息，高层偏向语义信息。 偏爱语法信息 很多模型都偏爱低层的语法信息。 小数据集提升 参考 原始论文Deep contextualized word representations illustrated-bert ELMo Slide","tags":[{"name":"语言模型","slug":"语言模型","permalink":"http://plmsmile.github.io/tags/语言模型/"},{"name":"迁移学习","slug":"迁移学习","permalink":"http://plmsmile.github.io/tags/迁移学习/"},{"name":"ELMo","slug":"ELMo","permalink":"http://plmsmile.github.io/tags/ELMo/"},{"name":"词向量","slug":"词向量","permalink":"http://plmsmile.github.io/tags/词向量/"},{"name":"LSTM","slug":"LSTM","permalink":"http://plmsmile.github.io/tags/LSTM/"}]},{"title":"QANet","date":"2018-08-30T05:47:34.000Z","path":"2018/08/30/49-qanet/","text":"常年SQuAD榜单排名第一的模型。QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension 论文模型 概览 机器阅读任务就不说了。这个模型的主要创新点在于 卷积（可分离卷积）捕捉局部信息 （并行计算，加速） Self-Attention捕捉全局信息 数据扩增 一个Encoder Block主要是，其中Transformer的EncoderBlock只有Attention和FFN，没有卷积。 Positional Encoder 可分离卷积 （多个，提高内存效率和泛化性） Self-Attention 前向神经网络 Input Embedding 词向量 Glove 300维，Fix；UNK词向量可以训练 字向量 CNN字符向量，200维，可以训练 每个单词的字符最多16个 对单词的16个字符的向量过卷积（可分离卷积） 选择所有字符中最大的向量作为单词的最终字符向量 拼接 对词向量和字符向量拼接起来，\\([x_w;x_c] \\in \\mathcal{R}^{d_w+d_c}\\)。再过两层的HighwayNetwork，得到最终的单词向量表示。 Embedding Encoder 每一个Encoder块是由卷积、Self-Attention、全连接层组成，一共有4个Encoder块。输入向量维数是\\(d=500(200+300)\\)，输出是\\(d=128\\) 可分离卷积：kernal size=7，d = 128。变成128维向量 Self-Attention：8头注意力，键值对注意力 全连接：输出也是128 QANet:层归一化+残差连接：\\(f(\\rm{LayerNorm}(x)) + x\\) Transformer 是Add&amp;Norm，\\(\\rm{LayerNorm(f(x) +x)}\\) Attention Layer Context: \\(n\\)个单词，Question：m个单词。\\(C \\in \\mathcal{R}^{n\\times d}\\)，\\(Q \\in \\mathcal{R}^{m \\times d}\\) 关联性矩阵 采用的是BiDAF的计算策略： \\[ S = f(q, c) = W_0 [q, c, q \\odot c] \\in \\mathcal{R} ^{n \\times m} \\] DCN： \\(S = C \\cdot Q^T \\in \\mathcal{R}^{n \\times m}\\) Context2Query Attention C2Q的attention weights，对行做softmax \\[ A^Q = \\rm{softmax}(S) \\in \\mathcal{R}^{n \\times m} \\] C2Q Attention（Context） \\[ S^C = A^Q \\cdot Q \\in \\mathcal{R} ^{n \\times d} \\] Query2Context Attention Q2C Attention weights，对列做Softmax \\[ A^C = \\rm{softmax}(S^T) \\in \\mathcal{R}^{m \\times n} \\] Q2C Attention（Query） \\[ S^Q = A^C \\cdot C \\in \\mathcal{R}^{m \\times d} \\] Context的Coattention，参考自DCN的Coattention \\[ C^C = A^Q \\cdot S^Q \\in \\mathcal{R}^{n \\times d} \\] 最终得到两个对Context的编码 普通Attention：\\(A = S^C \\in \\mathcal{R}^{n \\times d}\\) Coattention：\\(B = C^C \\in \\mathcal{R}^{n \\times d}\\) Model Encoder 输入是3个关于Context的矩阵信息： 原始Context：\\(C \\in \\mathcal{R}^{n\\times d}\\) Context的Attention： \\(A \\in \\mathcal{R}^{n\\times d}\\) Context的Coattention：\\(B \\in \\mathcal{R}^{n \\times d}\\) 每个单词的编码信息为上面三个矩阵的一个拼接： \\[ f(w) = [c, a, c \\odot a, c \\odot b] \\] 一个有7个Encoder-Block，每个Encoder-Block：2个卷积层、Self-Attention、FFN。其它参数和Embedding Encoder一样。 一共有3个Model-Encoder，共享所有参数。输出依次为\\(M_0, M_1, M_2\\) Output Layer 这一层是和特定任务相关的。输出各个位置作为开始和结束位置的概率： \\[ p^1 = \\rm{softmax}(W_1[M_0; M_1]), \\quad p^2 = \\rm{softmax}(W_1[M_0; M_2]) \\] 目标函数 \\[ L(\\theta) = -\\frac{1}{N} \\sum_{i}^N [\\log(p^1_{y_i^1}) + \\log(p^2_{y_i^2})] \\]","tags":[{"name":"Self-Attention","slug":"Self-Attention","permalink":"http://plmsmile.github.io/tags/Self-Attention/"},{"name":"可分离卷积","slug":"可分离卷积","permalink":"http://plmsmile.github.io/tags/可分离卷积/"}]},{"title":"Transformer","date":"2018-08-29T07:41:32.000Z","path":"2018/08/29/48-attention-is-all-you-need/","text":"大名鼎鼎的Transformer，Attention Is All You Need Transformer概览 论文结构 总览结构 图解总览 其实也是一个Encoder-Decoder的翻译模型。 由一个Encoders和一个Decoders组成。 Encoders由多个Encoder块组成。 Encoder 总体结构 1个Encoder由Self-Attention和FFN组成 一个Encoder的结果再给到下一个Encoder Encoder-Decoder 编码实例 对一个句子进行编码 Self-Attention会对每一个单词进行编码，得到对应的向量。\\(\\mathbf{x_1}, \\mathbf{x_2}, \\mathbf{x_3} \\to \\mathbf{z_1}, \\mathbf{z_2}, \\mathbf{z_3}\\)，再给到FFN，会得到一个Encoder的结果\\(\\mathbf{r_1}, \\mathbf{r_2}, \\mathbf{r_3}\\)， 再继续给到下一个Encoder Attention Self-Attention 1. 乘以3个矩阵生成3个向量：Query、Key、Value 2. 计算与每个位置的score 编码一个单词时，会计算它与句子中其他单词的得分。会得到每个单词对于当前单词的关注程度。 3. 归一化和softmax得到每个概率 4. 依概率结合每个单词的向量 Attention图示 矩阵形式 其实就是一个注意力矩阵公式 多头注意力 其实就是多个KV注意力。从两个方面提升了Attention Layer的优点 让模型能够关注到句子中的各个不同位置 Attention Layer可以有多个不同的表示子空间representation subspaces 多头注意力矩阵形式 经过多头注意力映射，会生成多个注意力\\(Z_0, Z_1, \\cdots, Z_7\\)。 把这些注意力头拼接起来，再乘以一个大矩阵，最终融合得到一个信息矩阵。会给到FFN进行计算。 注意力总结 Attention图示 前向神经网络 Position-wise Feed-Forward Network，会对每个位置过两个线性层，其中使用ReLU作为激活函数。 \\[ \\rm{FFN}(x) = \\rm{Linear}(\\rm{ReLU}(\\rm{Linear}(x))) = \\rm{max}(0, xW_1 + b_1)W_2 + b_2 \\] 位置编码 词向量+位置信息=带位置信息的词向量 \\[ \\rm{PE}(pos, 2i) = \\sin (\\rm{pos} / 10000^{\\frac{2i}{d}}) \\] \\[ \\rm{PE}(pos, \\rm{2i+1}) = \\cos (\\rm{pos} / 10000^{\\frac{2i}{d}}) \\] 示例 再把sin和cos的两个值拼接起来，就得到如下图所示。 Encoder-Block 残差连接 层归一化 层归一化 总览 Decoder 单步 多步 Linear-Softmax 模型样例","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"Self-Attention","slug":"Self-Attention","permalink":"http://plmsmile.github.io/tags/Self-Attention/"},{"name":"机器翻译","slug":"机器翻译","permalink":"http://plmsmile.github.io/tags/机器翻译/"},{"name":"Transformer","slug":"Transformer","permalink":"http://plmsmile.github.io/tags/Transformer/"},{"name":"多头注意力","slug":"多头注意力","permalink":"http://plmsmile.github.io/tags/多头注意力/"},{"name":"残差连接","slug":"残差连接","permalink":"http://plmsmile.github.io/tags/残差连接/"},{"name":"层归一化","slug":"层归一化","permalink":"http://plmsmile.github.io/tags/层归一化/"},{"name":"位置编码","slug":"位置编码","permalink":"http://plmsmile.github.io/tags/位置编码/"}]},{"title":"Bidirectional Attention Flow","date":"2018-05-22T12:01:43.000Z","path":"2018/05/22/47-bidaf/","text":"阅读理解中很经典很有名的BiDAF模型。 模型 问题定义 \\[ P = (w^P_1, w^P_2, \\cdots, w^P_m) \\] \\[ Q = (w^Q_1,w^Q_2, \\cdots, w^Q_n) \\] \\[ A = (w^A_1, w^A_2, \\cdots, w^A_k) \\] \\[ p_{start} \\quad p_{end} \\] 望燕 枝俏 索寒 盘峰 漫枫 照月、焕月、（揽月） 后阳 （蚍蜉 ）撼树 永诀 （龙岩 ）上杭 黔驴","tags":[{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://plmsmile.github.io/tags/机器阅读理解/"},{"name":"BiDAF","slug":"BiDAF","permalink":"http://plmsmile.github.io/tags/BiDAF/"}]},{"title":"R-Net (Gated Self-Matching Networks)","date":"2018-05-15T02:46:49.000Z","path":"2018/05/15/46-rnet-selfmatch/","text":"微软亚研院和北大的阅读理解模型R-Net。 Gated Attention-based RNN 来获得question-aware passage representation，即编码P Self-matching Attention来修正编码P，即P与自己做match，有效从全文中编码信息 Pointer Network预测开始和结束位置 论文地址： Gated Self-Matching Networks for Reading Comprehension and Question Answering R-NET: MACHINE READING COMPREHENSION WITH SELF-MATCHING NETWORKS Introduction 经典模型 1. Match-LSTM Match-LSTM and Answer Pointer笔记 2. Dynamic Coatteion Network DCN笔记。Coattention同时处理P和Q，动态迭代预测答案的位置。 3. Bi-Directional Attention Flow Network 本文模型概要 1. BiRNN 分别编码P和Q 分别编码Question和Passage 2. gated matching layer 编码Q-Aware的Passage Gated Attention-based RNN。在Match-LSTM上添加了门机制。 段落有多个部分，根据与Q的相关程度，分配重要性权值 忽略不重要的，强调重要的部分 3. self-matching layer 再次从整个Passage中提取信息。它的缺点： RNN只能存储少部分上下文内容 一个候选答案不知道其他部分的线索 解决方法：对P做self-match。使用Gated Attention-based RNN对P和P自己做match。 4. pointer-network 模型 BiRNN，GARNN（P+Q），GARNN-Selfmatch（P+P），Pointer Network BiRNN编码Q和P \\(Q=\\{w_t^Q\\}_{t=1}^m\\)，\\(P=\\{w_t^P\\}_{t=1}^n\\)。 P是n个单词，Q是m个单词。 词向量和字符向量 词向量：\\(\\{e_t^Q\\}_{t=1}^m\\)、\\(\\{e_t^P\\}_{t=1}^n\\) 字符向量：\\(\\{c_t^Q\\}_{t=1}^m\\)、\\(\\{c_t^P\\}_{t=1}^n\\) 字符向量，使用RNN，用每个单词的最后时刻的隐状态，作为字符向量。有助于处理OOV词汇。 编码Question和Passage \\[ \\mathbf u_t^Q = \\rm{BiRNN }(u_{t-1}^Q, [e_t^Q, c_t^Q]) \\] \\[ \\mathbf u_t^P = \\rm{BiRNN }(u_{t-1}^P, [e_t^P, c_t^P]) \\] Gated Attention-based RNN 要基于U（\\(U^Q\\)）去编码P（\\(U^P\\)） ，得到Question-Aware的Passage编码，\\(V^P\\)。 1. Attention RNN \\(p_t\\)与\\(q_j\\)两个单词的相关性函数（能量函数） \\[ s_j^t = v^T \\tanh (W_u^Q\\mathbf u_j^Q + W_u^P \\mathbf u_t^P + W_v^P \\mathbf v_{t-1}^P), \\quad j = 1, \\cdots, m \\] \\(p_t\\)与所有Q单词的注意力权值\\(\\mathbf \\alpha^t\\) doc 2 query attention \\[ \\alpha_{j}^t = \\rm{softmax}(s_j^t) \\] \\(p_t\\)基于\\(\\mathbf \\alpha^t\\)对\\(Q\\)的信息汇总（注意力）attention pooling vector of the whole question \\[ \\mathbf c_t = \\sum_{i=1}^m \\alpha_i^t \\mathbf u_i^Q \\] 实际上：\\(\\mathbf c_t = \\rm{attn}(U^Q, [\\mathbf u_t^P, \\mathbf v_{t-1}^P])​\\)。 注意力$c_t $和上一时刻隐状态 \\(\\mathbf v_{t-1}^P\\)，输入RNN，计算当前的信息 \\[ \\mathbf v_t^P = \\rm{RNN}(\\mathbf v_{t-1}^P, \\mathbf c_t) \\] 每个\\(\\mathbf v_t^P\\)动态地合并了来自整个Q的匹配信息。 2. Match RNN Match-LSTM。在输入RNN计算时，把当前\\(\\mathbf u_t^P\\)也输入进去，带上Passage的信息。输入是\\(\\rm{input}=[\\mathbf u_t^P, \\mathbf c_t]\\)。 \\[ \\mathbf v_t^P = \\rm{RNN}(\\mathbf v_{t-1}^P, [\\mathbf u_t^P, \\mathbf c_t]) \\] 3. Gated Attention-based RNN 用门机制去控制每个\\(p_t\\)的重要程度。 \\[ g_t = \\rm{sigmoid}(W_g \\cdot [\\mathbf u_t^P, \\mathbf c_t]) \\] \\[ [\\mathbf u_t^P, \\mathbf c_t]^* = g_t \\odot [\\mathbf u_t^P, \\mathbf c_t] \\] \\[ \\mathbf v_t^P = \\rm{RNN}(\\mathbf v_{t-1}^P, [\\mathbf u_t^P, \\mathbf c_t]^*) \\] GARNN的门机制 与GRU和LSTM不同 门机制是基于当前\\(p_t\\)和它的对应的Q的注意力向量\\(\\mathbf c_t\\)（包含当前\\(p_t\\)和Q的关系） 模拟了阅读理解中，只有\\(P\\)的一部分才与问题相关的特点 最终得到了question-aware passage representation ：\\(\\{\\mathbf v_t^P\\}_{t=1}^n\\)。它的缺点如下： 对Passage的上下文感知太少 候选答案对它窗口之外的线索未知 Question和Passage在词法、句法上有区别 Self-Matching Attention 为了充分利用Passage的上下文信息。\\(\\{\\mathbf v_t^P\\}_{t=1}^n\\) 对P做self-match。使用Gated Attention-based RNN对P和P自己做match。 注意力计算 \\[ s_j^t = v^T \\tanh (W_v^P \\mathbf v_j^P + W_v^{\\bar P} \\mathbf v_t^P), \\quad j = 1, \\cdots, n \\] \\[ \\alpha_{j}^t = \\rm{softmax}(s_j^t) \\] \\[ \\mathbf c_t = \\sum_{i=1}^n \\alpha_i^t \\mathbf v_i^P \\] RNN计算 \\[ \\mathbf h_t^P = \\rm{BiRNN}(\\mathbf h_{t-1}^P, [\\mathbf v_t^P, \\mathbf c_t]^*) \\] Self-Matching根据当前p单词、Q，从整个Passage中提取信息。最终得到Passage的表达\\(H^P\\)。 Output Layer 其实就是个Pointer Network的边界模型，预测起始位置\\(p^1\\)和结束位置\\(p^2\\)。用RNN计算两次。 1. 基于Q计算初始隐状态 初始hidden state是Question的attention-pooling vector \\[ \\mathbf h_{t-1}^Q = \\mathbf r^Q \\] 基于Q的编码和一组参数\\(V_r^Q\\)，利用注意力机制计算\\(\\mathbf r^Q\\) \\[ \\mathbf r^Q = \\rm{attn}(U^Q, V_r^Q) \\] \\[ s_j = \\mathbf v^T \\tanh(W_u^Q \\mathbf u_j^Q + W_v^Q V_r^Q), \\quad j = 1, \\cdots, m \\] \\[ \\alpha_i = \\rm{softmax}(s_i) = \\frac{\\exp(s_i)}{\\sum_{j=1}^m \\exp(s_j)} \\] \\[ \\mathbf r^Q = \\sum_{i=1}^m \\alpha_i \\mathbf u_i^Q \\] 2. RNN计算开始位置和结束位置 计算t时刻的attention-pooling passage （注意力\\(\\mathbf c_t\\)） \\[ s_j^t = \\mathbf v^T \\tanh(W_h^P\\mathbf h_j^P + W_h^a \\mathbf h_{t-1}^a) \\] \\[ \\alpha_i^t = \\rm{softmax}(s_j^t) \\] \\[ \\mathbf c_t = \\sum_{i=1}^n \\alpha_i^t \\mathbf h_i^P \\] RNN前向计算 \\[ \\mathbf h_t^a = \\rm{RNN} (\\mathbf h_{t-1}^a, \\mathbf c_t) \\] 基于注意力权值去选择位置 \\[ p^t = \\arg \\max_{i}(a_i^t) \\] 实验 实现细节 数据集 训练集80%，验证集10%，测试10% 分词 斯坦福的CoreNLP中的tokenizer 词向量 预训练好的Glove Vectors。训练中保持不变。 字符向量 单层的双向GRU，末尾隐状态作为该单词的字符向量 BiRNN编码Question和Passage 3层的双向GRU Hidden Size大小 所有都是75 Dropout 每层之间的DropOut比例是0.2 优化器 AdaDelta。初始学习率为1，衰减率\\(\\beta = 0.95\\)，\\(\\epsilon = 1e^{-6}\\) 效果 对比分析","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"机器阅读","slug":"机器阅读","permalink":"http://plmsmile.github.io/tags/机器阅读/"},{"name":"Gated Attention RNN","slug":"Gated-Attention-RNN","permalink":"http://plmsmile.github.io/tags/Gated-Attention-RNN/"},{"name":"Self-Matching","slug":"Self-Matching","permalink":"http://plmsmile.github.io/tags/Self-Matching/"},{"name":"RNet","slug":"RNet","permalink":"http://plmsmile.github.io/tags/RNet/"},{"name":"Pointer Network","slug":"Pointer-Network","permalink":"http://plmsmile.github.io/tags/Pointer-Network/"}]},{"title":"Match-LSTM and Answer Pointer","date":"2018-05-09T06:20:56.000Z","path":"2018/05/09/45-match-lstm/","text":"最初的Match-LSTM阅读理解模型。 论文地址：Machine Comprehension Using Match-LSTM and Answer Pointer 背景 阅读理解任务 后面会详细补充。 传统解决问答的方法：语言分析、特征工程等，具体包括句法分析、命名实体识别、问题分类、语义分析等。 Squad数据集 答案是文章中出现的任意长度片段 Wiki文章为主 众包人工标注产生 每个问题3人标注，降低了人工标注误差 数量较大：500多篇文章，2万多个段落，10万个问题 鼓励用自己的语言提问 Match-LSTM 1. 文本蕴含任务 一个前提集合P，一个假设H。去预测P里是否能蕴含出H。 2. Match-LSTM 有K个前提\\(\\{P_1, \\cdots, P_K\\}\\)，1个假设\\(H\\)。假设的长度为m。遍历假设的每一个词汇\\(h_i\\) 在\\(h_i\\)处，利用注意力机制，综合K个前提，得到一个向量\\(p_i\\) 聚合匹配\\([h_i, p_i]\\)一起，给到LSTM 其实类似于Attention-Based NMT的解码过程。 Pointer-Net 从一个输入序列中，选择一个位置作为输出。 序列模型：选择多个位置，就组成一个序列 边界模型：选择开始和结束位置，中间的片段是答案 模型 段落\\(P\\)有m个单词，问题\\(Q\\)有n个单词。 LSTM编码层 单向LSTM编码 \\[ H^p = \\rm{LSTM}(P), \\quad H^q = \\rm{LSTM}(Q) \\] 取每一时刻的隐状态，得到对文章和问题的编码。\\(H^p \\in \\mathbb R^{m \\times h}, H^q \\in \\mathbb R^{n \\times h}\\)。\\(h\\)是编码的维度。 Match-LSTM层 这一层实际上是一个LSTM，输入依次是P中的各个单词\\(p_i\\)。每一时刻，利用注意力机制计算相对应的Q的编码。 问题--前提，段落--假设，看问题蕴含P的哪些部分。 先计算注意力权值 \\[ \\overrightarrow{ G_i} = \\tanh (W^qH^q + (W^p\\mathbf h_i^p + W^r \\overrightarrow{\\mathbf h_{i-1}^r} + \\mathbf b^p) \\otimes \\mathbf e_Q) \\] \\[ \\overrightarrow{ \\mathbf \\alpha_i} = \\rm{softmax}(\\mathbf w^T \\overrightarrow{ G_i} + b \\otimes \\mathbf e_Q) \\] 利用注意力机制，计算所有Q基于当前\\(p_i\\)的注意力，把注意力和\\(\\mathbf h_i^p\\)拼接起来 \\[ \\overrightarrow {\\mathbf z_i} = [\\mathbf h_i^p, \\underbrace{H^q \\overrightarrow{ \\mathbf \\alpha_i}}_{\\color{blue}{\\rm{attention}}}] \\] 把match后的结果，输入到LSTM， \\[ \\overrightarrow {\\mathbf h_i^r} = \\rm{LSTM}(\\overrightarrow {\\mathbf z_i}, \\overrightarrow {\\mathbf h_{i-1}^r}) \\] 定义从右向左，得到\\(\\overleftarrow {\\mathbf h_i^r}\\)。最终，拼接两个方向的向量，得到 \\[ H^r = [\\overrightarrow{H^r}, \\overleftarrow{H^r}] \\quad \\in \\mathbb R^{m \\times 2h} \\] Answer-Pointer层 输入Match-LSTM层对Passage的编码结果\\(H^r\\)，输出一个序列。 序列模型 不断生成一个序列\\(\\mathbf a = (a_1, a_2, \\cdots)\\)，表示P中的位置。 在P的末尾设置一个停止标记，如果选择它，则停止迭代。新的\\(\\bar H^r \\in \\mathbb R^{(m+1) \\times 2h}\\) 1、计算注意力权值\\(\\mathbf \\beta_k\\)，\\(\\beta_{k,j}\\)表示，选\\(p_j\\)作为\\(a_k\\)的概率 \\[ F_k = \\tanh(V \\bar H^r + (W^a \\mathbf h_{k-1}^a + \\mathbf b^a) \\otimes \\mathbf e_{(m+1)}) \\] \\[ \\mathbf \\beta_k = \\rm{softmax}(\\mathbf v^TF_k + \\mathbf c \\otimes \\mathbf e_{(m+1)}) \\] 2、使用注意力机制得到当前时刻需要的\\(H^r\\)的信息，结合上一时刻的隐状态，输入到LSTM中 \\[ \\mathbf h_k^a = \\overrightarrow{\\rm{LSTM}} ( \\underbrace{\\bar H^r \\mathbf \\beta_k^T}_{\\color{blue}{\\rm{attention}}}, \\mathbf h_{k-1}^r) \\] 答案的概率计算如下： \\[ p(\\mathbf a \\mid H^r) = \\prod_{k} p(a_k \\mid a_1, \\cdots, a_{k-1}, H^r) \\] \\[ p(a_k = j \\mid a_1, \\cdots, a_{k-1}, H^r) = \\beta_{k,j} \\] 目标函数： \\[ - \\sum_{n=1}^N \\log p(\\mathbf a_n \\mid P_n, Q_n) \\] 边界模型 不用预测完整的序列，只预测开始和结束位置就可以了。 \\[ p(\\mathbf a \\mid H^r) = p(a_s \\mid H^r) \\cdot p(a_e \\mid a_s, H^r) \\]","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"机器阅读","slug":"机器阅读","permalink":"http://plmsmile.github.io/tags/机器阅读/"},{"name":"Match-LSTM","slug":"Match-LSTM","permalink":"http://plmsmile.github.io/tags/Match-LSTM/"},{"name":"Pointer Net","slug":"Pointer-Net","permalink":"http://plmsmile.github.io/tags/Pointer-Net/"}]},{"title":"强化学习在NLP中的应用","date":"2018-05-03T06:00:55.000Z","path":"2018/05/03/44-reinforce-nlp/","text":"强化学习在自然语言处理中的应用。 阿里小蜜的任务型问答 小蜜包含QA问答、开放域聊天、任务型对话。 任务型对话 1、TaskBot：由任务驱动的多轮对话，每一轮去读取用户的slot信息，直到槽填满，全部ok 2、Action Policy：强化学习去管理多轮对话，小蜜每一轮给出一个动作，询问用户或者完成订单 3、Belief Tracker：深度学习去提取slot信息，LSTM-CRF标注 1. TaskBot 任务型对话是指由任务驱动的多轮对话。在对话中帮助用户完成某个任务，比如订机票、订酒店等。 传统：用slot filling来做，但需要大量人工模板、规则和训练语料 小蜜：基于强化学习和Neural Belief Tracker的端到端可训练的TaskBot方案 在每轮对话中，都需要抽取用户当前给的slot状态（任务需要的组件信息）。不断地去填满所有的slot，最后去下单。 2. Action Policy - 强化学习 系统如何给用户合适的回复：接着询问用户、出订单。使用强化学习去管理这个多轮对话。各个定义如下： 智能体：小蜜（系统） 策略：小蜜给用户的回答，反问哪个slot、出订单 环境：用户 状态：用户回答中提取出的slot状态（信息） 反馈：继续聊天、退出、下单 3. Belief Tracker - 深度学习 Belief Tracker用来提取用户的slot状态，实际是一个序列标注问题。使用LSTM-CRF进行标注。传统是slot filling 系统结构 系统分为下面三层。 数据预处理层 ： 分词、实体抽取等。 端到端的对话管理层 ：强化学习 任务生成层 强化学习包括： Intent Network ：处理用户输入 Neural Belief Tracker ：记录读取slot信息 Policy Network ：决定小蜜的回答：反问哪个slot 或 出订单。 Intent Network 阿里小蜜意图分类。使用CNN学一个sentence embedding来表示用户的意图。后面给到Policy Network。 Belief Tracker 使用BiLSTM-CRF来进行标记句子，提取出slot信息。 句子 first class fares from Boston to Denver Slots B-机舱类别 I-机舱类别 O O B-出发地 O B-目的地 Policy Network 四个关键：episode、reward、state、action。 1. 一轮交互的定义 episode开始：识别出用户意图为购买机票 episode结束：用户成功购买机票 或 退出会话 2. 反馈 获取用户的反馈非常关键。 收集线上用户的反馈，如用户下单、退出等行为 使用预训练环境 预训练环境的两部分反馈 Action Policy ：策略梯度 更新模型。正反馈\\(r=1\\)，负反馈\\(r=-1\\) Belief Tracker：仅使用正反馈作为正例，出现错误由小二标出正确的slots 3. 状态 当前slot：Intent Network得到的Sentence Embedding，再过Belief Tracker得到的slot信息。 使用当前slot+历史slot，过线性层，softmax，到各个Action。 4. 动作 订机票，Action是离散的。主要是：对各个Slot的反问和下单。 整体模型 符号定义 \\(q_i\\) ：当前用户的问题 \\(a_{i-1}\\) ：上一轮问题的答案 \\(S_i\\) ：历史slot信息 \\[ \\begin{align} &amp; O_i = \\rm{IntentNet}(q_i) \\\\ &amp; C_i = \\rm{BeliefTracker}(q_i, a_{i-1}) \\\\ &amp; X_i = O_i \\oplus C_i \\oplus S_{i-1} \\\\ &amp; H_i = \\rm{Linear} (X_i) \\\\ &amp; P(\\cdot) = \\rm{Softmax}(H_i) \\end{align} \\]","tags":[{"name":"强化学习","slug":"强化学习","permalink":"http://plmsmile.github.io/tags/强化学习/"},{"name":"TaskBot","slug":"TaskBot","permalink":"http://plmsmile.github.io/tags/TaskBot/"}]},{"title":"意图识别和槽填充","date":"2018-05-02T06:11:38.000Z","path":"2018/05/02/43-intent-detection-slot-filling/","text":"Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling 背景 语义理解的两个方面 1. 语义理解的两个主要方面 在对话系统中，Spoken language understanding（语言理解）很重要。主要是下面两个关键点： 理解说话人的意图 -- 意图检测（Intent Detection） 从句子中提取语义成分 -- 槽填充（Slot Filling） 2. 意图检测 意图检测是一个语义句子的分类问题。可以用SVM、DNN来进行分类。 3. 槽填充 槽填充是要读取句子中的一些语义成分，是一个序列标注问题。可以用MEMMs来做。 4. 处理 传统一般是用两个模型去分别处理意图检测和槽填充，现在可以使用一个模型Encoder-Decoder去同时解决这两个问题。 5. 对齐和注意力 序列标注具有明确的对齐信息。 输入n，输出n，相同长度。输入和输出每一个位置严格对齐。Alignment-based RNN。 输入n，输出m，不同长度，本身不具有对齐信息。需要注意力机制来进行对齐。Attention-based Encoder-Decoder。 槽填充 1. 问题 槽填充是一个序列标注问题，具有明确的对齐信息。 句子 first class fares from Boston to Denver Slots B-机舱类别 I-机舱类别 O O B-出发地 O B-目的地 意图：订机票。 本质上是学得一个映射函数\\(\\cal {X \\to Y}\\)。训练样本：\\(\\{ (\\mathbf x^{(n)}, \\mathbf y^{(n)}), n=1,\\cdots, N \\}\\)。 2. RNN 槽填充 符号定义 \\(\\mathbf x\\) ：输入序列 \\(\\mathbf y\\) ：输出序列 \\(y_t\\) ：第t个单词的slot lable 预测\\(y_t\\)，需要\\(\\mathbf x\\)和\\(y_{t-1}\\)。 训练是找到一个最大的使概率似然最大的参数\\(\\theta\\) ： \\[ \\arg \\max_{\\theta} \\prod P(y_t \\mid y_{t-1}, \\mathbf x; \\theta) \\] 预测是找到最大概率的标记序列\\(\\mathbf y\\) \\[ \\mathbf {\\hat y} = \\arg \\max_{\\mathbf y} P(\\mathbf y \\mid \\mathbf x) \\] 3. RNN Encoder-Decoder 槽填充 序列标注有明确的对齐信息，所以先没有使用注意力机制。把\\(\\mathbf x\\)编码为语义向量\\(\\mathbf c\\)： \\[ P(\\mathbf y) = \\prod_{t=1}^T P(y_t \\mid y_{t-1}, \\mathbf c) \\] Seq2Seq可以处理不同长度的映射信息，这时没有明确的对齐信息。但是可以使用注意力机制来进行软对齐Soft Alignment。 两种方法 Seq2Seq方法 Encoder-Decoder with Aligned Inputs 1. 编码 使用双向RNN对输入序列进行编码，\\(\\mathbf {h_i} = [fh_i, bh_i]\\)。 2. 意图识别 最后时刻的隐状态\\(\\mathbf {h_T}\\)携带了整个句子的信息，使用它进行意图分类。 3. 槽填充 用单向RNN作为Decoder。初始\\(\\mathbf s_0= \\mathbf h_T\\)。有3种方式： 只有注意力输入 只有对齐输入 有注意力和对齐两个输入 4. 带注意力和对齐输入的RNN槽填充计算方式 \\[ s_0 = h_T \\] 计算注意力的上下文\\(\\mathbf c_i\\) \\[ \\alpha_{ij} = \\rm{softmax}(e_{ij}) \\] \\[ e_{ij} = g(\\mathbf s_{i-1}, \\mathbf h_k) \\] \\[ \\mathbf c_i = \\sum_{j=1}^T\\alpha_{ij} \\mathbf h_j \\] 计算新的状态 \\[ s_i = f(\\mathbf s_{i-1}, y_{i-1}, \\mathbf h_i, \\mathbf c_i) \\] 基于注意力的RNN","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"注意力","slug":"注意力","permalink":"http://plmsmile.github.io/tags/注意力/"},{"name":"意图检测","slug":"意图检测","permalink":"http://plmsmile.github.io/tags/意图检测/"},{"name":"槽填充","slug":"槽填充","permalink":"http://plmsmile.github.io/tags/槽填充/"},{"name":"RNN","slug":"RNN","permalink":"http://plmsmile.github.io/tags/RNN/"},{"name":"对齐","slug":"对齐","permalink":"http://plmsmile.github.io/tags/对齐/"}]},{"title":"强化学习算法小结","date":"2018-04-24T08:28:41.000Z","path":"2018/04/24/42-reinforce-conclusion-simple/","text":"强化学习算法的简单总结，主要包括基于值函数/策略函数的学习方法、Actor-Critic算法。 强化学习笔记： 强化学习基础 、基于值函数的学习方法、基于值函数的学习方法 强化学习的目标 强化学习的目标是学习到一个策略\\(\\pi_{\\theta}(a\\mid s)\\)，来最大化这个策略的期望回报。希望智能体能够获得更多的回报。本质上是策略搜索。 \\[ J(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} [\\sum_{t=0}^{T-1}\\gamma ^tr_{t+1}] \\] \\[ J(\\theta) = \\int p_{\\theta}(\\tau) \\cdot G(\\tau) \\, {\\rm d}\\tau \\] 基于值函数的学习方法 策略迭代 已知模型。利用贝尔曼方程（算均值）迭代计算出\\(V(s)\\)，再算出\\(Q(s,a)\\)。选择最好的动作\\(a\\)去优化策略\\(\\pi(s)\\)。 \\[ \\forall s, \\quad V^\\pi(s) = E_{a \\sim \\pi(a \\mid s)}E_{s\\prime \\sim p(s\\prime \\mid s, a)}[ r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] \\[ \\forall s, \\qquad \\pi(s) = \\arg \\max_\\limits{a} Q(s, a) \\] 值迭代 已知模型。利用贝尔曼最优方程迭代算出\\(V(s)\\)，再算出\\(Q(s,a)\\)。选择最好的动作\\(a\\)去优化策略\\(\\pi(s)\\)。 \\[ \\forall s \\in S, \\quad V^*(s) = \\max_\\limits{a} E_{s^\\prime \\sim p(s^\\prime \\mid s, a)}[r(s, a, s^\\prime) + \\gamma V^*(s^\\prime)] \\] \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] \\[ \\forall s, \\quad \\pi(s) = \\arg \\max_\\limits{a} Q(s, a) \\] 蒙特卡罗 未知模型。从\\((s,a)\\)随机游走，采集N个样本。使用所有轨迹回报平均值近似估计\\(Q(s,a)\\) ，再去改进策略。重复，直至收敛。 \\[ Q^\\pi(s, a) \\approx \\hat Q^\\pi(s, a) = \\frac{1}{N} \\sum_{n=1}^NG(\\tau^{(n)}) \\] 时序差分算法 无需知道完整轨迹就能对策略进行评估。 时序差分学习=动态规划-贝尔曼估计\\(G(\\tau)\\) + 蒙特卡罗采样-增量计算\\(Q(s,a)\\) 贝尔曼估计轨迹总回报\\(G(\\tau)\\) \\[ G(\\tau) \\leftarrow r(s, a, s^\\prime) + \\gamma \\cdot Q(s^\\prime, a^\\prime) \\] 增量计算\\(Q(s,a)\\) \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (\\underbrace{ r+ \\gamma \\cdot Q(s^\\prime, a^\\prime)}_{\\color{blue}{实际值}} - \\underbrace{Q(s, a)}_{\\color{blue}{预期值}}) \\] SARSA 同策略的时序差分算法，是Q学习的改进。 1、当前状态\\(s\\)，当前动作\\(a\\) （初始时选择\\(a=\\pi^\\epsilon(s)\\)，后续是更新得到的） 2、执行动作\\(a\\)，得到新状态\\(s^\\prime\\)，得到奖励\\(r(s,a,s^\\prime)\\) 4、依概率选择新动作\\(a = \\pi^\\epsilon(s^\\prime)\\)，新状态新动作的值函数：\\(Q(s^\\prime, a^\\prime)\\) 5、更新Q函数 \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha\\cdot \\left( r + \\gamma \\cdot Q(s^\\prime, a^\\prime) - Q(s, a) \\right) \\] 6、更新状态和动作：\\(s = s^\\prime, a = a^\\prime\\) Q学习 1、当前状态\\(s\\)，选择当前动作\\(a = \\pi^\\epsilon(s)\\) 2、执行动作\\(a\\)、得到新状态\\(s^\\prime\\)和奖励 \\(r(s,a,s^\\prime)\\) 3、不依概率选择新动作，而是直接选择最大的值函数\\(\\max_\\limits{a^\\prime}Q(s^\\prime, a^\\prime)\\) 4、更新Q函数 \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha\\cdot \\left( r + \\gamma \\cdot \\max_{a^\\prime} Q(s^\\prime, a^\\prime) - Q(s, a) \\right) \\] 5、更新状态：\\(s = s^\\prime\\) Q网络 使用神经网络\\(Q_{\\phi}(\\mathbf{s,a})\\)去近似值函数\\(Q(s,a)\\)。两个问题：实际目标值不稳定；样本之间具有强相关性。 \\[ L(s, a, s^\\prime; \\phi) = \\left( \\underbrace{r + \\gamma \\cdot \\max_{a^\\prime} Q_\\phi(\\mathbf s^\\prime, \\mathbf a^\\prime)}_{\\color{blue}{实际目标值}} - \\underbrace{Q_\\phi(\\mathbf s, \\mathbf a)}_{\\color{blue}{\\text{网络值}}} \\right)^2 \\] DQN 深度Q网络： 目标网络冻结-稳定目标值。\\(Q_{\\phi}(\\mathbf{s,a})\\)训练网络，\\(Q_{\\hat \\phi}(\\mathbf{s,a})\\)目标值网络。定期更新参数\\(\\hat \\phi \\leftarrow \\phi\\) 经验池的经验回放-去除样本相关性- 每次采集一条数据放入经验池，再从经验池取数据进行训练。 生成新数据加入经验池 1、状态\\(s\\)， 选择动作\\(a = \\pi^\\epsilon(s)\\) 2、执行动作\\(a\\)， 得到\\(r\\)和\\(s^\\prime\\) 3、\\((s,a, r, s^\\prime)\\) 加入经验池\\(\\cal D\\) 采经验池中采样一条数据计算 1、从\\(\\cal D\\)中采样一条数据，\\((ss,aa, rr, ss^\\prime)\\)。 （去除样本相关性） 2、计算实际目标值\\(Q_{\\hat \\psi}(\\mathbf{ss, aa})\\)。 （解决目标值不稳定的问题） \\[ Q_{\\hat \\psi}(\\mathbf{ss, aa}) = \\begin{cases} &amp; rr, &amp; ss^\\prime 为终态 \\\\ &amp; rr + \\gamma \\cdot \\max_\\limits{\\mathbf a^\\prime}Q_{\\hat \\phi}(\\mathbf {ss^\\prime}, \\mathbf {a^\\prime}), &amp;其它 \\end{cases} \\] 3、损失函数如下，梯度下降法去训练Q网络 \\[ J(\\phi) = \\left ( Q_{\\phi}(\\mathbf {ss}, \\mathbf {aa}) - y \\right)^2 =\\left ( Q_{\\phi}(\\mathbf {ss}, \\mathbf {aa}) - Q_{\\hat \\psi}(\\mathbf{ss, aa}) \\right)^2 \\] 状态前进 \\(s \\leftarrow s^\\prime\\) 更新目标Q网络的参数 每隔C步更新：\\(\\hat \\phi \\leftarrow \\phi\\) 基于策略函数的学习方法 策略搜索本质上是一个优化问题，无需值函数可以直接优化策略。参数化的策略可以处理连续状态和动作。 策略梯度 ：是一种基于梯度的强化学习方法。 策略连续可微假设：假设\\(\\pi_{\\theta}(a \\mid s)\\)是一个关于\\(\\theta\\)的连续可微函数。 最大化策略的期望回报 \\[ J(\\theta) = \\int p_{\\theta}(\\tau) \\cdot G(\\tau) \\, {\\rm d}\\tau \\] 策略梯度 \\[ \\frac{\\partial J(\\theta)}{\\partial \\theta} \\triangleq E_{\\tau \\sim p_{\\theta}(\\tau)} \\left[ \\color{blue}{\\frac{\\partial}{\\partial \\theta} \\log p_{\\theta}(\\tau)} \\cdot G(\\tau)\\right] \\] \\[ \\frac{\\partial}{\\partial \\theta} \\log p_{\\theta}(\\tau) = \\sum_{t=0}^{T-1} \\frac{\\partial}{\\partial \\theta} \\color{blue}{ \\log\\pi_{\\theta}(a_t \\mid s_t)} \\] \\[ \\begin{align} \\frac{\\partial J(\\theta)}{\\partial \\theta} &amp; = E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[ \\sum_{t=0}^{T-1} \\left(\\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\cdot \\gamma^t G(\\tau_{t:T})\\right) \\right] \\end{align} \\] REINFORCE算法 期望用采样的方式来近似，随机采样N个轨迹。 \\[ \\begin{align} \\frac{\\partial J(\\theta)}{\\partial \\theta} &amp; \\approx \\frac{1}{N}\\sum_{n=1}^ N \\left[ \\sum_{t=0}^{T-1} \\left(\\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a^{(n)}_t \\mid s^{(n)}_t) \\cdot \\gamma^t G(\\tau^{(n)}_{t:T})\\right) \\right] \\end{align} \\] 1、根据\\(\\pi_\\theta(a\\mid s)\\)生成一条完整的轨迹 ：\\(\\tau = s_0, a_0, s_1,a_1, \\cdots, s_{T-1}, a_{T-1}, s_{T}\\) 2、在每一时刻更新参数 (0~T) 先计算当前时刻的回报\\(G(\\tau_{t:T})\\)，再更新参数： \\[ \\theta \\leftarrow \\theta + \\alpha \\cdot \\gamma^tG(\\tau_{t:T}) \\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\] 缺点： 需要完整的轨迹 不同轨迹之间的策略梯度方差大，导致训练不稳定 带基准函数的REINFORCE算法 每个时刻\\(t\\)的策略梯度 \\[ \\frac{\\partial J_t(\\theta)}{\\partial \\theta} = E_{s_t,a_t}\\left[ \\alpha \\cdot \\gamma^tG(\\tau_{t:T}) \\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\right] \\] 基准函数 为了减小策略梯度的方差 引入与\\(a_t\\)无关的基准函数\\(b(s_t) = V(s_t)\\) 越相关方差越小，所以选择值函数 每一时刻的策略梯度为： \\[ \\frac{\\partial \\hat J_t(\\theta)}{\\partial \\theta} = E_{s_t,a_t}\\left[ \\alpha \\cdot \\gamma^t \\left( \\color{blue} {G(\\tau_{t:T}) - b(s_t)}\\right)\\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\right] \\] 1、根据策略\\(\\pi_\\theta(a\\mid s)\\)生成一条完整轨迹 ：\\(\\tau = s_0, a_0, s_1,a_1, \\cdots, s_{T-1}, a_{T-1}, s_{T}\\) 2、在每一时刻更新参数 计算当前时刻的轨迹回报\\(G(\\tau_{t:T})\\) ，再利用基准函数(值函数)进行修正，得到\\(\\delta\\) \\[ \\delta \\leftarrow G(\\tau_{t:T}) - V_{\\phi} (s_t) \\] 更新值函数\\(V_\\phi(s)\\)的参数\\(\\phi\\) \\[ \\phi \\leftarrow \\phi + \\beta \\cdot \\delta \\cdot \\frac{\\partial}{ \\partial \\phi} V_{\\phi}(s_t) \\] 更新策略函数\\(\\pi_\\theta(a \\mid s)\\)的参数\\(\\theta\\) \\[ \\theta \\leftarrow \\theta + \\alpha \\cdot \\gamma^t\\delta \\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\] 缺点： 需要根据策略采集一条完整的轨迹。 Actor-Critic算法 演员-评论家算法结合了策略梯度和时序差分算法。不需要一条完整的轨迹，可以单步更新参数，无需等到回合结束才进行更新。 演员 根据\\(s\\)和策略\\(\\pi_\\theta(a\\mid s)\\)，执行动作\\(a\\)，环境变为\\(s^\\prime\\)，得到奖励\\(r\\) 评论员 根据真实奖励\\(r\\)和之前的标准，打分（估计回报）：\\(r + \\gamma V_\\phi(s^\\prime)\\) ，再调整自己的打分标准\\(\\phi\\)。\\(\\min_{\\phi} \\left(\\hat G(\\tau_{t:T}) - V_{\\phi}(s_{t}) \\right)^2\\) 使评分更加接近环境的真实回报。 演员 演员根据评论的打分，调整自己的策略\\(\\pi_\\theta\\)，争取下次做得更好。\\(\\theta \\leftarrow \\theta + \\alpha \\cdot \\gamma^t \\left( G(\\tau_{t:T}) - V_{\\phi} (s_t)\\right) \\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t)\\) 1. 执行策略，生成样本 \\[ s, a, r, s^\\prime \\] 2. 估计回报，生成\\(\\delta\\) \\[ G(s) = r + \\gamma V_\\phi(s^\\prime), \\quad \\delta = G(s) - V_{\\phi}(s) \\] 3. 更新值函数和策略 \\[ \\phi \\leftarrow \\phi + \\beta \\cdot \\delta \\cdot \\frac{\\partial V_{\\phi}(s)}{\\partial \\phi} \\] \\[ \\theta \\leftarrow \\theta + \\alpha \\cdot \\lambda \\delta \\cdot \\frac{\\partial }{\\partial \\theta} \\log \\pi_\\theta(a\\mid s) \\] 4. 更新折扣率和状态 \\[ \\lambda \\leftarrow \\lambda \\cdot \\gamma, \\quad s \\leftarrow s^\\prime \\]","tags":[{"name":"策略梯度","slug":"策略梯度","permalink":"http://plmsmile.github.io/tags/策略梯度/"},{"name":"策略迭代","slug":"策略迭代","permalink":"http://plmsmile.github.io/tags/策略迭代/"},{"name":"值迭代","slug":"值迭代","permalink":"http://plmsmile.github.io/tags/值迭代/"},{"name":"蒙特卡罗","slug":"蒙特卡罗","permalink":"http://plmsmile.github.io/tags/蒙特卡罗/"},{"name":"时序差分","slug":"时序差分","permalink":"http://plmsmile.github.io/tags/时序差分/"},{"name":"SARSA","slug":"SARSA","permalink":"http://plmsmile.github.io/tags/SARSA/"},{"name":"Q学习","slug":"Q学习","permalink":"http://plmsmile.github.io/tags/Q学习/"},{"name":"Q网络","slug":"Q网络","permalink":"http://plmsmile.github.io/tags/Q网络/"},{"name":"DQN","slug":"DQN","permalink":"http://plmsmile.github.io/tags/DQN/"}]},{"title":"基于策略函数的学习方法","date":"2018-04-22T05:44:08.000Z","path":"2018/04/22/41-strategy-learning/","text":"基于策略函数的学习方法和Actor-Critc算法。 强化学习基础 、基于值函数的学习方法 基于策略函数的学习方法 强化学习目标 强化学习的目标是学习到一个策略\\(\\pi_{\\theta}(a\\mid s)\\)，来最大化这个策略的期望回报。希望智能体能够获得更多的回报。本质上是策略搜索。 \\[ J(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} [\\sum_{t=0}^{T-1}\\gamma ^tr_{t+1}] \\] \\[ J(\\theta) = \\int p_{\\theta}(\\tau) \\cdot G(\\tau) \\, {\\rm d}\\tau \\] 策略搜索本质上是一个优化问题，无需值函数可以直接优化策略。参数化的策略可以处理连续状态和动作。 基于梯度的优化 无梯度的优化 策略梯度 1. 思想和假设 策略梯度 ：是一种基于梯度的强化学习方法。 策略连续可微假设：假设\\(\\pi_{\\theta}(a \\mid s)\\)是一个关于\\(\\theta\\)的连续可微函数。 2. 优化目标 最大化策略的期望回报。 \\[ J(\\theta) = \\int p_{\\theta}(\\tau) \\cdot G(\\tau) \\, {\\rm d}\\tau \\] 3. 策略梯度推导 采用梯度上升法来优化参数\\(\\theta\\)来使得\\(J(\\theta)\\)最大。 策略梯度\\(\\frac{\\partial J(\\theta)}{\\partial \\theta}\\)的推导如下： 1、参数\\(\\theta\\)的优化方向是总回报\\(G(\\tau)\\)大的轨迹\\(\\tau\\)，其概率\\(p_\\theta(\\tau)\\)也就越大。 \\[ \\begin{align} \\frac{\\partial J(\\theta)}{\\partial \\theta} &amp; = \\frac{\\partial} {\\partial \\theta} \\int p_{\\theta}(\\tau) \\cdot G(\\tau) \\, {\\rm d}\\tau = \\int \\left(\\frac{\\partial}{\\partial \\theta} p_{\\theta}(\\tau)\\right) \\cdot G(\\tau) \\, {\\rm d}\\tau \\\\ &amp; = \\int \\color{blue}{p_{\\theta}(\\tau)} \\cdot \\left(\\color{blue}{\\frac{1} {p_{\\theta}(\\tau)}}\\frac{\\partial}{\\partial \\theta} p_{\\theta}(\\tau)\\right) \\cdot G(\\tau) \\, {\\rm d}\\tau \\\\ &amp; = \\int p_{\\theta}(\\tau) \\cdot \\left( \\color{blue}{ \\frac{\\partial}{\\partial \\theta} \\log p_{\\theta}(\\tau) }\\right) \\cdot G(\\tau) \\, {\\rm d}\\tau \\\\ &amp; \\triangleq E_{\\tau \\sim p_{\\theta}(\\tau)} \\left[ \\color{blue}{\\frac{\\partial}{\\partial \\theta} \\log p_{\\theta}(\\tau)} \\cdot G(\\tau)\\right] \\end{align} \\] 2、梯度只和策略相关，轨迹的梯度 == 各个时刻的梯度的求和 \\[ \\begin{align} \\frac{\\partial}{\\partial \\theta} \\log p_{\\theta}(\\tau) &amp; = \\frac{\\partial}{\\partial \\theta} \\log \\left( p(s_0) \\cdot \\prod_{t=0}^{T-1} \\underbrace {\\pi_{\\theta}(a_t \\mid s_t) }_{\\color{blue}{执行动作}} \\underbrace{p(s_{t+1} \\mid s_t,a_t)}_{\\color{blue}{环境改变}}\\right) \\\\ &amp; = \\frac{\\partial}{\\partial \\theta} \\log \\left( \\log p(s_0) + \\color{blue}{\\sum_{t=0}^{T-1} \\log\\pi_{\\theta}(a_t \\mid s_t) }+ \\sum_{t=0}^{T-1} \\log p(s_{t+1} \\mid s_t,a_t) \\right) \\\\ &amp; = \\sum_{t=0}^{T-1} \\frac{\\partial}{\\partial \\theta} \\color{blue}{ \\log\\pi_{\\theta}(a_t \\mid s_t)} \\end{align} \\] 3、策略梯度 == 轨迹的梯度*轨迹的回报 的期望 \\[ \\begin{align} \\frac{\\partial J(\\theta)}{\\partial \\theta} &amp; = E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[ \\left(\\sum_{t=0}^{T-1} \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\right) \\cdot G(\\tau)\\right] \\\\ &amp; = E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[ \\left(\\sum_{t=0}^{T-1} \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\right) \\cdot \\left( \\color{blue}{G(\\tau_{1:k-1}) + \\gamma^k \\cdot G(\\tau_{k:T})}\\right) \\right] \\\\ &amp; = E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[ \\sum_{t=0}^{T-1} \\left(\\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\cdot \\gamma^t G(\\tau_{t:T})\\right) \\right] \\end{align} \\] 其中\\(G(\\tau_{t:T})​\\)是从时刻\\(t​\\)作为起始时刻收到的总回报 \\[ G(\\tau_{t:T}) = \\sum_{i=t}^{T-1} \\gamma ^{i-t} r_{i+1} \\] 4. 总结 \\[ \\frac{\\partial J(\\theta)}{\\partial \\theta} \\triangleq E_{\\tau \\sim p_{\\theta}(\\tau)} \\left[ \\color{blue}{\\frac{\\partial}{\\partial \\theta} \\log p_{\\theta}(\\tau)} \\cdot G(\\tau)\\right] \\] \\[ \\frac{\\partial}{\\partial \\theta} \\log p_{\\theta}(\\tau) = \\sum_{t=0}^{T-1} \\frac{\\partial}{\\partial \\theta} \\color{blue}{ \\log\\pi_{\\theta}(a_t \\mid s_t)} \\] \\[ \\begin{align} \\frac{\\partial J(\\theta)}{\\partial \\theta} &amp; = E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[ \\sum_{t=0}^{T-1} \\left(\\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\cdot \\gamma^t G(\\tau_{t:T})\\right) \\right] \\end{align} \\] REINFORCE算法 期望可以通过采样的方法来近似。对于当前策略\\(\\pi_\\theta\\)，可以随机游走采集N个轨迹。\\(\\tau^{(1)},\\cdots, \\tau^{(N)}\\) \\[ \\begin{align} \\frac{\\partial J(\\theta)}{\\partial \\theta} &amp; \\approx \\frac{1}{N}\\sum_{n=1}^ N \\left[ \\sum_{t=0}^{T-1} \\left(\\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a^{(n)}_t \\mid s^{(n)}_t) \\cdot \\gamma^t G(\\tau^{(n)}_{t:T})\\right) \\right] \\end{align} \\] 可微分策略函数\\(\\pi_\\theta(a\\mid s)​\\)，折扣率\\(\\gamma​\\)，学习率\\(\\alpha​\\) 初始化参数\\(\\theta\\)， 训练，直到\\(\\theta\\)收敛 1、根据\\(\\pi_\\theta(a\\mid s)\\)生成一条轨迹 \\[ \\tau = s_0, a_0, s_1,a_1, \\cdots, s_{T-1}, a_{T-1}, s_{T} \\] 2、在每一时刻更新参数 (0~T) 计算\\(G(\\tau_{t:T})\\) \\(\\theta \\leftarrow \\theta + \\alpha \\cdot \\gamma^tG(\\tau_{t:T}) \\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t)\\) REINFORCE算法缺点：不同路径之间的方差很大，导致训练不稳定；需要根据一个策略采集一条完整的轨迹。 带基准线的REINFORCE算法 值函数作为基准函数，去减小策略梯度的方差 由于不同轨迹之间的方差很大，导致训练不稳定，使用基准函数去减小策略梯度的方差。 1. 减小方差的办法 目标：估计函数\\(f\\)的期望，同时要减小\\(f\\)的方差。 方法 引入已知期望的函数\\(g\\) \\(\\hat f = f - \\alpha(g - E[g])\\) 推导可知： \\(E[f] = E[\\hat f]\\) 用\\(g\\)去减小\\(f\\)的方差， \\(D(f)=Var(f)\\) \\[ D(\\hat f) = D(f) - 2\\alpha \\cdot \\rm{Cov}(f,g) + \\alpha^2 \\cdot D(g) \\] \\[ 令\\frac{\\partial D(\\hat f)}{\\partial \\alpha} = 0 \\quad \\to \\quad \\frac{D(\\hat f)}{D(f)} = 1 - \\rho^2(f, g) \\] 所以，\\(f\\)和\\(g\\)的相关性越高，\\(D(\\hat f)\\)越小。 2. 带基准线的REINFORCE算法核心 每个时刻\\(t\\)的策略梯度 \\[ \\frac{\\partial J_t(\\theta)}{\\partial \\theta} = E_{s_t,a_t}\\left[ \\alpha \\cdot \\gamma^tG(\\tau_{t:T}) \\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\right] \\] 为了减小策略梯度的方差，引入一个和\\(a_t\\)无关的基准函数\\(b(s_t)\\)，策略梯度为： \\[ \\frac{\\partial \\hat J_t(\\theta)}{\\partial \\theta} = E_{s_t,a_t}\\left[ \\alpha \\cdot \\gamma^t \\left( \\color{blue} {G(\\tau_{t:T}) - b(s_t)}\\right)\\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\right] \\] 因为\\(b(s_t)\\)与\\(a_t\\)无关，可以证明得到：（使用积分求平均，\\(\\int_{a_t} \\pi_{\\theta}(a_t \\mid s_t) \\,{\\rm d} a_t= 1\\)） \\[ E_{a_t}\\left[ b(s_t)\\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\right] = \\frac{\\partial}{\\partial \\theta} (b(s_t) \\cdot 1) = 0 \\] 所以得到：\\(\\frac{\\partial J_t(\\theta)}{\\partial \\theta} = \\frac{\\partial \\hat J_t(\\theta)}{\\partial \\theta}​\\) 4. 基准函数的选择 为了减小策略梯度的方差，希望\\(b(s_t)\\)与\\(G(\\tau_{t:T})\\) 越相关越好，所以选择\\(b(s_t) = V(s_t)\\)（值函数）。 1、可学习的函数\\(V_{\\phi}(s_t)\\)来近似值函数，类似于Q网络 \\[ \\phi^* = \\arg \\min_{\\phi} \\left(V(s_t) - V_{\\phi}(s_t)\\right)^2 \\] 2、蒙塔卡罗方法进行估计值函数 也就是增量计算Q(s,a)嘛。 5. 带基准线的REINFORCE算法步骤 输入 状态空间和动作空间，\\(\\cal {S,A}\\) 可微分的策略函数\\(\\pi_\\theta(a \\mid s)\\) 可微分的状态值函数\\(V_{\\phi}(s_t)\\) 折扣率\\(\\gamma\\)，学习率\\(\\alpha, \\beta\\) 随机初始化参数\\(\\theta, \\phi\\) 不断训练，直到\\(\\theta\\)收敛 1、根据策略\\(\\pi_\\theta(a\\mid s)\\)生成一条完整轨迹 ：\\(\\tau = s_0, a_0, s_1,a_1, \\cdots, s_{T-1}, a_{T-1}, s_{T}\\) 2、在每一时刻更新参数 计算当前时刻的轨迹回报\\(G(\\tau_{t:T})\\) ，再利用基准函数(值函数)进行修正，得到\\(\\delta\\) \\[ \\delta \\leftarrow G(\\tau_{t:T}) - V_{\\phi} (s_t) \\] 更新值函数\\(V_\\phi(s)\\)的参数\\(\\phi\\) \\[ \\phi \\leftarrow \\phi + \\beta \\cdot \\delta \\cdot \\frac{\\partial}{ \\partial \\phi} V_{\\phi}(s_t) \\] 更新策略函数\\(\\pi_\\theta(a \\mid s)\\)的参数\\(\\theta\\) \\[ \\theta \\leftarrow \\theta + \\alpha \\cdot \\gamma^t\\delta \\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\] 缺点： 需要根据策略采集一条完整的轨迹。 Actor-Critic算法 思想 演员-评论员算法是一种结合策略梯度和时序差分学习的强化学习方法。 开始，演员随机表演，评论员随机打分；不断学习，评论员评分越来越准，演员的动作越来越好。 演员：策略函数\\(\\pi_\\theta(s,a)\\)，学习一个策略来得到尽可能高的回报 评论员：值函数\\(V_\\phi(s)\\)，评估当前策略函数（演员）的好坏 演员 根据\\(s\\)和策略\\(\\pi_\\theta(a\\mid s)\\)，执行动作\\(a\\)，环境变为\\(s^\\prime\\)，得到奖励\\(r\\) 评论员 根据真实奖励\\(r\\)和之前的标准，打分（估计回报）：\\(r + \\gamma V_\\phi(s^\\prime)\\) ，再调整自己的打分标准\\(\\phi\\)。\\(\\min_{\\phi} \\left(\\hat G(\\tau_{t:T}) - V_{\\phi}(s_{t}) \\right)^2\\) 使评分更加接近环境的真实回报。 演员 演员根据评论的打分，调整自己的策略\\(\\pi_\\theta\\)，争取下次做得更好。 优点：可以单步更新参数，不需要等到回合结束才进行更新。 值函数的三个功能 1. 估计轨迹真实回报（打分） \\[ \\hat G(\\tau_{t:T}) = r_{t+1} + \\gamma V_{\\phi}(s_{t+1}) \\] 2. 更新值函数参数\\(\\phi\\) （调整打分标准） \\[ \\min_{\\phi} \\left(\\hat G(\\tau_{t:T}) - V_{\\phi}(s_{t}) \\right)^2 \\] 3. 更新策略参数\\(\\theta\\)时，作为基函数来减少策略梯度的方差（调整策略） \\[ \\theta \\leftarrow \\theta + \\alpha \\cdot \\gamma^t \\left( G(\\tau_{t:T}) - V_{\\phi} (s_t)\\right) \\cdot \\frac{\\partial}{\\partial \\theta} \\log\\pi_{\\theta}(a_t \\mid s_t) \\] 算法实现步骤 输入 状态空间和动作空间，\\(\\cal {S,A}\\) 可微分的策略函数\\(\\pi_\\theta(a \\mid s)\\) 可微分的状态值函数\\(V_{\\phi}(s_t)\\) 折扣率\\(\\gamma\\)，学习率\\(\\alpha &gt;0, \\beta&gt;0\\) 随机初始化参数\\(\\theta, \\phi\\) 迭代直到\\(\\theta\\)收敛，初始状态\\(s\\), \\(\\lambda=1\\) 从s开始，直到\\(s\\)为终止状态 1、状态s，选择动作\\(a = \\pi_\\theta(a\\mid s)\\) 2、执行动作\\(a\\)，得到即时奖励\\(r\\)和新状态\\(s^\\prime\\) 3、利用值函数作为基函数计算\\(\\delta\\)，\\(\\delta \\leftarrow r + \\gamma V_{\\phi}(s^\\prime) - V_{\\phi}(s)\\) 4、更新值函数：\\(\\phi \\leftarrow \\phi + \\beta \\cdot \\delta \\cdot \\frac{\\partial V_{\\phi}(s)}{\\partial \\phi}\\) 5、更新策略函数：\\(\\theta \\leftarrow \\theta + \\alpha \\cdot \\lambda \\delta \\cdot \\frac{\\partial }{\\partial \\theta} \\log \\pi_\\theta(a\\mid s)\\) 6、更新折扣率和状态：\\(\\lambda \\leftarrow \\lambda \\cdot \\gamma, \\quad s \\leftarrow s^\\prime\\) 强化学习算法总结 方法总览 1. 通用步骤 执行策略，生成样本 估计回报 更新策略 2. 值函数与策略函数的比较 值函数的方法 策略更新，导致值函数的改变比较大，对收敛性有一定的影响 策略函数的方法 策略更新，更加平稳。 缺点：策略函数的解空间很大，难以进行充分采样，导致方差较大，容易陷入局部最优解。 四个典型方法 与监督学习的区别 强化学习 监督学习 样本 与环境进行交互产生样本，进行试错学习 人工收集并标注 反馈 只有奖励，并且是延迟的 需要明确的指导信息（每个状态对应一个动作）","tags":[{"name":"策略函数","slug":"策略函数","permalink":"http://plmsmile.github.io/tags/策略函数/"},{"name":"策略梯度","slug":"策略梯度","permalink":"http://plmsmile.github.io/tags/策略梯度/"},{"name":"REINFORCE","slug":"REINFORCE","permalink":"http://plmsmile.github.io/tags/REINFORCE/"},{"name":"基准函数","slug":"基准函数","permalink":"http://plmsmile.github.io/tags/基准函数/"},{"name":"Actot-Critic","slug":"Actot-Critic","permalink":"http://plmsmile.github.io/tags/Actot-Critic/"},{"name":"强化学习算法总结","slug":"强化学习算法总结","permalink":"http://plmsmile.github.io/tags/强化学习算法总结/"}]},{"title":"基于值函数的学习","date":"2018-04-21T05:14:36.000Z","path":"2018/04/21/40-value-learning/","text":"基于值函数的学习方法：贝尔曼方程，动态规划、蒙特卡洛、时续差分、Q网络。 强化学习基于值函数的学习方法。最重要要是SARSA、Q学习、DQN。但是这些都依赖于前面的动态规划和蒙特卡罗方法。 强化学习基础笔记 贝尔曼和贝尔曼最优方程 \\(V(s)\\)函数和\\(Q(s,a)\\)函数 贝尔曼方程（选择所有可能的均值） 贝尔曼最优方程（直接选择最大值） V函数与Q函数 V函数：以s为初始状态，执行策略\\(\\pi\\)得到的期望回报（所有轨迹回报的均值） \\[ V^\\pi(s) = E_{\\tau \\sim p(\\tau)} [\\sum_{t=0}^{T-1}r_{t+1} \\mid \\tau_{s_0} = s] \\] Q函数：以s为初始状态，执行动作a，执行策略\\(\\pi\\)得到的期望回报 \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] 利用V函数去计算Q函数 \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] 贝尔曼方程 \\(V(s)\\)的贝尔曼方程，选择所有a的期望回报， 也是Q函数的均值，\\(V(s)=E_a[Q(s, a)]\\) \\[ V^\\pi(s) = E_{a \\sim \\pi(a \\mid s)}E_{s\\prime \\sim p(s\\prime \\mid s, a)}[ r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] \\[ V^\\pi(s) = E_{a \\sim \\pi(a \\mid s)}[Q^\\pi(s, a)] \\] \\(Q(s,a)\\)函数的贝尔曼方程 \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma E_{a\\prime \\sim \\pi(a\\prime \\mid s\\prime)}[Q^\\pi(s\\prime, a\\prime)]] \\] 贝尔曼最优方程 \\(V(s)\\)函数的贝尔曼最优方程，实际上是直接选择所有a中的最大回报 ： \\[ V^*(s) = \\max_\\limits{a} E_{s^\\prime \\sim p(s^\\prime \\mid s, a)}[r(s, a, s^\\prime) + \\gamma V^*(s^\\prime)] \\] \\(Q(s,a)\\)函数的贝尔曼最优方程 \\[ Q^*(s, a) = E_{s^\\prime \\sim p(s^\\prime \\mid s, a)}[r(s, a, s^\\prime) + \\gamma \\max_\\limits{a\\prime}Q^*(s^\\prime, a^\\prime)] \\] 值函数的学习方法 穷举所有策略选择最好策略（没用） 迭代优化策略，根据策略的值函数去优化策略 （重点） 动态规划（已知状态转移概率\\(p(s^\\prime \\mid s, a)\\)和奖励函数\\(r(s, a, s^\\prime)\\)） 蒙特卡罗方法（不知模型），先采一些样本，再优化 时序差分学习算法：SARAS和Q学习 深度Q网络 值函数（\\(V^\\pi(s)\\)、\\(Q^\\pi(s, a)\\) ）用来对策略\\(\\pi(a \\mid s)\\)进行评估。 1. 穷举策略法 如果策略有限，可以对所有策略进行评估，选出最优策略 \\[ \\forall s, \\qquad \\pi^* = \\arg \\max_\\limits{\\pi} V^\\pi(s) \\] 策略空间\\(\\vert \\mathcal A\\vert^{\\vert \\mathcal S\\vert}\\)非常大，根本无法搜索。 2. 迭代法优化策略 步骤如下，直到收敛 随机初始化一个策略 计算该策略的值函数：动态规划， 蒙特卡罗 根据值函数来设置新的策略 比如 给一个初始策略\\(\\pi(a\\mid s)\\)， 根据\\(Q^\\pi(s, a)\\)去不断迭代去优化，得到新的策略函数\\(\\pi^\\prime (a\\mid s)\\)（确定性策略），直到收敛。 \\[ \\pi^\\prime (a\\mid s) = \\begin {cases} &amp; 1 &amp; a = \\arg \\max_\\limits{\\hat a}Q^\\pi(s, \\hat a) \\\\ &amp; 0 &amp; \\text{others}\\\\ \\end {cases} \\] 新策略的值函数会不断变大： \\[ Q^{\\pi^\\prime}(s, \\hat a) \\ge Q^\\pi(s, \\hat a) \\] 动态规划算法 总体思想 思想：已知模型，通过贝尔曼方程来迭代计算值函数，通过值函数去优化策略为固定策略 两种方法：策略迭代-贝尔曼方程（所有可能的均值），值迭代-贝尔曼最优方程（直接） 缺点：要求模型已知，效率太低 基于模型的强化学习，叫做模型相关的强化学习，或有模型的强化学习。 1. 动态规划思想 已知模型：状态转移概率\\(p(s \\prime \\mid s, a)\\) 和奖励\\(r(s, a, s\\prime)\\) 可以通过贝尔曼方程或贝尔曼最优方程来迭代计算值函数\\(V(s)\\) ，再通过\\(V(s)\\)去计算\\(Q(s,a)\\) 通过值函数来优化策略，一般为优化为固定策略\\(\\pi(s)=a\\) 2. 两种方法 策略迭代算法 ： 贝尔曼方程更新值函数，算出所有值函数，计算均值 值迭代算法：贝尔曼最优方程更新值函数，直接优化计算最大值 3. 缺点 要求模型已知：\\(p(s \\prime \\mid s, a)\\)、 \\(r(s, a, s\\prime)\\) 效率太低：可以通过神经网络来近似计算值函数 策略迭代算法 已知模型：状态转移概率\\(p(s^\\prime \\mid s, a)\\)和奖励函数\\(r(s, a, s^\\prime)\\) 使用贝尔曼方程迭代计算\\(V^{\\pi}(s)\\)，求均值， \\(V^{\\pi}(s) = E_{a}E_{s^\\prime}[r(s,a,s^\\prime)+ \\gamma V^\\pi(s\\prime)]\\) 利用\\(V^{\\pi}(s)\\)去计算\\(Q^{\\pi}(s,a)\\)。求均值，\\(Q^\\pi(s, a) = E_{s\\prime} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)]\\) 根据\\(Q^\\pi(s, a)\\)更新策略\\(\\pi(s)=a\\)，选择最好的动作a，更新为固定策略 1. 初始化策略 \\[ \\forall s, \\forall a, \\quad \\pi(a \\mid s) = \\frac{1}{\\vert \\cal A\\vert} \\] 2. 使用贝尔曼方程迭代计算值函数\\(V^\\pi(s)\\) \\[ \\forall s, \\quad V^\\pi(s) = E_{a \\sim \\pi(a \\mid s)}E_{s\\prime \\sim p(s\\prime \\mid s, a)}[ r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] 3. 利用值函数\\(V^\\pi(s)\\)计算\\(Q^\\pi(s, a)\\) \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] 4. 根据\\(Q^\\pi(s, a)\\)更新策略，选择最好的动作a，更新为固定策略 \\[ \\forall s, \\qquad \\pi(s) = \\arg \\max_\\limits{a} Q(s, a) \\] 值迭代算法 最优值函数：最优策略对应的值函数 贝尔曼最优方程：\\(V^*(s) = \\max_\\limits{a} E_{s^\\prime \\sim p(s^\\prime \\mid s, a)}[r(s, a, s^\\prime) + \\gamma V^*(s^\\prime)]\\) ，直接选择最大的a 值迭代算法：最优方程更新值函数\\(V(s)\\)， 计算\\(Q(s,a)\\)， 更新策略\\(\\pi(s)=a\\) 1. 最优值函数 最优策略\\(\\pi^*\\)对应的值函数就是最优值函数 \\[ V^*(s) = \\max_\\limits{a} Q^*(s, a) \\] 2. 贝尔曼最优方程 \\[ V^*(s) = \\max_\\limits{a} E_{s^\\prime \\sim p(s^\\prime \\mid s, a)}[r(s, a, s^\\prime) + \\gamma V^*(s^\\prime)] \\] \\[ Q^*(s, a) = E_{s^\\prime \\sim p(s^\\prime \\mid s, a)}[r(s, a, s^\\prime) + \\gamma \\max_\\limits{a\\prime}Q^*(s^\\prime, a^\\prime)] \\] 3. 值迭代算法 值迭代算法：使用贝尔曼最优方程去更新值函数，收敛时的值函数就是最优值函数，对应的策略也是最优的策略。 1、 初始化值函数 \\[ \\forall s \\in S, \\quad V(s) = 0 \\] 2、 使用贝尔曼最优方程更新\\(V(s)\\)，直到收敛 \\[ \\forall s \\in S, \\quad V^*(s) = \\max_\\limits{a} E_{s^\\prime \\sim p(s^\\prime \\mid s, a)}[r(s, a, s^\\prime) + \\gamma V^*(s^\\prime)] \\] 3、 计算\\(Q(s,a)\\) \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] 4、 更新策略\\(\\pi(s)=a\\) \\[ \\forall s, \\quad \\pi(s) = \\arg \\max_\\limits{a} Q(s, a) \\] 5、 输出策略\\(\\pi\\) 蒙特卡罗采样学习方法 蒙特卡罗方法：随机游走采集样本，估计 \\(Q^\\pi(s, a) \\approx \\frac{1}{N} \\sum_{n=1}^NG(\\tau^{(n)})\\)，根据Q去改进策略 \\(\\epsilon 贪心法\\): 依概率选择\\(\\pi^\\epsilon (s) = \\pi(s)\\) 同策略和异策略：采样与改进策略相同为同策略 需要拿到完整的轨迹才能对策略评估更新模型，效率较低 模型无关的强化学习也称为无模型的强化学习。蒙特卡罗方法： 在不知\\(p(s \\prime \\mid s, a)\\)、 \\(r(s, a, s\\prime)\\) 的情况下， 需要智能体和环境进行交互，并且收集一些样本。然后根据这些样本去求解马尔可夫决策过程最优策略 Q函数\\(Q^\\pi(s, a)\\)，初始状态为\\(s\\)， 执行动作\\(a\\)后，策略\\(\\pi\\)能得到的期望总回报。 \\[ Q^\\pi(s, a) = E_{\\tau \\sim p(\\tau)} [G(\\tau) \\mid \\tau_{s_0} = s, \\tau_{a_0} = a] \\] 模型未知，Q函数可以通过采样来计算。 1. 蒙特卡罗方法 1、从状态\\(s\\)、 动作\\(a\\)开始随机游走探索环境， 采集N个样本（N个轨迹） 2、得到N个轨迹\\(\\tau^{(1)}, \\cdots, \\tau^{(N)}\\)，得到它们的总回报\\(G(\\tau^{(1)}), \\cdots, G(\\tau^{(N)})\\) 3、利用轨迹的总回报去估计出\\(Q^\\pi(s, a)\\) 。 \\(Q^\\pi(s, a) \\approx \\hat Q^\\pi(s, a) = \\frac{1}{N} \\sum_{n=1}^NG(\\tau^{(n)})\\) 4、基于\\(Q^\\pi(s, a)\\) 去改进策略， \\(\\epsilon\\)贪心法 5、在新的策略下，再去采集样本、去估计Q，再去改进策略，直到收敛 2. 利用和探索 如果采用确定性策略 : 则每次试验得到的轨迹是一样的 只能计算出\\(Q^\\pi(s, \\pi(s))\\) ，无法计算出\\(Q^\\pi(s, a\\prime)\\)，即无法计算出其它的\\(a\\prime\\)的Q函数 只对当前环境进行了利用，而没有探索 而试验的轨迹应该覆盖所有的状态和动作，以找到更好的策略 采用\\(\\epsilon\\)贪心法 \\[ \\pi^\\epsilon(s) = \\begin {cases} &amp; \\pi(s) &amp; \\text{依概率 } 1-\\epsilon \\\\ &amp; a\\prime \\quad\\text{(随机选择)} &amp; \\text{依概率 } \\epsilon\\\\ \\end {cases} \\] 3. 同策略和异策略 同策略：采样策略\\(\\pi^\\epsilon(s)\\)， 改进策略\\(\\pi^\\epsilon(s)\\)， 相同 异策略：采样策略\\(\\pi^\\epsilon(s)\\)， 改进策略\\(\\pi(s)\\)， 不同。可以使用重要性采样、重要性权重来优化\\(\\pi\\) 时序差分学习算法 总体思想 无需知道完整轨迹就能对策略进行评估。时序差分学习=蒙特卡罗+动态规划 贝尔曼估计轨迹的回报。\\(G(\\tau_{0:T}^{(N)}) = r(s, a, s^\\prime) + \\gamma \\cdot \\hat Q^\\pi_{N-1}(s^\\prime, a^ \\prime)\\) 增量计算\\(\\hat Q_N^\\pi(s,a)\\)。 \\(\\hat Q_N^\\pi(s,a) = \\hat Q_{N-1}^\\pi(s,a) + \\alpha \\cdot \\left(G(\\tau ^{(N)}) - \\hat Q_{N-1}^\\pi(s,a) \\right)\\) 蒙特卡罗方法需要拿到完整的轨迹，才能对策略进行评估。 时序差分学习（temporal-difference learning）结合了动态规划和蒙特卡罗方法。 1. 增量计算\\(\\hat Q_N^\\pi(s,a)\\) 蒙特卡罗方法：从状态\\(s\\)，动作\\(a\\)开始，随机游走，采样N个样本 \\(G(\\tau ^{(N)})\\) ：第N次试验的总回报 \\(\\hat Q_N^\\pi(s,a)\\) ：第N次试验后值函数的平均值，推导如下： \\[ \\begin{align} \\hat Q_N^\\pi(s,a) &amp; = \\frac{1}{N} \\sum_{i=1}^NG(\\tau ^{(i)}) \\\\ &amp; = \\frac{1}{N} \\left(G(\\tau ^{(N)}) + \\color{blue}{\\sum_{i=1}^{N-1}G(\\tau ^{(i)})} \\right) \\\\ &amp; = \\frac{1}{N} \\left(G(\\tau ^{(N)}) + \\color{blue}{(N-1) \\hat Q_{N-1}^\\pi(s,a)} \\right) \\\\ &amp; = \\hat Q_{N-1}^\\pi(s,a) + \\frac{1}{N} \\left(G(\\tau ^{(N)}) - \\hat Q_{N-1}^\\pi(s,a)\\right) \\end{align} \\] 值函数\\(\\hat Q_{N}^\\pi (s, a)\\) ： 第N次后的平均 = N-1次后的平均 + 一个增量， \\(\\alpha\\)是一个较小的权值。 \\[ \\hat Q_N^\\pi(s,a) = \\hat Q_{N-1}^\\pi(s,a) + \\alpha \\cdot \\left(G(\\tau ^{(N)}) - \\hat Q_{N-1}^\\pi(s,a) \\right) \\] 增量 ：实际回报与估计回报直接的误差。 \\[ \\delta = G(\\tau ^{(N)}) - \\hat Q_{N-1}^\\pi(s,a) \\] 2. 贝尔曼方程估计\\(G(\\tau ^{(N)})\\) 从\\(s, a\\)开始，采样下一步的状态和动作\\((s^\\prime, a^\\prime)\\) ，得到奖励\\(r(s, a, s^\\prime)\\)。 无需得到完整的轨迹去计算总回报，使用贝尔曼方程去估计第N次试验后面\\((s^\\prime, a^\\prime)\\)的总回报。 使用N-1次实验后的\\(\\hat Q^\\pi_{N-1}(s^\\prime, a^\\prime)\\)，去估计第N次试验中后续\\((s^\\prime, a^\\prime)\\)的总回报 \\(G(\\tau_{1:T}^{(N)} \\mid \\tau_{s_1} = s^\\prime, \\tau_{a_1} = a^\\prime)\\)。 \\[ \\begin{align} G(\\tau_{0:T}^{(N)}) &amp; =r(s, a, s^\\prime) + \\gamma \\cdot \\color{blue}{G(\\tau_{1:T}^{(N)} \\mid \\tau_{s_1} = s^\\prime, \\tau_{a_1} = a^\\prime)}\\\\ &amp; \\approx r(s, a, s^\\prime) + \\gamma \\cdot \\color{blue}{\\hat Q^\\pi_{N-1}(s^\\prime, a^ \\prime)} \\end{align} \\] 3. 两种算法 SARSA：同策略。采样下一个动作：\\(a^\\prime = \\pi^\\epsilon (s^\\prime)\\)，值函数更新\\(Q(s^\\prime, a^\\prime)\\)，更新的Q是关于策略\\(\\pi^\\epsilon\\)的 Q学习算法：直接选择最大的值函数\\(\\max_\\limits{a^\\prime}Q(s^\\prime, a^\\prime)\\)更新，更新的Q是关于策略\\(\\pi\\)的。 4. 蒙特卡罗方法和时序差分方法比较 蒙特卡罗方法：需要完整路径才能知道总回报，不依赖马尔可夫性质 时序差分学习：只需要一步就能知道总回报，依赖于马尔可夫性质 5. 总结 贝尔曼估计总回报（马尔可夫性，动态规划） \\[ G(\\tau) \\leftarrow r(s, a, s^\\prime) + \\gamma \\cdot Q(s^\\prime, a^\\prime) \\] 增量更新值函数（蒙特卡罗） \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (G(\\tau) - Q(s, a)) \\] \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (\\underbrace{ r+ \\gamma \\cdot Q(s^\\prime, a^\\prime)}_{\\color{blue}{实际值}} - \\underbrace{Q(s, a)}_{\\color{blue}{预期值}}) \\] \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (r+ \\gamma \\cdot Q(s^\\prime, a^\\prime) - Q(s, a) \\] SARSA算法 当前\\(s, a\\)， 奖励\\(r(s, a, s^\\prime)\\)， 新的\\(s^\\prime, a^\\prime\\)， 优化\\(Q(s,a)\\) 贝尔曼估计实际奖励\\(G(\\tau)\\)：\\(r+ \\gamma \\cdot Q(s^\\prime, a^\\prime)\\) 增量计算Q：\\(Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (r+ \\gamma \\cdot Q(s^\\prime, a^\\prime) - Q(s, a))\\) 更新策略\\(\\pi(s)\\) ：\\(\\pi(s) = \\arg \\max_\\limits{a \\in \\cal A} Q(s, a)\\) SARAS：优化所有\\(Q(s,a)\\)直到收敛。对每一个\\(s,a\\)，每一步状态转移，计算Q，直到s为终止状态 SARASState Action Reward State Action算法，是一种同策略的时序差分学习算法。 1. 思想目的 要算出\\(\\hat Q_N^\\pi(s,a)\\)，只需知道下面三项： 当前状态和动作\\((s, a)\\) 得到的奖励\\(r(s, a, s^\\prime)\\) 下一步的状态和动作\\((s^\\prime, a^\\prime)\\) 不断优化Q函数，减少实际值和预期值的差距。 2. 核心计算 结合增量计算\\(\\hat Q_N^\\pi(s,a)\\) ， 贝尔曼方程估计\\(G(\\tau ^{(N)})\\) \\[ \\hat Q_N^\\pi(s,a) = \\hat Q_{N-1}^\\pi(s,a) + \\alpha \\cdot \\left(G(\\tau ^{(N)}) - \\hat Q_{N-1}^\\pi(s,a) \\right) \\] \\[ G(\\tau_{0:T}^{(N)}) = r(s, a, s^\\prime) + \\gamma \\cdot \\hat Q^\\pi_{N-1}(s^\\prime, a^ \\prime) \\] 得到： \\[ \\hat Q_N^\\pi(s,a) = (1-\\alpha)\\cdot \\hat Q_{N-1}^\\pi(s,a) + \\alpha \\cdot \\left( r(s, a, s^\\prime) + \\gamma \\cdot \\hat Q^\\pi_{N-1}(s^\\prime, a^ \\prime)\\right) \\] 简单点： \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (G(\\tau) - Q(s, a)) \\] \\[ G(\\tau) \\leftarrow r(s, a, s^\\prime) + \\gamma \\cdot Q(s^\\prime, a^\\prime) \\] \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (\\underbrace{ r+ \\gamma \\cdot Q(s^\\prime, a^\\prime)}_{\\color{blue}{实际值}} - \\underbrace{Q(s, a)}_{\\color{blue}{预期值}}) \\] 3. SARSA算法步骤 输入：状态空间\\(\\cal S\\)， 动作空间\\(\\cal A\\)，折扣率\\(\\gamma\\)， 学习率\\(\\alpha\\) 输出：策略\\(\\pi(s)\\) 1 初始化 随机初始化\\(Q(s,a)\\)，平均初始化策略\\(\\pi(s)\\) \\[ \\forall s, \\forall a, \\quad \\pi(a \\mid s) = \\frac{1}{\\vert \\cal A\\vert} \\] 2 计算所有\\(Q(s,a)\\)， 直到全部收敛 选择初始状态\\(s\\)，和动作\\(a=\\pi^\\epsilon(s)\\)。从\\((s, a)\\)开始向后执行，直到\\(s\\)为终止状态 a. 执行动作，得到新状态和新动作 当前状态\\(s\\)，动作\\(a\\) 执行动作\\(a\\)：得到奖励\\(r\\)和新状态\\(s^\\prime\\) 选择新动作：\\(a^\\prime=\\pi^\\epsilon(s^\\prime)\\) b. 增量计算 \\(Q(s, a)\\) \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha\\cdot \\left( r + \\gamma \\cdot Q(s^\\prime, a^\\prime) - Q(s, a) \\right) \\] c. 更新策略\\(\\pi(s)\\) \\[ \\pi(s) = \\arg \\max_\\limits{a \\in \\cal A} Q(s, a) \\] d. 状态前进 \\[ s \\leftarrow s^\\prime, \\quad a \\leftarrow a^\\prime \\] Q学习算法 SARAS：\\(s,a \\to r, s^\\prime\\)， 选择新状态\\(a^\\prime = \\pi^\\epsilon(s^\\prime)\\)，值函数\\(Q(s^\\prime, a^\\prime)\\) Q：\\(s\\)，选择当前动作\\(a= \\pi^\\epsilon(s)\\)，\\(\\to r, s^\\prime\\)，直接选择最大的值函数\\(\\max_\\limits{a^\\prime}Q(s^\\prime, a^\\prime)\\) SARAS是Q学习算法的一种改进。 1. SARAS 1、当前状态\\(s\\)，当前动作\\(a\\) （初始时选择\\(a=\\pi^\\epsilon(s)\\)，后续是更新得到的） 2、执行动作\\(a\\)，得到新状态\\(s^\\prime\\)，得到奖励\\(r(s,a,s^\\prime)\\) 4、依概率选择新动作\\(a = \\pi^\\epsilon(s^\\prime)\\)，新状态新动作的值函数：\\(Q(s^\\prime, a^\\prime)\\) 5、更新Q函数 \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha\\cdot \\left( r + \\gamma \\cdot Q(s^\\prime, a^\\prime) - Q(s, a) \\right) \\] 6、更新状态和动作：\\(s = s^\\prime, a = a^\\prime\\) 2. Q学习 1、当前状态\\(s\\)，选择当前动作\\(a = \\pi^\\epsilon(s)\\) 2、执行动作\\(a\\)、得到新状态\\(s^\\prime\\)和奖励 \\(r(s,a,s^\\prime)\\) 3、不依概率选择新动作，而是直接选择最大的值函数\\(\\max_\\limits{a^\\prime}Q(s^\\prime, a^\\prime)\\) 4、更新Q函数 \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha\\cdot \\left( r + \\gamma \\cdot \\max_{a^\\prime} Q(s^\\prime, a^\\prime) - Q(s, a) \\right) \\] 5、更新状态：\\(s = s^\\prime\\) 深度Q网络 Q网络 1. Q网络 \\(\\mathbf s, \\mathbf a\\) 是状态动作\\(s,a\\)的向量表达。 用函数\\(Q_\\phi(\\mathbf s, \\mathbf a) \\approx Q^\\pi (s, a)\\) 。 参数为\\(\\phi\\)的神经网络\\(Q_\\phi(\\mathbf s, \\mathbf a)\\)，则称为Q网络。输入两个向量，输出为1个实数。 \\[ Q_\\phi(\\mathbf s) = \\begin{bmatrix} Q_\\phi(\\mathbf s, \\mathbf a_1) \\\\ \\vdots \\\\ Q_\\phi(\\mathbf s, \\mathbf a_m) \\\\ \\end{bmatrix} \\approx \\begin{bmatrix} Q^\\pi(s, a_1) \\\\ \\vdots \\\\ Q^\\pi(s, a_1) \\\\ \\end{bmatrix} \\] 2. 两种逼近 学习一组参数\\(\\phi\\)使得\\(Q_\\phi(\\mathbf s, \\mathbf a)\\)逼近值函数\\(Q^\\pi(s, a)\\)。 蒙特卡罗方法：\\(\\hat Q^\\pi(s, a) = \\frac{1}{N} \\sum_{n=1}^NG(\\tau^{(n)})\\)，总回报的平均 时序差分方法：\\(E[r + \\gamma Q_\\phi(\\mathbf s^\\prime, \\mathbf a^\\prime)]\\) 3. Q学习的目标函数 以Q学习为例，采用随机梯度下降来优化，目标函数（减小差距）如下： \\[ L(s, a, s^\\prime; \\phi) = \\left( \\underbrace{r + \\gamma \\cdot \\max_{a^\\prime} Q_\\phi(\\mathbf s^\\prime, \\mathbf a^\\prime)}_{\\color{blue}{实际目标值}} - \\underbrace{Q_\\phi(\\mathbf s, \\mathbf a)}_{\\color{blue}{\\text{网络值}}} \\right)^2 \\] 一般，标记是一个标量，不包含参数；不依赖于网络参数，与网络独立。 \\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m\\left ( \\underbrace{y^{(i)}}_{\\color{blue}{\\text{实际目标值}}} - \\underbrace{f_\\theta(\\mathbf x^{(i)}) }_{\\color{blue}{网络值}} \\right)^2 \\] 两个问题： 实际目标值不稳定。参数学习的目标依赖于参数本身。label本身也包含参数 样本之间有很强的相关性 Deep Q Network 深度Q网络（deep Q-networks, DQN） 目标网络冻结。在一个时间段，固定目标值中的参数 经验回放。构建经验池来去除数据相关性 经验池。最近的经历组成的数据集 带经验回放的DQN算法 带经验回放的深度Q网络。 1. 初始化经验池、Q网络参数、目标Q网络参数 经验池： \\(\\cal D\\)，容量为N Q网络参数：\\(\\phi\\) 目标Q网络的参数：\\(\\hat \\phi = \\phi\\) 2. 要让\\(\\forall s, \\forall a, \\; Q_\\phi(\\mathbf s,\\mathbf a)\\)都收敛 每一次初始化起始状态为\\(s\\)， 遍历直到\\(s\\)为最终态 3. 每一时刻 生成新数据加入经验池 1、状态\\(s\\)， 选择动作\\(a = \\pi^\\epsilon(s)\\) 2、执行动作\\(a\\)， 得到\\(r\\)和\\(s^\\prime\\) 3、\\((s,a, r, s^\\prime)\\) 加入经验池\\(\\cal D\\) 采经验池中采样一条数据计算 1、从\\(\\cal D\\)中采样一条数据，\\((ss,aa, rr, ss^\\prime)\\)。 （去除样本相关性） 2、计算实际目标值\\(Q_{\\hat \\psi}(\\mathbf{ss, aa})\\)。 （解决目标值不稳定的问题） \\[ Q_{\\hat \\psi}(\\mathbf{ss, aa}) = \\begin{cases} &amp; rr, &amp; ss^\\prime 为终态 \\\\ &amp; rr + \\gamma \\cdot \\max_\\limits{\\mathbf a^\\prime}Q_{\\hat \\phi}(\\mathbf {ss^\\prime}, \\mathbf {a^\\prime}), &amp;其它 \\end{cases} \\] 3、损失函数如下，梯度下降法去训练Q网络 \\[ J(\\phi) = \\left ( Q_{\\phi}(\\mathbf {ss}, \\mathbf {aa}) - y \\right)^2 =\\left ( Q_{\\phi}(\\mathbf {ss}, \\mathbf {aa}) - Q_{\\hat \\psi}(\\mathbf{ss, aa}) \\right)^2 \\] 状态前进 \\(s \\leftarrow s^\\prime\\) 更新目标Q网络的参数 每隔C步更新：\\(\\hat \\phi \\leftarrow \\phi\\) 4. DQN算法中经验池的优点 1、去除样本相关性，避免陷入局部最优 经验池中抽取样本代替当前样本进行训练，打破了与相邻训练样本的相关性，避免陷入局部最优。 2、经验回放类似于监督学习 s 总结 策略迭代 已知模型。利用贝尔曼方程（算均值）迭代计算出\\(V(s)\\)，再算出\\(Q(s,a)\\)。选择最好的动作\\(a\\)去优化策略\\(\\pi(s)\\)。 \\[ \\forall s, \\quad V^\\pi(s) = E_{a \\sim \\pi(a \\mid s)}E_{s\\prime \\sim p(s\\prime \\mid s, a)}[ r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] \\[ \\forall s, \\qquad \\pi(s) = \\arg \\max_\\limits{a} Q(s, a) \\] 值迭代 已知模型。利用贝尔曼最优方程迭代算出\\(V(s)\\)，再算出\\(Q(s,a)\\)。选择最好的动作\\(a\\)去优化策略\\(\\pi(s)\\)。 \\[ \\forall s \\in S, \\quad V^*(s) = \\max_\\limits{a} E_{s^\\prime \\sim p(s^\\prime \\mid s, a)}[r(s, a, s^\\prime) + \\gamma V^*(s^\\prime)] \\] \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] \\[ \\forall s, \\quad \\pi(s) = \\arg \\max_\\limits{a} Q(s, a) \\] 蒙特卡罗 未知模型。从\\((s,a)\\)随机游走，采集N个样本。使用所有轨迹回报平均值近似估计\\(Q(s,a)\\) ，再去改进策略。重复，直至收敛。 \\[ Q^\\pi(s, a) \\approx \\hat Q^\\pi(s, a) = \\frac{1}{N} \\sum_{n=1}^NG(\\tau^{(n)}) \\] 时序差分算法 无需知道完整轨迹就能对策略进行评估。 时序差分学习=动态规划-贝尔曼估计\\(G(\\tau)\\) + 蒙特卡罗采样-增量计算\\(Q(s,a)\\) 贝尔曼估计轨迹总回报\\(G(\\tau)\\) \\[ G(\\tau) \\leftarrow r(s, a, s^\\prime) + \\gamma \\cdot Q(s^\\prime, a^\\prime) \\] 增量计算\\(Q(s,a)\\) \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (\\underbrace{ r+ \\gamma \\cdot Q(s^\\prime, a^\\prime)}_{\\color{blue}{实际值}} - \\underbrace{Q(s, a)}_{\\color{blue}{预期值}}) \\] SARSA 同策略的时序差分算法，是Q学习的改进。 1、当前状态\\(s\\)，当前动作\\(a\\) （初始时选择\\(a=\\pi^\\epsilon(s)\\)，后续是更新得到的） 2、执行动作\\(a\\)，得到新状态\\(s^\\prime\\)，得到奖励\\(r(s,a,s^\\prime)\\) 4、依概率选择新动作\\(a = \\pi^\\epsilon(s^\\prime)\\)，新状态新动作的值函数：\\(Q(s^\\prime, a^\\prime)\\) 5、更新Q函数 \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha\\cdot \\left( r + \\gamma \\cdot Q(s^\\prime, a^\\prime) - Q(s, a) \\right) \\] 6、更新状态和动作：\\(s = s^\\prime, a = a^\\prime\\) Q学习 1、当前状态\\(s\\)，选择当前动作\\(a = \\pi^\\epsilon(s)\\) 2、执行动作\\(a\\)、得到新状态\\(s^\\prime\\)和奖励 \\(r(s,a,s^\\prime)\\) 3、不依概率选择新动作，而是直接选择最大的值函数\\(\\max_\\limits{a^\\prime}Q(s^\\prime, a^\\prime)\\) 4、更新Q函数 \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha\\cdot \\left( r + \\gamma \\cdot \\max_{a^\\prime} Q(s^\\prime, a^\\prime) - Q(s, a) \\right) \\] 5、更新状态：\\(s = s^\\prime\\) Q网络 使用神经网络\\(Q_{\\phi}(\\mathbf{s,a})\\)去近似值函数\\(Q(s,a)\\)。两个问题：实际目标值不稳定；样本之间具有强相关性。 \\[ L(s, a, s^\\prime; \\phi) = \\left( \\underbrace{r + \\gamma \\cdot \\max_{a^\\prime} Q_\\phi(\\mathbf s^\\prime, \\mathbf a^\\prime)}_{\\color{blue}{实际目标值}} - \\underbrace{Q_\\phi(\\mathbf s, \\mathbf a)}_{\\color{blue}{\\text{网络值}}} \\right)^2 \\] DQN 深度Q网络： 目标网络冻结-稳定目标值。\\(Q_{\\phi}(\\mathbf{s,a})\\)训练网络，\\(Q_{\\hat \\phi}(\\mathbf{s,a})\\)目标值网络。定期更新参数\\(\\hat \\phi \\leftarrow \\phi\\) 经验池的经验回放-去除样本相关性- 每次采集一条数据放入经验池，再从经验池取数据进行训练。 生成新数据加入经验池 1、状态\\(s\\)， 选择动作\\(a = \\pi^\\epsilon(s)\\) 2、执行动作\\(a\\)， 得到\\(r\\)和\\(s^\\prime\\) 3、\\((s,a, r, s^\\prime)\\) 加入经验池\\(\\cal D\\) 采经验池中采样一条数据计算 1、从\\(\\cal D\\)中采样一条数据，\\((ss,aa, rr, ss^\\prime)\\)。 （去除样本相关性） 2、计算实际目标值\\(Q_{\\hat \\psi}(\\mathbf{ss, aa})\\)。 （解决目标值不稳定的问题） \\[ Q_{\\hat \\psi}(\\mathbf{ss, aa}) = \\begin{cases} &amp; rr, &amp; ss^\\prime 为终态 \\\\ &amp; rr + \\gamma \\cdot \\max_\\limits{\\mathbf a^\\prime}Q_{\\hat \\phi}(\\mathbf {ss^\\prime}, \\mathbf {a^\\prime}), &amp;其它 \\end{cases} \\] 3、损失函数如下，梯度下降法去训练Q网络 \\[ J(\\phi) = \\left ( Q_{\\phi}(\\mathbf {ss}, \\mathbf {aa}) - y \\right)^2 =\\left ( Q_{\\phi}(\\mathbf {ss}, \\mathbf {aa}) - Q_{\\hat \\psi}(\\mathbf{ss, aa}) \\right)^2 \\] 状态前进 \\(s \\leftarrow s^\\prime\\) 更新目标Q网络的参数 每隔C步更新：\\(\\hat \\phi \\leftarrow \\phi\\)","tags":[{"name":"值函数","slug":"值函数","permalink":"http://plmsmile.github.io/tags/值函数/"},{"name":"策略迭代","slug":"策略迭代","permalink":"http://plmsmile.github.io/tags/策略迭代/"},{"name":"值迭代","slug":"值迭代","permalink":"http://plmsmile.github.io/tags/值迭代/"},{"name":"蒙特卡罗","slug":"蒙特卡罗","permalink":"http://plmsmile.github.io/tags/蒙特卡罗/"},{"name":"时序差分","slug":"时序差分","permalink":"http://plmsmile.github.io/tags/时序差分/"},{"name":"SARSA","slug":"SARSA","permalink":"http://plmsmile.github.io/tags/SARSA/"},{"name":"Q学习","slug":"Q学习","permalink":"http://plmsmile.github.io/tags/Q学习/"},{"name":"Q网络","slug":"Q网络","permalink":"http://plmsmile.github.io/tags/Q网络/"},{"name":"DQN","slug":"DQN","permalink":"http://plmsmile.github.io/tags/DQN/"},{"name":"动态规划","slug":"动态规划","permalink":"http://plmsmile.github.io/tags/动态规划/"}]},{"title":"阅读理解模型总结","date":"2018-04-12T05:54:12.000Z","path":"2018/04/12/39-squard-models/","text":"QANet BiDAF AoA DCN","tags":[{"name":"QANet","slug":"QANet","permalink":"http://plmsmile.github.io/tags/QANet/"},{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://plmsmile.github.io/tags/机器阅读理解/"}]},{"title":"卷积神经网络总结","date":"2018-04-11T07:42:36.000Z","path":"2018/04/11/38-convolution/","text":"卷积基本概念和常见的卷积神经网络 卷积神经网络 全连接网络的两个问题： 参数太多：训练效率低、容易过拟合 局部不变形特征：全连接很难提取出图片的不变性特征 三个特性 1. 局部性 图片特征只在局部。图片特征决定图片类别，这些图片特征在一些局部的区域中。 局部连接。 2. 相同性 用同样的检测模式去检测不同图片的相同特征。只是这些特征出现在图片的不同位置。 参数共享。 3. 不变性 对于一张大图片，进行下采样，图片的性质基本保持不变。 下采样保持不变性。 卷积 一维卷积：卷积核、步长、首位0填充 三种卷积：窄卷积、宽卷积、等长卷积 二维卷积 1. 一维卷积 卷积核：参数\\([1, 0, -1]\\)就是一个卷积核 或 滤波器 步长：卷积核滑动的间隔 零填充：在输入向量两端进行补零 2. 三种卷积 输入n，卷积大小m，步长s，输入神经元各两端填补p个0 窄卷积：s=1，不补0，输出长度为n-m+1 宽卷积：s=1，两端补0，\\(p=m-1\\)， 输出长度为n+m-1 等长卷积：s=1，两端补0，\\(p=\\frac{m-1}{2}\\)， 输出长度为n 一般卷积默认为窄卷积。 3. 二维卷积 输入一张图片（假设深度为1），\\(X \\in \\mathbb R^{M \\times N}\\)， 卷积核\\(W \\in \\mathbb R ^{m \\times n}\\) ，则卷积（互相关代替）结果为： \\[ y_{ij} = \\sum_{u=1}^m \\sum_{v=1}^n w_{uv} x_{i+u-1, j+v-1} \\] 一个卷积核提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。 卷积后的结果称为特征映射（feature map）。 卷积层 一个卷积核 \\(W_p \\in \\mathbb R ^{m \\times n \\times D}\\)， 对D个通道做卷积，结果相加求和，过激活函数，得到一个特征图\\(Y^p \\in\\mathbb R^{M^\\prime \\times N^\\prime}\\) 多个卷积核：得到P个特征图 输入图片(feature map)是\\(X \\in \\mathbb R^{M \\times N \\times D}\\)，深度是D 1. 一个卷积核 用1个卷积核\\(W_p \\in \\mathbb R ^{m \\times n \\times D}\\)（实际上是D个\\(\\mathbb R^{m\\times n}\\)）去卷积这张图片（所有深度） 对各个深度的卷积结果进行相加求和，再加上偏置 过激活函数，输出最终的FM，是\\(Y^p\\) 2. 多个卷积核 多个卷积核可以提取出多种不同的特征。输入图片是\\(X \\in \\mathbb R^{M \\times N \\times D}\\)， 有P个不同的卷积核\\(W_p \\in \\mathbb R ^{m \\times n \\times D}\\)， 实际上是四维的[m, n, D, P]，后两维是in_channel、out_channel 输出P个特征图\\(\\mathbb R^{M^\\prime \\times N^\\prime \\times P}\\) 对每一个卷积核\\(W \\in \\mathbb R ^{m \\times n \\times D}\\)，对D个深度\\(\\mathbb R ^{m \\times n}\\)分别做卷积，对D个卷积结果进行求和相加，经过激活函数，得到一个特征图 \\(Y^p \\in\\mathbb R^{M^\\prime \\times N^\\prime}\\) 一共需要\\(P \\times D \\times (m \\times n) + P\\)个参数 卷积代替全连接 局部连接：卷积核只与输入的一个局部做连接，计算出FM中的一个值，局部性 权值共享：同一个卷积核与图片的各个位置进行连接，权值是一样的，提取出同样的特征 1. 局部连接 卷积层的神经元只与输入数据的一个局部区域做连接 因为图片的局部性，图片的特征在局部 FM中的每一个值，只与输入的局部相关。而不是与所有的相关 2. 权值共享 一个卷积核会分多次对输入数据的各个部分做卷积操作 对每个部分的连接参数实际上是相同的，因为是同一个卷积核 因为图片的相同性，同样的卷积核可以检测出相同的特征，只是特征在不同的位置 汇聚层 卷积层的不足：FM的维数很高 汇聚层的作用：选择特征、降低特征数量、减少参数数量、避免过拟合 两种汇聚方式：最大和平均。 1. 卷积层的不足 减少网络连接数量 但是FM中的神经元个数依然很多 如果直接接分类器全连接，则维数会很高，容易过拟合 2. 汇聚层的作用 汇聚层(pooling layer)，也作子采样层(subsampling layer)。作用是： 进行特征选择 降低特征数量 进而减少参数数量、避免过拟合 拥有更大感受野，大图片缩小，保持不变性 3. 两种汇聚方式 最大汇聚：一个区域内所有神经元的最大值 平均汇聚：一个区域内所有神经元的平均值 过大采样区会急剧减少神经元的数量，造成过多的信息损失！ 典型的卷积网络结构 由多个卷积块组成，一个卷积块： 连续2~5个卷积层，ReLU激活函数 0~1个汇聚层 目前，趋向于使用更小的卷积核，比如\\(1\\times 1, 3 \\times 3\\)。汇聚层的比例也逐渐降低，趋向于全卷积网络。 常见卷积网络 LeNet Alex Net 使用ReLU作为非线性激活函数、Dropout防止过拟合、数据增强提高模型准确率。 AlexNet分组卷积 对所有通道进行分组，进行分组卷积，执行标准卷积操作 在最后时刻才使用两个全连接融合通道的信息 降低了模型的泛化能力 Inception Net 如何选择卷积核大小非常关键： 一个卷积层同时使用多种尺寸的卷积核 先过\\(1\\times 1\\)卷积减少卷积层参数量 Inception Net由多个Inception模块堆叠而成。一个Inception同时使用\\(1\\times 1\\)、\\(3\\times 3\\)、\\(5\\times 5\\) 的卷积，如下： \\(3\\times 3\\) 、\\(5\\times 5\\) 卷积前，先进行\\(1\\times 1\\)卷积的作用： 减少输入数据的深度 减少各个深度的冗余信息，先进行一次特征抽取 后续还有各种各样的Inception Net，最终演变成Xception Net。 Inception Net的极限就是，对每个channel做一个单独的卷积。 Res Net 越深的网络可以用ResNet来训练。ResNet可以很深的原因 残差连接通过给非线性的卷积层增加直连边的方式 来提高信息的传播效率 可以减小梯度消失问题 Xception 卷积需要同时考虑所有通道吗？ 输入图片(feature map)是\\(X \\in \\mathbb R^{M \\times N \\times D}\\)，深度是D 1. 传统卷积核会同时考虑所有通道 传统1个卷积核会对所有channel的FM做同样的卷积 得到D个卷积结果 再对D个卷积结果进行相加求过激活函数得到一个FM 2. 深度可分离卷积核 Depth Separable Convolution 输入数据有D个FM，输出P个FM。深度可分离卷积(DepthWise Convolution) 如下： 对\\(X\\)的每个channel，分别做一个单独的卷积，得到D个新的FM 对D个新的FM，做\\(1\\times 1\\)的传统卷积(PointWise Convolution)，\\(P \\times D \\times (1 \\times 1)\\) 最终输出P个FM （通道数变换） 卷积操作不一定需要同时考虑通道和区域。可分离卷积。 3. 可分离卷积参数大大减小 输入通道\\(D=3\\)，输出通道\\(P=256\\)，卷积核大小为\\(3 \\times 3\\) 传统卷积参数：\\(256 \\times 3 \\times (3 \\times 3) = 6912\\) DepthWise卷积参数：\\(3 \\times 3 \\times 3 +256 \\times 3 \\times (1 \\times 1) =795\\)， 降低九分之一 同时，效果更好。 Shuffle Net 1. AlexNet分组卷积 对所有通道进行分组，进行分组卷积，执行标准卷积操作 在最后时刻才使用全连接融合通道的信息 降低了模型的泛化能力 2. ShuffleNet 分组卷积 ShuffleNet = 分组卷积（通道分组）+ 深度可分离卷积（Depthwise+PointWise） 对通道进行分组卷积时 每一个组执行深度可分离卷积，而不是标准传统卷积 每一次层叠分组卷积时，都进行channel shuffle 实际上每个组各取一个也能实现shuffle SENet Inception、ShuffleNet等网络中，对所有通道产生的特征都是不分权重直接相加求和的。 为什么所有通道的特征对模型的作用是相等的呢？ 总结 参考自知乎卷积网络中十大拍案叫绝的操作 1. 卷积核 大卷积核用多个小卷积核代替 单一尺寸卷积核用多尺寸卷积核代替 固定形状卷积核趋于用可变形卷积核 使用\\(1\\times 1\\)卷积核 2. 卷积层通道 标准卷积使用深度可分离卷积代替 使用分组卷积 分组卷积前使用channel shuffle 通道加权计算 3. 卷积层连接 使用skip connection，让模型更深 densely connection，使每一层都融合其它层的特征输出","tags":[{"name":"可分离卷积","slug":"可分离卷积","permalink":"http://plmsmile.github.io/tags/可分离卷积/"},{"name":"卷积","slug":"卷积","permalink":"http://plmsmile.github.io/tags/卷积/"},{"name":"1*1卷积","slug":"1-1卷积","permalink":"http://plmsmile.github.io/tags/1-1卷积/"},{"name":"分组卷积","slug":"分组卷积","permalink":"http://plmsmile.github.io/tags/分组卷积/"},{"name":"LeNet","slug":"LeNet","permalink":"http://plmsmile.github.io/tags/LeNet/"},{"name":"AlexNet","slug":"AlexNet","permalink":"http://plmsmile.github.io/tags/AlexNet/"},{"name":"InceptionNet","slug":"InceptionNet","permalink":"http://plmsmile.github.io/tags/InceptionNet/"},{"name":"ResNet","slug":"ResNet","permalink":"http://plmsmile.github.io/tags/ResNet/"},{"name":"XceptionNet","slug":"XceptionNet","permalink":"http://plmsmile.github.io/tags/XceptionNet/"},{"name":"ShuffleNet","slug":"ShuffleNet","permalink":"http://plmsmile.github.io/tags/ShuffleNet/"}]},{"title":"强化学习","date":"2018-04-01T01:30:47.000Z","path":"2018/04/01/37-reinforce-learning/","text":"强化学习的基础知识。基本要素、轨迹、值函数、V函数和Q函数、贝尔曼方程。 强化学习定义 概览 强化学习是指一个智能体从与环境的交互中不断学习去完成特定的目标。 强化学习不需要给出正确策略作为监督信息，只需要给出策略的（延迟）回报，并通过调整策略来取得最大化的期望回报。 智能体、环境 环境状态\\(s\\)，智能体的动作\\(a\\)，智能体的策略\\(\\pi(a\\mid s)\\)，状态转移概率\\(p(s_{t+1}\\mid s_t, a_t)\\)，即使奖励\\(r(s, a, s \\prime)\\) 智能体和环境 智能体 感知环境的状态和反馈的奖励，进行学习和决策 决策 ：根据 -- 环境状态 -- 做出不同的动作 学习： 根据 -- 反馈奖励 -- 调整策略 环境 智能体外部的所有事物 收到 -- 智能体的动作 -- 改变状态 给 -- 智能体 -- 反馈奖励 5个基本要素 状态\\(s\\) 环境的状态，状态空间\\(\\mathcal S\\)， 离散/连续 动作\\(a\\) 智能体的行为，动作空间\\(\\mathcal A\\)， 离散/连续 策略\\(\\pi(a\\mid s)\\) 智能体 根据 -- 环境状态s -- 决定下一步的动作a 的函数 状态转移概率\\(p(s \\prime \\mid s, a)\\) 根据 -- 当前状态\\(s\\)和智能体的动作\\(a\\) -- 环境状态变为\\(s \\prime\\)的概率 即时奖励\\(r(s, a, s\\prime)\\) 环境给智能体的奖励，标量函数。根据 -- 环境当前状态 、智能体执行的动作、环境新状态 智能体的策略 \\(\\pi(a \\mid s)\\) 智能体根据环境状态决定下一步的动作。分为确定性策略和随机性策略。 \\[ \\pi(a\\mid s) \\triangleq p(a\\mid s) , \\quad \\quad \\sum_{a \\in \\mathcal A} \\pi(a\\mid s) = 1 \\] 马尔科夫决策过程 马尔可夫过程，\\(p(s_{t+1}\\mid s_t)\\) 马尔可夫决策过程，\\(p(s_{t+1} \\mid s_t, a_t)\\) 轨迹，给初始状态，智能体与环境的一次交互过程 马尔可夫过程 状态序列\\(s_0, s_1, \\cdots, s_t\\)具有马尔可夫性，\\(s_{t+1}\\)只依赖于\\(s_t\\) \\[ p(s_{t+1}\\mid s_t, \\cdots, s_0) = p(s_{t+1} \\mid s_t) \\] ## 马尔可夫决策过程 \\(s_{t+1}\\)依赖于\\(s_t\\)和\\(a_t\\)， 即环境新状态依赖于当前状态和当前智能体的动作。 \\[ p(s_{t+1} \\mid s_t, a_t, \\cdots, s_0, a_0) = p(s_{t+1}\\mid s_t, a_t) \\] 智能体与环境的交互是一个马尔可夫决策过程。 轨迹 给定策略\\(\\pi(a\\mid s)\\)， 轨迹是智能体与环境的一次交互过程，是一个马尔可夫决策过程，如下： \\[ \\tau = s_0, a_0, s_1, r_1, \\cdots, s_{T-1}, a_{T-1}, s_{T}, r_{T} \\] 其中\\(r_t = r(s_{t-1}, a_{t-1}, s_t)\\)是时刻\\(t\\)的即时奖励。 轨迹的概率 初始状态 所有时刻概率的乘积 智能体执行动作，环境更新状态 \\[ p(\\tau) = p(s_0) \\prod_{t=0}^{T-1}\\pi(a_t \\mid s_t) p(s_{t+1} \\mid s_t, a_t) \\] 目标函数 一个轨迹的总回报。\\(G(\\tau) = \\sum_{t=0}^{T-1} r_{t+1}\\) 一个策略的期望回报。\\(E_{\\tau \\sim p(\\tau)} [G(\\tau)]\\)。 所有轨迹的回报的期望 强化学习的目标。学一个策略\\(\\pi_{\\theta}(a \\mid s)\\)， 最大化这个策略的期望回报 轨迹的总回报 1. 某一时刻的奖励 \\(r_t = r(s_{t-1}, a_{t-1}, s_t)\\)是\\(t\\)时刻， 环境给智能体的奖励。 给定策略\\(\\pi(a\\mid s)\\)， 智能体与环境一次交互过程(回合，试验)为轨迹\\(\\tau\\) 2. 一条轨迹的总回报 总回报是一条轨迹所有时刻的累积奖励和。 \\[ G(\\tau) = \\sum_{t=0}^{T-1} r(s_{t-1}, a_{t-1}, s_t) = \\sum_{t=0}^{T-1} r_{t+1} \\] 3. 一条轨迹的折扣回报 折扣回报引入折扣率，降低远期回报的权重（T无限大时）。 \\[ G(\\tau) = \\sum_{t=0}^{T-1} \\gamma^t r_{t+1}, \\quad \\quad \\gamma \\in [0, 1] \\] 折扣率\\(\\gamma\\) \\(\\gamma \\sim 0\\)， 在意短期回报 \\(\\gamma \\sim 1\\)， 在意长期回报 策略的期望回报 给一个策略\\(\\pi(a\\mid s)\\)， 有多个轨迹。 一个策略的期望回报：该策略下所有轨迹总回报的期望值。 \\[ E_{\\tau \\sim p(\\tau)} [G(\\tau)] =E_{\\tau \\sim p(\\tau)} [\\sum_{t=0}^{T-1}r_{t+1}] \\] 强化学习的目标 强化学习的目标是学习到一个策略\\(\\pi_{\\theta}(a\\mid s)​\\)，来最大化这个策略的期望回报。希望智能体能够获得更多的回报。 \\[ J(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} [\\sum_{t=0}^{T-1}\\gamma ^tr_{t+1}] \\] 值函数 状态值函数。\\(V^\\pi(s)\\), 初始状态为s，执行策略\\(\\pi\\)得到的期望回报。 贝尔曼方程迭代计算值函数 状态-动作值函数。\\(Q^\\pi(s, a)\\)， 初始状态为s，进行动作a，执行策略\\(\\pi\\)得到的期望回报 V函数与Q函数的关系。\\(V^\\pi(s) = E_{a \\sim \\pi(a \\mid s)}[Q^\\pi(s, a)]\\) 值函数的作用。评估策略\\(\\pi(a \\mid s)\\)， 对好的动作a（\\(Q^\\pi(s, a)\\)大 ），增大其概率\\(\\pi(a \\mid s)\\) 状态值函数 状态值函数\\(V^\\pi(s)\\)是初始状态为\\(s\\)，执行策略\\(\\pi\\)得到的期望回报。（因为有多个轨迹，每个轨迹的初始状态都是\\(\\tau_{s_0} = s\\)） \\[ V^\\pi(s) = E_{\\tau \\sim p(\\tau)} [\\sum_{t=0}^{T-1}r_{t+1} \\mid \\tau_{s_0} = s] \\] 贝尔曼方程计算值函数 当前状态的值函数，可以通过下个状态的值函数进行递推计算。 核心：\\(V^\\pi(s) \\sim r(s, a, s \\prime) + V^\\pi(s\\prime)\\)。 有动态规划的意思 关键在于状态转移：\\(s \\sim s\\prime\\) 选动作、选新状态 ： \\(s \\sim a\\)， \\(s, a \\sim s\\prime\\) 策略\\(\\pi(a\\mid s)\\) 和状态转移概率\\(p(s\\prime \\mid s, a)\\) 对这两层可能性的所有值函数，求期望即可 给定策略\\(\\pi(a\\mid s)\\)、状态转移概率\\(p(s\\prime \\mid s, a)\\)、奖励\\(r(s, a, s\\prime)\\)， 迭代计算值函数： \\[ V^\\pi(s) = E[ r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] V函数的贝尔曼方程 \\[ V^\\pi(s) = E_{a \\sim \\pi(a \\mid s)}E_{s\\prime \\sim p(s\\prime \\mid s, a)}[ r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] 状态-动作值函数 状态-动作值函数是 初始状态为\\(s\\)并进行动作\\(a\\)， 执行策略\\(\\pi\\)得到的期望总回报。 也称为Q函数。 \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] Q函数的贝尔曼方程 \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma E_{a\\prime \\sim \\pi(a\\prime \\mid s\\prime)}[Q^\\pi(s\\prime, a\\prime)]] \\] ## V函数与Q函数 \\(V(s)\\)函数要 先确定动作\\(s \\sim a\\)， 再确定新状态\\(s, a \\sim s\\prime\\) \\(Q(s,a)\\)函数是确定动作a后的V函数 V函数是所有动作a的Q函数的期望 \\[ V^\\pi(s) = E_{a \\sim \\pi(a \\mid s)}[Q^\\pi(s, a)] \\] ## 值函数的作用 值函数用来对策略\\(\\pi(a\\mid s)\\)进行评估。 如果在状态s，有一个动作a使得\\(Q^\\pi(s, a) &gt; V^\\pi(s)\\) s状态，执行动作a 比 s状态 所有动作的期望，都要好。状态a高于所有状态的平均值 说明执行动作a比当前策略\\(\\pi(a \\mid s)\\)好 调整参数使\\(\\pi(a \\mid s)\\)的概率增加 贝尔曼和贝尔曼最优方程 \\(V(s)\\)函数和\\(Q(s,a)\\)函数 贝尔曼方程（选择所有可能的均值） 贝尔曼最优方程（直接选择最大值） V函数与Q函数 V函数：以s为初始状态，执行策略\\(\\pi\\)得到的期望回报（所有轨迹回报的均值） \\[ V^\\pi(s) = E_{\\tau \\sim p(\\tau)} [\\sum_{t=0}^{T-1}r_{t+1} \\mid \\tau_{s_0} = s] \\] Q函数：以s为初始状态，执行动作a，执行策略\\(\\pi\\)得到的期望回报 \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] 利用V函数去计算Q函数 \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] 贝尔曼方程 \\(V(s)\\)的贝尔曼方程，选择所有a的期望回报， 也是Q函数的均值，\\(V(s)=E_a[Q(s, a)]\\) \\[ V^\\pi(s) = E_{a \\sim \\pi(a \\mid s)}E_{s\\prime \\sim p(s\\prime \\mid s, a)}[ r(s, a, s\\prime) + \\gamma V^\\pi(s\\prime)] \\] \\[ V^\\pi(s) = E_{a \\sim \\pi(a \\mid s)}[Q^\\pi(s, a)] \\] \\(Q(s,a)\\)函数的贝尔曼方程 \\[ Q^\\pi(s, a) = E_{s\\prime \\sim p(s\\prime \\mid s, a)} [r(s, a, s\\prime) + \\gamma E_{a\\prime \\sim \\pi(a\\prime \\mid s\\prime)}[Q^\\pi(s\\prime, a\\prime)]] \\] 贝尔曼最优方程 \\(V(s)\\)函数的贝尔曼最优方程，实际上是直接选择所有a中的最大回报 ： \\[ V^*(s) = \\max_\\limits{a} E_{s^\\prime \\sim p(s^\\prime \\mid s, a)}[r(s, a, s^\\prime) + \\gamma V^*(s^\\prime)] \\] \\(Q(s,a)\\)函数的贝尔曼最优方程 \\[ Q^*(s, a) = E_{s^\\prime \\sim p(s^\\prime \\mid s, a)}[r(s, a, s^\\prime) + \\gamma \\max_\\limits{a\\prime}Q^*(s^\\prime, a^\\prime)] \\] 深度强化学习 有些任务的状态和动作非常多，并且是连续的。普通方法很难去计算。 可以使用更复杂的函数（深度神经网络）使智能体来感知更复杂的环境状态，建立更复杂的策略。 深度强化学习 强化学习 -- 定义问题和优化目标 深度学习 -- 解决状态表示、策略表示等问题","tags":[{"name":"智能体","slug":"智能体","permalink":"http://plmsmile.github.io/tags/智能体/"},{"name":"环境","slug":"环境","permalink":"http://plmsmile.github.io/tags/环境/"},{"name":"值函数","slug":"值函数","permalink":"http://plmsmile.github.io/tags/值函数/"},{"name":"贝尔曼方程","slug":"贝尔曼方程","permalink":"http://plmsmile.github.io/tags/贝尔曼方程/"},{"name":"V函数","slug":"V函数","permalink":"http://plmsmile.github.io/tags/V函数/"},{"name":"Q函数","slug":"Q函数","permalink":"http://plmsmile.github.io/tags/Q函数/"}]},{"title":"阿里小蜜论文","date":"2018-03-31T06:23:20.000Z","path":"2018/03/31/36-alime-chat/","text":"AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine AliMe Chat 概览 1. IR Model Information Retrieval。有一个QA对知识库，给一个问题，选择最相似的问题Pair，得出答案。 缺点：很难处理那些不在QA知识库里面的Long tail问句 2. Generation Model (Seq2Seq) ：基于Question生成一个回答 缺点：会产生一些不连贯或者没意义的回答 3. 小蜜的混合模型 集成了IR和生成式模型。 收到一个句子 IR模型：从QA知识库中选择一些答案作为候选答案 打分模型：利用Attentive Seq2Seq对候选答案进行打分 最高得分大于阈值，直接输出该得分 生成式模型：否则，利用生成式模型生成一个回答 QA知识库 从用户和员工的对话数据中，提取一些问题和答案。也会把几个问题连在一起。最终共9164834个QA对。 IR模型 1. IR步骤 建立索引：每个单词 -- 多个问题 收到一个问句计算它的词集：分词 -- 去掉停用词 -- 扩展同义词 利用词集和索引去找到若干个QA对 利用BM25算法，去计算问句和所有候选QA对里问题的相似度 选择最相似的QA对 2. BM25算法 BM25通常用来搜索相关性评分。 一个query和一个d 。把query分割成\\(K\\)个语素\\(q_i\\)（中文是分词） \\[ \\rm{score}(q, d) = \\sum_{i}^K w_i \\cdot r(q_i, d) \\] \\(w_i​\\)是判断一个词与一个文档的相关性权重。这里使用IDF来计算。 \\(N\\)是总文档数，\\(N(q_i)\\)为包含词\\(q_i\\)的文档数 \\(f_i\\) 为\\(q_i\\)在d中的出现频率，\\(g_i\\)为\\(q_i\\)在\\(q\\)中的出现频率 \\(d_l\\) 为\\(d\\)的长度，\\(\\rm{avg}(d_l)\\)是所有文档的长度 \\(k_1, k_2, b\\) 为调节因子，一般\\(k_1=2,b=0.75\\) \\[ w_i = \\rm{IDF}(q_i) = \\log \\frac{N - N(q_i) + 0.5}{N(q_i) + 0.5} \\] \\[ r(q_i, d) = \\frac{f_i \\cdot (k_1 + 1)} {f_i + K} \\cdot \\frac{g_i \\cdot (k_2 +1 )}{g_i + k_2} \\] \\[ K = k_1 \\cdot (1 - b + b \\cdot \\frac{d_l}{\\rm{avg}(d_l)}) \\] 特别地，一般\\(q_i\\)只在\\(q\\)中出现一次，即\\(g_i = 1\\)， 则 \\[ r(q_i, d) = \\frac{f_i \\cdot (k_1 + 1)} {f_i + K} \\] 调节因子 K ：相同\\(f_i\\)的情况下，文档越长，相似性越低 b：越大，提高长文档与\\(q_i\\)的相似性 生成式模型 1. Attentive Seq2Seq 输入问句的语义信息： \\((h_1, h_2, \\cdots, h_m)\\) 上一时刻的单词和隐状态：\\(y_{i-1},s_{i-1}\\) 计算注意力分布：\\(\\alpha_{ij} = a(s_{i-1}, h_j)\\) 语义信息：\\(c_i = \\sum_{j=1}^m \\alpha_{ij}h_j\\) 预测当前单词：\\(p(y_i=w_i \\mid \\theta_i) = p(y_i=w_i \\mid y_1, \\cdots, y_{i-1}, c_i) = f(y_{i-1}, s_{i-1}, c_i)\\) 2. 数据padding 利用Tensorflow的Bucket Mechanism组织。选择(5,5),(5,10),(10,15),(20,30),(45,60)。 3. Softmax 训练时，softmax词表使用目标词汇+512个随机词汇。 4. BeamSearch 解码 每个时刻选择top-k(k=10) 打分模型 打分模型，对所有候选答案计算一个得分，然后选择得分最高的答案。 生成式模型，在解码时会计算各个单词的概率。打分模型和生成式模型使用同一个模型。 打分模型，计算候选回答中每个单词在Decoder时出现的概率。再求平均值作为该回答的得分。 \\[ s^{\\text{avg}(p)} = \\frac{1}{n} \\sum_{i=1}^n p(y_i = w_i \\mid \\theta_i) \\] 评价方法 5个评价规则： 语法正确 意义相关 标准的表达 上下文无关 context independent not overly generalized 答案的三个级别： 2 ：适合。满足所有规则 1： 一般。满足前三项，不满足后面两项其中一项 0：不适合 top-1概率 \\[ P_{\\rm{top}_1} = \\frac{N_{合适} + N_{一般}}{N_{所有}} \\] 阿里小蜜助手 小蜜助手 小蜜主要包括：助手(Task)服务、客户服务、聊天服务。支持声音、文本输入，支持多轮对话。 系统概览 1. 系统概览 输入层：接收多个终端和多种形式的输入 意图分类层：Rules Parser 直接解析意图，失败则通过 Intention Classifier 解析意图 处理问题组件层：语义解析、知识图引擎、信息提取引擎、Slot Filling引擎、聊天引擎 知识库：QA对，知识图。 2. 问题的信息流 收到一个问句后 1 使用Business Rules Parser （trie-based）去解析q，如果匹配到一个模式 q是一个任务型的问题（助手服务）：给Slot Filling Engine（槽填充） 直接给答案 q是一个促销活动：给到Sales Promotion ，回答准备好的答案 q是请求人工：则先询问客户有什么问题 2 没有匹配到一个模式，给到意图分类器去识别意图，也就是识别出意图的场景（比如退货、退款、人工等） 3 如果场景是要转人工，则直接转人工 4 否则，q给到语义解析器，去识别是否包含语义标签（知识图谱中的实体） 如果识别出语义标签，则通过知识图谱去找答案，如果找到直接输出 如果知识图谱没有答案 如果有上下文，结合上下文和q再去解析语义，再给到语义解析器解析语义标签 如果没有上下文，则判断是否要询问用户 如果要询问，则通过模板去询问用户 如果不用询问，则通过IR去提取信息，如果有答案，则输出；如果没有，则转人工 5 如果不包含语义标签 如果要聊天，则通过聊天引擎去产生结果 否则，通过词模板去输出结果 意图分类 对一个问句结合上下文（前面的文件）去识别出它的意图。有3个大范围： 助手。我要订机票 信息咨询、解决方案。怎么找回密码 聊天。我不开心 每一个大的范围都会进行商业细化。比如助手服务会包含订机票、手机充值。 意图分类由商业规则解析器和意图分类器组成。前者解析失败，才会执行后者 规则解析器：一颗很大的trie树，写了很多的规则 意图分类器：CNN CNN，使用fast-text训练的词向量，fine tuned in CNN 输入1：问题q 输入2：问题q和上下文(之前的问题)的语义标签 CNN的好处 也可以捕获上下文信息（前一个和后一个），足够好、够用的结果就行 CNN快啊，QPS=200，Query per Second 多个卷积池化层或者RNN能实现一个更好的结果，但是扩展性不好？为什么？ TaskBot 主要是以强化学习为中心的端到端的对话管理，由下面三个部分组成： Intent network 处理用户的输入 使用单层CNN对用户的问句进行编码，得到用户的意图语义向量 Neural belief tracker 提取记录用户的slot信息 使用BiLSTM-CRF来提取用户每次输入的slot信息 根据上一轮系统的回答和当前用户的问句生成当前的Context信息，给到后面的Policy Network 优点是：BiLSTM可以挖掘出当前词的Context信息，而CRF能有效地对标记序列进行建模 Policy networker 决定系统的操作，继续反问用户或者直接产生相应的实际操作 这也是强化学习的核心点，主要包含Episode，Reward，State和Action四个部分。 Episode 在某个场景下，识别出用户该场景的意图，则认为一个Episode开始；执行目的操作或者退出，则认为Episode结束 Reward 收集线上用户的反馈，并根据正负给出相应的Reward。特别注意要使用预训练的环境 State 结合当前新的Slot状态（Context）、历史的Slot信息和用户的当前问句信息，使用线性层+Softmax直接算出各个Actions的概率 Action Action就是系统可以给用户的一些反馈操作，比如继续询问用户、执行一个真实的操作等等。 该Taskbot的瓶颈主要是难以确定用户退出的原因，从而很难给出一些确定的惩罚。","tags":[{"name":"chatbot","slug":"chatbot","permalink":"http://plmsmile.github.io/tags/chatbot/"},{"name":"qa","slug":"qa","permalink":"http://plmsmile.github.io/tags/qa/"},{"name":"IR","slug":"IR","permalink":"http://plmsmile.github.io/tags/IR/"},{"name":"BM25","slug":"BM25","permalink":"http://plmsmile.github.io/tags/BM25/"},{"name":"seq2seq","slug":"seq2seq","permalink":"http://plmsmile.github.io/tags/seq2seq/"},{"name":"CNN","slug":"CNN","permalink":"http://plmsmile.github.io/tags/CNN/"}]},{"title":"网络优化","date":"2018-03-30T05:54:34.000Z","path":"2018/03/30/35-nerual-network-optim/","text":"参数初始化、数据预处理、逐层归一化、各种优化方法、超参数优化。 任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法 神经网络的问题 神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过优化和正则化来提升网络。 优化问题 优化问题的难点 网络是一个非凸函数，深层网络的梯度消失问题，很难优化 网络结构多样性，很难找到通用优化方法 参数多、数据大，训练效率低 参数多，存在高维变量的非凸优化 低维空间非凸优化：存在局部最优点，难在初始化参数和逃离局部最优点 高维空间非凸优化：难在如何逃离鞍点。 鞍点是梯度为0，但一些维度是最高点，另一些维度是最低点。 梯度下降法很难逃离鞍点。 梯度下降法面临的问题 如何初始化参数 预处理数据 如何选择合适的学习率，避免陷入局部最优 泛化问题 神经网络拟合能力很强，容易过拟合。解决过拟合的5个方法 参数初始化 我之前的参数初始化笔记 对称权重问题 全0产生的对称权重问题 参数千万不能全0初始化。如果全0初始化，会导致隐层神经元激活值都相同，导致深层神经元没有区分性。这就是对称权重现象。 通俗点： 每个神经元输出相同 -- BP时梯度也相同 -- 参数更新也相同 神经元之间就失去了不对称性的源头 应该对每个参数随机初始化，打破这个对称权重现象，使得不同神经元之间区分性更好。 参数区间的选择 参数太小时 使得Sigmoid激活函数丢失非线性的能力。在0附近近似线性，多层神经网络的优势也不存在。 参数太大时 Sigmoid的输入会变得很大，输出接近1。梯度直接等于0。 选择一个合适的初始化区间非常重要。如果，一个神经元输入连接很多，那么每个输入连接上的权值就应该小一些。 高斯分布初始化 高斯分布也就是正态分布。 初始化一个深度网络，比较好的方案是保持每个神经元输入的方差为一个常量。 如果神经元输入是\\(n_{in}\\)， 输出是\\(n_{out}\\)， 则按照\\(N(0, \\sqrt{\\frac {2}{n_{in} + n_{out}}})\\) 来初始化参数。 均匀分布初始化 在\\([-r, r]\\)区间内，采用均匀分布来初始化参数 Xavier均匀分布初始化 会自动计算超参数\\(r\\)， 来对参数进行\\([-r, r]\\)均匀分布初始化。 设\\(n^{l}\\)为第\\(l\\) 层神经元个数， \\(n^{l-1}\\) 是第\\(l-1\\)层神经元个数。 logsitic激活函数 ：\\(r = \\sqrt{\\frac{6}{n^{l-1} + n^l}}\\) tanh激活函数： \\(r = 4 \\sqrt{\\frac{6}{n^{l-1} + n^l}}\\) \\(l\\)层的一个神经元\\(z^l\\)，收到\\(l-1\\)层的\\(n^{l-1}\\)个神经元的输出\\(a_i^{l-1}\\), \\(i \\in [1, n^{(l-1)}]\\)。 \\[ z^l = \\sum_{i=1}^n w_i^l a_i^{l-1} \\] 为了避免初始化参数使得激活值变得饱和，尽量使\\(z^l\\)处于线性区间，即神经元的输出\\(a^l = f(z^l) \\approx z^l\\)。 假设\\(w_i^l\\)和\\(a_i^{l-1}\\)相互独立，均值均为0，则a的均值为 \\[ E[a^l] = E[\\sum_{i=1}^n w_i^l a_i^{l-1}] = \\sum_{i=1}^d E[\\mathbf w_i] E[a_i^{l-1}] = 0 \\] \\(a^l\\)的方差 \\[ \\mathrm{Var} [a^l] = n^{l-1} \\cdot \\mathrm{Var} [w_i^l] \\cdot \\mathrm{Var} [a^{l-1}_i] \\] 输入信号经过该神经元后，被放大或缩小了\\(n^{l-1} \\cdot \\mathrm{Var} [w_i^l]\\)倍。 为了使输入信号经过多层网络后，不被过分放大或过分缩小，应该使\\(n^{l-1} \\cdot \\mathrm{Var} [w_i^l]=1\\)。 综合前向和后向，使信号在前向和反向传播中都不被放大或缩小，综合设置方差： \\[ \\mathrm{Var} [w_i^l] = \\frac{2} {n^{l-1} + n^l} \\] 数据预处理 为什么要归一化 每一维的特征的来源和度量单位不同，导致特征分布不同。 未归一化数据的3个坏处 样本之间的欧式距离度量不准。取值范围大的特征会占主导作用。类似于信息增益和信息增益比 降低神经网络的训练效率 降低梯度下降法的搜索效率 未归一化对梯度下降的影响 取值范围不同：大多数位置的梯度方向不是最优的，要多次迭代才能收敛 取值范围相同：大部分位置的梯度方向近似于最优搜索方向，每一步都指向最小值，训练效率大大提高 归一化要做的事情 各个维度特征归一化到同一个取值区间 消除不同特征的相关性 标准归一化 实际上是由中心化和标准化结合的。 把数据归一化到标准正态分布。\\(X \\sim N(0, 1^2)\\) 计算均值和方差 \\[ \\mu = \\frac{1}{N} \\sum_{i=1}^n x^{(i)} \\\\ \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^n(x^{(i)} - \\mu)^2 \\] 归一化数据，减均值除以标准差。如果\\(\\sigma = 0\\)， 说明特征没有区分性，应该直接删掉。 \\[ \\hat x^{(i)} = \\frac {x^{(i)} - \\mu}{ \\sigma } \\] 缩放归一化 把数据归一化到\\([0, 1]\\) 或者\\([-1, 1]\\) 直接。 \\[ x^{(i)} = \\frac {x^{(i)} - \\min(x)}{\\max(x) - \\min (x)} \\] 白化 白化用来降低输入数据特征之间的冗余性。白化主要使用PCA来去掉特征之间的相关性。我的白化笔记 处理后的数据 特征之间相关性较低 所有特征具有相同的方差 白化的缺点 可能会夸大数据中的噪声。所有维度都拉到了相同的数值范围。可能有一些差异性小、但大多数是噪声的维度。可以使用平滑来解决。 逐层归一化 原因 深层神经网络，中间层的输入是上一层的输出。每次SGD参数更新，都会导致每一层的输入分布发生改变。 像高楼，低楼层发生较小偏移，就会导致高楼层发生较大偏移。 如果某个层的输入发生改变，其参数就需要重新学习，这也是内部协变量偏移问题。 在训练过程中，要使得每一层的输入分布保持一致。简单点，对每一个神经层进行归一化。 批量归一化 层归一化 其它方法 批量归一化 针对每一个维度，对每个batch的数据进行归一化+缩放平移。 批量归一化Batch Normalization ，我的BN详细笔记。 对每一层（单个神经元）的输入进行归一化 \\[ \\begin{align} &amp; \\mu = \\frac{1}{m} \\sum_{i=1}^m x_i &amp; \\text{求均值} \\\\ &amp; \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2 &amp; \\text{求方差} \\\\ &amp; \\hat x = \\frac{x - E(x)} {\\sqrt{\\sigma^2 + \\epsilon}} &amp; \\text{标准归一化} \\\\ &amp; y = \\gamma \\hat x+ \\beta &amp; \\text{缩放和平移} \\end{align} \\] 缩放参数\\(\\gamma\\) ，和平移参数 \\(\\beta\\) 的作用 强行归一化会破坏刚学习到的特征。用这两个变量去还原应该学习到的数据分布 归一化会聚集在0处，会减弱神经网络的非线性性质。缩放和平移解决这个问题 注意： BN是对中间层的单个神经元进行归一化 要求批量样本数量不能太小，否则难以计算单个神经元的统计信息 如果层的净输入的分布是动态变化的，则无法使用批量归一化。如循环神经网络 层归一化 对每个样本，对所有维度做一个归一化，即对同层的所有神经元的输入做归一化。 层归一化是对一个中间层的所有神经元进行归一化 批量归一化是对一个中间层的单个神经元进行归一化 设第\\(l\\)层的净输入为\\(\\mathbf z^{(l)}\\)， 求第\\(l\\)层所有输入的均值和方差 \\[ \\begin{align} &amp; \\mu^{(l)} = \\frac{1}{n^l} \\sum_{i=1}^{n^l} z_i^{(l)} &amp; \\text{第l层输入的均值} \\\\ &amp; \\sigma^{(l)^2} = \\frac{1}{n^l} \\sum_{i=1}^{n^l} (z_i^{(l)} - \\mu^{(l)})^2 &amp; \\text{第l层输入的方差} \\\\ \\end{align} \\] 层归一化 如下，其中\\(\\gamma, \\beta\\)是缩放和平移的参数向量，与\\(\\mathbf z^{(l)}\\)维数相同 \\[ \\hat {\\mathbf z}^{(l)} = \\rm{LayerNorm}_{\\gamma, \\beta} (\\mathbf z^{(l)}) = \\frac {\\mathbf z^{(l) - \\mu^{(l)}}}{\\sqrt{\\sigma ^{(l)^2} + \\epsilon}} \\cdot \\gamma + \\beta \\] 层归一化的RNN \\[ \\mathbf z_t = U\\mathbf h_{t-1} + W \\mathbf x_t \\\\ \\mathbf h_t = f (\\rm{LN}_{\\gamma, \\beta}(\\mathbf z_t))) \\] RNN的净输入一般会随着时间慢慢变大或变小，导致梯度爆炸或消失。 层归一化的RNN可以有效缓解梯度消失和梯度爆炸。 批归和层归对比 思想类似，都是标准归一化 + 缩放和平移。 批量归一化：针对每一个维度，对batch的所有数据做归一化 层归一化：针对每一个样本，对所有维度做归一化。可以用在RNN上，减小梯度消失和梯度爆炸。 其它归一化 权重归一化 对神经网络的连接权重进行归一化。 局部相应归一化 对同层的神经元进行归一化。但是局部响应归一化，用在激活函数之后，对邻近的神经元进行局部归一化。 梯度下降法的改进 梯度下降法 Mini-Batch梯度下降法。设\\(f(\\mathbf x ^{(i)}, \\theta)\\) 是神经网络。 在第\\(t\\)次迭代(epoch)时，选取\\(m\\)个训练样本\\(\\{\\mathbf x^{(i)}, y^{(i)} \\}_{i=1}^m\\)。 计算梯度\\(\\mathbf g_t\\) \\[ \\mathbf g_t = \\frac{1}{m} \\sum_{i \\in I_t} \\frac {\\partial J(y^{(i)}, f(\\mathbf x ^{(i)}, \\theta))}{\\partial \\theta} + \\lambda \\|\\theta\\| ^2 \\] 更新参数，其中学习率\\(\\alpha \\ge 0\\) ： \\[ \\theta_t = \\theta_{t-1} - \\alpha \\mathbf g_t \\] \\[ \\theta_t = \\theta_{t-1}+ \\Delta \\theta_t \\] 1. BGD Batch Gradient Descent 意义：每一轮选择所有整个数据集去计算梯度更新参数 优点 凸函数，可以保证收敛到全局最优点；非凸函数，保证收敛到局部最优点 缺点 批量梯度下降非常慢。因为在整个数据集上计算 训练次数多时，耗费内存 不允许在线更新模型，例如更新实例 2. SGD Stochastic Gradient Descent 意义：每轮值选择一条数据去计算梯度更新参数 优点 算法收敛快（BGD每轮会计算很多相似样本的梯度，冗余的） 可以在线更新 有一定几率跳出比较差的局部最优而到达更好的局部最优或者全局最优 缺点 容易收敛到局部最优，并且容易困在鞍点 3. Mini-BGD Mini-Batch Gradient Descent 意义： 每次迭代只计算一个mini-batch的梯度去更新参数 优点 计算效率高，收敛较为稳定 缺点 更新方向依赖于当前batch算出的梯度，不稳定 4. 梯度下降法的难点 学习率\\(\\alpha\\)难以选择。太小，导致收敛缓慢；太大，造成较大波动妨碍收敛 学习率一直相同是不合理的。出现频率低的特征，大学习率；出现频率小的特征，小学习率 按迭代次数和loss阈值在训练时去调整学习率。然而次数和阈值难以设定，无法适应所有数据 很难逃离鞍点。梯度为0，一些特征是最高点（上升），一些特征是最低点（下降） 更新方向依赖于当前batch算出的梯度，不稳定 主要通过学习率递减和动量法来优化梯度下降法。 可以看出 SGD，整体下降，但局部会来回震荡 MBGD，一个batch来说，batch越大，下降越快，越平滑 MBGD，整体来说，batch越小，下降越明显 学习率递减 0 指数加权平均 求10天的平均温度，可以直接利用平均数求，每天的权值是一样的，且要保存所有的数值才能计算。 \\[ v_{avg} = \\frac {v_1 + \\cdots + v_{100}}{100} \\] 设\\(v_t\\)是到第t天的平均温度，\\(\\theta_t\\)是第t天的真实温度，\\(\\beta=0.9\\)是衰减系数。 则有指数加权平均： \\[ v_t = \\beta * v_{t-1} + (1-\\beta) \\theta_t \\] \\[ v_{100} = 0.1 \\cdot \\theta_{100} + 0.1(0.9)^1 \\cdot \\theta_{99} + 0.1 (0.9)^2 \\cdot \\theta_{98} + 0.1(0.9)^3 \\cdot \\theta_{97} + \\ldots \\] 离当前越近，权值越大。越远，权值越小（指数递减），也有一定权值。 1. 按迭代次数递减 设置\\(\\beta = 0.96\\)为衰减率 反时衰减 \\[ \\alpha_t = \\alpha_0 \\cdot \\frac {1} {1 + \\beta \\times t} \\] 指数衰减 : \\[ \\alpha_t = \\alpha_0 \\cdot \\beta^t \\] 自然指数衰减 \\[ \\alpha_t = \\alpha_0 \\cdot e^{-\\beta \\cdot t} \\] 2. AdaGrad Adaptive Gradient 意义：每次迭代时，根据历史梯度累积量来减小学习率，减小梯度。梯度平方的累计值来减小梯度 初始学习率\\(\\alpha_0\\)不变，实际学习率减小。\\(\\alpha = \\frac {\\alpha_0} {\\sqrt {G_t + \\epsilon}}\\) \\[ G_t = \\sum_{i=1}^t g_i^2 \\] \\[ \\Delta \\theta_t = - \\frac {\\alpha_0}{\\sqrt {G_t + \\epsilon}} \\cdot g_t \\] 优点 累积梯度\\(G_t\\)的\\(\\frac{1}{\\sqrt{G_t + \\epsilon}}\\)实际上构成了一个约束项 前期\\(G_t\\)较小， 约束值大，能够放大梯度 后期\\(G_t\\)较大， 约束值小，能够约束梯度 适合处理稀疏梯度 缺点 经过一些迭代，学习率会变非常小，参数难以更新。过早停止训练 依赖于人工设置的全局学习率\\(\\alpha_0\\) \\(\\alpha_0\\)设置过大，约束项大，则对梯度的调节太大 3. RMSprop 意义：计算梯度\\(\\mathbf g_t\\)平方的指数递减移动平均， 即梯度平方的平均值来减小梯度 \\[ G_t = \\beta G_{t-1} + (1-\\beta) \\cdot \\mathbf g_t^2 \\] \\[ \\Delta \\theta_t = - \\frac {\\alpha_0}{\\sqrt {G_t + \\epsilon}} \\cdot \\mathbf g_t \\] 优点 解决了AdaGrad学习率一直递减过早停止训练的问题，学习率可大可小 训练初中期，加速效果不错，很快；训练后期，反复在局部最小值抖动 适合处理非平稳目标，对于RNN效果很好 缺点 依然依赖于全局学习率\\(\\alpha_0\\) 4. AdaDelta 意义 不初始化学习率。计算梯度更新差平方的指数衰减移动平均来作为分子学习率， \\[ G_t = \\beta G_{t-1} + (1-\\beta) \\cdot \\mathbf g_t^2 \\] \\[ \\Delta X_{t-1}^2 = \\beta \\Delta X_{t-2}^2 + (1-\\beta) \\Delta \\theta_{t-1}^2 \\] \\[ \\Delta \\theta_t = - \\frac { \\sqrt {\\Delta X_{t-1}^2 + \\epsilon}}{\\sqrt {G_t + \\epsilon}} \\cdot \\mathbf g_t \\] 优点 初始学习率\\(\\alpha_0\\)改成了动态计算的\\(\\sqrt {\\Delta X_{t-1}^2 + \\epsilon}\\) ，一定程度上平抑了学习率的波动。 动量法 结合前面更新的方向和当前batch的方向，来更新参数。 解决了MBGD的不稳定性，增加了稳定性。可以加速或者减速。 1. 普通动量法 设\\(\\rho = 0.9\\)为动量因子，计算负梯度的移动加权平均 \\[ \\Delta \\theta_t = \\rho \\cdot \\Delta \\theta_{t-1} - \\alpha \\cdot \\mathbf g_t \\] 当前梯度与最近时刻的梯度方向： 前后梯度方向一致：参数更新幅度变大，会加速 前后梯度方向不一致：参数更新幅度变小，会减速 优点： 迭代初期，梯度方向一致，动量法加速，更快到达最优点 迭代后期，梯度方向不一致，在收敛值附近震荡，动量法会减速，增加稳定性 当前梯度叠加上上次的梯度，可以近似地看成二阶梯度。 Adam Adaptive Momentum Estimation = RMSProp + Momentum， 即自适应学习率+稳定性（动量法）。 意义：计算梯度\\(\\mathbf g_t\\)的指数权值递减移动平均(动量)，计算梯度平方\\(\\mathbf g_t^2\\)的指数权值递减移动平均(自适应alpha) 设\\(\\beta_1 = 0.9\\)， \\(\\beta_2 = 0.99\\) 为衰减率 \\[ M_t = \\beta_1M_{t-1} + (1-\\beta_1) \\mathbf g_t \\quad \\quad \\sim E(\\mathbf g_t) \\] \\[ G_t = \\beta_2 G_{t-1} + (1-\\beta_2) \\mathbf g_t^2 \\quad \\quad \\sim E(\\mathbf g_t^2) \\] \\[ \\hat M_t = \\frac {M_t}{1 - \\beta_1^t}, \\quad \\hat G_t = \\frac{G_t}{1 - \\beta_2^t} \\quad \\quad \\text{初始化偏差修正} \\] \\[ \\Delta \\theta_t = - \\frac {\\alpha_0}{\\sqrt{\\hat G_t + \\epsilon}} \\hat M_t \\] 优点 有RMSprop的处理非稳态目标的优点，有Adagrad处理稀疏梯度的优点 对内存需求比较小，高效地计算 为不同的参数计算不同的自适应学习率 适用于大多数的非凸优化 超参数好解释，只需极少量的调参 梯度截断 一般按模截断，如果\\(\\|\\mathbf g_t\\|^2 &gt; b\\)， 则 \\[ \\mathbf g_t = \\frac{b}{\\|\\mathbf g_t\\|} \\mathbf g_t \\] 超参数优化 优化内容和难点 优化内容 网络结构：神经元之间连接关系、层数、每层的神经元数量、激活函数类型等 优化参数：优化方法、学习率、小批量样本数量 正则化系数 优化难点 参数优化是组合优化问题，没有梯度下降法来优化，没有通用的有效的方法 评估一组超参数配置的实际代价非常高 配置说明 有\\(K\\)个超参数， 每个超参数配置表示为1个向量\\(\\mathbf x \\in X\\) \\(f(\\mathbf x)\\) 是衡量超参数配置\\(\\mathbf x\\)效果的函数 \\(f(\\mathbf x)\\)不是\\(\\mathbf x\\)的连续函数，\\(\\mathbf x\\)也不同。 无法使用梯度下降等优化方法 超参数设置-搜索 超参数设置：人工搜索、网格搜索、随机搜索。 缺点：没有利用到不同超参数组合之间的相关性，搜索方式都比较低效。 1. 网格搜索 对于\\(K\\)个超参数，第\\(k\\)个参数有\\(m_k\\)种取值。总共的配置数量： \\[ N = m_1 \\times m_2 \\times \\cdots \\times m_K \\] 如果超参数是连续的，可以根据经验选择一些经验值，比如学习率 \\[ \\alpha \\in \\{0.01, 0.1, 0.5, 1.0\\} \\] 对这些超参数的不同组合，分别训练一个模型，测试在开发集上的性能。选取一组性能最好的配置。 2. 随机搜索 有的超参数对模型影响力有限（正则化），有的超参数对模型性能影响比较大。网格搜索会遍历所有的可能性。 随机搜索：对超参数进行随机组合，选择一个性能最好的配置。 优点：比网格搜索好，更容易实现，更有效。 贝叶斯优化 根据当前已经试验的超参数组合，来预测下一个可能带来的最大收益的组合。 贝叶斯优化过程：根据已有的N组试验结果来建立高斯过程，计算\\(f(\\mathbf x)\\)的后验分布。 动态资源分配 在早期阶段，估计出一组配置的效果会比较差，则中止这组配置的评估。把更多的资源留给其他配置。 这是多臂赌博机的泛化问题：最优赌博机。在给定有限次数的情况下，玩赌博机，找到收益最大的臂。 神经架构搜索 通过神经网络来自动实现网络架构的设计。 变长字符串 -- 描述神经网络的架构 控制器 -- 生成另一个子网络的架构描述 控制器 -- RNN来实现 控制器训练 -- 强化学习来完成 奖励信号 -- 生成的子网络在开发集上的准确率","tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://plmsmile.github.io/tags/神经网络/"},{"name":"数据预处理","slug":"数据预处理","permalink":"http://plmsmile.github.io/tags/数据预处理/"},{"name":"归一化","slug":"归一化","permalink":"http://plmsmile.github.io/tags/归一化/"},{"name":"优化方法","slug":"优化方法","permalink":"http://plmsmile.github.io/tags/优化方法/"},{"name":"Adam","slug":"Adam","permalink":"http://plmsmile.github.io/tags/Adam/"},{"name":"动量法","slug":"动量法","permalink":"http://plmsmile.github.io/tags/动量法/"},{"name":"学习率","slug":"学习率","permalink":"http://plmsmile.github.io/tags/学习率/"}]},{"title":"各种注意力总结","date":"2018-03-25T06:14:12.000Z","path":"2018/03/25/33-attention-summary/","text":"一切都应该尽可能简单，但不能过于简单。 本文主要是总结：注意力机制、注意力机制的变体、论文中常见的注意力 注意力机制系统介绍 问题背景 计算能力不足 神经网络有很强的能力。但是对于复杂任务，需要大量的输入信息和复杂的计算流程。计算机的计算能力是神经网络的一个瓶颈。 减少计算复杂度 常见的：局部连接、权值共享、汇聚操作。 但仍然需要：尽量少增加模型复杂度（参数），来提高模型的表达能力。 简单文本分类可以使用单向量表达文本 只需要一些关键信息即可，所以一个向量足以表达一篇文章，可以用来分类。 阅读理解需要所有的语义 文章比较长时，一个RNN很难反应出文章的所有语义信息。 对于阅读理解任务来说，编码时并不知道会遇到什么问题。这些问题可能会涉及到文章的所有信息点，如果丢失任意一个信息就可能导致无法正确回答问题。 网络容量与参数成正比 神经网络中可以存储的信息称为网络容量。 存储的多，参数也就越多，网络也就越复杂。 LSTM就是一个存储和计算单元。 注意力和记忆力解决信息过载问题 输入的信息太多(信息过载问题)，但不能同时处理这些信息。只能选择重要的信息进行计算，同时用额外空间进行信息存储。 信息选择：聚焦式自上而下地选择重要信息，过滤掉无关的信息。注意力机制 外部记忆 ： 优化神经网络的记忆结构，使用额外的外部记忆，来提高网络的存储信息的容量。 记忆力机制 比如，一篇文章，一个问题。答案只与几个句子相关。所以只需把相关的片段挑选出来交给后续的神经网络来处理，而不需要把所有的文章内容都给到神经网络。 注意力 注意力机制Attention Mechanism 是解决信息过载的一种资源分配方案，把计算资源分配给更重要的任务。 注意力：人脑可以有意或无意地从大量的输入信息中，选择小部分有用信息来重点处理，并忽略其它信息 聚焦式注意力 自上而下有意识的注意力。有预定目的、依赖任务、主动有意识的聚焦于某一对象的注意力。 一般注意力值聚焦式注意力。聚焦式注意力会根据环境、情景或任务的不同而选择不同的信息。 显著性注意力 自下而上无意识的注意力。由外界刺激驱动的注意力，无需主动干预，也和任务无关。如Max Pooling和Gating。 鸡尾酒效应 鸡尾酒效应可以理解这两种注意力。 在吵闹的酒会上 噪音很多，依然可以听到朋友谈话的内容 没有关注背景声音，但是突然有人叫自己（重要信息），依然会马上注意到 普通注意力机制 把目前的最大汇聚Max Pooling和门控Gating 近似地看做自下而上的基于显著性的注意力机制。 为了节省资源，选择重要的信息给到后续的神经网络进行计算，而不需要把所有的内容都给到后面的神经网络。 输入N个信息 \\(X_{1:N} = [\\mathbf{x}_1, \\cdots, \\mathbf{x}_N]\\)， 问题\\(\\mathbf{q}\\)。 要从\\(X\\)中选择一些和任务相关的信息输入给神经网络。 计算注意力分布 \\(\\alpha_i\\) : 选择第\\(i\\)个信息的概率，也称为注意力分布 ，\\(z\\)表示被选择信息的索引位置 \\[ \\alpha_i = p(z = i \\mid X, \\mathbf{q}) = \\rm{softmax}\\left(s(\\mathbf{x}_i, \\mathbf{q})\\right) = \\frac{\\exp\\left(s(\\mathbf{x}_i, \\mathbf{q})\\right)}{\\sum_{j=1}^N \\exp\\left(s(\\mathbf{x}_j, \\mathbf{q})\\right)} \\] NMT里面三种score打分函数 : \\[ \\color{blue}{\\rm{score}(h_t, \\bar h_s)} = \\begin{cases} h_t^T \\bar h_s &amp; \\text{dot} \\\\ h_t^T W_a \\bar h_s &amp; \\text{general} \\\\ v_a^T \\tanh (W_a [h_t; \\bar h_s]) &amp; \\text{concat} \\\\ \\end{cases} \\] 加性模型 \\[ s(\\mathbf{x}_i, \\mathbf{q}) = v^T\\rm{tanh} (W\\mathbf{x}_i + U\\mathbf{q}) \\] 点击模型 \\[ s(\\mathbf{x}_i, \\mathbf{q}) = \\mathbf{x}_i^T \\mathbf{q} \\] 计算注意力 Soft Attention 是对所有的信息进行加权求和。Hard Attention是选择最大信息的那一个。 使用软性注意力选择机制，对输入信息编码为，实际上也是一个期望。 \\[ \\rm{attn} (X, \\mathbf q) = \\sum_{i=1}^N \\alpha_i \\mathbf x_i = E_{z\\sim p(z\\mid X, \\mathbf{q})} [X] \\] 应用与优点 传统机器翻译Encoder-Decoder的缺点： 编码向量容量瓶颈问题：所有信息都需要保存在编码向量中 长距离依赖问题：长距离信息传递时，信息会丢失 注意力机制和PyTorch实现机器翻译 注意力机制直接从源语言信息中选择相关的信息作为辅助，有下面几个好处： 解码过程中每一步都直接访问源语言所有位置上的信息。无需让所有信息都通过编码向量进行传递。 缩短了信息的传递距离。源语言的信息可以直接传递到解码过程中的每一步 图像描述生成 注意力机制变体 多头注意力 Multi-head Attention利用多个查询\\(\\mathbf{q}_{1:M}={\\mathbf{q}_1, \\cdots, \\mathbf{q}_M}\\)来并行地从输入信息中选取多个信息。每个注意力关注输入信息的不同部分。比如Attention Is All You Need。 硬性注意力 硬性注意力是只关注到一个位置上。 选取最高概率的输入信息 在注意力分布上随机采样 缺点：loss与注意力分布之间的函数关系不可导，无法使用反向传播训练。一般使用软性注意力。 需要：硬性注意力需要强化学习来进行训练。 键值对注意力 输入信息：键值对(Key, Value)。 Key用来计算注意力分布\\(\\alpha_i\\)，值用来生成选择的信息。 \\[ \\rm{attn} (\\mathbf{(K, V)}, \\mathbf q) = \\sum_{i=1}^N \\alpha_i \\mathbf v_i = \\sum_{i=1}^N\\frac{\\exp\\left(s(\\mathbf{k}_i, \\mathbf{q})\\right)}{\\sum_{j=1}^N \\exp\\left(s(\\mathbf{k}_j, \\mathbf{q})\\right)} \\mathbf v_i \\] 结构化注意力 普通注意力是在输入信息上的一个多项分布，是一个扁平结构。 如果输入信息，本身就有层次化的结构，词、句子、段落、篇章等不同粒度的层次。这时用层次化的注意力来进行更好的信息选择。 也可以使用一种图模型，来构建更加复杂的结构化注意力分布。 指针网络 前面的都是计算注意力对信息进行筛选：计算注意力分布，利用分布对信息进行加权平均。 指针网络pointer network是一种序列到序列的模型，用来指出相关信息的位置。也就是只做第一步。 输入： \\(X_{1:n} = [\\mathbf{x}_1, \\cdots, \\mathbf{x}_n]\\) 输出：\\(c_{1:m} = c_1, c_2, \\cdots, c_m, \\; c_i \\in [1,n]\\)， 输出是序列的下标。如输入123，输出312 条件概率 \\[ p(c_{1:m} \\mid \\mathbf x_{1:n}) = \\prod_{i=1}^m p(c_i \\mid c_{1:i-1}, \\mathbf x_{1:n}) \\approx \\prod_{i=1}^m p(c_i \\mid \\mathbf x_{c_1}, \\cdots, \\mathbf x_{c_{i-1}}\\mathbf x_{1:n}) \\] \\[ p(c_i \\mid c_{1:i-1}, \\mathbf x_{1:n}) = \\rm{softmax}(s_{i,j}) \\] 第i步时，每个输入向量的得分（未归一化的注意力分布）： \\[ s_{i,j} = v^T\\rm{tanh} (W\\mathbf{x}_j + U\\mathbf{e}_i) \\] 其中向量\\(\\mathbf e_i\\)是第i个时刻，RNN对\\(\\mathbf x_{c_1}, \\cdots, \\mathbf x_{c_{i-1}}\\mathbf x_{1:n}\\) 的编码。 各种注意力计算模型 注意力的本质 有\\(k\\)个\\(d\\)维的特征向量\\(\\mathbf h_i \\;(i \\in [1,k])\\)，想要整合这k个特征向量的信息。得到一个向量\\(\\mathbf h^*\\)，一般也是d维。 简单粗暴：对k个向量求平均。当然不合理啦。 加权平均：\\(\\mathbf h^* = \\sum_{i=1}^k \\alpha_i \\mathbf h_i\\) 。合理 所以最重要的就是合理地求出\\(\\alpha_i\\)，根据所关心的对象\\(\\mathbf q\\)(可能是自身)去计算注意力分布 针对每个\\(\\mathbf h_i\\)， 计算出一个得分，\\(s_i\\)。 \\(h_i\\)与\\(q\\)越相关，得分越高。 \\(\\alpha_i = \\rm{softmax}(s_i)\\) \\[ s_i = \\rm{score}(\\mathbf h_i, \\mathbf q) \\] 打分函数的计算：(NMT里面三种score打分函数 ) Local-based Attention ，没有外部的关注对象，自己关注自己。 General Attention， 有外部的关注对象，直接乘积，全连接层。 Concatenation-based Attention， 有关注的对象，先concat或相加再过连接层。 Local-based 没有外部的信息，每个向量的得分与自己相关，与外部无关。 比如：Where is the football? ，where和football在句子中起总结性作用。Attention只与句子中的每个词有关。 一个句子，有多个词，多个向量。通过自己计算注意力分布，再对这些词的注意力进行加权求和，则可以得到这个句子的最终表达。 \\[ s_i = f(\\mathbf h_i) = \\rm{a}(W^T \\mathbf h_i + b) \\] \\[ \\mathbf h^* = \\sum_{i=1}^n s_i \\cdot \\mathbf h_i \\] a是激活函数。 sigmoid, tanh, relu, maxout， y=x（无激活函数）。 1 一个得分简单求法 A Context-Aware Attention Network For Interactive Question Answering 利用自己计算注意力分布 \\[ \\gamma_j = \\rm{softmax} (\\mathbf v^T \\mathbf g_j^q) \\] 利用新的注意力分布去计算最终的attention向量 \\[ \\mathbf u = W_{ch} \\sum_{j=1}^N \\gamma_j \\mathbf g_j^q + \\mathbf b_c ^q \\] 2 两个得分合并为一个得分 Dynamic Attention Deep Model for Article Recommendation by Learning Human Editors’ Demonstration 计算两个得分 \\[ \\lambda _{m_t}^M = w_{m_t}^M \\cdot \\mathbf o + b_{m_t}^M , \\quad \\lambda_t ^T = w_t^T \\cdot \\mathbf o + b_t^T \\] 权值合并，求注意力分布： \\[ p_t = \\rm{softmax} (\\alpha \\lambda _{m_t}^M + (1-\\alpha) \\lambda_t ^T) \\] 3 论文图片 CAN for QA Dynamic Attention General Attention 有外部的信息，\\(\\mathbf h_i\\) 与 \\(\\mathbf q\\)进行乘积得分。 机器翻译的应用 \\[ \\rm{score}(\\mathbf h_i, \\mathbf q) = \\mathbf h_i^T W\\mathbf q \\] Concatenation-based 要关注的外部对象是\\(\\mathbf h_t\\)， 可以随时间变化，也可以一直不变(question)。 \\[ s_i = f(\\mathbf h_i, \\mathbf h_t) = \\mathbf v^T \\rm a(W_1 \\mathbf h_i + W_2 \\mathbf h_t + b) \\] 1 多个元素组成Attention Attentive Collaborative Filtering Multimedia Recommendation with Item- and Component-Level Attention_sigir17 Item-Level Attention。可以看到需要加什么Attention，直接向公式里面一加就可以了。 \\[ a(i, l) = w_1^T \\phi(W_{1u} \\mathbf u_i + W_{1v} \\mathbf v_l + W_{1p} \\mathbf p_l + W_{1x} \\mathbf {\\bar x}_l + \\mathbf b_1) + \\mathbf c_1 \\] \\[ \\alpha(i, l) = \\frac {\\exp (a(i, l))} {\\sum_{n \\in R(i)} \\exp (a(i, n))} \\] 多层Attention 有\\(m\\)个句子，每个句子有\\(k\\)个词语。 Word-level Attention 每个句子，有k个词语，每个词语一个词向量，使用Local-based Attention ， 可以得到这个句子的向量表达\\(\\mathbf s_i\\)。 Sentence-level Attention 有\\(m\\)个句子，每个句子是一个句子向量\\(\\mathbf s_i\\)。 可以再次Attention，得到文档的向量表达\\(\\mathbf d\\)， 也可以得到每个句子的权值\\(\\alpha_i\\)。 得到这些信息之后，再具体问题具体分析。 1. 文章摘要生成 Leveraging Contextual Sentence Relations for Extractive Summarization Using a Neural Attention Model_SIGIR2017 输入一篇文档，输出它的摘要。 第一层：Local-based Attention， 生成每个句子的vector 第二层：当前句子作为中心，2n+1个句子。输入RNN（不明白）。将中心句子作为attention，来编码上下文。通过上下文对中心句子进行打分。作为该句子对整个文本的重要性 CAN的实时问答 A Context-Aware Attention Network For Interactive Question Answering 第一层Attention 对句子过GRU，每一时刻的output作为词的编码。再使用Local-Attention对这些词，得到问句的表达\\(\\mathbf u\\)。 第二层Attention 由于上下文有多个句子。 首先，对一个句子进行过GRU，得到每一时刻单词的语义信息\\(\\alpha^t\\)， 然后利用Concat-Attention对这些单词计算，得到这句话的语义信息\\(\\mathbf y_t\\)。 再把当前句子的语义信息给到句子的GRU 第三次Attention 经过GRU，得到每个句子的表达\\(\\mathbf s_t\\)。 再使用Concat-Attention来得到每个句子的注意力分配\\(\\mathbf \\beta_t\\), 然后加权求和得到 整个Context的表达\\(\\mathbf m\\)。 输出 结合\\(\\mathbf {m, u}\\)通过GRU去生成答案 Period Symbol ：是正确答案，直接输出 Question Mask： 输出是一个问题，要继续问用户相应的信息 用户重新给了反馈之后，对所有词汇信息使用simple attention mechanism， 即平均加权，所有的贡献都是一样的。得到反馈的向量表达\\(\\mathbf f\\)。 使用新的反馈向量和原始的问句向量，结合，重新生成新的context的语义表达\\(\\mathbf m\\)。 最终得到新的\\(\\mathbf {m, u}\\) 去重新回答。 \\[ \\mathbf r = \\tanh (W_{rf}f + \\mathbf b_r^{(f)}) \\] \\[ \\beta_t = \\rm{softmax}(\\mathbf u^T \\mathbf s_t + \\mathbf r^T \\mathbf s_t) \\]","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"Attention","slug":"Attention","permalink":"http://plmsmile.github.io/tags/Attention/"}]},{"title":"Dynamic Coattention Network (Plus)","date":"2018-03-15T00:33:16.000Z","path":"2018/03/15/32-dynamic-coattention-network/","text":"先放四张图，分别是DCN的Encoder、Decoder，DCN+的Encoder和Objective。后面再详细总结 Dynamic Coattention Networks For Question Answering DCN+: Mixed Objective and Deep Residual Coattention for Question Answering DCN Coattention Encoder Dynamic Pointing Decoder HMN DCN+ DCN的问题 loss没有判断真正的意义 DCN使用传统交叉熵去优化optimization，只考虑答案字符串的匹配程度。但是实际上人的评判evaluation却是看回答的意义。如果只考虑span，则有下面两个问题： 精确答案：没影响 但是对正确答案周围重叠的单词，却可能认为是错误的。 句子：Some believe that the Golden State Warriors team of 2017 is one of the greatest teams in NBA history 问题：which team is considered to be one of the greatest teams in NBA history 正确答案：the Golden State Warriors team of 2017 其实Warriors也是正确答案， 但是传统交叉熵却认为它还不如history。 DCN没有建立起Optimization和 evaluation的联系。 这也是Word Overlap。 单层coattention表达力不强 DCN+的优化点 Mixed Loss 交叉熵+自我批评学习（强化学习）。Word真正意义相似才会给一个好的reward。 强化学习会鼓励意义相近的词语，而dis不相近的词语 交叉熵让强化学习朝着正确的轨迹发展 Deep Residual Coattention Encoder 多层表达能力更强，详细看下面的优点。 Deep Residual Encoder 优点 两个别人得出的重要结论： stacked self-attention 可以加速信号传递 减少信号传递路径，可以增加长依赖 比DCN的两个优化点： coattention with self-attention和多层coattention 。可以对输入有richer representations 对每层的coattention outputs进行残差连接。缩短了信息传递路径。 Coattention深层理解 当时理解了很久都不懂，后来一个下午，一直看，结合机器翻译实现和实际例子矩阵计算，终于理解了Attention、Coattention。 参考了我的下面三篇笔记。 注意力机制在机器翻译中的思想和代码实现总结 图文介绍Attention Coattention的两种形式 单个Coattention层计算 经过双向RNN后，得到两个语义编码：文档\\(E_0^D \\in \\mathbb R^{m\\times e}\\)， 问题编码\\(E^Q_0 \\in \\mathbb R ^{n \\times h}\\) 。 \\[ E_1^D = \\rm{biGRU_1}(E_0^D) \\quad\\in \\mathbb R^{m \\times h} \\] \\[ E_1^Q = \\tanh(\\rm{W \\; \\rm{biGRU_1(Q_E)+b)}} \\quad \\in \\mathbb R^{n \\times h} \\] 计算关联得分矩阵A \\[ A = E_1^D (E_1^Q)^T \\in \\mathbb R^{m \\times n} \\] \\[ \\begin{bmatrix} 0 &amp; 0 \\\\ 2 &amp; 3 \\\\ 0 &amp; 2 \\\\ 1 &amp; 1 \\\\ 3 &amp; 3 \\\\ \\end{bmatrix}_{5 \\times 2} \\cdot \\begin{bmatrix} 1&amp; 3 \\\\ 1 &amp; 1 \\\\ 1&amp; 3 \\\\ \\end{bmatrix}_{3 \\times 2}^T = \\begin{bmatrix} 0&amp; 0 &amp;0 \\\\ 11&amp; 5 &amp;11 \\\\ 6&amp; 2 &amp;6 \\\\ 4&amp; 2 &amp;4 \\\\ 12&amp; 6 &amp;12 \\\\ \\end{bmatrix}_{5\\times 3} \\] 做行Softmax，得到Q对D的权值分配概率\\(A^Q\\)， attention_weights 每一行是一个文档单词w 元素值是所有问句单词对当前文档单词w的注意力分配权值 元素值是每个问句单词的权值概率 \\[ \\begin{bmatrix} 0.3333 &amp; 0.3333 &amp; 0.3333 \\\\ 0.4994 &amp;0.0012 &amp; 0.4994 \\\\ 0.4955 &amp;0.0091 &amp;0.4955 \\\\ 0.4683 &amp;0.0634 &amp;0.4683\\\\ 0.4994 &amp;0.0012 &amp;0.4994 \\\\ \\end{bmatrix}_{5\\times 3} \\] 计算D的summary， \\(S^D = A^Q \\cdot Q\\) \\[ S^D = A^Q \\cdot Q \\] D所需要的新的语义，参考机器翻译的新语义理解 \\(A^Q\\)的每一行去乘以Q的每一列去表达单词w 用Q去表达D，每个\\(D_w\\)都是Q的所有单词对w的线性表达，权值就是\\(A^Q\\) 所以\\(S^D\\)也是D的summary， 也称作D需要context 同理，对列做softmax， 得到D对Q的权值分配概率\\(A^D\\)， 得到Q的summary， \\(S^Q = A^D \\cdot D\\) 这时，借鉴alternation-coattention思想 去计算对D的Coattention context\\(C^D\\) ： \\[ C^D = S^Q \\cdot A^Q \\] 实际上，\\(C^D\\)与\\(S^D\\)类似，都是Summary， 都是context。 只是\\(C^D\\)使用的是新的\\(S^Q\\)， 而不是\\(E^Q_1\\)。 Coattention Encoder总结 使用两层coattention， 最后再残差连接，经过LSTM输出。 第一层 \\[ E_1^D = \\rm{biGRU_1}(E_0^D) \\quad\\in \\mathbb R^{m \\times h} \\\\ E_1^Q = \\tanh(\\rm{W \\cdot \\rm{biGRU_1(E_0^Q)+b)}} \\quad \\in \\mathbb R^{n \\times h} \\] \\[ \\rm{coattn_1} (E_1^D, E_1^Q) = S_1^D, S_1^Q, C_1^Q \\\\ \\] 第二层 \\[ E_2^D = \\rm{biGRU_2}(E_1^D) \\quad\\in \\mathbb R^{m \\times h} \\\\ E_2^Q = \\tanh (W \\cdot \\rm{biGRU_2}(E_1^Q) + b) \\quad\\in \\mathbb R^{m \\times h} \\] \\[ \\rm{coattn_2} (E_2^D, E_2^Q) = S_2^D, S_2^Q, C_2^Q \\\\ \\] 残差连接所有的D \\[ c = \\rm {concat}((E_1^D, E_2^D, S_1^D, S_2^D, C_1^D, C_2^D) \\] LSTM编码输出，得到Encoder的输出 \\[ U = \\rm{biGRU}(c) \\quad \\in \\mathbb R^{m \\times 2h} \\] Mixed Objective","tags":[{"name":"DCN","slug":"DCN","permalink":"http://plmsmile.github.io/tags/DCN/"},{"name":"coattention","slug":"coattention","permalink":"http://plmsmile.github.io/tags/coattention/"},{"name":"QA","slug":"QA","permalink":"http://plmsmile.github.io/tags/QA/"}]},{"title":"协同注意力简介","date":"2018-03-14T08:56:27.000Z","path":"2018/03/14/31-co-attention-vqa/","text":"只是记录一下Co-Attention，后续再补上本篇论文的全部笔记吧。 论文：Hierarchical Question-Image Co-Attention for Visual Question Answering 我的相关笔记：Attention-based NMT阅读笔记和NLP中的Attention笔记 Co-Attention 这里以VQA里面的两个例子记录一下Co-Attention。即图片和问题。 注意力和协同注意力 注意力 注意力机制就像人带着问题去阅读， 先看问题，再去文本中有目标地阅读寻找答案。 机器阅读则是结合问题和文本的信息，生成一个关于文本段落各部分的注意力权重，再对文本信息进行加权。 注意力机制可以帮助我们更好地去捕捉段落中和问题相关的信息。 协同注意力 协同注意力是一种双向的注意力， 再利用注意力去生成文本和问句的注意力。 给文本生成注意力权值 给问句生成注意力权值 协同注意力分为两种方式： Parallel Co-Attention : 两种数据源A和B，先结合得到C，再基于结合信息C对A和B分别生成对应的Attention。同时生成注意力 Alternating Co-Attention： 先基于A产生B的attention，得到新的B；再基于新B去产生A的attention。两次交替生成注意力 Parallel Co-Attention 图片特征：\\(V \\in \\mathbb {R}^{d\\times N}\\) ，问句特征：\\(Q \\in \\mathbb R^{d \\times T}\\) 。 同时生成图片和问题的注意力。 先计算关联矩阵： \\[ C = \\rm{tanh}(Q^T W_b V) \\in \\mathbb R^{T \\times N} \\] 计算注意力权值 \\(a^v\\)和\\(a^q\\) 方法1：直接选择最大值。\\(a^v_n = \\max \\limits_i(C_{i, n})\\) ，\\(a_t^q = \\max \\limits_i (C_{t, j})\\) 方法2：把关联矩阵当做特征给到网络中，进行计算注意力权值，再进行softmax。更好 \\[ H^v = \\rm{tanh} (W_vV + (W_qQ)C), \\quad \\quad H^q = \\rm{tanh} (W_qQ + (W_vV)C^T) \\] \\[ a^v = \\rm{softmax}(w_{hv}^TH^v), \\quad \\quad a^q = \\rm{softmax}(w^T_{hq}H^q) \\] 利用注意力和原特征向量去计算新的特征向量 \\[ \\mathbf {\\hat v} = \\sum_{n=1}^N a^v_n \\mathbf v_n, \\quad \\quad \\mathbf { \\hat q} = \\sum_{t=1}^Tq_t^q \\mathbf q_t \\] Alternating Co-Attention 交替生成图片和问题的注意力。 把问题归纳成一个单独向量\\(\\mathbf {q}\\) 基于\\(\\mathbf q\\) 去和图片特征\\(V\\)去生成图像特征\\(\\mathbf {\\hat v}\\) 基于\\(\\mathbf v\\)和问题特征\\(Q\\)去生成问题特征\\(\\mathbf {\\hat q}\\) 具体地，给一个\\(X\\)和attention guidance\\(\\mathbf g\\) ，通过\\(\\mathbf {\\hat x} = f(X, \\mathbf g)\\)去得到特征向量\\(\\mathbf {\\hat x}\\) \\[ H = \\rm {tanh} (W_x X+ (W_g \\mathbf g) \\mathbb 1^T) \\] \\(\\mathbf a ^x\\) 是特征\\(X\\)的注意力权值 ： \\[ \\mathbf a^x = \\rm(softmax)(w^T_{hx} H) \\] 新的注意力向量 (attended image (or question) vector) : \\[ \\mathbf {\\hat x} = \\sum a_i^x \\mathbf x_i \\] 对应本例子如下： \\(X = Q, \\; g = 0 \\to \\mathbf q\\) \\(X = V, \\; g = \\mathbf q \\to \\mathbf {\\hat v}\\) \\(X = Q, \\; g = \\mathbf {\\hat v} \\to \\mathbf {\\hat q}\\)","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"注意力","slug":"注意力","permalink":"http://plmsmile.github.io/tags/注意力/"},{"name":"VQA","slug":"VQA","permalink":"http://plmsmile.github.io/tags/VQA/"}]},{"title":"使用Dynamic Memory Network实现一个简单QA","date":"2018-03-13T08:07:29.000Z","path":"2018/03/13/30-dynamic-memory-network/","text":"本文概要：介绍DMN的基本原理，使用PyTorch进行实现一个简单QA 论文：Ask Me Anything: Dynamic Memory Networks for Natural Language Processing 模型简介 概要说明 许多NLP问题都可以看做一个Question-Answer问题。Dynamic Memory Network 由4部分组成。 输入模块 对输入的句子facts(先embedding)使用GRU进行编码，得到encoded_facts，给到后面的情景记忆模块。 问题模块 对输入的问题question使用GRU进行编码，得到encoded_question， 给到后面的情景记忆模块 和回答模块 。 情景记忆模块 Episodic Memory Module由memory和attention组成。 attention：会选择更重要的facts memory：根据question、facts和 旧memory来生成新momery 。初始：memory=encoded_question 会在facts上迭代多次去计算memory。 每一次迭代会提取出新的信息。 输出最终的momery， 给到回答模块。 回答模块 memory + question， 在GRUCell上迭代原本的回答长度次， 得到最终的预测结果。 输入模块 输入 一个句子，有\\(T_I\\)个单词 \\(T_I\\)个句子，则把这些句子合并成一个大句子。在每个句子的末尾添加一个句子结束标记&lt;/s&gt;。如上图蓝色的部分 GRU计算隐状态 句子过RNN时，对于每一时刻\\(t\\)的单词\\(w_t\\) ，有\\(h_t\\) : \\[ h_t = \\rm{RNN}(w_t, h_{t-1}) \\] 输出 使用RNN的h = hidden states 作为输入句子的向量表达，也就是encoded_facts 一个句子，输出所有时刻的\\(h_t\\) 多个句子，输出每个句子结束标记&lt;/s&gt;时刻的\\(h_t\\)。 问题模块 输入 输入一个句子question，有\\(T_Q\\)个单词。 GRU计算隐状态 \\[ q_t = \\rm{RNN}(w_t^Q, q_{t-1}) \\] 输出Q编码 最后时刻的隐状态\\(q_{T_Q}\\)作为句子的编码。 情景记忆模块 总体思路 记忆模块收到两个编码表达：encoded_facts和encoded_question ， 也就是\\(h\\)和\\(q\\)。 模块会生成一个记忆memory，初始时memory = encoded_question 记忆模块在encoded_facts上反复迭代多轮，每一轮去提取新的信息episode， 更新memory 遍历所有facts， 对于每一个的fact， 不停地更新当前轮的信息e 计算新的信息：\\(e_{new}=\\rm{RNN}(fact, e)\\) ，使用当前fact和当前信息 计算新信息的保留比例注意门\\(g\\) 更新信息：\\(e = g * e_{new} + (1-g) * e\\) 计算保留比例g：结合当前fact 、memory、 question 去生成多个特征，再过一个两层前向网络G得到一个比例数值 更新memory ，\\(m^i = \\rm{GRU}(e, m^{i-1})\\) 特征函数与前向网络 保留比例门g充当着attention的作用 。 特征函数\\(z(c, m, q)\\)， 其中c就是当前的fact ，（论文里面是9个特征）： \\[ z(c, m, q) = [c \\circ q, c \\circ m, \\vert c-q\\vert, \\vert c-m\\vert] \\] 前向网络\\(g=G(c, m ,q)\\) ： \\[ t = \\rm{tanh}(W^1z(c, m, q) + b^1) \\\\ g = G(c, m, q) = \\sigma(W^2 t + b^2) \\] e更新 在每个fact遍历中，e会结合fact和旧e去生成新的信息\\(e_{new}\\)，再结合旧\\(e\\)和新\\(e_{new}\\) 去生成最终的\\(e^i\\) ： \\[ e_{new}=\\rm{RNN}(fact, e) \\] \\[ e = g * e_{new} + (1-g) * e \\] 记忆更新 每一轮迭代后，结合旧记忆和当前轮的信息e去更新记忆： \\[ m^i = \\rm{GRU}(e, m^{i-1}) \\] 迭代停止条件 设置最大迭代次数\\(T_M\\) 在输入里面追加停止迭代信号，如果注意门选择它，则停止。 回答模块 回答模块结合memory和question，来生成对问题的答案。也是通过GRU来生成答案的。 设a 是answer_gru的hidden state，初始\\(a_0= m^{T_M}\\) \\[ y_t = \\rm{softmax}(W^a a_t) \\\\ a_t = \\rm{GRU} ([y_{t-1}, q], a_{t-1}) \\] 使用交叉熵去计算loss，进行优化。 实现细节 我的github源代码 ，实现参考自DSKSD的代码 。 数据处理 原始数据 使用过的数据是facebook的bAbi Tasks Data 1-20里面的 en-10k下的qa5_three-arg-relations_train.txt 和test数据。 12345678910111213141516171819202122232425262728293031323334353637381 Bill travelled to the office.2 Bill picked up the football there.3 Bill went to the bedroom.4 Bill gave the football to Fred.5 What did Bill give to Fred? football 46 Fred handed the football to Bill.7 Jeff went back to the office.8 Who received the football? Bill 69 Bill travelled to the office.10 Bill got the milk there.11 Who received the football? Bill 612 Fred travelled to the garden.13 Fred went to the hallway.14 Bill journeyed to the bedroom.15 Jeff moved to the hallway.16 Jeff journeyed to the bathroom.17 Bill journeyed to the office.18 Fred travelled to the bathroom.19 Mary journeyed to the kitchen.20 Jeff took the apple there.21 Jeff gave the apple to Fred.22 Who did Jeff give the apple to? Fred 2123 Bill went back to the bathroom.24 Bill left the milk.25 Who received the apple? Fred 211 Mary travelled to the garden.2 Mary journeyed to the kitchen.3 Bill went back to the office.4 Bill journeyed to the hallway.5 Jeff went back to the bedroom.6 Fred moved to the hallway.7 Bill moved to the bathroom.8 Jeff went back to the garden.9 Jeff went back to the kitchen.10 Fred went back to the garden.11 Mary got the football there.12 Mary handed the football to Jeff.13 What did Mary give to Jeff? football 12 比如1-25是一个大的情景 没有问号的都是陈述句，是情景数据fact。只有.号， 都是简单句 带问号的：是问句，带有答案和答案所在句子。使用tab分割 加载原始数据 1234567891011121314151617181920212223242526272829303132def load_raw_data(file_path, seq_end='&lt;/s&gt;'): ''' 从文件中读取文本数据，并整合成[facts, question, answer]一条一条的可用数据，原始word形式 Args: file_path -- 数据文件 seq_end -- 句子结束标记 Returns: data -- list，元素是[facts, question, answer] ''' source_data = open(file_path).readlines() print (file_path, \":\", len(source_data), \"lines\") # 去掉换行符号 source_data = [line[:-1] for line in source_data] data = [] for line in source_data: index = line.split(' ')[0] if index == '1': # 一个新的QA开始 facts = [] #qa = [] if '?' in line: # 当前QA的一个问句 # 问题 答案 答案所在句子的编号 \\t分隔 tmp = line.split('\\t') question = tmp[0].strip().replace('?', '').split(' ')[1:] + ['?'] answer = tmp[1].split() + [seq_end] facts_for_q = deepcopy(facts) data.append([facts_for_q, question, answer]) else: # 普通的事件描述，简单句，只有.和空格 sentence = line.replace('.', '').split(' ')[1:] + [seq_end] facts.append(sentence) return data 把数据转成id格式 1234567891011121314151617def triple_word2id(triple_word_data, th): '''把文字转成id Args: triple_word_data -- [(facts, q, a)] word形式 th -- textheler Returns: triple_id_data -- [(facts, q, a)]index形式 ''' # 把各个word转成数字id for t in triple_word_data: # 处理facts句子 for i, fact in enumerate(t[0]): t[0][i] = th.sentence2indices(fact) # 问题与答案 t[1] = th.sentence2indices(t[1]) t[2] = th.sentence2indices(t[2]) return triple_word_data 根据batch_size取数据 1234567891011121314151617181920def get_data_loader(data, batch_size=1, shuffle=False): ''' 以batch的格式返回数据 Args: data -- list格式的data batch_size -- shuffle -- 每一个epoch开始的时候，对数据进行shuffle Returns: 数据遍历的iterator ''' if shuffle: random.shuffle(data) start = 0 end = batch_size while (start &lt; len(data)): batch = data[start:end] start, end = end, end + batch_size yield batch if end &gt;= len(data) and start &lt; len(data): batch = data[start:] yield batch 对每一个batch进行padding 这部分有点复杂。要求问题、答案、fact的长度一致，每个问题的fact的数量也要一样。 其实和模型也有关，模型写的有点坑，就是每条数据的所有fact应该连接在一起成为一个大的fact送进GRU里，在每个fact后面加上结束标记。但是我这却分开了，分成了多个标记好的fact，也怪当时没有仔细看好论文，这个也是参考别人的实现。循环也导致训练贼慢，但是现在忙着找实习，就先不改了。后面好好写DMNPLUS吧。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def pad_batch_data(raw_batch_data, th): ''' 对数据进行padding，问题、答案、fact长度分别一致，同时每条数据的fact的数量一致。输入到网络的时候要用 Args: raw_batch_data -- [[facts, q, a]]，都是以list wordid表示 th -- TextHelper Returns: all_facts -- [b, nfact, flen]，pad后的facts，Variable all_facts_mask -- [b, nfact, flen]，facts的mask，Variable questions -- [b, qlen]，pad后的questions，Variable questions_mask -- [b, qlen]，questions的mask，Variable answers -- [b, alen]，pad后的answers，Variable ''' all_facts, questions, answers = [list(i) for i in zip(*raw_batch_data)] batch_size = len(raw_batch_data) # 1. 计算各种长度。一个QA的facts数量，fact、Q、A句子的最大长度 n_fact = max([len(facts) for facts in all_facts]) flen = max([len(f) for f in flatten(all_facts)]) qlen = max([len(q) for q in questions]) alen = max([len(a) for a in answers]) padid = th.word2index(th.pad) # 2. 对数据进行padding all_facts_mask = [] for i in range(batch_size): # 2.1 pad fact facts = all_facts[i] for j in range(len(facts)): t = flen - len(facts[j]) if t &gt; 0: all_facts[i][j] = facts[j] + [padid] * t # fact数量pad while (len(facts) &lt; n_fact): all_facts[i].append([padid] * flen) # 计算facts内容是否是填充给的，填充为1，不填充为0 mask = [tuple(map(lambda v: v == padid, fact)) for fact in all_facts[i]] all_facts_mask.append(mask) # 2.2 pad question q = questions[i] if len(q) &lt; qlen: questions[i] = q + [padid] * (qlen - len(q)) # 2.3 pad answer a = answers[i] if len(a) &lt; alen: answers[i] = a + [padid] * (alen - len(a)) # 3. 把list数据转成Variable all_facts = get_variable(torch.LongTensor(all_facts)) all_facts_mask = get_variable(torch.ByteTensor(all_facts_mask)) answers = get_variable(torch.LongTensor(answers)) questions = torch.LongTensor(questions) questions_mask = [(tuple(map(lambda v: v == padid, q))) for q in questions] questions_mask = torch.ByteTensor(questions_mask) questions, questions_mask = get_variable(questions), get_variable(questions_mask) return all_facts, all_facts_mask, questions, questions_mask, answers 模型 模型定义 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class DMN(nn.Module): def __init__(self, vocab_size, embed_size, hidden_size, padding_idx, seqbegin_id, dropout_p=0.1): ''' Args: vocab_size -- 词汇表大小 embed_size -- 词嵌入维数 hidden_size -- GRU的输出维数 padding_idx -- pad标记的wordid seqbegin_id -- 句子起始的wordid dropout_p -- dropout比率 ''' super(DMN, self).__init__() self.vocab_size = vocab_size self.hidden_size = hidden_size self.seqbegin_id = seqbegin_id self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx) self.input_gru = nn.GRU(embed_size, hidden_size, batch_first=True) self.question_gru = nn.GRU(embed_size, hidden_size, batch_first=True) self.gate = nn.Sequential( nn.Linear(hidden_size * 4, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 1), nn.Sigmoid() ) self.attention_grucell = nn.GRUCell(hidden_size, hidden_size) self.memory_grucell = nn.GRUCell(hidden_size, hidden_size) self.answer_grucell = nn.GRUCell(hidden_size * 2, hidden_size) self.answer_fc = nn.Linear(hidden_size, vocab_size) self.dropout = nn.Dropout(dropout_p) self.init_weight() def init_hidden(self, batch_size): '''GRU的初始hidden。单层单向''' hidden = torch.zeros(1, batch_size, self.hidden_size) hidden = get_variable(hidden) return hidden def init_weight(self): nn.init.xavier_uniform(self.embed.state_dict()['weight']) components = [self.input_gru, self.question_gru, self.gate, self.attention_grucell, self.memory_grucell, self.answer_grucell] for component in components: for name, param in component.state_dict().items(): if 'weight' in name: nn.init.xavier_normal(param) nn.init.xavier_uniform(self.answer_fc.state_dict()['weight']) self.answer_fc.bias.data.fill_(0) 前向计算参数 123456789101112131415161718def forward(self, allfacts, allfacts_mask, questions, questions_mask, alen, n_episode=3): ''' Args: allfacts -- [b, n_fact, flen]，输入的多个句子 allfacts_mask -- [b, n_fact, flen]，mask=1表示是pad的，否则不是 questions -- [b, qlen]，问题 questions_mask -- [b, qlen]，mask=1：pad alen -- Answer len seqbegin_id -- 句子开始标记的wordid n_episodes -- Returns: preds -- [b * alen, vocab_size]，预测的句子。b*alen合在一起方便后面算交叉熵 ''' # 0. 计算常用的信息，batch_size，一条数据nfact条句子，每个fact长度为flen，每个问题长度为qlen bsize = allfacts.size(0) nfact = allfacts.size(1) flen = allfacts.size(2) qlen = questions.size(1) 输入模块 12345678910111213141516171819202122# 1. 输入模块，用RNN编码输入的句子# TODO 两层循环，待优化encoded_facts = []# 对每一条数据，计算facts编码for facts, facts_mask in zip(allfacts, allfacts_mask): facts_embeds = self.embed(facts) facts.embeds = self.dropout(facts_embeds) hidden = self.init_hidden(nfact) # 1.1 把输入(多条句子)给到GRU # b=nf, [nf, flen, h], [1, nf, h] outputs, hidden = self.input_gru(facts_embeds, hidden) # 1.2 每条句子真正结束时(real_len)对应的输出，作为该句子的hidden。GRU：ouput=hidden real_hiddens = [] for i, o in enumerate(outputs): real_len = facts_mask[i].data.tolist().count(0) real_hiddens.append(o[real_len - 1]) # 1.3 把所有单个fact连接起来，unsqueeze(0)是为了后面的所有batch的cat hiddens = torch.cat(real_hiddens).view(nfact, -1).unsqueeze(0) encoded_facts.append(hiddens) # [b, nfact, h] encoded_facts = torch.cat(encoded_facts) 问句模块 1234567891011# 2. 问题模块，对问题使用RNN编码questions_embeds = self.embed(questions)questions_embeds = self.dropout(questions_embeds)hidden = self.init_hidden(bsize)# [b, qlen, h], [1, b, h]outputs, hidden = self.question_gru(questions_embeds, hidden)real_questions = []for i, o in enumerate(outputs): real_len = questions_mask[i].data.tolist().count(0) real_questions.append(o[real_len - 1]) encoded_questions = torch.cat(real_questions).view(bsize, -1) 12345678910111213141516171819202122# 3. Memory模块memory = encoded_questionsfor i in range(n_episode): # e e = self.init_hidden(bsize).squeeze(0) # [nfact, b, h] encoded_facts_t = encoded_facts.transpose(0, 1) # 根据memory, episode，计算每一时刻的e。最终的e和memory来计算新的memory for t in range(nfact): # [b, h] bfact = encoded_facts_t[t] # TODO 计算4个特征，论文是9个 f1 = bfact * encoded_questions f2 = bfact * memory f3 = torch.abs(bfact - encoded_questions) f4 = torch.abs(bfact - memory) z = torch.cat([f1, f2, f3, f4], dim=1) # [b, 1] 对每个fact的注意力 gt = self.gate(z) e = gt * self.attention_grucell(bfact, e) + (1 - gt) * e # 每一轮的e和旧memory计算新的memory memory = self.memory_grucell(e, memory) 回答模块 123456789101112131415161718192021# 4. Answer模块# [b, h]answer_hidden = memorybegin_tokens = get_variable(torch.LongTensor([self.seqbegin_id]*bsize))# [b, h]last_word = self.embed(begin_tokens)preds = []for i in range(alen): inputs = torch.cat([last_word, encoded_questions], dim=1) answer_hidden = self.answer_grucell(inputs, answer_hidden) # to vocab_size probs = self.answer_fc(answer_hidden) # [b, v] probs = F.log_softmax(probs.float()) _, indics = torch.max(probs, 1) last_word = self.embed(indics) # for cross entropy preds.append(probs.view(bsize, 1, -1)) #print (preds[0].data.shape) preds = torch.cat(preds, dim=1)return preds.view(bsize * alen, -1) 配置信息 12345678910111213141516171819202122232425262728293031323334353637class DefaultConfig(object): '''配置文件''' # 数据信息 train_file = \"./datasets/tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_train.txt\" test_file = \"./datasets/tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_test.txt\" # 一些特殊符号 seq_end = '&lt;/s&gt;' seq_begin = '&lt;s&gt;' pad = '&lt;pad&gt;' unk = '&lt;unk&gt;' # DataLoader信息 batch_size = 128 shuffle = False # TODO num_workers = 1 # model embed_size = 64 hidden_size = 64 # 对inputs推理的轮数 n_episode = 3 dropout_p = 0.1 # train max_epoch = 500 learning_rate = 0.001 min_loss = 0.01 print_every_epoch = 5 # cuda信息 use_cuda = True device_id = 0 # model_path model_path = \"./models/DMN.pkl\" 训练 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def train(opt, th, train_data): ''' 训练 Args: opt -- 配置信息 th -- TextHelper实例 train_data -- 训练数据，[[facts, question, answer]] ''' # 加载原始数据 seqbegin_id = th.word2index(th.seq_begin) model = DMN(th.vocab_size, opt.embed_size, opt.hidden_size, seqbegin_id, th.word2index(th.pad)) if opt.use_cuda: model = model.cuda(opt.device_id) optimizer = optim.Adam(model.parameters(), lr = opt.learning_rate) loss_func = nn.CrossEntropyLoss(ignore_index=th.word2index(th.pad)) for e in range(opt.max_epoch): losses = [] for batch_data in get_data_loader(train_data, opt.batch_size, opt.shuffle): # batch内的数据进行pad，转成Variable allfacts, allfacts_mask, questions, questions_mask, answers = \\ pad_batch_data(batch_data, th) # 前向 preds = model(allfacts, allfacts_mask, questions, questions_mask, answers.size(1), opt.n_episode) # loss optimizer.zero_grad() loss = loss_func(preds, answers.view(-1)) losses.append(loss.data.tolist()[0]) # 反向 loss.backward() optimizer.step() avg_loss = np.mean(losses) if avg_loss &lt;= opt.min_loss or e % opt.print_every_epoch == 0 or e == opt.max_epoch - 1: info = \"e=&#123;&#125;, loss=&#123;&#125;\".format(e, avg_loss) losses = [] print (info) if e == opt.max_epoch - 1 and avg_loss &gt; opt.min_loss: print (\"epoch finish, loss &gt; min_loss\") torch.save(model, opt.model_path) break elif avg_loss &lt;= opt.min_loss: print (\"Early stop\") torch.save(model, opt.model_path) break 预测和效果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def cal_test_accuracy(model, test_data, th, n_episode=DefaultConfig.n_episode): '''测试，测试数据''' batch_size = 1 model.eval() correct = 0 for item in get_data_loader(test_data, batch_size, False): facts, facts_mask, question, question_mask, answer = pad_batch_data(item, th) preds = model(facts, facts_mask, question, question_mask, answer.size(1), n_episode) #print (answer.data.shape, preds.data.shape) preds = preds.max(1)[1].data.tolist() answer = answer.view(-1).data.tolist() if preds == answer: correct += 1 print (\"acccuracy = \", correct / len(test_data)) def test_one_data(model, item, th, n_episode=DefaultConfig.n_episode): ''' 测试一条数据 Args: model -- DMN模型 item -- [facts, question, answer] th -- TextHelper Returns: None ''' # batch_size = 1 model.eval() item = [item] facts, facts_mask, question, question_mask, answer = pad_batch_data(item, th) preds = model(facts, facts_mask, question, question_mask, answer.size(1), n_episode) item = item[0] preds = preds.max(1)[1].data.tolist() fact = item[0][0] facts = [th.indices2sentence(fact) for fact in item[0]] facts = [\" \".join(fact) for fact in facts] q = \" \".join(th.indices2sentence(item[1])) a = \" \".join(th.indices2sentence(item[2])) preds = \" \".join(th.indices2sentence(preds)) print (\"Facts:\") print (\"\\n\".join(facts)) print (\"Question:\", q) print (\"Answer:\", a) print (\"Predict:\", preds) print () 在本数据集上效果较好，但是数据量小、句子简单，还没有在别的数据集上面进行测试。等忙完了测试一下。","tags":[{"name":"QA","slug":"QA","permalink":"http://plmsmile.github.io/tags/QA/"},{"name":"DMN","slug":"DMN","permalink":"http://plmsmile.github.io/tags/DMN/"}]},{"title":"决策树笔记","date":"2018-03-05T07:44:59.000Z","path":"2018/03/05/29-desicion-tree/","text":"决策树的特征选择、生成、剪枝。熵、信息增益、基尼指数。ID3、C4.5、CART。 决策树背景 概览 树的意义 决策树是一棵if-then树。 内部节点代表一个属性或特征，叶节点代表一个类。 决策树也是给定各个特征的情况下，某类别的概率。即条件概率\\(P(Y \\mid X)\\)。 树的生成 构建根节点，选择最优特征。按照特征划分子集，继续选择新的最优特征，直到没有或者全部被正确分类。 剪枝 决策树的生成对应于模型的局部选择，会尽量拟合训练数据，导致模型复杂和过拟合。 决策树的剪枝对应于模型的全局选择， 自下而上删掉一些节点。 熵和信息增益 在每个节点，要选择一个最优特征生成。 ID3使用信息增益最大选择最优特征 C4.5使用信息增益率最大来选择最优特征 CART回归树 ，平方误差最小 CART分类树， 基尼指数最小 信息量 信息量是随机变量\\(X\\)不确定性的度量。 \\[ I(X) = - \\log p(x) \\] 熵 熵是信息量的期望，也是随机变量不确定性的度量。熵偏向离散属性， 基尼指数偏向连续属性。 \\[ H(X) = - \\sum_{x \\in X} p(x) \\log p(x) \\] 条件熵 条件熵是在给定随机变量\\(X\\)的情况下，随机变量\\(Y\\)的不确定性。 \\[ H(Y \\mid X) = \\sum_{i = 1}^K p(x_i) H(Y \\mid X = x_i) \\] \\(X\\)共有K类，\\(p(x_i)\\)表示\\(X\\)属于第\\(i\\)类的概率。\\(H(Y\\mid X=x_i)\\)表示\\(X=x_i\\)时\\(Y\\)的子集的熵。 经验熵和经验条件熵 由数据估计（极大似然估计）得到的熵和条件熵。 如数据集D，有K个类别。经验熵是 \\[ H(D) = -\\sum_{k=1}^K \\frac{\\vert C_k\\vert}{\\vert D\\vert} \\log_2 \\frac{\\vert C_k\\vert}{\\vert D\\vert} \\] 特征A根据取值把数据集D划分为n个子集，则给定特征A时数据集D的经验条件熵是： \\[ H(D \\mid A) = \\sum_{i=1}^n \\frac{\\vert D_i\\vert}{\\vert D\\vert} H(D_i) = -\\sum_{i=1}^n \\frac{\\vert D_i\\vert}{\\vert D\\vert} \\sum_{k=1}^K \\frac{\\vert D_{ik}\\vert}{\\vert D_i\\vert} \\log_2 \\frac{\\vert D_{ik}\\vert}{\\vert D_i\\vert} \\] 信息增益 信息增益是给定特征A，使得数据集D不确定性减少的程度。信息增益 = 划分前熵 - 划分后熵 = 熵 - 条件熵 \\[ g(D, A) = H(D) - H(D \\mid A) \\] 特征A的信息增益越大，不确定性减少越多，A的分类能力就越强。 信息增益的问题 对于取值很多的特征，比如连续型数据(时间)。每一个取值几乎都可以确定一个样本。即这个特征就可以划分所有的样本数据。 信息增益不适合连续型、取值多的特征 使得所有分支下的样本集合都是纯的，极端情况每一个叶子节点都是一个样本 数据更纯，信息增益更大，选择它作为根节点，结果就是庞大且深度很浅的树 信息增益比 数据集\\(D\\)关于特征A的熵，\\(n\\)是特征A的取值个数： \\[ H_A(D) = -\\sum_{i=1}^n \\frac{\\vert D_i\\vert}{\\vert D\\vert} \\log_2 \\frac{\\vert D_i\\vert}{\\vert D\\vert} \\] 信息增益比 = 信息增益 / 划分前熵 = 信息增益 / D关于特征A的熵 ： \\[ g_R(D, A) = \\frac {g(D, A)}{H_A(D)} = \\frac {H(D) - H(D \\mid A)}{H_A(D)} \\] 解决信息增益的问题：特征A分的类别越多，\\(D​\\)关于A的熵就越大，作为分母，所以信息增益\\(g_R(D, A)​\\) 就越小。在信息增益的基础上增加了一个分母惩罚项。 信息增益比的问题：实际上偏好可取类别数目较少的特征。 基尼指数 CART分类树使用基尼指数来选择最优特征。 基尼指数也是度量不确定性。 熵偏向离散属性， 基尼指数偏向连续属性。 概率分布基尼指数 分类中，有\\(K\\)类。 样本属于第\\(k\\)类的概率为\\(p_k\\)。 \\[ \\rm{Gini}(p) = \\sum_{k=1}^K p_k(1-p_k) = 1 - \\sum_{k=1}^Kp_k^2 \\] 样本集合基尼指数 集合D，有\\(K\\)类，\\(D_k\\) 是第k类的样本子集。则D的基尼指数 \\[ \\rm{Gini}(D) = 1 - \\sum_{k=1}^K \\left(\\frac{\\vert D_k\\vert}{\\vert D\\vert} \\right)^2 \\] 特征A条件基尼指数 特征A取值为某一可能取值为a。 根据A是否取值为a把D划分为\\(D_1\\)和\\(D_2\\)两个集合。 在特征A的条件下，D的基尼指数如下： \\[ \\rm{Gini}(D, A) = \\frac{\\vert D_1\\vert}{\\vert D\\vert} \\rm{Gini}(D_1) + \\frac{\\vert D_2\\vert}{\\vert D\\vert} \\rm{Gini}(D_2) \\] \\(\\rm{Gini}(D, A)​\\)是集合D根据特征A分割后，集合D的不确定性。 ID3算法 决策树的生成，ID3算法以信息增益最大为标准选择特征。递归构建，不断选择最优特征对训练集进行划分。 递归终止条件： 当前节点的所有样本，属于同一类别\\(C_k\\)，无需划分。该节点为叶子节点，类标记为\\(C_k\\) 当前属性集为空，或所有样本在属性集上取值相同 当前节点的样本集合为空，没有样本 在集合D中，选择信息增益最大的特征\\(A_g\\) ： 增益小于阈值，则不继续向下分裂，到达叶子节点。该节点的标记为该节点所有样本中的majority class\\(C_k\\)。 这也是预剪枝 增益大于阈值，按照特征\\(A_g\\)的每一个取值\\(A_g=a_i\\)把D划分为各个子集\\(D_i\\)，去掉特征\\(A_g\\) 继续对每个内部节点进行递归划分。 C4.5算法 C4.5是ID3的改进，C4.5以信息增益率最大为标准选择特征。 ID3/C4.5决策树剪枝 决策树的生成，会过多地考虑如何提高对训练数据的分类，从而构建出非常复杂的决策树。就容易过拟合。 剪枝就是裁掉一些子树和叶节点，并将其根节点或父节点作为叶节点。剪枝分为预剪枝和后剪枝。 预剪枝 在生成树的时候，设定信息增益的阈值，如果某节点的某特征的信息增益小于该阈值，则不继续分裂，直接设为叶节点。选择该节点的D中类别数量最多的类别 （majority class）作为类别标记。 后剪枝 树构建好以后，基于整体，极小化损失函数，自下而上地进行剪枝。 树T的参数表示 叶节点的个数\\(\\vert T \\vert\\) 叶节点\\(t\\) 叶节点\\(t\\)上有\\(N_t\\)个样本 有\\(K\\)类 叶节点t上的经验熵\\(H_t(T)\\) \\(\\alpha \\ge 0\\) 为惩罚系数 叶节点t上的经验熵 \\[ H_t(T) = -\\sum_{k=1}^K \\frac{N_{tk}}{N_t} \\log \\frac{N_{tk}}{N_t} \\] 模型对训练数据的拟合程度\\(C(T)\\) ，所有叶节点的经验熵和： \\[ C(T) = \\sum_{t=1}^{\\vert T \\vert} N_tH_t(T) \\] 最终损失函数 = 拟合程度 + 惩罚因子： \\[ C_\\alpha(T) = C(T) + \\alpha \\vert T\\vert \\] 参数\\(\\alpha\\)权衡了训练数据的拟合程度和模型复杂度。 \\(\\alpha\\)大，决策树简单，拟合不好 \\(\\alpha\\)小，决策树复杂，过拟合 剪枝步骤 计算每个节点的经验熵 递归从树的叶节点向上回缩。叶节点回缩到父节点：整体树：回缩前\\(T_1\\) ，回缩后\\(T_2\\) \\(C_\\alpha(T_2) \\le C_\\alpha(T_1)\\)， 则回缩到父节点， 父节点变成新的叶节点。 CART-回归树 Classification and regression tree分类与回归树。 回归-平方误差最小 分类-基尼指数最小 二叉树 内部节点：是 - 否。如特征$A a $或 \\(A &gt; a\\) 模型 把输入空间划分为M个单元\\(R_1,R_2,\\cdots, R_M\\)， 每个单元有多个样本，有一个固定的输出值\\(c_m\\)。 \\[ \\hat c_m = \\rm{avg} (y_i), \\; y_i \\in R_m \\] 树模型 ： \\[ f(x) = \\sum_{m=1}^M c_m I(x \\in R_m) \\] 划分单元 寻找最优切分变量j和最优切分点s 。 选择第\\(j\\)个变量\\(x^{(j)}\\)和其取值\\(s\\)， 作为切分变量和切分点，划分为两个空间 \\(R_1, R_2\\)，输出分别为\\(c_1, c_2\\) : \\[ R_1(j, s) = \\{x \\mid x^{(j)} \\le s \\}, \\quad \\quad R_2(j, s) = \\{x \\mid x^{(j)} &gt; s \\} \\] 求最优，平方误差最小 ： \\[ \\min_\\limits{j, s} \\left[ \\min_\\limits{c_1} \\sum_{x_i \\in R_1(j, s)} (y_i - c_1)^2 + \\min_\\limits{c_2} \\sum_{x_i \\in R_1(j, s)} (y_i - c_1)^2 \\right] \\] 对每个区域重复划分过程，直到停止。也叫作最小二乘回归树。 CART-分类树 基尼指数最小原则 。 对每一个数据集D，对每一个特征A，对每一个A的取值\\(A=a\\) 是或者否，划分两个自己\\(D_1\\)和\\(D_2\\) 计算在特征\\(A=a\\)条件下的基尼指数\\(\\color{blue} {\\rm{Gini}(D, A=a)}\\) 选择基尼指数最小特征A及其取值a，作为最优特征和最优切分点 从现节点划分为两个子节点 CART剪枝 剪枝总体步骤 从生成的决策树\\(T_0\\)开始， 从底端向上开始剪枝，直到\\(T_0\\)的根节点。损失函数决定是否剪枝 形成子树序列\\(\\{T_0, T_1, \\cdots, T_n\\}\\) 交叉验证子树序列，选择最优子树 K-折交叉验证法 数据集划分为K个子集。每个子集分别做一次验证集，其余K-1组作为训练集。得到K个模型。 剪枝损失函数 \\[ C_\\alpha(T) = C(T) + \\alpha \\vert T\\vert \\] \\(C(T)\\)为所有叶节点的经验熵和 ： \\[ C(T) = \\sum_{t=1}^{\\vert T \\vert} N_tH_t(T) \\] \\(\\alpha\\)权衡训练数据拟合程度和模型复杂度。 整体树\\(T_0\\)的任意内部节点t， \\(\\alpha\\)从0开始，每次一个小区间\\([\\alpha_i, \\alpha_{i+1})\\) ： t为单节点树时损失：\\(C\\alpha(t) = C(t) + \\alpha\\) t为根节点子树时损失：\\(C_\\alpha(T_t) = C(T_t) + \\alpha \\vert T_t\\vert\\) \\(\\alpha=0\\)时， \\(C\\alpha(t) &lt; C_\\alpha(T_t)\\) 。因为，树大，精确，损失小。 随着\\(\\alpha\\)的增大，会达到： \\(C\\alpha(t) = C_\\alpha(T_t)\\) 求得临界点\\(\\alpha​\\) \\[ \\alpha = \\frac{C(T) - C(T_t)} {\\vert T_t\\vert - 1} \\] 对每个内部节点求： \\[ g(t) = \\frac{C(T) - C(T_t)} {\\vert T_t\\vert - 1} \\] 在\\(T_0\\)中减去最小的\\(g(t)\\)对应的子树\\(T_t\\) ， 作为\\(T_1\\) t节点作为叶子节点，类标记为majority class 最后再交叉验证所有的子树序列即可","tags":[{"name":"决策树","slug":"决策树","permalink":"http://plmsmile.github.io/tags/决策树/"},{"name":"ID3","slug":"ID3","permalink":"http://plmsmile.github.io/tags/ID3/"},{"name":"C4.5","slug":"C4-5","permalink":"http://plmsmile.github.io/tags/C4-5/"},{"name":"CART","slug":"CART","permalink":"http://plmsmile.github.io/tags/CART/"},{"name":"熵","slug":"熵","permalink":"http://plmsmile.github.io/tags/熵/"},{"name":"信息增益","slug":"信息增益","permalink":"http://plmsmile.github.io/tags/信息增益/"},{"name":"基尼指数","slug":"基尼指数","permalink":"http://plmsmile.github.io/tags/基尼指数/"}]},{"title":"机器学习知识点汇总整理","date":"2018-03-03T13:32:52.000Z","path":"2018/03/03/28-ml-interview-notes/","text":"一些机器学习的知识点总结 Tensorflow 1 Tensorflow的计算图 Tensorflow通过计算图的形式来表示计算。是一个有向图。节点代表一个计算，边代表计算之间的依赖关系。 构建计算图 执行计算图，session.run 12345678910x_data = np.float32(np.random.rand(2,100))y_data = np.dot([0.1 , 0.2] , x_data) + 0.3b = tf.Variable(tf.zeros([1]))W = tf.Variable(tf.random_uniform([1,2],-1.0,1.0))y = tf.matmul(W,x_data) + bloss = tf.reduce_mean(tf.square(y - y_data))optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss) KNN 看中间的绿色属于哪一类？ 和谁近就属于谁 看k个 1 欧式距离和曼哈顿距离 欧式距离是两点之间的距离 \\[ d = \\sqrt {(x_1-x_2)^2 +(y_1-y_2)^2 } \\] 曼哈顿距离也称作城市街区距离，两个十字路口的实际要走的距离 \\[ d = \\vert x_1 - x_2\\vert + \\vert y_1 -y_2 \\vert \\] 2 K值的选择 k较小：用较小范围进行预测。容易过拟合，模型复杂。 k较大：用较大范围进行预测。较远不相似的也会起作用，会发生错误。模型简单。 k=N：完全不可取。每个都是属于最多样本的类别 一般选择比较小的数值。如采用交叉验证法来选择最优的k值。 （一部分训练，一部分测试） Logistic Regression 我的LR笔记 问题列表 1 简介LR 2 LR与SVM的区别和联系 3 LR与线性回归的区别和联系 1 简介LR 问一个女生喜欢你吗，SVM会告诉你喜欢或者不喜欢。很粗暴。 LR则会告诉你，有多喜欢你，多不喜欢你。就是告诉你一个可能性。多喜欢你是取决于她的看重点权值和你身上有的东西x。 \\[ p(y=1\\mid x;\\theta)= \\sigma(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}} \\quad\\quad = h_\\theta(x) \\] \\[ p(y=1) = h(x), \\quad p(y=0) = 1-h(x) \\] 使用极大似然法进行参数估计：样本恰好使联合概率密度（似然函数）取得最大值。概率密度相乘。 对数似然函数： \\[ \\begin{align} L(\\theta) &amp; = \\log \\prod_{i=1}^N [h(x)^{y_i} ][1-h(x)]^{1-y_i} \\\\ &amp; = \\sum_{i=1}^N \\left( y_i \\log h(x) + (1-y_i)\\log(1-h(x)) \\right) \\\\ &amp; = \\sum_{i=1}^N (y_i \\theta^Tx_i - \\log(1+e^{\\theta^Tx})) \\end{align} \\] 使\\(L(\\theta)\\)最大，取负数就是其损失函数 ： \\[ J(\\theta) = -\\frac{1}{m} L(\\theta) = -\\frac{1}{m} \\sum_{i=1}^N \\left( y_i \\log h(x) + (1-y_i)\\log(1-h(x)) \\right) \\] 令导数为0，发现无法解析求解。 \\[ \\begin{align} \\frac{\\partial L(\\theta)}{\\partial \\theta} &amp; =\\sum_{i=1}^N y_i x_i - \\sum_{i=1}^N \\frac{e^{\\theta^Tx}}{1 + e^{\\theta^Tx}} x_i \\\\ &amp; = \\sum_{i=1}^N x_i (y_i - \\sigma(\\theta^Tx) ) \\end{align} \\] 只能借助迭代法，如梯度下降法和拟牛顿法来进行求解。 2 LR和SVM的比较 相同点 都是分类算法，都是监督学习算法 如果不考虑核函数，LR和SVM都是线性分类算法，决策面都是线性的 LR和SVM都是判别式模型。 不关心数据怎么生成，只关心数据之间的差别。用差别来进行分类。 判别模型： KNN、 LR 、SVM 。 生成式模型：朴素贝叶斯、 HMM 。 不同点 损失函数不同 \\[ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^N \\left( y_i \\log h(x) + (1-y_i)\\log(1-h(x)) \\right) \\] \\[ L(w, b, \\lambda) =\\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i \\left(y_i(w^Tx_i+b) - 1\\right) \\] LR基于概率理论，用sigmoid函数表示，通过极大似然法估计出参数的值。 SVM基于间隔最大化原理，认为最大几何间隔的分类面为最优分类面。 SVM只在乎边界线附近的点(SV)，LR在乎所有的点。 SVM不直接依赖于数据分布；LR受所有点影响，如果不同类别不平衡，要对数据做balancing 处理非线性问题时，SVM使用核函数，LR不使用核函数 SVM只有少数SV进行核计算，LR如果用核函数，则所有的点都会进行计算，代价太高 线性SVM依赖数据的距离测度，要对数据做归一化，而LR不用 SVM损失函数自带L2正则项，而LR需要额外添加 3 LR与线性回归的联系 联系 LR本质上是一个线性核回归模型 区别 目标函数：线性回归最小二乘，LR是似然函数 线性回归整个实数范围内进行预测；LR预测值限定为[0,1] ，sigmoid的非线性形式。轻松处理0/1分类问题","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"}]},{"title":"剑指offer4(51-64)","date":"2018-03-02T13:34:02.000Z","path":"2018/03/02/aim2offer4/","text":"剑指offer(51-64) 数组中的逆序对-51 牛客网数组中的逆序对 逆序对，前面&gt;后面。给一个数组，求出所有逆序对的个数。如{7,5,6,4}， 有75-76-74-54-64这5对。 使用冒泡排序思想，每一次交换，就说明有一个逆序对，统计交换次数。 利用归并排序思想，","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"}]},{"title":"SVM笔记","date":"2018-03-01T12:42:20.000Z","path":"2018/03/01/27-svm-notes/","text":"Support Vector Machine简单笔记。 特征空间上的间隔最大的线性分类器。学习策略是间隔最大化，转化为一个凸二次规划问题的求解。 SVM概览 线性分类器 逻辑回归的图像和公式如下，预测的分类为1的概率。 \\[ h_\\theta(x) = g(\\theta^Tx), \\quad g(z) = \\frac{1}{1+e^{-z}}, \\quad g(z) = \\begin{cases} 1, &amp; z\\ge 0 \\\\ -1, &amp; z &lt; 0 \\\\ \\end{cases} \\] \\[ y = \\begin{cases} 1, \\; &amp; h_\\theta(x) \\ge 0.5, \\;即\\; \\theta^Tx \\ge 0\\\\ 0, \\; &amp; h_\\theta(x) &lt; 0.5, \\; 即 \\; \\theta^Tx &lt; 0 \\\\\\end{cases} \\] 其中\\(\\theta^Tx=w^Tx+b=0\\) 是一个超平面。 用分类函数表示\\(f(x)=w^Tx+b\\) 。 \\(w\\)是这个超平面的法向量。 即对于任意一个x，有如下预测类别： \\[ \\hat y=\\begin{cases} 1, &amp; f(x) \\ge 0\\\\ -1, &amp; f(x) &lt; 0 \\\\ \\end{cases} \\] 函数间隔与几何间隔 函数间隔 超平面\\(w^Tx+b=0\\)确定后， \\(\\vert w\\cdot x+b\\vert\\)表示点x到平面的距离，表示分类可靠性。距离越远，分类越可信。\\(y\\)与\\(w\\cdot x+b\\)的符号的一致性表示分类的正确性。 超平面\\((w,b)\\)关于样本点\\((x_i, y_i)\\)的函数间隔\\(\\hat \\gamma_i\\)如下： \\[ \\hat \\gamma_i = y_i (w^T \\cdot x_i + b) \\] 超平面关于所有样本点的函数间隔\\(\\hat \\gamma​\\) ： \\[ \\hat \\gamma = \\min \\hat \\gamma_i \\] 函数间隔的问题：w和b成比例改变，超平面未变，但函数间隔已变。 几何间隔 对函数间隔除以法向量的二范数，则得到超平面与点\\((x_i,y_i)\\)的几何间隔\\(\\gamma_i\\) ： \\[ \\gamma_i = \\frac{\\hat \\gamma_i}{\\|w\\|} = \\frac{y_i(w^T\\cdot x_i + b)}{\\|w\\|} \\] 超平面关于所有样本点的几何间隔： \\[ \\gamma = \\min \\gamma_i \\] 几何间隔才是直观上点到超平面的距离。 最大间隔分类器 分类时，超平面离数据点的间隔越大，分类的确信度也越大。 所以要最大化这个几何间隔，目标函数如下： \\[ L = \\max_\\limits{w, b} \\gamma, \\quad s.t,\\quad \\gamma_i \\ge \\gamma \\] 用函数间隔\\(\\hat \\gamma\\)描写为： \\[ L = \\max_\\limits{w, b} \\frac{\\hat \\gamma}{\\|w\\|}, \\quad s.t, \\quad \\hat \\gamma_i \\ge \\hat \\gamma, \\; \\text{ 其中 }\\hat \\gamma_i = y_i(w^T \\cdot x_i + b) \\] 函数间隔\\(\\hat \\gamma​\\)的取值并不会影响最优化问题的解。 \\(\\lambda w, \\lambda b \\to \\lambda \\hat \\gamma​\\) 目标函数 取函数间隔为1，\\(\\hat \\gamma = 1\\)， 则有目标函数： \\[ L = \\max_\\limits{w,b} \\frac{1}{\\|w\\|}, \\quad s.t, \\quad y_i(w^Tx_i+b) \\ge 1 \\] 支持向量是虚线边界上的点，则有： \\[ \\begin{cases} y_i(w^Tx_i+b)=1, &amp; 支持向量 \\\\ y_i(w^Tx_i+b) &gt;1, &amp; 其他点 \\\\ \\end{cases} \\] 分类 \\[ \\hat y=\\begin{cases} 1, &amp; f(x) \\ge 0\\\\ -1, &amp; f(x) &lt; 0 \\\\ \\end{cases} \\] 线性SVM 拉格朗日对偶性 1 原始问题 \\(f(x), c_i(x), h_j(x)\\)都连续可微。 最优化： \\[ \\min_\\limits{x\\in R} f(x) \\] 有很多个约束条件（不等式约束和等式约束）： \\[ c_i(x) \\le 0 ,\\quad h_j(x) = 0 \\] 求解原始问题 将约束问题无约束化。 引入拉格朗日函数，其中\\(\\alpha_i (\\ge 0)\\)和\\(\\beta_j\\)是拉格朗日乘子 \\[ L(x, \\alpha, \\beta) = f(x) + \\sum\\alpha_ic_i(x) + \\sum \\beta_j h_j(x) \\] 定义关于\\(x\\)的函数\\(\\theta_p(x)\\)： \\[ \\theta_p(x) = \\max_\\limits{\\alpha,\\beta:\\alpha_i\\ge0} L(x, \\alpha, \\beta) \\] \\[ \\theta_p(x) = \\begin{cases} f(x), &amp;x满足约束 \\\\ +\\infty, &amp; 其他 \\\\ \\end{cases} \\] \\(f(x)\\)求最小，则对\\(\\theta_p(x)\\)求最小。 原始问题： 先固定x，优化出参数\\(\\alpha, \\beta\\)，再优化x。 \\[ \\min_\\limits{x} \\; \\theta_p(x) = \\min_\\limits{x} \\max_\\limits{\\alpha, \\beta:\\alpha_i\\ge0} L(x, \\alpha, \\beta) \\] 所以原始最优化问题 变为 拉格朗日函数的极小极大问题。 定义原始问题的最优解\\(p^*\\) ： \\[ p^* = \\min_\\limits{x} \\theta_p(x) \\] 2 对偶问题 定义关于\\(\\alpha, \\beta\\)的函数\\(\\theta_d(\\alpha, \\beta)\\) \\[ \\theta_d(\\alpha, \\beta) = \\min_x L(x, \\alpha, \\beta) \\] 对偶问题：先固定参数\\(\\alpha, \\beta\\) ，优化出x，再优化出参数。 先优化x。 \\[ \\max_\\limits{\\alpha, \\beta:\\alpha_i\\ge0} \\theta_d(\\alpha, \\beta) = \\max_\\limits{\\alpha, \\beta:\\alpha_i\\ge0} \\min_x L(x, \\alpha, \\beta) \\] 原始问题： 先固定x，优化出参数\\(\\alpha, \\beta\\)，再优化x。先优化参数。 \\[ \\min_\\limits{x} \\; \\theta_p(x) = \\min_\\limits{x} \\max_\\limits{\\alpha, \\beta:\\alpha_i\\ge0} L(x, \\alpha, \\beta) \\] 定义对偶问题的最优值： \\[ d^* = \\max_\\limits{\\alpha, \\beta:\\alpha_i\\ge0} \\theta_d(\\alpha, \\beta) \\] 3 原始问题与对偶问题的关系 因为： \\[ \\theta_d(\\alpha, \\beta) = \\min_x L(x, \\alpha, \\beta) \\le \\max_\\limits{\\alpha,\\beta:\\alpha_i\\ge0} L(x, \\alpha, \\beta) = \\theta_p(x) \\] 定理1：如果原始问题与对偶问题均有最优值，则有：\\(d^* \\le p^*\\) \\[ d^* = \\max_\\limits{\\alpha, \\beta:\\alpha_i\\ge0} \\min_x L(x, \\alpha, \\beta) \\le \\min_\\limits{x} \\max_\\limits{\\alpha, \\beta:\\alpha_i\\ge0} L(x, \\alpha, \\beta) = p^* \\] 推论1：如果\\(d^* = p^*\\)， 那么\\(x^*, \\alpha^*, \\beta^*\\)分别是原始问题和对偶问题的最优解。 通过对偶问题，来解决原始问题。 4 KKT条件 满足什么条件，才能使\\(d^* = p^*\\)呢 ？ 首先满足下面的大条件： 假设\\(f(x)\\)和\\(c_i(x)\\)都是凸函数， \\(h_j(x)\\)是仿射函数；假设不等式约束\\(c_i(x)\\)是严格可行的。 定理2：则存在解，\\(x^*\\)是原始问题的最优解，\\(\\alpha^*, \\beta^*\\)是对偶问题的最优解。 并且： \\[ d^* = p^* = L(x^*, \\alpha^*, \\beta^*) \\] KKT条件：则\\(x^*\\)是原始问题、\\(\\alpha^*, \\beta^*\\)是对偶问题的最优解的充分必要条件是\\(x^*, \\alpha^*, \\beta^*\\)满足下面的KKT条件： \\[ \\begin{align} &amp; 偏导为0条件\\\\ &amp; \\nabla_x L(x^*, \\alpha^*, \\beta^*) = 0 \\\\ &amp; \\nabla_\\alpha L(x^*, \\alpha^*, \\beta^*) = 0 \\\\ &amp; \\nabla_\\beta L(x^*, \\alpha^*, \\beta^*) = 0 \\\\ &amp; 约束条件 \\\\ &amp; c_i(x^*) \\le 0 \\\\ &amp; h_j(x^*) = 0 \\\\ &amp; \\alpha_i^* \\ge 0 \\\\ &amp; \\rm{KKT}对偶互补条件 \\\\ &amp; \\alpha_i^* c_i(x^*) = 0 \\\\ \\end{align} \\] 由KKT对偶互补条件可知，若\\(\\alpha_i^* &gt;0\\)， 则\\(c_i(x^*)=0\\) 。SVM推导会用到。 若\\(\\alpha_i&gt;0\\)， 则对应的\\(x_i\\)是支持向量， 有\\(y_i(w^*\\cdot x+ b^*) = 1\\)。 所有的非支持向量，都有\\(\\alpha_i =0\\)。 原始问题到对偶问题 先前的目标函数： \\[ J = \\max_\\limits{w,b} \\frac{1}{\\|w\\|}, \\quad s.t, \\quad y_i(w^Tx_i+b) \\ge 1 \\] 最大变为最小，则有原始问题如下。目标函数是二次的，约束条件是线性的。所以是个凸二次规划问题。 \\[ J = \\min_{w,b} \\frac{1}{2} \\|w\\|^2, \\quad s.t, \\quad y_i(w^Tx_i+b) \\ge 1 \\] 构造拉格朗日函数 ： \\[ L(w, b, \\lambda) =\\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i \\left(y_i(w^Tx_i+b) - 1\\right) \\] 原始问题 \\[ \\theta_p(w,b) = \\max_{\\lambda_i \\ge 0} L(w, b, \\alpha) \\] \\[ p^* = \\min_{w, b} \\theta_p(w, b) = \\min_{w, b} \\max_{\\lambda_i \\ge 0} L(w, b, \\alpha) \\] 对偶问题 \\[ \\theta_d(\\alpha) = \\min_{w,b} L(w, b, \\alpha) \\] \\[ d^* = \\max_{\\alpha_i \\ge 0} \\theta_d(\\alpha) = \\max_{\\alpha_i \\ge 0} \\min_{w,b} L(w, b, \\alpha) \\] 我们知道\\(d^* \\le p^*​\\)， 有时相等。原始问题可以转化为对偶问题求解，好处是：近似解，好求解。 求解对偶问题 拉格朗日函数： \\[ L(w, b, \\lambda) =\\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i \\left(y_i(w^Tx_i+b) - 1\\right) \\] 化简后： \\[ L(w, b, \\lambda) =\\frac{1}{2} \\|w\\|^2 -\\sum_{i=1}^n\\alpha_iy_iw^Tx_i- \\sum_{i=1}^n\\alpha_iy_ib + \\sum_{i=1}^n\\alpha_i \\] 目标函数： \\[ d^* = \\max_{\\alpha_i \\ge 0} \\theta_d(\\alpha) = \\max_\\limits{\\alpha_i \\ge 0} \\min_\\limits{w,b} L(w, b, \\alpha) \\] 主要是三个步骤： 固定参数\\(\\alpha\\)， 求 极小化\\(\\min_{w,b} L(w, b, \\alpha)\\)的w和b 带入w和b，对\\(L\\)求参数\\(\\alpha\\) 的极大化 利用SMO算法求解对偶问题中的拉格朗日乘子\\(\\alpha\\) 1 极小求出w和b \\(\\min_\\limits{w,b} L(w, b, \\alpha)\\) 对w和b求偏导，使其等于0。 \\[ \\frac{\\partial L}{\\partial w} = w -\\sum_{i=1}^n\\alpha_iy_ix_i \\begin{equation}\\xlongequal {令}{} 0 \\end{equation} \\quad \\to \\quad w = \\sum_{i=1}^n\\alpha_iy_ix_i \\] \\[ \\frac{\\partial L}{\\partial b} = - \\sum_{i=1}^n \\alpha_iy_i \\xlongequal {令}{} 0 \\quad \\to \\quad \\sum_{i=1}^n \\alpha_iy_i=0 \\] 特别地范式求导：\\(\\frac{\\partial \\|w\\|^2}{\\partial w} = 2w​\\) \\[ \\frac{\\partial \\|w\\|^2}{\\partial w} = w \\] 把上面两个结论，带入原式进行化简，得到： \\[ \\begin{align} L(w, b, \\alpha) &amp;=\\frac{1}{2}w^Tw - \\sum_{i=1}^n\\alpha_iy_iw^Tx_i- \\sum_{i=1}^n\\alpha_iy_ib + \\sum_{i=1}^n\\alpha_i \\\\ &amp; = \\frac{1}{2} w^T\\sum_{i=1}^n\\alpha_iy_ix_i - w^T \\sum_{i=1}^n\\alpha_iy_ix_i - b\\sum_{i=1}^n\\alpha_iy_i + \\sum_{i=1}^n\\alpha_i \\quad\\text{(带入w，提出b，带入0)}\\\\ &amp; = -\\frac{1}{2}\\left(\\sum_{i=1}^n\\alpha_iy_ix_i\\right)^T\\left(\\sum_{i=1}^n\\alpha_iy_ix_i\\right) + \\sum_{i=1}^n\\alpha_i \\quad{(只有x是向量，直接转置)}\\\\ &amp;= -\\frac{1}{2}\\left(\\sum_{i=1}^n\\alpha_iy_ix_i^T\\right)\\left(\\sum_{i=1}^n\\alpha_iy_ix_i\\right) + \\sum_{i=1}^n\\alpha_i \\\\ &amp; = \\sum_{i=1}^n\\alpha_i -\\frac{1}{2}\\sum_{i=1}^n\\sum_{i=1}^n\\alpha_i \\alpha_j y_i y_j x_i^T x_j \\end{align} \\] 得到只用\\(\\alpha\\)表示的拉格朗日函数： \\[ L(w, b, \\alpha) =\\sum_{i=1}^n\\alpha_i -\\frac{1}{2}\\sum_{i=1}^n\\sum_{i=1}^n\\alpha_i \\alpha_j y_i y_j x_i^T x_j \\] 2 求出对\\(\\alpha​\\)的极大 \\(\\max_{\\alpha_i \\ge 0} \\theta_d(\\alpha) = \\max_\\limits{\\alpha_i \\ge 0} \\min_\\limits{w,b} L(w, b, \\alpha)​\\) 对偶问题 如下： 目标函数： \\[ \\max_\\limits{\\alpha} \\; \\sum_{i=1}^n\\alpha_i -\\frac{1}{2}\\sum_{i=1}^n\\sum_{i=1}^n\\alpha_i \\alpha_j y_i y_j x_i^T x_j \\] 约束条件： \\[ \\begin{align} &amp;\\alpha_i \\ge 0 \\\\ &amp; \\sum_{i=1}^n \\alpha_iy_i = 0\\\\ \\end{align} \\] 利用SMO算法求出拉格朗日乘子\\(\\alpha^*\\)。 3 求出w和b，得到分离超平面和决策函数 根据前面的公式得到\\(w^*\\)： \\[ w* =\\sum_{i=1}^n\\alpha_i^*y_ix_i \\] 选一个\\(\\alpha^*_j &gt; 0\\)对应的点\\((x_j, y_j)\\) 就是支持向量。由于支持向量\\(y_j(w^*\\cdot x+ b^*) -1 = 0\\) ，\\(y_j^2=1\\) 得到\\(b^*\\) ： \\[ b^* = y_j - \\sum_{i=1}^n\\alpha_i^*y_i(x_i\\cdot x_j), \\quad\\quad \\text{($x_i\\cdot x_j$是向量内积，后面同理)} \\] 通过公式可以看出，决定w和b的是支持向量， 其它点对超平面是没有影响的。 分离超平面 \\[ f(x) = \\sum_{i=1}^n\\alpha_i^*y_i(x_i\\cdot x) + b^* = 0 \\] 分类决策函数 \\[ f(x) = \\rm{sign}\\left(\\sum_{i=1}^n\\alpha_i^*y_i(x_i\\cdot x) + b^* \\right) \\] 简单总结 目标函数 \\[ J = \\min_{w,b} \\frac{1}{2} \\|w\\|^2, \\quad s.t, \\quad y_i(w^Tx_i+b) \\ge 1 \\] 拉格朗日函数 \\[ L(w, b, \\lambda) =\\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i \\left(y_i(w^Tx_i+b) - 1\\right) \\] 转化为对偶问题求解， 需要会求解过程、会推导公式。 \\[ \\max_{\\alpha_i \\ge 0} \\min_{w,b} L(w, b, \\alpha) \\] 主要是下面4个求解步骤：十分重要!!! 固定\\(\\alpha\\)， L对w和b求偏导，得到两个等式 结果带入L，消去w和b，得到只有\\(\\alpha\\)的L 利用SMO求出\\(\\alpha^*\\) 利用\\(\\alpha^*\\)和支持向量，算出w和b。得出分离超平面和分界函数。 求导后消去w和b，得到L \\[ L(w, b, \\alpha) =\\sum_{i=1}^n\\alpha_i -\\frac{1}{2}\\sum_{i=1}^n\\sum_{i=1}^n\\alpha_i \\alpha_j y_i y_j x_i^T x_j \\] 利用SMO求得\\(\\alpha^*\\)后， 带回原式，得到w和b： \\[ w* =\\sum_{i=1}^n\\alpha_i^*y_ix_i, \\quad b^* = y_j - \\sum_{i=1}^n\\alpha_i^*y_i(x_i\\cdot x_j), \\] 实际上最重要是向量内积来进行决策 \\[ f(x) = \\rm{sign}\\left(\\sum_{i=1}^n\\alpha_i^*y_i \\color{red}{(x_i\\cdot x}) + b^* \\right) \\] 目标函数 \\[ \\max_\\limits{\\alpha_i \\ge 0}L(w, b, \\lambda) = \\max_\\limits{\\alpha_i \\ge 0} \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i \\color{red}{\\left(y_i(w^Tx_i+b) - 1\\right)} \\] 两种数据点 支持向量：红色为0，\\(\\alpha_i &gt; 0\\)。 后面为0。 其它点：红色大于1，\\(\\alpha_i=0\\)。 后面为0。 非线性SVM 核函数 问题 大部分数据不是线性可分的，前面的超平面根本不存在。可以用一个超曲面进行分离，这就是非线性可分问题。 SVM可以通过核函数把输入映射到高维特征空间，最终在高维特征空间中构造最优分离超平面。 需要映射和学习线性SVM： 把输入映射到特征空间F 在特征空间F中使用线性学习器分类 \\[ f(x) = \\sum_{i=1}^n\\alpha_i^*y_i \\color{red}{(\\phi(x_i)\\cdot \\phi(x)}) + b^* \\] 核函数的功能 核函数在特征空间中直接计算内积，就像在原始输入点的函数中一样，两个步骤合二为一： \\[ K(x, z) = \\phi(x) \\cdot \\phi(z) \\] 分类函数： \\[ f(x) = \\sum_{i=1}^n\\alpha_i^*y_i \\color{red}{k(x_i, x)} + b^* \\] 对偶问题： \\[ \\begin{align} &amp; \\max_\\limits{\\alpha} \\; \\sum_{i=1}^n\\alpha_i -\\frac{1}{2}\\sum_{i=1}^n\\sum_{i=1}^n\\alpha_i \\alpha_j y_i y_j k(x_i, x) \\\\ &amp; s.t, \\quad \\alpha_i \\ge 0, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{align} \\] 核函数处理非线性数据 简单例子 上面的数据线性不可分，两个维度(a, b)。 应该用二次曲线(特殊圆)来区分： \\[ w_1a + w_2a^2 +w_3b + w_4b^2 + w_5ab + b=0 \\] 看做映射到了五维空间： \\[ w_1z_1 + w_2z_2 + w_3z_3 + w_4z_4 + w_5z_5 + b = \\sum_{i=1}^5 w_iz_i + b = 0 \\] 如下图：（实际映射到了三维空间的图），可以使用一个平面来分开： 问题 五维是由1维和2维进行组合，就可以解决问题。所以对输入数据无脑组合映射到高维可以吗？当然是不可以的。维数太高，根本没法计算，不能无脑组合映射。 核函数的功能 看核函数： \\[ k(x_1, x_2) = (x_1 \\cdot x_2 + 1)^2 \\] 核函数和上面映射空间的结果是一样的！区别： 映射计算：先映射到高维空间，然后根据内积进行计算 核函数：直接在原来的低维空间中计算，而不需显示写出映射后的结果。避开了在高维空间中的计算！ 常用核函数 1 线性核 \\[ k(x_1, x_2) = x_1 \\cdot x_2 \\quad\\quad\\text{(原始空间的内积)} \\] 目的：映射前和映射后，形式上统一了起来。写个通用模板，再带入不同的核就可以了。 2 高斯核 \\[ k(x_1, x_2) = \\exp \\left( - \\frac{\\|x_1 - x_2\\|^2}{2\\sigma^2}\\right) \\] 高斯核函数，非常灵活，应用很广泛。可以映射到无穷维。 \\(\\sigma\\) 的选择 太大：权重衰减快，相当于映射到低维子空间 太小：将任意数据线性可分，容易陷入严重过拟合 3 多项式核 \\[ k(x_1, x_2) = ((x_1, x_2) + R)^d \\] 核函数总结 问题的出现 数据线性不可分，要映射到高维空间中去 不能无脑低维组合映射到高维空间，维度太大根本没法计算 核函数的功能 将特征向由低维向高维的转换 直接在低位空间中进行计算 实际的分类效果却是在高维上 避免了直接在高维空间中的复杂计算 SVM曲线，逻辑回归和决策树是直线。SVM的效果好。 松弛变量软间隔最大化 定义 数据可能有一些噪声特异点outlier导致不是线性可分或者效果不好。 如果不处理outlier，则会非常影响SVM。因为本身支持向量就只有几个。 给每个数据点加上松弛变量\\(\\xi_i \\ge 0\\)， 使函数间隔+松弛变量大于等于1，即约束条件： \\[ y_i(w \\cdot x_i + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0 \\] 为每个松弛变量\\(\\xi_i\\)支付一个代价\\(\\xi_i\\)， 新的目标函数和约束条件如下： \\[ \\min \\frac{1}{2} \\|w\\|^2 + C\\sum_{i=1}^n \\xi_i \\] \\[ s.t, \\quad \\quad y_i(w \\cdot x_i + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0 \\] 惩罚系数C是一个常数 C大时，对误分类的惩罚增大 C来调节权衡：使间隔尽量大；误分类点个数尽量少 求解 定义新的拉格朗日函数： \\[ L(w,b,\\xi,\\alpha, r) = \\frac{1}{2} \\|w\\|^2 + C\\sum_{i=1}^n \\xi_i -\\sum_{i=1}^n \\alpha_i \\left(y_i(w^Tx_i+b) - 1 + \\xi_i\\right) - \\sum_{i=1}^nr_i\\xi_i \\] 和前面对偶问题求解一样，求导求解： \\[ \\frac{\\partial L}{\\partial w} = 0 \\quad \\to \\quad w = \\sum_{i=1}^n\\alpha_iy_ix_i \\] \\[ \\frac{\\partial L}{\\partial b} = 0 \\quad \\to \\quad \\sum_{i=1}^n\\alpha_iy_i = 0 \\] \\[ \\frac{\\partial L}{\\partial \\xi} = 0 \\quad \\to \\quad C -\\alpha_i - r_i = 0 \\] 带入，得到新的L \\[ \\max_{\\alpha} L = \\sum_{i=1}^n\\alpha_i -\\frac{1}{2}\\sum_{i=1}^n\\sum_{i=1}^n\\alpha_i \\alpha_j y_i y_j( x_i \\cdot x_j) \\] 约束条件： \\[ 0 \\le \\alpha_i \\le C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\] SVM的深层理解 感知机算法 感知机算法是一个二类分类的线性模型，也是找一个超平面进行划分数据： \\[ f(x) = \\rm{sign}(w\\cdot x + b) \\] 损失函数是所有误分类点到超平面的总距离： \\[ \\min_\\limits{w, b} L(w, b) = - \\sum_{x_i \\in M}y_i(w\\cdot x_i + b) \\] 可以使用SGD对损失函数进行优化。 当训练数据集线性可分时，感知机算法是收敛的。可以在一定迭代次数上，找到一个超平面，有很多个解。 感知机的超平面不是最优效果，最优是SVM。 损失函数 数据\\(x\\)， 预测值\\(f(x)=\\hat y\\)， 真实值\\(y\\)。 常见损失 01损失 \\[ L(y, \\hat y) = \\begin{cases} 1, &amp; y \\neq \\hat y \\\\ 0, &amp; y = \\hat y \\end{cases} \\] 平方损失 \\[ L(y, \\hat y) = (y - \\hat y)^ 2 \\] 绝对损失 \\[ L(y, \\hat y) = \\vert y - \\hat y\\vert \\] 对数损失 \\[ L(y, \\hat y) = -\\log P(y \\hat x) \\] 期望损失 期望损失也称为风险函数，需要知道联合概率分布\\(P(X, Y)\\)， 一般不知道。 \\[ R_{\\rm{exp}} = E_p[L(y, \\hat y)] = \\int_{(x,y)} L(y, \\hat y) P(x, y) {\\rm d}x{\\rm d}y \\] 经验损失 经验损失也成为经验风险 ，所以监督学习就是要经验风险最小化。 \\[ R_{\\rm emp}(f) = \\frac{1}{N} \\sum_{i=1}^NL(y_i, \\hat y_i) \\] 结构风险最小化 样本数量太小时，容易过拟合。需要加上正则化项，也称为惩罚项。 模型越复杂，越大。 \\[ R_{\\rm srm}(f) = \\frac{1}{N} \\sum_{i=1}^NL(y_i, \\hat y_i) + \\lambda J(f) \\] \\(\\lambda\\ge0\\) 是系数，权衡经验风险和模型复杂度。 监督学习，就是要使结构风险最小化。 SVM也是最优化+损失最小。 可以从损失函数和优化算法角度去看SVM、boosting、LR，可能会有不同的收获。 SVM的合页损失函数 从最优化+损失最小的角度去理解SVM。 最小二乘法 最小二乘法，就是通过最小化误差的平方来进行数学优化。对参数进行求偏导，进行求解。 SMO 模型 \\[ \\min \\frac{1}{2} \\|w\\|^2 + C\\sum_{i=1}^n \\xi_i \\] \\[ s.t, \\quad \\quad y_i(w \\cdot x_i + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0 \\] 序列最小最优化SMO (Sequential minimal optimization)，解决求解\\(\\alpha\\)的问题： \\[ \\min_{\\alpha} L = \\frac{1}{2}\\sum_{i=1}^n\\sum_{i=1}^n\\alpha_i \\alpha_j y_i y_jK(x_i, x_j) - \\sum_{i=1}^n\\alpha_i \\] \\[ s.t, \\quad \\quad0 \\le \\alpha_i \\le C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\] 如果所有变量的解都满足KKT条件，则最优化问题的解已经得到。 思想 每次抽取两个乘子\\(\\alpha_1, \\alpha_2\\)，然后固定其他乘子，针对这两个变量构建一个子二次规划问题，进行求解。不断迭代求解子问题，最终解得原问题。 选择乘子 \\(\\alpha_1\\)选择违反KKT条件最严重的，\\(\\alpha_2\\)选择让\\(\\alpha_1\\)变化最大的。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"SVM","slug":"SVM","permalink":"http://plmsmile.github.io/tags/SVM/"},{"name":"拉格朗日对偶性","slug":"拉格朗日对偶性","permalink":"http://plmsmile.github.io/tags/拉格朗日对偶性/"},{"name":"对偶问题","slug":"对偶问题","permalink":"http://plmsmile.github.io/tags/对偶问题/"},{"name":"支持向量","slug":"支持向量","permalink":"http://plmsmile.github.io/tags/支持向量/"},{"name":"核函数","slug":"核函数","permalink":"http://plmsmile.github.io/tags/核函数/"},{"name":"感知机","slug":"感知机","permalink":"http://plmsmile.github.io/tags/感知机/"},{"name":"损失函数","slug":"损失函数","permalink":"http://plmsmile.github.io/tags/损失函数/"}]},{"title":"IDE配置","date":"2018-02-10T13:39:42.000Z","path":"2018/02/10/12-ide-envs/","text":"VSCode-CPP vscode-cpp 使用vscode写cpp程序。参考自vscode-cpp知乎 1 下载安装vscode 2 下载安装MINGW 3 VSCode安装cc++官方扩展 4 配置json文件 c_cpp_properties.json， 编译环境 ctrl+shift+p ，选择edit configurations， 更新windows的配置 12345678910111213141516171819202122232425262728&#123; \"name\": \"Win32\", \"intelliSenseMode\": \"clang-x64\", \"includePath\": [ \"$&#123;workspaceRoot&#125;\", \"C:/MinGW/lib/gcc/mingw32/6.3.0/include/c++\", \"C:/MinGW/lib/gcc/mingw32/6.3.0/include/c++/mingw32\", \"C:/MinGW/lib/gcc/mingw32/6.3.0/include/c++/backward\", \"C:/MinGW/lib/gcc/mingw32/6.3.0/include\", \"C:/MinGW/include\", \"C:/MinGW/lib/gcc/mingw32/6.3.0/include-fixed\" ], \"defines\": [ \"_DEBUG\", \"UNICODE\", \"__GNUC__=6\", \"__cdecl=__attribute__((__cdecl__))\" ], \"browse\": &#123; \"path\": [ \"C:/MinGW/lib/gcc/mingw32/6.3.0/include\", \"C:/MinGW/lib/gcc/mingw32/6.3.0/include-fixed\", \"C:/MinGW/include/*\" ], \"limitSymbolsToIncludedHeaders\": true, \"databaseFilename\": \"\" &#125;&#125; launch.json， 执行结果文件。选择左边的调试ctrl+shift+d， 选择添加配置，选择gdbll什么的。粘贴下面的内容。 123456789101112131415161718&#123; \"version\": \"0.2.0\", \"configurations\": [ &#123; \"name\": \"C++ Launch (GDB)\", // 配置名称，将会在启动配置的下拉菜单中显示 \"type\": \"cppdbg\", // 配置类型，这里只能为cppdbg \"request\": \"launch\", // 请求配置类型，可以为launch（启动）或attach（附加） \"targetArchitecture\": \"x86\", // 生成目标架构，一般为x86或x64，可以为x86, arm, arm64, mips, x64, amd64, x86_64 \"program\": \"$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.out\", // 将要进行调试的程序的路径 \"miDebuggerPath\":\"c:\\\\MinGW\\\\bin\\\\gdb.exe\", // miDebugger的路径，注意这里要与MinGw的路径对应 \"args\": [], // 程序调试时传递给程序的命令行参数，一般设为空即可 \"stopAtEntry\": false, // 设为true时程序将暂停在程序入口处，一般设置为false \"cwd\": \"$&#123;workspaceRoot&#125;\", // 调试程序时的工作目录，一般为$&#123;workspaceRoot&#125;即代码所在目录 \"externalConsole\": true, // 调试时是否显示控制台窗口，一般设置为true显示控制台 \"preLaunchTask\": \"g++\" // 调试会话开始前执行的任务，一般为编译程序，c++为g++, c为gcc &#125; ]&#125; tasks.json， 编译参数。F5运行调试代码， 选择配置任务，使用模板创建tasks文件，选择others。粘贴下面的内容。 1234567891011121314151617&#123; \"version\": \"0.1.0\", \"command\": \"g++\", \"args\": [\"-g\",\"$&#123;file&#125;\",\"-o\",\"$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.out\", \"-std=c++0x\"], // gcc编译命令参数 \"problemMatcher\": &#123; \"owner\": \"cpp\", \"fileLocation\": [\"relative\", \"$&#123;workspaceRoot&#125;\"], \"pattern\": &#123; \"regexp\": \"^(.*):(\\\\d+):(\\\\d+):\\\\s+(warning|error):\\\\s+(.*)$\", \"file\": 1, \"line\": 2, \"column\": 3, \"severity\": 4, \"message\": 5 &#125; &#125;&#125;","tags":[{"name":"vscode","slug":"vscode","permalink":"http://plmsmile.github.io/tags/vscode/"}]},{"title":"Linux使用笔记","date":"2018-01-25T08:07:07.000Z","path":"2018/01/25/09-linux-notes/","text":"Linux环境配置，tmux+zsh+autojump Tmux使用 tmux list-keys 可以看到所有的按键。 使用tmux进行复制粘贴 1234567891011121314# 1. 复制# 进入vim模式ctrl x + [ # 输入Space开始选择space# 输入enter选择完成enter# 2. 粘贴到另外一个窗口的vim文件里面# i进入vim的插入模式i# 粘贴到文件里ctrl x + ] 即ctrl x + [ 是进入复制模式，可以进行上下翻页。ctrl x + ] 是粘贴。 交换window 123# &#123;&#125;实际操作是shift+[&#123;按键，和复制模式有点相似ctrl x + &#123;ctrl x + &#125; 搭建环境 tmux 默认就有，没有的话就再去安装。主要是离线安装。在公司的时候有，估计又得重新谷歌了。 .tmux.conf zsh 首先要确保是否有zsh。没有的话，得自己先安装zsh。如果没有root权限，不好搞。 1cat /etc/shells 安装oh-my-zsh 1wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh 配置.zshrc，.zshrc autojump","tags":[{"name":"linux","slug":"linux","permalink":"http://plmsmile.github.io/tags/linux/"},{"name":"tmux","slug":"tmux","permalink":"http://plmsmile.github.io/tags/tmux/"}]},{"title":"剑指offer3(21-40)","date":"2018-01-07T04:22:49.000Z","path":"2018/01/07/aim2offer3/","text":"剑指offer算法题(21-40) 把奇数放在偶数的前面-21 问题 输入：一个数组，乱序，有奇数和偶数 输出：把所有的奇数放在前面，所有的偶数放在后面 暴力思路 从头到尾遍历数组 遇到奇数，访问下一个 遇到偶数，把它后面的数字都向前挪动以为，该偶数放到末尾 冒泡思路 for (int i = n-1; i &gt; 0; i--) 依次放置好后面n-1个数即可 从for (int j = 0; j &lt; i; j++) ，遇到j-1是偶数，j是奇数，则交换 辅助数组 新数组存偶数，原数组删除偶数，最后把新数组的偶数追加到原数组 遍历原数组，遇到偶数，存到新数组，删除原数组中的偶数 双指针快排思路 类似于快速排序的思路，但不是稳定的。 指针1，在前面，向后移动，前面应是奇数 指针2，在后面，向前移动，后面应是偶数 指针1指偶数，指针2指奇数，交换两个数 直到指针相遇 关键代码 1234567891011121314151617181920212223242526/* * 对数组进行重新排序，把奇数放在前面，偶数在后面 * Args: * a -- 数组 * f -- 函数指针，什么样的条件放在后面，如是偶数、是正数，解耦 */void reorder_array(vector&lt;int&gt; &amp; a, bool (*f)(int)) &#123; int l = 0; int r = a.size() - 1; while (l &lt; r) &#123; // 从左到右找到第一个偶数 while (l &lt; r &amp;&amp; !f(a[l])) &#123; l++; &#125; // 从右到左，找到第一个奇数 while (r &gt; l &amp;&amp; f(a[r])) &#123; r--; &#125; // 交换 if (l &lt; r) &#123; int t = a[l]; a[l] = a[r]; a[r] = t; &#125; &#125;&#125; 归并排序思路 归并排序稳定，也很快，所以使用归并排序。 分成长度为1、2、4的序列，各自都排好，依次两两合并。 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class Solution &#123;public: /* * 奇偶性 */ int get_parity(int n) &#123; return (n &amp; 1) == 1; &#125; /* * a中前后有两个序列，分别有序，对其进行merge */ void merge(vector&lt;int&gt; &amp;a, int start, int mid, int end) &#123; // 临时存储新的排序数据 int *t = new int[end - start + 1]; int i = start; int j = mid + 1; // 临时数组的索引 int k = 0; while (i &lt;= mid &amp;&amp; j &lt;= end) &#123; // 奇偶性 int pi = get_parity(a[i]); int pj = get_parity(a[j]); if (pi == 1 &amp;&amp; pj == 0) &#123; t[k++] = a[i++]; &#125; else if (pi == 0 &amp;&amp; pj == 1)&#123; t[k++] = a[j++]; &#125; else if (pi == 1 &amp;&amp; pj == 1) &#123; t[k++] = a[i++]; &#125; else if (pi == 0 &amp;&amp; pj == 0) &#123; t[k++] = a[i++]; &#125; &#125; while (i &lt;= mid) &#123; t[k++] = a[i++]; &#125; while (j &lt;= end) &#123; t[k++] = a[j++]; &#125; for (i = 0; i &lt; end - start + 1; i++) &#123; a[start + i] = t[i]; &#125; delete [] t; return; &#125; /* * 对a的长度为gap的子序列，两两合并 * Args: * a -- 数组 * gap -- 1个子序列的长度 * Returns: * None */ void merge_groups(vector&lt;int&gt; &amp;a, int gap) &#123; // 两组的长度 int twolen = 2 * gap; int i; // 对相邻的两个gap进行合并 for (i = 0; i + twolen - 1 &lt; a.size(); i += twolen) &#123; int start = i; int mid = start + gap - 1; int end = i + twolen - 1; merge(a, start, mid, end); &#125; // 若最后一次不足两个gap，即1个gap和部分gap if (i + gap - 1 &lt; a.size() - 1) &#123; merge(a, i, i + gap - 1, a.size() - 1); &#125; &#125; void reOrderArray(vector&lt;int&gt; &amp;a) &#123; // 分割为长度为i的子序列，两两进行合并 for (int i = 1; i &lt; a.size(); i *= 2) &#123; merge_groups(a, i); &#125; &#125; &#125;; 链表中倒数第k个节点-22 不知道链表长度，要求在\\(O(n)\\)内找到倒数第k个节点，注意代码的鲁棒性。 双指针思路 两个指针，l先走k-1步，r再从头节点出发，始终保持距离为k-1。 r走到末尾时，l就是倒数第k个节点。 注意头结点为空和k非法(为0、k超出等)的情况。 双指针求链表中间节点 两个指针，l走一步，r走两步。r走到末尾的时候，l正好走到中间。 双指针总结 当一个指针遍历链表不能解决问题的时候，就使用两个指针。 同时走、一个速度快 一个先走、速度一样 关键代码 123456789101112131415161718192021222324252627282930313233343536373839/* * 返回链表中的倒数第k个节点 * 双指针思路，注意代码的鲁棒性 * Args: * phead -- 头指针 * k -- 无符号整数 * Returns: * nullptr or 第k个节点 */ListNode* kth_from_end(ListNode* phead, unsigned int k) &#123; // 1. phead为空或者k不合法，都返回空，k不能为0，否则k-1是一个巨大的数 if (phead == nullptr || k == 0) &#123; return nullptr; &#125; // 2. 双指针 ListNode* pr = phead; ListNode* pl = phead; unsigned int count = 1; // 3. pl先走 while (pl &amp;&amp; count &lt;= k - 1) &#123; pl = pl-&gt;next; count++; &#125; // 4. 不足k个节点，返回空 if (pl == nullptr) &#123; return nullptr; &#125; // 5. 左右指针一起走，保持k-1的距离 while (pl-&gt;next) &#123; pl = pl-&gt;next; pr = pr-&gt;next; &#125; return pr;&#125; 链表中环的入口节点-23 如果一个链表有环，则返回这个环的入口节点 双指针法 确定有环 双指针，同时走，l一次1步，r一次2步。 r走到末尾，则无环 r与l相遇，则有环。相遇节点在环内 确定环内节点数量 相遇节点在环内，从相遇节点开始遍历一圈，计数。再次回到相遇节点，就能知道环内节点数量k。 找到环的入口节点 双指针，r先走k步，l再走。l与r相遇时，就是环的入口节点 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/* * 双指针判断链表是否有环时，返回两个指针相遇的节点 * Args: * phead -- 头节点 * Returns: * pmeet -- nullptr or 相遇的节点 */ListNode* meet_node(ListNode* phead) &#123; // 空节点 or 1个节点，无法成环 if (phead == nullptr || phead-&gt;next == nullptr) &#123; return nullptr; &#125; // 两个指针，l一次走一步，r一次走两步 ListNode* pl = phead; ListNode* pr = phead-&gt;next-&gt;next; ListNode* pmeet = nullptr; while (pl &amp;&amp; pr) &#123; if (pl == pr) &#123; pmeet = pl; break; &#125; pl = pl-&gt;next; pr = pr-&gt;next; // pr走第二步的时候，先判断一下 if (pr) &#123; pr = pr-&gt;next; &#125; &#125; return pmeet;&#125;/* * 获取链表中环内节点的数量，已经确保有环 * Args: * pmeet -- 链表中环内的一个节点 * Returns: * count -- 环内的节点的数量 */int get_circle_node_count(ListNode* pmeet) &#123; if (pmeet == nullptr || pmeet-&gt;next == nullptr) &#123; return 0; &#125; ListNode* p = pmeet-&gt;next; int count = 1; while (p != pmeet) &#123; p = p-&gt;next; count++; &#125; return count;&#125;/* * 双指针法获得链表中环的入口节点 * Args: * phead -- 头节点 * Returns: * pentry -- nullptr 或者 环的入口节点 */ListNode* circle_entry_node(ListNode* phead) &#123; ListNode* pmeet = meet_node(phead); // 无环 if (pmeet == nullptr) &#123; return nullptr; &#125; // 后面的操作已经确保有环 // 环内节点的数量 int k = get_circle_node_count(pmeet); // pl先走k步 ListNode* pl = phead; ListNode* pr = phead; for (int i = 1; i &lt;= k; i++) &#123; pr = pr-&gt;next; &#125; // 同时走，直到相遇在入口节点 ListNode* pentry = nullptr; while (pl &amp;&amp; pr) &#123; if (pl == pr) &#123; pentry = pl; break; &#125; pl = pl-&gt;next; pr = pr-&gt;next; &#125; return pentry;&#125; 推导法和断链法 leetcode推导法 翻转链表-24 Reverse Linked List 思路 每次遍历保存三个节点：pre, pnow, pnext ，遍历到pnow 保存pnow的next pnow指向pre pre = pnow 把pnext赋值给pnow，下一次循环 注意当pnext为空时，已走到末尾，此时的pnow应该是新的head。 12345678910111213141516171819ListNode* reverseList(ListNode* head) &#123; ListNode* pre = nullptr; ListNode* pnow = head; ListNode* pnext = nullptr; while (pnow) &#123; // 保存pnow的next pnext = pnow-&gt;next; pnow-&gt;next = pre; // 新的头节点 if (pnext == nullptr) &#123; head = pnow; &#125; // pnow作为新的pre pre = pnow; // pnext作为下一轮遍历的pnow pnow = pnext; &#125; return head;&#125; 合并两个有序链表-25 见这里 树的子结构-26 Subtree of Another Tree 两棵树s和t，判断t是否是s的一个子结构 思路 层次遍历s的每个节点p 判断p是否和t完全相同 判断两颗树相同 两个都为空，相同 其中一个为空，不同 值不同，不同 则 return 左子树相同 &amp;&amp; 右子树相同 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/* * 判断两颗树是否相等 */bool is_same(TreeNode* p1, TreeNode* p2) &#123; if (p1 == nullptr &amp;&amp; p2 == nullptr) &#123; return true; &#125; if (p1 == nullptr || p2 == nullptr) &#123; return false; &#125; if (p1-&gt;val != p2-&gt;val) &#123; return false; &#125; return is_same(p1-&gt;left, p2-&gt;left) &amp;&amp; is_same(p1-&gt;right, p2-&gt;right);&#125;/* * 层次遍历以s的每个节点为根节点的子树是否和t相同 */bool isSubtree(TreeNode* s, TreeNode* t) &#123; queue&lt;TreeNode*&gt; nodes; // 放根节点 if (s != nullptr) &#123; nodes.push(s); &#125; while (!nodes.empty()) &#123; // 访问根节点 TreeNode* p = nodes.front(); nodes.pop(); if (is_same(p, t)) &#123; return true; &#125; // 向队列中追加左右孩子 if (p-&gt;left) &#123; nodes.push(p-&gt;left); &#125; if (p-&gt;right) &#123; nodes.push(p-&gt;right); &#125; &#125; return false;&#125; 二叉树的镜像-27 二叉树的镜像，就是左右孩子交换节点嘛。 思路 使用先序遍历的思想，这里是二叉树各种遍历 使用栈，根节点入栈 栈不为空，出栈一个元素p 交换其左右孩子节点 右孩子入栈，左孩子入栈 [关键代码] 1234567891011121314151617181920212223242526/* * 先序遍历求二叉树的镜像 */void mirror(TreeNode *head) &#123; TreeNode* p = head; stack&lt;TreeNode*&gt; st; if (p != nullptr) &#123; st.push(p); &#125; // 栈不为空 while (!st.empty()) &#123; TreeNode* now = st.top(); st.pop(); // 交换其左右节点 TreeNode* t = now-&gt;left; now-&gt;left = now-&gt;right; now-&gt;right = t; // 左右节点入栈 if (now-&gt;right) &#123; st.push(now-&gt;right); &#125; if (now-&gt;left) &#123; st.push(now-&gt;left); &#125; &#125;&#125; 对称的二叉树-28 leetcode对称的二叉树 判断一个二叉树是不是对称的 思路 普通的遍历都是先左后右， 对称的话，得用先右后左。 一棵树对称，则先左后右的先序序列、先右后左的先序序列应该一样。 即左右子树对称相等。 遍历的时候 根节点为空，对称 否则，看sym(root.left, root.right) 两个节点其中一个为空，则看p1 == p2 两个都不为空，则先看根节点的值 最后则交叉看左右子树，sym(p1.left, p2.right) &amp;&amp; sym(p1.right, p2.left) [关键代码] 123456789101112131415161718192021222324252627282930/* * 判断一棵树是否对称 */bool isSymmetric(TreeNode* root) &#123; if (root == nullptr) &#123; return true; &#125; return symmetric_equal(root-&gt;left, root-&gt;right);&#125;/* * 判断两棵树对称相等 * Args: * p1, p2 -- 一般是一棵树的左右子树 * Returns: * true or false */bool symmetric_equal(TreeNode* p1, TreeNode* p2) &#123; // 有空的 if (p1 == nullptr || p2 == nullptr) &#123; return p1 == p2; &#125; // 先看根节点的值 if (p1-&gt;val != p2-&gt;val) &#123; return false; &#125; // 看左右子树对称相等 return symmetric_equal(p1-&gt;left, p2-&gt;right) &amp;&amp; symmetric_equal(p1-&gt;right, p2-&gt;left); &#125; 非递归代码 12345678910111213141516public boolean isSymmetric(TreeNode root) &#123; if (root == null) return true; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root.left); stack.push(root.right); while (!stack.empty()) &#123; TreeNode n1 = stack.pop(), n2 = stack.pop(); if (n1 == null &amp;&amp; n2 == null) continue; if (n1 == null || n2 == null || n1.val != n2.val) return false; stack.push(n1.left); stack.push(n2.right); stack.push(n1.right); stack.push(n2.left); &#125; return true;&#125; 顺时针打印矩阵-29 牛客网打印矩阵 思路 用左上和右下的坐标定位出一圈要打印的数据。一圈打完以后，分别沿着对角线向中间靠拢一个单位。 关键代码 12345678910111213141516171819202122232425262728293031vector&lt;int&gt; print_marix(vector&lt;vector&lt;int&gt;&gt; matrix) &#123; vector&lt;int&gt; res; if (matrix.empty() || matrix[0].empty()) &#123; return res; &#125; int row = matrix.size(); int col = matrix[0].size(); // 通过左上、右下去锁定当前圈的元素 int left = 0, top = 0, right = col - 1, bottom = row - 1; while (left &lt;= right &amp;&amp; top &lt;= bottom) &#123; // 左到右 for (int i = left; i &lt;= right; i++) res.push_back(matrix[top][i]); // 上到下 for (int i = top + 1; i &lt;= bottom; i++) res.push_back(matrix[i][right]); // 右到左，有多行时 if (top != bottom) for (int i = right - 1; i &gt;= left; i--) res.push_back(matrix[bottom][i]); // 下到上，有多列时 if (left != right) for (int i = bottom - 1; i &gt; top; i--) res.push_back(matrix[i][left]); // 左上角、右下角移动 left++, top++, right--, bottom--; &#125; return res;&#125; 包含min函数的栈-30 Min Stack 实现能够得到栈中最小元素的数据结构，要求入栈、出栈、获得最小元素都是O(1) 思考过程 定义一个变量，去存储最小元素，每次对其更新。可是，当最小元素出栈以后呢，怎么去得到新的最小元素呢？这样就需要把次最小元素也存储下来。就需要把每次最小的元素，按顺序存储下来， 按照入栈的顺序 入栈时，入栈当前最小的元素 出栈时，把当前最小的元素也出栈，留下的是下一个状态的最小元素 思路 数据栈，正常存数据 最小栈，存放各个时刻的最小元素，栈顶一直是当前的最小元素 入栈： 当前最小元素：min(min_st.top(), new) ，最小栈压入当前的最小元素 出栈：数据栈元素出栈，同时最小栈出掉当前的最小元素 [关键代码] 12345678910111213141516171819202122232425262728293031323334353637383940class MinStack &#123;private: // 数据栈 stack&lt;int&gt; st_data; // 存储每个状态最小元素的栈 stack&lt;int&gt; st_min;public: // 初始化数据结构 MinStack() &#123; &#125; void push(int x) &#123; st_data.push(x); // 当前的最小元素入栈 if (st_min.empty() || x &lt; st_min.top()) &#123; st_min.push(x); &#125; else &#123; st_min.push(st_min.top()); &#125; &#125; void pop() &#123; if (st_data.empty() || st_min.empty()) &#123; return; &#125; // 当前数据栈和最小元素栈都出栈 st_data.pop(); st_min.pop(); &#125; int top() &#123; return st_data.top(); &#125; int getMin() &#123; return st_min.top(); &#125;&#125;; 栈的压入和弹出序列-31 输入两个序列，第一个为栈的入栈序列，判断第二个是否为其出栈序列。入栈所有数字都不相等 思路 入栈序列，出栈序列。当前需要出栈元素为i i在栈内，则直接出栈 i不在栈内，则把如栈序列 前面到i 的元素全部入栈，再重复1 入栈序列全都入栈了，依然没有i，则不是弹出序列 示例 入栈：1 2 3 4 5， 出栈 4 5 3 2 1 4不在栈顶，前面元素入栈 操作 栈内元素 剩余出栈元素 剩余入栈元素 4不在栈顶，4入栈 1 2 3 4 4 , 5 3 2 1 5 4出栈 1 2 3 5 3 2 1 5 5不在栈顶，5入栈 1 2 3 5 5, 3 2 1 - 5出栈 1 2 3 3 2 1 - 3出栈 1 2 2 1 - 2出栈 1 1 - 1出栈 - - - 入栈：1 2 3 4 5， 出栈 4 3 5 1 2 操作 栈内元素 剩余出栈元素 剩余入栈元素 4不在栈顶，4入栈 1 2 3 4 4 , 3 5 1 2 5 4出栈 1 2 3 3 5 1 2 5 3出栈 1 2 5 1 2 5 5出栈，不在栈顶， 入栈 1 2 5 5, 1 2 5出栈 1 2 1 2 1出栈，不在栈顶，已经无可入元素， 终止 [关键代码] 遍历每个出栈元素now 使其在栈顶，对如栈序列进行入栈，直到now 如果now依然不在栈顶，则不是 如果now在栈顶，则出栈 继续遍历下一个出栈now 12345678910111213141516171819202122232425262728293031323334353637383940414243/* * 判断vpop是否是入栈序列vpush的出栈序列 * Args: * vpush -- 入栈序列 * vpop -- 要判断的出栈序列 * Returns: * true or false */bool is_poporder(const vector&lt;int&gt;&amp; vpush, const vector&lt;int&gt;&amp; vpop) &#123; bool res = false; stack&lt;int&gt; st; // 入栈的元素 int k = 0; for (int i = 0; i &lt; vpop.size(); i++) &#123; // 当前要出栈的元素 int now = vpop[i]; // now不在栈顶，则从入栈序列中入栈 if (st.empty() || st.top() != now) &#123; while (k &lt; vpush.size()) &#123; st.push(vpush[k]); if (vpush[k] == now) &#123; k++; break; &#125; k++; &#125; &#125; // now依然不在栈顶 if (st.empty() || now != st.top()) &#123; res =false; break; &#125; // now 在栈顶，出栈 st.pop(); if (i == vpop.size() - 1) &#123; res = true; &#125; &#125; return res;&#125; 从上到下打印二叉树-32 leetcode层次遍历 有3个题 层次遍历序列 每次遍历一层 分行层次遍历打印 每次打印一层 z型遍历二叉树 见leetcode的z型遍历二叉树 二叉搜索树的后序遍历-33 给一个数组，判断是否是二叉搜索树的后序遍历 后序遍历：最后一个是根节点 BST：左 &lt; 根 &lt; 右 123 8 6 105 7 9 11 给一个数组：5 7 6 9 11 10 8 根节点是8， 5 7 6前面小于8的，是左子树，全部小于8 9 11 10中间大于8的，是右子树，全部大于8 再依次取判断左右子树是否是BST 思路 判断nums[start, end] 是否是BST的后序遍历序列 空返回false，一个元素返回true。 找到根节点root = nums[end] 从start开始，找左子树的最后一个元素i-1， 直到end-1， 每个元素都小于根节点 从i开始， 找右子树，直到end-1， 每个元素都要大于根节点。 右子树时，前面左子树已经ok（有或者没有），所以后面的元素都是右子树的，都要大于根节点。 如果后面有小于根节点的，则不满足右子树，返回false 递归判断左右子树是否是BST，返回结果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/* * 是否是BST的后序遍历序列 * Args: * nums: 要判断的目标序列，没有重复的数字 * start: 序列的起始值 * end: 序列的结束位置 * Returns: * true or false */bool is_bst_postorder(const vector&lt;int&gt; &amp;nums, int start, int end) &#123; // 不合法 if (nums.empty() || start &lt; 0 || end &gt; nums.size() - 1 || start &gt; end) &#123; return false; &#125; // 只有一个元素 if (start == end) &#123; return true; &#125; int root = nums[end]; // 1. 找到左子树 int i = start; for (; i &lt;= end - 1; i++) &#123; if (nums[i] &gt; root) break; &#125; // 2. 找到右子树，全部都大于root int j = i; for (; j &lt;= end - 1; j++) &#123; // 右子树中有小于root的值，不合法 if (nums[i] &lt; root) &#123; return false; &#125; &#125; // 3. 判断左右子树 bool left = true; if (i &gt; start) &#123; // 判断右子树 left = is_bst_postorder(nums, start, i - 1); &#125; bool right = true; if (j &gt; i) &#123; // 判断左子树 right = is_bst_postorder(nums, i, j - 1); &#125; return left &amp;&amp; right;&#125; 二叉树中的路径求和-34 参考leetcode中各种pathsum汇总 复杂链表的复制-35 链表：值，下一个节点，任意一个节点。复制这样的一个链表。 思路1 暴力解法 先把链表连接起来，不管任意指针 遍历链表，为每个新节点找到任意指针的节点，连接起来 同时知道旧节点N、新节点N1、旧节点的任意指针节点S 遍历新链表，找到任意指针S1 把N1和S1连接起来 时间复杂度\\(O(n^2)\\)，遍历新链表，找到任意指针S1的节点。 思路2 Hash解法 上面耗时间：找任意指针S1。用HashMap建立(N, N1)的配对，就能够\\(O(1)\\)查找到S1。从而总时间为\\(O(n)\\) 思路3 最优-新旧链表先连再断 连接新节点。把N1连接到N后面，a-a1-b-b1-c-c1 设置随机节点。设置N1的S1，实际上，a-s, a-a1，s-s1 ，所以能够找到a1-s1 新旧节点断开。把N和N1断开，得到a1-b1-c1 注意：设置随机节点时，随机节点可能为空。断链时，下一个节点可能为空。 [关键代码] 123456789101112131415161718192021222324252627282930313233343536373839404142434445RandomListNode* clone(RandomListNode* head) &#123; if (head == nullptr) &#123; return nullptr; &#125; // 1. 新建节点，连接到原节点的后面 RandomListNode* p = head; while (p) &#123; RandomListNode* p1 = new RandomListNode(p-&gt;label); // 连接 p1-&gt;next = p-&gt;next; p-&gt;next = p1; p = p1-&gt;next; &#125; // 2. 为新节点设置随机节点 p = head; while (p) &#123; RandomListNode* p1 = p-&gt;next; RandomListNode* s = p-&gt;random; // 注意random可能为空 if (s != nullptr) &#123; p1-&gt;random = s-&gt;next; &#125; p = p1-&gt;next; &#125; // 3. 新旧节点断开 p = head; RandomListNode* head1 = p-&gt;next; while (p) &#123; // 新节点 RandomListNode* p1 = p-&gt;next; // 原节点，连接到原下一个节点 p-&gt;next = p1-&gt;next; // 新节点的下一个节点，可能下一个节点为空 if (p-&gt;next) &#123; p1-&gt;next = p-&gt;next-&gt;next; &#125; else &#123; p1-&gt;next = nullptr; &#125; p = p-&gt;next; &#125; return head1;&#125; 二叉搜索树转双向链表-36 给一个二叉搜索树，转化为一个排好序的双向链表。左孩子-前一个节点，右孩子-后一个节点。 牛客网二叉搜索树与双向链表 ， 类似题型：有序链表转平衡BST BST的中序遍历就是自动有序的。 递归思路 中序遍历，使用pre来记录链表中的最后一个元素。 遍历到根节点时，递归转换左子树 pre与root连接，pre.right=root, root.left=pre, pre=root 。注意pre为空时，pre=root 递归创建右子树 再从pre找到第一个节点。 [关键代码] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/* * 把树转化为链表，递归版本 * Args: * root -- 树 * Returns: * 转换后的链表的头结点 */TreeNode* convert(TreeNode* root) &#123; if (root == nullptr) &#123; return nullptr; &#125; TreeNode* pre = nullptr; // 转换为链表，pre为最后一个节点 convert_inorder(root, pre); // 从末尾找到第一个节点 while (pre &amp;&amp; pre-&gt;left) &#123; pre = pre-&gt;left; &#125; return pre;&#125;/* * 递归转换 * Args: * root -- 当前节点 * pre -- 上一个节点，引用类型，改变值。 * Returns: * None */void convert_inorder(TreeNode* root, TreeNode* &amp;pre) &#123; if (root == nullptr) &#123; return; &#125; // 左子树 convert_inorder(root-&gt;left, pre); // 当前节点 if (pre != nullptr) &#123; root-&gt;left = pre; pre-&gt;right = root; pre = root; &#125; else &#123; pre = root; &#125; // 右子树 convert_inorder(root-&gt;right, pre);&#125; 非递归思路 中序遍历时，记录pre节点，每次进行修改即可，访问到p时，则连接到pre即可。 p不为空，p入栈，p=p.left， 扫描左孩子 p为空，p从栈顶获得，p=st.top()， 显然p没有左孩子或者左孩子已经出栈遍历过，p访问出栈。此时，把p与pre连接即可， 注意pre为空 扫描右孩子，p = p.right 最后从末尾，找到头结点 12345678910111213141516171819202122232425262728293031323334353637TreeNode* convert_stack(TreeNode* root) &#123; if (root == nullptr) &#123; return nullptr; &#125; stack&lt;TreeNode*&gt; st; TreeNode* p = root; // 上一个节点 TreeNode* pre = nullptr; while (p || !st.empty()) &#123; if (p) &#123; // p入栈 st.push(p); // 扫描左孩子 p = p-&gt;left; &#125; else &#123; // p为空，出栈元素，p为根节点，左孩子已经访问结束或者没有左孩子 p = st.top(); st.pop(); if (pre == nullptr) &#123; pre = p; &#125; else &#123; pre-&gt;right = p; p-&gt;left = pre; pre = p; &#125; // 扫描右孩子 p = p-&gt;right; &#125; &#125; // 找到头结点 while (pre &amp;&amp; pre-&gt;left) &#123; pre = pre-&gt;left; &#125; return pre;&#125; 序列化和反序列化二叉树-37 leetcode序列化和反序列化二叉树笔记 数字全排列-38 leetcode数字全排列笔记 数组中出现次数超过一半的数-39 Leetcode笔记 最小的k个数-40 牛客最小的k个数 给一个数组，找到最小的k个数 思路1 快排思路 修改原数组。partition左边即可。O(n) 。快速排序 。关键代码 如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/* * 通过快速排序来找到最小的k个数，改变原数组 * Args: * a -- 数组 * k -- 最小的k个数 * Returns: * res -- 最小的k个数 */vector&lt;int&gt; get_leastnums_by_partition(vector&lt;int&gt;&amp; a, int k) &#123; // 条件合法性判断 if (a.empty() || k &lt; 1 || k &gt; a.size()) &#123; return vector&lt;int&gt;(); &#125; int l = 0, r = a.size() - 1; int i = partition(a, l, r); // 直到左边是最小的k个数，包含a[i] while (i + 1 != k) &#123; if (i + 1 &gt; k) &#123; // 左边有超过k个数，左边继续划分 r = i - 1; &#125; else if (i + 1 &lt; k) &#123; // 左边不足k个，划分右边，加一些给左边 l = i + 1; &#125; i = partition(a, l, r); &#125; // 把左边给到res中 vector&lt;int&gt; res(k); std::copy(a.begin(), a.begin() + k, res.begin()); return res;&#125;/* * 快排的partition，左边小于，中间x，右边大于 * Args: * a -- 数组 * l -- 左边起始值 * r -- 右边结束值 * Returns: * a[l]的最终位置 */int partition(vector&lt;int&gt;&amp; a, int l, int r) &#123; int x = a[l]; while (l &lt; r) &#123; // 从右向左，找到小于x的值，放到a[l]上 while (l &lt; r &amp;&amp; a[r] &gt;= x) &#123; r--; &#125; if (l &lt; r) &#123; a[l++] = a[r]; &#125; // 从左向右，找到大于x的值，放到a[r]上 while (l &lt; r &amp;&amp; a[l] &lt;= x) &#123; l++; &#125; if (l &lt; r) &#123; a[r--] = a[l]; &#125; &#125; a[l] = x; return l;&#125; 思路2 最大堆 不修改原数组，用最大堆找到最小的k个数。O(nlogk) 。自己的堆操作和stl堆操作。 关键代码 如下： 123456789101112131415161718192021222324252627282930/* * 通过最大堆来获得数组中最小的k个数 */vector&lt;int&gt; get_leastknums_by_heap(vector&lt;int&gt;&amp; a, int k) &#123; if (a.empty() || k &lt;= 0 || k &gt; a.size()) &#123; return vector&lt;int&gt;(); &#125; // 选择前k个元素，初始化堆 vector&lt;int&gt; res(k); std::copy(a.begin(), a.begin() + k, res.begin()); std::make_heap(res.begin(), res.end()); // 剩余元素入堆 for (auto it = a.begin() + k; it != a.end(); it++) &#123; auto n = *it; printf(\"n=%d, max=%d\\n\", n, res[0]); // 大于最大值，无需入堆 if (n &gt;= res[0]) &#123; continue; &#125; // n小于最大堆的最大值，入堆 // 最大元素出堆，默认放到末尾 std::pop_heap(res.begin(), res.end()); // 新元素入堆 res[k - 1] = n; std::push_heap(res.begin(), res.end()); &#125; return res;&#125; 数据流中的中位数-41 给一个数据流，找到数组排序之后的中间的数值。奇数，中间；偶数，中间两个数的平均值。 牛客网数据流中的中位数 数据结构 插入时间 获取中位数时间 方法备注 未排序数组 O(1) O(n) 快排partition方法 排序数组 O(n) O(1) 排序链表 O(n) O(1) 两个指针指向中间节点 二叉搜索树 平均O(logn)，最差O(n) 平均O(logn)，最差O(n) AVL树 O(logn) O(1) 最大堆和最小堆 O(logn) O(1) 堆思路 两个容器（最大堆和最小堆），数据量平均分配，左边全部小于右边，左右两边无需排好序， 根据左边最大的，右边最小的 来找到中位数。 总体思路 左边小，右边大。左边最大堆max，右边最小堆min。 先放右边，后方左边。 奇数：右边min[0]； 偶数：求平均(max[0] + min[0]) / 2 插入右边 索引为偶数时，放右边。从0开始 num较大，直接插入右边min堆 num较小，插入左边max堆，再把max插入右边min堆。num &lt; 左边max[0] 插入左边 索引为奇数时，放左边min堆。 num较小，直接插入左边min堆 num较大，插入右边min堆，再把min插入右边max堆。num &gt; 右边min[0] Githubd代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374class Solution &#123;private: // 位于左边的最大堆 vector&lt;int&gt; max; // 位于右边的最小堆 vector&lt;int&gt; min;public: /* * 序号是偶数时，写入右边的最小堆 */ void insert2min(int num) &#123; // num较小，写到左边的最大堆 if (max.size() &gt; 0 &amp;&amp; num &lt; max[0]) &#123; max.push_back(num); // num入堆 std::push_heap(max.begin(), max.end(), less&lt;int&gt;()); // max出堆 std::pop_heap(max.begin(), max.end(), less&lt;int&gt;()); num = max.back(); max.pop_back(); &#125; // num写入最小堆 min.push_back(num); std::push_heap(min.begin(), min.end(), greater&lt;int&gt;()); &#125; /* * 序号是奇数时，写入左边的最大堆 */ void insert2max(int num) &#123; // num较大，先写到右边的最小堆 if (min.size() &gt; 0 &amp;&amp; num &gt; min[0]) &#123; min.push_back(num); // num 入最小堆 std::push_heap(min.begin(), min.end(), greater&lt;int&gt;()); // min 出堆 std::pop_heap(min.begin(), min.end(), greater&lt;int&gt;()); num = min.back(); min.pop_back(); &#125; // num写入最大堆 max.push_back(num); std::push_heap(max.begin(), max.end(), less&lt;int&gt;()); &#125; /* * 写入一个元素，先右后左，右边比左边多一个或相等 */ void Insert(int num) &#123; int idx = max.size() + min.size(); // 奇数，写入左边的最大堆 if ((idx &amp; 1) == 1) &#123; insert2max(num); &#125; else &#123; insert2min(num); &#125; &#125; /* * 获取中间元素，奇数在右边min[0]，偶数求平均 */ double GetMedian() &#123; int size = min.size() + max.size(); if (size == 0) &#123; return -1; &#125; else if (size &amp; 1) &#123; return min[0]; &#125; else &#123; return (min[0] + max[0]) / 2.0; &#125; &#125;&#125;; 连续子数组的最大和-42 给一个数组，有正数有负数。求所有连续子数组和的最大值。O(n) 牛客网连续子数组的最大和 ，github代码 思路1 累加切换思路 当前累加和，最大累加和。遍历遇到数n 如果cursum &lt;= 0， 则cursum = n， 否则 cursum += n 如果cursum &gt; bestsum， 则bestsum = cursum 123456789101112131415161718192021int maxsum_subarray(const vector&lt;int&gt;&amp; a) &#123; if (a.empty()) &#123; return 0; &#125; int cursum = a[0]; int bestsum = a[0]; for (int i = 1; i &lt; a.size(); i++) &#123; if (cursum &lt;= 0) &#123; cursum = a[i]; &#125; else &#123; cursum += a[i]; &#125; if (cursum &gt; bestsum) &#123; bestsum = cursum; &#125; &#125; return bestsum;&#125; 思路2 动态规划 设f[i]表示以\\(a_i\\) 结尾的子数组的最大和，我们要求max[fi] \\[ f(i) = \\begin{cases} &amp; a_i, &amp; f(i-1) \\le 0 \\quad or \\quad i = 0\\\\ &amp; f(i-1) + a_i, &amp; f(i-1) &gt; 0 \\quad and \\quad i \\ne 0 \\\\ \\end{cases} \\] 123456789101112131415int maxsum_subarray_dp(const vector&lt;int&gt;&amp; a) &#123; if (a.empty()) &#123; return 0; &#125; // p[i]=k，以i结尾的所有连续子数组中的最大值为k vector&lt;int&gt; p(a.size(), 0); for (int i = 0; i &lt; a.size(); i++) &#123; if (i == 0 || p[i-1] &lt;= 0) &#123; p[i] = a[i]; &#125; else if (p[i-1] &gt; 0) &#123; p[i] = p[i-1] + a[i]; &#125; &#125; return *std::max_element(p.begin(), p.end());&#125; 1-n整数中1出现的次数-43 输入一个n，求问1-n数字中，1出现的总个数。 牛客网1-n整数中1出现的次数 ，github源码 思路1 暴力无offer思路 用个count计数，遍历所有的数字，求每一个数字的1的个数，再求和。O(nlogn) 12345678910111213/* * 求余计算数字n中1的个数 */int numberof1(unsigned int n) &#123; int count = 0; while (n) &#123; if (n % 10 == 1) &#123; count++; &#125; n = n / 10; &#125; return count;&#125; 思路2 分治递归思路 例子，找到1-21345中1的个数。分为两段：1346-21345（当前解），1-1345 （递归解） 思路总结 21345，先求位数5和最高位2， 1346-21345，计算最高位为1的数量h1。h &gt;= 2时， \\(10^4\\)。 h==1时， 做减法。 1346-21345，计算其余位为1的数量o1。选1位为1，剩余位0-9任选，排列组合。\\(4\\cdot10^3 \\cdot 2\\) 递归计算1-1345中1的个数r1 返回h1+o1+r1 12345678910111213141516171819202122232425262728293031/* * 递归计算从1-n中1的个数 */int numberof1_between1andn(int n) &#123; if (n &lt;= 9) &#123; return n &gt;= 1 ? 1 : 0; &#125; // 以21345为例，分为1346-21345(本次求解)和1-1345(递归求解)两段 // 1. 计算位数和最高位 string ns = std::to_string(n); int len = ns.size(); int h = ns[0] - '0'; // 2. 计算最高位为1的数量 int h1 = 0; if (h &gt;= 2) &#123; h1 = std::pow(10, len-1); &#125; else &#123; h1 = n - pow(10, len-1) + 1; &#125; // 3. 计算其他位为1的个数，选1位为1，其余位0-9任选 int o1 = h * (len-1) * pow(10, len-2); // 4. 递归求1-1346中1的位数 int p = h * pow(10, len-1); int r = n - h * pow(10, len-1); int r1 = numberof1_between1andn(r); return h1 + o1 + r1;&#125; 数字序列中某一位的数字-44 数字序列：01234567891011121314.... 给位数，返回该位置上的数字。 例子， 第1001位 1位数，10个，不在此，找后面的1001-10=991位 2位数，180个，不在此，找后面的991-180=881位 3位数，2700个，在此。881/3=270，881%3=1。即，从100开始的第270个数中的第2位。即370中的第2位7 github代码 注意每次递减index时，减的是位数*总数量，即index -= numcount * digits 在某个d位数中时，先算出d位数，再算出在d位数字内的res_index，反正是除法求余。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/* * 数字流012345...某一位的数字 * Args: * index -- 数字序列中索引，从0开始 * Returns: * res -- 某一位的数字 */int digit_in_sequence(int index) &#123; if (index &lt; 0) &#123; return -1; &#125; int res = 0; // 位数 int digits = 1; while (true) &#123; // d位数的总数量 int numcount = count_num(digits); if (index &lt;= numcount * digits - 1) &#123; // index在某个d位数中 // d位数的某个数的index int num_index = index / digits; // 数字内的某位数 int res_index = index % digits; // 目标d位数 int num = num_in_digits(num_index, digits); res = to_string(num)[res_index] - '0'; break; &#125; else &#123; // index在某个d+1位数中 index -= digits * numcount; digits++; &#125; &#125; return res;&#125;/* * 获得几位数的所有数字数量 * Args: * digits -- 位数 * Returns: * count -- 该位数所有数字的数量 */int count_num(int digits) &#123; if (digits &lt;= 0) &#123; return -1; &#125; if (digits == 1) &#123; return 10; &#125; // 加0.5是这个版本的编译器pow的问题 int count = (int) (std::pow(10, digits - 1) + 0.5); return 9 * count;&#125;/* * 第1个d位数 */int first_d_num(int digits) &#123; if (digits == 1) &#123; return 0; &#125; return (int) (pow(10, digits - 1) + 0.5);&#125;/** * d位数中的第index个数 * Args: * index -- 从0开始 * Returns: * num -- d位数中的第index个数字 */int num_in_digits(int index, int digits) &#123; int first = first_d_num(digits); return first + index;&#125; 把数组排成最小的数-45 给一个正整数数组，把它们拼接起来，求得一个最小的数字。 牛客网排成最小的数 1 全排列思路 实际上可以获得所有的排列方法，然后选择最小的。但是这样会比较慢。一共n!个排列组合。参考全排列。 2 排序思路 因为实际上最终是，按照一个规则把所有数字排列起来。 把所有数字转换成字符串 定义一个compare方法，判断哪个数字应该排列在前面。字符串本身的比较即可。 对所有数字排序，使用sort(nums.begin(), nums.end(), cmp) 按照排好序的数字来拼接到一个字符串 注意：隐藏的大数问题。用字符串来保存。 github源代码 12345678910111213141516171819202122232425262728293031323334/* * 把数组排列成最小的数 */string array2minnum(const vector&lt;int&gt; &amp;a) &#123; vector&lt;string&gt; nums; for (auto n: a) &#123; nums.push_back(to_string(n)); &#125; sort(nums.begin(), nums.end(), cmp); ostringstream res; for (auto s:nums) &#123; res &lt;&lt; s; &#125; return res.str(); &#125;/* * 用于从小到大排序的比较函数。n1&lt;n2，返回true * 小于的意义：谁小谁在前。 * Args: * n1, n2 -- 两个数字，由字符串表示 * Returns: * true or false */bool cmp(const string&amp; n1, const string&amp; n2) &#123; string n1n2 = n1 + n2; string n2n1 = n2 + n1; if (n1n2 &lt; n2n1) &#123; // n1 &lt; n2，n1应该在前面 return true; &#125; else &#123; return false; &#125;&#125; 把数字翻译成字符串-46 基本规则：0-a, 1-b, 2-c, ... , 25-z。一个数字(可能是多位数)有多种翻译的可能。给一个数字字符串，求出有多少种翻译方法。比如12258有5种翻译方法。bccfi,bwfi,bczi,mcfi,mzi 1 递归计算 1 - 2258, 12 - 258 。第一个数字有2种翻译方法，再递归地去翻译剩下的部分。 由于存在重复的子问题，所以递归不好。从上到下计算也不好。我们要用从最小的子问题自下而上解决问题 从上到下思考，自下而上循环计算。 设c[i] 是a[i]开始到末尾的所有翻译种类的数量。 每个a[i]都能单独翻译，c[i] = c[i+1] 如果a[i]a[i+1]可以一起翻译，则c[i] += c[i+2] \\[ f(i) = \\begin{cases} f(i+1) + f(i+2), &amp;a_ia_{i+1} \\text{可以一起翻译}\\\\ f(i+1),&amp;a_i \\text{只能单独翻译}\\\\ \\end{cases} \\] github代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* * 把数字翻译成字母，找到有多少种翻译方法 */int get_trans_count(int number) &#123; string num = to_string(number); if (number &lt; 0 || num.empty()) &#123; return 0; &#125; if (num.length() == 1) &#123; return 1; &#125; // c[i]=k，i~n的翻译法有k种 vector&lt;int&gt; c(num.size()); for (int i = c.size() - 1; i &gt;= 0; i--) &#123; // 0. 计算a[i]能否和a[i+1]一起翻译 bool flag = false; if (i+1 &lt; c.size()) &#123; int now = num[i] - '0'; int back = num[i+1] - '0'; int sum = now * 10 + back; if (sum &gt;= 10 &amp;&amp; sum &lt;= 25) &#123; // a[i]与a[i+1]一起翻译 flag = true; &#125; &#125; // 1. 每个a[i]都可以单独翻译 if (i == c.size() - 1) &#123; c[i] = 1; &#125; else &#123; c[i] = c[i+1]; &#125; // 2. a[i]a[i+1]一起翻译，则与c[i+2]有关 if (flag == true) &#123; if (i+2 &lt; c.size()) &#123; c[i] += c[i+2]; &#125; else &#123; c[i] += 1; &#125; &#125; &#125; // 返回0-n的翻译数量 return c[0];&#125; 礼物的最大价值-47 m*n的棋盘，每个格子上有一个礼物，每个礼物都有一个价值。从左上角开始走到右下角。每次只能向下-向右走。走一个格子，拿一个礼物。问，最大能拿多大的价值。 设v[i,j]是格子ij上的价值，设m[i,j] 是到达ij后的总路线最大价值。只能从上面或左边到达。 \\[ m[i, j] = \\max(m[i-1,j], m[i, j-1]) +v[i, j] \\] 一行一行、一列一列的计算。初始化left和up为0，如果有，则赋值。计算m[i,j] = max(left, up)+v[i,j] github源码 12345678910111213141516171819202122232425262728293031323334353637383940/* * 给一个礼物价值矩阵，找到最大价值 * Args: * v -- 各个位置上的礼物矩阵 * Returns: * res -- (0,0)-(m,n)的最大礼物价值 */int max_gifts_value(const vector&lt;vector&lt;int&gt;&gt; &amp;v) &#123; if (v.empty() || v[0].empty()) &#123; return 0; &#125; vector&lt;vector&lt;int&gt;&gt; m; int row = v.size(); int col = v[0].size(); // 初始化为0 for (int i = 0; i &lt; row; i++) &#123; vector&lt;int&gt; cur_row(col, 0); m.push_back(cur_row); &#125; // 一行一行地计算 for (int i = 0; i &lt; row; i++) &#123; // 一列一列地算 for (int j = 0; j &lt; col; j++) &#123; // 从上边和左边来的 int up = 0; int left = 0; if (i &gt; 0) &#123; up = m[i-1][j]; &#125; if (j &gt; 0) &#123; left = m[i][j-1]; &#125; // 选大的+v m[i][j] = std::max(up, left) + v[i][j]; &#125; &#125; return m[row-1][col-1];&#125; 最长不重复的子字符串-48 给一个字符串，找到里面最长子字符串的长度。 leetcode最长子字符串笔记 第n个丑数-49 leetcode丑数笔记 第一个只出现一次的字符-50 找到字符串中，第一个只出现一次的字符 用空间换时间，HashMap， 统计出现次数；再遍历字符串，找出现次数为1的字符。 1234567891011121314151617181920212223242526/* * 字符串中出现次数为1的字符 */char first_char(const string&amp; s) &#123; if (s.empty()) &#123; return '\\0'; &#125; // 每个字符的出现次数 map&lt;char, int&gt; c; // 计算 for (auto ch:s) &#123; if (c.count(ch) == 0) &#123; c[ch] = 1; &#125; else &#123; c[ch] += 1; &#125; &#125; // 遍历找 for (auto ch : s) &#123; if (c[ch] == 1) &#123; return ch; &#125; &#125; return '\\0';&#125; 字符流中出现的第一个不重复字符 1234567891011121314151617181920212223242526272829303132class Solution&#123;private: // char出现次数 map&lt;char, int&gt;m; // 保留字符串 string s;public: /* * 写入一个字符 */ void insert(char ch) &#123; s += ch; if (m.count(ch) == 0) &#123; m[ch] = 1; &#125; else &#123; m[ch] += 1; &#125; &#125; /* * 找到第一个出现一次的字符 */ char first_char() &#123; char ch = '#'; for (auto c:s) &#123; if (m[c] == 1) &#123; ch = c; break; &#125; &#125; return ch; &#125;&#125;;","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"}]},{"title":"数据结构之搜索算法","date":"2017-12-31T07:20:28.000Z","path":"2017/12/31/algorithm-dfs/","text":"搜索算法、装载问题 搜索算法 解空间树 子集树 比如n种可选物品的01背包，对其进行选择。解空间为长度为n的01向量表示。解空间树是一颗完全二叉树。 排列树 比如旅行商问题，对n进行排列。 解空间搜索方式 DFS 深度优先。回溯法。 BFS 广度优先或最小耗费优先。分支限界法。 在扩展节点处，先生成所有的儿子节点，再从活节点列表中选择一个最有利的节点作为新的扩展节点。 回溯法 就是穷举，深度优先去找到解空间中满足约束条件的所有解，再选择一个最好的出来。 类似于走迷宫：走到死路或者已经达不到最优解就及时回头。要注意剪枝。 搜索子集树 12345678910111213void dfs(int t) &#123; if (t &gt; n) &#123; output(x); return; &#125; // 0和1两种选择 for (int i = 0; i &lt;= 1; i++) &#123; x[t] = i; if (legal(t)) &#123; bfs(t+1); &#125; &#125;&#125; 搜索排列树 1234567891011121314void dfs(int t) &#123; if (t &gt; n) &#123; output(x); return; &#125; // 遍历所有t的可能：还剩下的可选择的内容，前面t-1已经确定了 for (int i = t; i &lt;= n; i++) &#123; swap(x[t], x[i]); if (legal(t)) &#123; bfs(t + 1); &#125; swap(x[t], x[i]); &#125;&#125; 分支限界法 广度优先或者最小消费优先去搜索解空间树，去找到使目标函数达到极大或者极小解，某种意义下的最优解。 每个活节点只有一次机会成为扩展节点，成为时，就一次性产生其所有儿子节点。舍弃不可行或导致非最优解的儿子节点，其余儿子节点加入活节点列表。从活节点列表中取出下一个节点作为新的扩展节点。 广度优先搜索 队列式分支限界法 最小消费优先搜索 优先队列式分支限界法。每个活节点有一个优先级。通常用最大堆来实现最大优先队列。 装载问题 问题描述 \\(n\\)个集装箱质量分别为\\(w_i\\)， 要装上总容量为\\(c\\)的轮船。 问，怎样装，才能使装得最多？ \\[ \\begin{align} &amp; \\max \\sum_{i}^nx_i\\cdot w_i \\\\ &amp; \\sum_{i}^nx_i\\cdot w_i \\le c \\\\ &amp; x_i \\in\\{0,1\\}, \\quad \\text{表示装或不装} \\end{align} \\] 贪心解法 贪心策略：集装箱从轻到重排序，轻者先装， 直到超重。 BFS 有2艘轮船，\\(\\sum_i^n w_i \\le c_1 + c_2\\)， 怎样才能把n个集装箱装到这2艘轮船。 可知最优方案：尽量把第一艘轮船装满，剩余的装到第二艘上。 定义Solution 12345678910111213141516171819202122232425262728class Solution &#123; public: // 集装箱数量 int n; // 集装箱的重量 vector&lt;int&gt; w; // 船的载重 int c; // 当前重量 int cw; // 最优重量 int bestw; // 回溯遍历index=i的箱子 void backtrack(int i); // 构造函数 Solution(const vector&lt;int&gt; &amp;w, int c):w(w), c(c) &#123; this-&gt;cw = 0; this-&gt;bestw = 0; this-&gt;n = w.size(); &#125;&#125;;int max_loading(const vector&lt;int&gt; &amp;w, int c) &#123; Solution solu(w, c); solu.backtrack(0); return solu.bestw;&#125; 无剪枝的dfs 12345678910111213141516void Solution::backtrack(int i) &#123; if (i == n) &#123; if (cw &gt; bestw) &#123; bestw = cw; &#125; return; &#125; // 选择i if (cw + w[i] &lt;= c) &#123; cw = cw + w[i]; backtrack(i + 1); cw = cw - w[i]; &#125; // 不选择i backtrack(i + 1);&#125; 剪枝的dfs 进入下一步遍历之前，检查\\(\\rm{cw+rest &gt; bestw}\\) ，如果剩下的所有加上都小于最优解的话，那么就不用进入了。 12345678910111213141516171819202122232425262728/* * 剪枝的dfs */void Solution::dfs(int i) &#123; if (i == n) &#123; if (cw &gt; bestw) &#123; bestw = cw; &#125; return; &#125; // 剩余 r = r - w[i]; // 选择i if (cw + w[i] &lt;= c) &#123; cw += w[i]; // 剪枝 if (cw + r &gt; bestw) &#123; dfs(i + 1); &#125; cw -= w[i]; &#125; // 不选择i，剪枝 if (cw + r &gt; bestw) &#123; dfs(i + 1); &#125; r = r + w[i];&#125;","tags":[{"name":"搜索","slug":"搜索","permalink":"http://plmsmile.github.io/tags/搜索/"}]},{"title":"树的总结","date":"2017-12-29T06:25:09.000Z","path":"2017/12/29/10-trees/","text":"二叉树的一些性质 二叉树 二叉树的性质 第i层，节点最多\\(2^{i-1}\\)个 深度为k的二叉树，最多有\\(2^k-1\\)个 节点 二叉树有n个节点，高度至少为\\(\\log_2(n+1)\\) \\(n_0 =n_2 + 1\\)，叶子节点和度为2的节点的关系 种类 满二叉树 高为h，有\\(2^h-1\\)个节点 完全二叉树 二叉查找树 \\(\\rm{left &lt; root &lt; right}\\) ， 没有相等的节点 二叉平衡树 左右子树的高度差的绝对值小于等于1 二叉查找树","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"},{"name":"数据结构","slug":"数据结构","permalink":"http://plmsmile.github.io/tags/数据结构/"},{"name":"树","slug":"树","permalink":"http://plmsmile.github.io/tags/树/"}]},{"title":"leetcode-01","date":"2017-12-29T06:08:03.000Z","path":"2017/12/29/leetcode-01/","text":"leetcode题目 有序列表转二叉查找树-109 Convert Sorted List to Binary Search Tree 二叉树的最小深度-111 Minimum Depth of Binary Tree 最小深度：根节点到某个叶子节点的最短路径。 为空，返回0 左孩子为空，则结果在右孩子 右孩子为空，则结果在左孩子 左右均不为空，返回小的+1 二叉树遍历 先序遍历-144 Binary Tree Preorder Traversal 根、左、右。栈。 先把根节点入栈 栈不为空时，出栈一个元素 访问该元素，右孩子进栈，左孩子进栈。 因为出栈，先出左孩子，再出右孩子。 关键代码 1234567891011121314151617181920vector&lt;int&gt; pre_order(TreeNode* root) &#123; vector&lt;int&gt; vpre; stack&lt;TreeNode*&gt; st; if (root != nullptr) &#123; st.push(root); &#125; while (!st.empty()) &#123; TreeNode* p = st.top(); vpre.push_back(p-&gt;val); st.pop(); // 右进、左进；出时：左先出 if (p-&gt;right) &#123; st.push(p-&gt;right); &#125; if (p-&gt;left) &#123; st.push(p-&gt;left); &#125; &#125; return vpre;&#125; 中序遍历-094 Binary Tree Inorder Traversal 思路 左、根、右。使用栈。 p=root p不为空，p入栈，一直向左走p = p.left，扫描它的左孩子，所有左孩子依次入栈 p为空时，p = st.top() ，p位于栈顶，显然没有左孩子或者左孩子已经遍历过，p访问出栈。 扫描右孩子 p = p.right 从根节点开始，一直向左，所有的左孩子入栈， 出栈一个节点，访问，它的右孩子入栈。 关键代码 123456789101112131415161718192021vector&lt;int&gt; inorder_traversal(TreeNode* root) &#123; vector&lt;int&gt; res; stack&lt;TreeNode*&gt; st; TreeNode* p = root; while (p || !st.empty()) &#123; if (p) &#123; // 根节点入栈 st.push(p); // 扫描左孩子 p = p-&gt;left; &#125; else &#123; // p位于栈顶，左孩子已经被遍历过或者没有左孩子，直接出栈访问 p = st.top(); res.push_back(p-&gt;val); st.pop(); // 扫描右孩子 p = p-&gt;right; &#125; &#125; return res;&#125; 后序遍历-145 Binary Tree Postorder Traversal 思路 左孩子、右孩子、根节点。使用栈。使用pre记录上一次遍历的节点。 根节点入栈 栈不为空，访问栈顶元素p 直接访问p的条件：p没有左右孩子 or 左右孩子刚刚遍历结束，只要pre是左或者右孩子即可 p可以直接访问，则访问出栈 p不能直接访问，则左右孩子入栈 关键代码 12345678910111213141516171819202122232425262728293031323334vector&lt;int&gt; post_order(TreeNode* root) &#123; vector&lt;int&gt; res; stack&lt;TreeNode*&gt; st; // 前一次访问的节点 TreeNode* pre = nullptr; if (root != nullptr) &#123; st.push(root); &#125; while (!st.empty()) &#123; TreeNode* p = st.top(); // 0. 检查是否可以直接访问p bool no_child = (p-&gt;left == nullptr &amp;&amp; p-&gt;right == nullptr); bool pre_is_child = (pre == p-&gt;left || pre == p-&gt;right); if (nullptr == pre) &#123; pre_is_child = false; &#125; // 1. p无左右子树 or 左右子树刚刚遍历完，直接访问p if (no_child || pre_is_child) &#123; res.push_back(p-&gt;val); pre = p; st.pop(); &#125; // 2. 需要将p的左右孩子入栈 else &#123; if (p-&gt;right) &#123; st.push(p-&gt;right); &#125; if (p-&gt;left) &#123; st.push(p-&gt;left); &#125; &#125; &#125; return res;&#125; 层次遍历-102 层次遍历，使用队列。 从上到下 Binary Tree Level Order Traversal 和从下到上 Binary Tree Level Order Traversal II。 如果只需要顺序放在一个数组里面，则不需要分层，直接层次遍历即可。 但是此题，需要分层构建vector。 数量记录思路 不是很好。 队列层次遍历 当前层在队列中的数量：cur_remain 下一层的数量：next_level cur_remain == 0时， 就切换到下一层 [关键代码] 1234567891011121314151617181920212223242526272829303132333435363738vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (root == nullptr) &#123; return res; &#125; queue&lt;TreeNode*&gt; q; q.push(root); res.push_back(vector&lt;int&gt;()); // 当前层，在队列里面的元素数量 int cur_remain = 1; // 下一层的元素数量 int next_level = 0; while (!q.empty()) &#123; TreeNode* now = q.front(); q.pop(); // 存入队列 res[res.size() - 1].push_back(now-&gt;val); // 左右孩子入队 if (now-&gt;left) &#123; q.push(now-&gt;left); next_level++; &#125; if (now-&gt;right) &#123; q.push(now-&gt;right); next_level++; &#125; // 当前层数量-- cur_remain--; // 切换到下一层 if (cur_remain == 0 &amp;&amp; !q.empty()) &#123; res.push_back(vector&lt;int&gt;()); cur_remain = next_level; next_level = 0; &#125; &#125; return res;&#125; 一次遍历一层的思路 很好，掌握！ 一次while循环，保证当前队列里面只有当前层的元素，用vector记录当前层的序列 q.size() 获得当前层元素数量，然后本次循环，只从队列里面出这么多元素。 依次遍历当前层的所有元素，出队，同时左右孩子入队 q为空，则所有层遍历结束 123456789101112131415161718192021222324252627282930/* * 保证当前队列的循环只有当前层的 */vector&lt;vector&lt;int&gt;&gt; level_order(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (root == nullptr) &#123; return res; &#125; queue&lt;TreeNode*&gt; q; q.push(root); // 一次while循环，出掉当前层的所有元素，下一层的元素全部入队 while (!q.empty()) &#123; // 当前层的数量和遍历结果序列 int level_num = q.size(); vector&lt;int&gt; curv; for (int i = 0; i &lt; level_num; i++) &#123; TreeNode* p = q.front(); // p的左右孩子入队列 if (p-&gt;left) q.push(p-&gt;left); if (p-&gt;right) q.push(p-&gt;right); // p出队，放到当前层的vector中 q.pop(); curv.push_back(p-&gt;val); &#125; // 放到末尾，就是从下到上 // res.insert(res.begin(), curv); res.push_back(curv); &#125; return res;&#125; 发现重复的数字-287 Find the Duplicate Number， 类似于aim2offer中查找重复的数字 数组a，有n+1个数，都在[1,n]范围内，只有一个重复的元素。找到它 二分思路 [1, n]这个范围有n个数。划分为两个范围[1, m]和[m+1, n] 每次去遍历整个数组，统计两个范围内的数字的数目 统计整个数组中元素在[1, m]范围内的个数\\(c_1\\) 统计整个数组中元素在[m+1, n]范围内的个数\\(c_2\\) [1, m]这m个数字的数量是c 如果 c &gt; m， 则1, m]内一定存在重复的数，e = m 否则，[m+1, n]一定存在重复的数，s = m + 1 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* * 找到一个重复的数字 * Args: * a -- 数组，n+1个元素，范围[1,n]，至少有一个重复的数字 * Returns: * dup -- 重复的数字 */int find_duplicate(const vector&lt;int&gt;&amp; a) &#123; int n = a.size() - 1; if (n &lt;= 0) &#123; return 0; &#125; int l = 1; int r = n; int dup = -1; // 不断缩小范围 while (l &lt;= r) &#123; int m = (l + r) &gt;&gt; 1; // 统计[l,m]在a中的出现次数 int count = count_range(a, l, m); if (l == r) &#123; if (count &gt;= 2) &#123; dup = l; break; &#125; &#125; // [l, m]有重复的 if (count &gt; (m - l + 1)) &#123; r = m; &#125; // [m+1, r]有重复的 else &#123; l = m + 1; &#125; &#125; return dup;&#125;/* * 找到数组a中，[min, max]这些数的出现次数 */int count_range(const vector&lt;int&gt;&amp; a, int min, int max) &#123; int count = 0; for (int i = 0; i &lt; a.size(); i++) &#123; if (a[i] &gt;= min &amp;&amp; a[i] &lt;= max) &#123; count++; &#125; &#125; return count;&#125; 合并两条有序链表-021 Merge Two Sorted Lists ，和归并排序的Merge操作 很类似。 考虑鲁棒性 思路 \\(l_1\\)与\\(l_2\\)若有一个为空的，则返回另一个 初始化新的head，选择\\(l_1\\)与\\(l_2\\)中第一个节点较小的那个 while循环，谁小选谁 结束之后，直接把未空的链表链接上即可 关键代码 123456789101112131415161718192021222324252627282930313233343536373839ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123; if (l1 == nullptr) &#123; return l2; &#125; if (l2 == nullptr) &#123; return l1; &#125; // 初始化head ListNode* head = nullptr; if (l1-&gt;val &lt; l2-&gt;val) &#123; head = l1; l1 = l1-&gt;next; &#125; else &#123; head = l2; l2 = l2-&gt;next; &#125; // 遍历两条链表，每次选择小的追加到p的后面 ListNode* p = head; while (l1 &amp;&amp; l2) &#123; if (l1-&gt;val &lt; l2-&gt;val) &#123; p-&gt;next = l1; l1 = l1-&gt;next; &#125; else &#123; p-&gt;next = l2; l2 = l2-&gt;next; &#125; p = p-&gt;next; &#125; // 某一条链表还有剩余 if (l1) &#123; p-&gt;next = l1; &#125; if (l2) &#123; p-&gt;next = l2; &#125; return head;&#125; 合并多条有序链表-023 Merge k Sorted Lists 我们已经会合并2条链表了，可以使用归并排序和二分查找的思想来合并多个列表。 示例 现在有1, 2, 3, 4, 5, 6条链表 第一步：1-6，2-5，3-4合并，得到新的1, 2, 3 第二步：1-3合并，2不动，得到新的1, 2 第三步：1-2合并， 得到新的2， 合并完成 返回list[0] 总结 直到len==1 合并到只有一条链表，合并到list[0] 对于当前len，折半两两合并，i和len-i-1合并，放到前面lists[i] len缩减一半，len=(len+1)/2 关键代码 12345678910111213141516ListNode* mergeKLists(vector&lt;ListNode*&gt;&amp; lists) &#123; if (lists.empty()) &#123; return nullptr; &#125; int len = lists.size(); // 直到只有一条链表 while (len &gt; 1) &#123; // 依次合并前后两条链表 for (int i = 0; i &lt; len / 2; i++) &#123; // 合并放到list[i] lists[i] = mergeTwoLists(lists[i], lists[len - i - 1]); &#125; len = (len + 1) / 2; &#125; return lists[0];&#125; Z型打印二叉树-103 Binary Tree Zigzag Level Order Traversal 参考层次遍历 一次遍历一层的思路。 一次遍历一层，得到当前层的遍历结果 单数，从左向右；偶数，从右向左 每次遍历一个元素，把左右孩子入队 根节点，向右走 第二层，向左走 向右走，从队头出，孩子先左后右，加到队尾 向左走，从队尾出，孩子先右后左，加到队首 两个栈的思路 栈1初始存放根节点，栈2为空 向右走，栈1全部出栈，先左后右孩子依次压入栈2，栈底-栈顶，栈2为2 3 向左走，栈2全部出栈，先右后左孩子依次压入栈1，栈1为7 6 5 4 向右走，栈1出栈，左右孩子依次压入栈2，栈2为8 9 10 11 12 13 14 15 向左走，栈2出栈，结束 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041/* * 使用两个栈z型层次打印二叉树 */vector&lt;vector&lt;int&gt;&gt; zigzagLevelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (root == nullptr) &#123; return res; &#125; stack&lt;TreeNode*&gt; st1; stack&lt;TreeNode*&gt; st2; st1.push(root); while (!st1.empty() || !st2.empty()) &#123; // 向右走 vector&lt;int&gt; curv; if (!st1.empty()) &#123; while (!st1.empty()) &#123; TreeNode* p = st1.top(); st1.pop(); curv.push_back(p-&gt;val); if (p-&gt;left) st2.push(p-&gt;left); if (p-&gt;right) st2.push(p-&gt;right); &#125; &#125; // 向左走 else &#123; while (!st2.empty()) &#123; TreeNode* p = st2.top(); st2.pop(); curv.push_back(p-&gt;val); if (p-&gt;right) st1.push(p-&gt;right); if (p-&gt;left) st1.push(p-&gt;left); &#125; &#125; res.push_back(curv); &#125; return res;&#125; 二叉树路径求和-112.113.437 题目1 从根节点到叶子求和-112 Path Sum， EASY 给一颗二叉树和一个sum值，判断是否有从根节点到叶子节点的路径，使得路径上的节点求和等于sum 思路 做减法 根节点为空，False 没有孩子，判断root.val == sum 有孩子，把sum减掉根节点的值，去判断左右子树是否有 ，sum = sum - root.val 关键代码 12345678910111213bool hasPathSum(TreeNode* root, int sum) &#123; // 1. 节点为空 if (root == nullptr) &#123; return false; &#125; // 2. 没有左右子树，直接判断 if (!root-&gt;left &amp;&amp; !root-&gt;right) &#123; return root-&gt;val == sum; &#125; // 3. 减小sum，去递归判断左右子树 int newsum = sum - root-&gt;val; return hasPathSum(root-&gt;left, newsum) || hasPathSum(root-&gt;right, newsum);&#125; 题目2 从根节点到叶子节点求和-保存路径-113 Path Sum-113， Medium 给二叉树和sum值，找到root-to-leaf的路径，使得和为sum。保存该路径 思路 用vector&lt;int&gt; path 来记录当前路径， vector&lt;vector&lt;int&gt;&gt; res 记录最终结果 根节点为空，返回 到达叶子节点，val == sum， 把当前节点加入path，当前path加入res， 否则返回 非叶子节点，把当前节点加入path，去左右子树中遍历，继续追加path直到叶子节点 非叶子节点，结束后，把当前节点从path中删除 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* * 找到树种从root到leaf的所有和为sum的路径 */vector&lt;vector&lt;int&gt;&gt; pathSum(TreeNode* root, int sum) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (root == nullptr) &#123; return res; &#125; vector&lt;int&gt; path; find_path(root, sum, path, res); return res;&#125;/* * 递归遍历节点，逐渐添加节点到当前的path，叶子节点，满足要求时，则把path追加到res中 * Args: * root -- 当前节点 * path -- 当前路径 * res -- 所有路径 * Returns: * None */void find_path(TreeNode* root, int sum, vector&lt;int&gt;&amp; path, vector&lt;vector&lt;int&gt;&gt; &amp;res) &#123; if (root == nullptr) &#123; return; &#125; // 到达叶子节点 if (!root-&gt;left &amp;&amp; !root-&gt;right) &#123; if (root-&gt;val == sum) &#123; path.push_back(root-&gt;val); res.push_back(path); path.pop_back(); &#125; return; &#125; // 当前节点加到path中 path.push_back(root-&gt;val); // 更新sum，到左右子树中去添加path int newsum = sum - root-&gt;val; if (root-&gt;left) &#123; find_path(root-&gt;left, newsum, path, res); &#125; if (root-&gt;right) &#123; find_path(root-&gt;right, newsum, path, res); &#125; // 当前节点从path中移除 path.pop_back();&#125; 题目3 求二叉树的所有和为sum的路径，任意起始节点-437 给一颗二叉树和sum，求出所有和为sum的路径数量，从任意节点开始和结束。 思路 层次遍历，以每一颗节点为起始值，找到以它开始的路径数量 节点为空，0 无孩子，不相等，0 无孩子，相等，1 有孩子，相等，c = 1， 不相等c = 0。 更新sum，继续递归查找左右孩子的count。返回c+count 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940/* * 找到树中，和为sum的所有路径数量 */int pathSum(TreeNode* root, int sum) &#123; if (root == nullptr) &#123; return 0; &#125; queue&lt;TreeNode*&gt; q; q.push(root); int count = 0; // 前序遍历 while (!q.empty()) &#123; TreeNode* now = q.front(); q.pop(); count += count_from_root(now, sum); if (now-&gt;left) q.push(now-&gt;left); if (now-&gt;right) q.push(now-&gt;right); &#125; return count;&#125;/* * 以root为起始节点，向下走，和为sum的路径的条数 */int count_from_root(TreeNode* root, int sum) &#123; // 空 if (root == nullptr) return 0; // 直接根节点就满足，无需看孩子 int c = 0; if (sum == root-&gt;val) &#123; // 相等 c = 1; &#125; else if (!root-&gt;left &amp;&amp; !root-&gt;right) &#123; // 无孩子，不相等 return 0; &#125; // c+左右孩子的 int newsum = sum - root-&gt;val; return c + count_from_root(root-&gt;left, newsum) + count_from_root(root-&gt;right, newsum);&#125; 有序链表转平衡BST-109 Convert Sorted List to Binary Search Tree， Medium。 类似题型：BST转有序双向链表 给一个有序链表，转化为平衡的二叉搜索树 思路 有序 -- BST的中序遍历；平衡 -- 以中间节点为根节点，分为左右子树递归去创建。 计算总结点数量 -- size， 递归去构建树go(0, size - 1) go(head, start, end) ，计算出中间节点mid-node， 构造树根root 左孩子root.left = go(head, start, mid-1)， 右孩子root.right = go(now.next, mid+1, end) 返回当前树根节点root 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* * 把有序链表转化为平衡的BST * Args: * head -- 链表 * Returns: * root -- 树的头结点 */ TreeNode* sortedListToBST(ListNode* head) &#123; if (head == nullptr) &#123; return nullptr; &#125; int size = 0; ListNode* p = head; while (p) &#123; ++size; p = p-&gt;next; &#125; return create_tree(head, 1, size);&#125;/* * 递归中序创建树 * Args: * head -- 链表头节点 * start -- 起始节点编号，从1开始 * end -- 结束节点编号 * Returns: * root -- 树的根节点 */TreeNode* create_tree(ListNode* head, int start, int end) &#123; // 0. 递归终止条件 if (head == nullptr || start &gt; end) &#123; return nullptr; &#125; // 1. 找到中间节点，构建根节点 int mid = (end + start) / 2; ListNode* node = head; for (int i = start + 1; i &lt;= mid; i++) &#123; node = node-&gt;next; &#125; TreeNode* root = new TreeNode(node-&gt;val); // 2. 递归构造左右子树 root-&gt;left = create_tree(head, start, mid - 1); root-&gt;right = create_tree(node-&gt;next, mid + 1, end); return root;&#125; 序列化二叉树-297 Serialize and Deserialize Binary Tree， Hard 把二叉树序列化为字符串，把字符串反序列化为一棵树 思路 前序遍历来保存序列，保存成一颗完全二叉树，空节点用$表示，使用空格进行分割。 序列化 序列化为一颗完全二叉树，先序递归。遇到空指针，则用$代替 先把字符放到stringstream里面，&lt;&lt;输入， 最后s.str()得到字符串 12345678910111213141516171819202122232425262728293031323334/* * 把一棵树序列化为一个字符串，前序完全二叉树序列 * Args: * root -- 树 * Returns: * str -- 序列化后的字符串 */string serialize(TreeNode* root) &#123; if (root == nullptr) &#123; return \"\"; &#125; stringstream buf; build_string(root, buf); return buf.str();&#125;/* * 递归把二叉树序列化到buf字符串中 * Args: * root -- 当前的根节点 * buf -- 字符串buffer * Returns: * None，都写到了buf中 */void build_string(TreeNode* root, stringstream&amp; buf) &#123; if (root == nullptr) &#123; buf &lt;&lt; \"$\" &lt;&lt; \" \"; return; &#125; buf &lt;&lt; root-&gt;val &lt;&lt; \" \"; build_string(root-&gt;left, buf); build_string(root-&gt;right, buf); return;&#125; 从字符串中解析得到序列，存到队列中 123456789101112/* * 分割字符串，把字符写到容器q里面 */void split(const string&amp; str, queue&lt;string&gt; &amp;q, const char delim = ' ') &#123; istringstream input; input.str(str); string line; while (std::getline(input, line, delim)) &#123; q.push(line); &#125; return;&#125; 反序列化 得到队列序列之后，可以对其进行递归反序列化构建树。先序序列，不是层次序列。 根-左-右，队列。出队，建立根节点 左-右，队列，递归建立左孩子 右，队列，建立右孩子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/* * 把String解析为一棵树 * Args: * data -- 序列化后的字符串 * Returns: * root -- 树 */TreeNode* deserialize(const string&amp; data) &#123; if (data.empty()) &#123; return nullptr; &#125; // 先序序列 queue&lt;string&gt; preorder; split(data, preorder); return build_tree(preorder);&#125;/* * 递归先序构造树 * Args: * prev -- 先序遍历序列 * Retursn: * root -- prev[i]为根构建的树 */TreeNode* build_tree(queue&lt;string&gt;&amp; pres) &#123; if (pres.size() == 0) &#123; return nullptr; &#125; string val = pres.front(); pres.pop(); // 当前为空节点 if (val == \"$\") &#123; return nullptr; &#125; // 有值 TreeNode* root = new TreeNode(std::stoi(val)); // 递归按照顺序构建左右子树 root-&gt;left = build_tree(pres); root-&gt;right = build_tree(pres); return root;&#125; 数字全排列-046 Permutations, Medium。 搜索树 给一个数组，返回全排列。每个数字都不相同 思路 全排列回溯法搜索。一个数组，搜索第t层的时候 前面t-1层都已经ok 遍历后面的所有元素，给到t层，去搜索 每次进行交换 github代码 123456789101112131415161718192021222324252627282930/* * 数组的全排列 */vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt; &amp;nums) &#123; vector&lt;vector&lt;int&gt;&gt; res; dfs(nums, 0, res); return res;&#125;/* * 回溯搜索排列树，遍历当前第i层的所有可能性，前面i-1已经全部确定好 * Args: * t -- 第几层，[0, n-1] * path -- 当前路径，[0,i-1]已经确定好，[i,n-1]是剩余的数字，遍历每一种可能给到i * res -- 总的结果 * Returns: * None */void dfs(vector&lt;int&gt;&amp; path, int t, vector&lt;vector&lt;int&gt;&gt;&amp; res) &#123; if (t &gt;= path.size()) &#123; res.push_back(path); return; &#125; for (int i = t; i &lt; path.size(); i++) &#123; std::swap(path[t], path[i]); dfs(path, t + 1, res); std::swap(path[t], path[i]); &#125;&#125; 重复数字全排列-047 重复数字全排列-047 给一个数组，里面有一些重复的数字，给出所有的排列可能 思路 重复的原因：当为t设置值的时候，遍历后面的所有元素给t赋值，但是后面都有一些重复的数值。 比如说 1 2 1 1， 开始是1，1会与最后的两个1再进行交换，然而其实是一样的。没必要了。 每次遍历交换的时候，只交换遍历后面不重复的元素。 github代码 123456789101112131415161718192021222324252627282930/* * 回溯搜索排列树，遍历当前第i层的所有可能性，前面i-1已经全部确定好 * Args: * t -- 第几层，[0, n-1] * path -- 当前路径，[0,i-1]已经确定好，[i,n-1]是剩余的数字，遍历每一种可能给到i * res -- 总的结果 * Returns: * None */void dfs(vector&lt;int&gt;&amp; path, int t, vector&lt;vector&lt;int&gt;&gt;&amp; res) &#123; if (t &gt;= path.size()) &#123; res.push_back(path); return; &#125; // 不重复的元素与其索引 set&lt;int&gt; vals; set&lt;int&gt; idx; for (int i = t; i &lt; path.size(); i++) &#123; if (vals.find(path[i]) == vals.end()) &#123; vals.insert(path[i]); idx.insert(i); &#125; &#125; for_each(idx.begin(), idx.end(), [&amp;](int i) &#123; std::swap(path[t], path[i]); dfs(path, t + 1, res); std::swap(path[t], path[i]); &#125;);&#125; 组合问题 给n个字符，要组成m个字符，问有多少种组成方法 有点类似于01背包的选择。 选择第一个字符，则在后面选择m-1个字符 不选择第一个字符，则在后面选择m个字符 正方体顶点和相等问题 给8个数字，正方体有8个顶点，数字放在顶点上。使得3对对面的顶点和相等。 也就是搜索，然后限定一些条件。 8皇后问题 8*8的象棋摆8个皇后，任意两个皇后不能在同一行、同一列或同一对角线上。问有多少种摆法 N皇后问题-051 n*n的棋盘摆n个皇后，任意两个皇后不能在同一行、同一列或同一对角线上。返回摆法。 N-Queens 每一行一个皇后，每一行有n个选择。就去dfs搜索所有的排列树。 path[t]=k， 第t行的皇后在第k列 k不能在前面皇后的：同一列、主对角线、副对角线 。别忘记副对角线。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/* * 返回n皇后的解法 */vector&lt;vector&lt;string&gt;&gt; solveNQueens(int n) &#123; // 1. 获得所有可能的位置 vector&lt;vector&lt;int&gt;&gt; locations; vector&lt;int&gt; path(n); std::iota(path.begin(), path.end(), 0); dfs(0, n, path, locations); // 2. 构造返回结果 vector&lt;vector&lt;string&gt;&gt; res; for (auto loc : locations) &#123; vector&lt;string&gt; solu; for (int i : loc) &#123; string line(n, '.'); line[i] = 'Q'; solu.push_back(line); &#125; res.push_back(solu); &#125; show(res[0]); return res;&#125;/* * dfs，设置t行的皇后位置 * Args: * t -- 第t行，从0开始 * n -- n皇后 * path -- 当前的路径方案 * res -- 总的方案 * Returns: * None */void dfs(int t, int n, vector&lt;int&gt;&amp; path, vector&lt;vector&lt;int&gt;&gt;&amp; res) &#123; if (t == n) &#123; res.push_back(path); return; &#125; // 前面t-1行已经ok，再后面的t-n个选择中选择遍历t for (int i = t; i &lt; n; i++) &#123; if (legal(t, path[i], path) == true) &#123; swap(path[i], path[t]); //cout &lt;&lt; \"t=\" &lt;&lt; t &lt;&lt; \", k=\" &lt;&lt; path[i] &lt;&lt; endl; //for_each(path.begin(), path.end(), [](int i)&#123;cout &lt;&lt; i &lt;&lt; \" \";&#125;); dfs(t + 1, n, path, res); swap(path[i], path[t]); &#125; &#125;&#125;/* * 合法性判断，同一列、主对角线、副对角线 * Args: * t -- 第t行，从0开始 * k -- 放在第k个列，从0开始 * path -- 当前的路径，[0,t-1]行已经放好 * Returns: * true or false */bool legal(int t, int k, const vector&lt;int&gt;&amp; path) &#123; for (int i = 0; i &lt;= t - 1; i++) &#123; // 1. 不能和之前的在同一列 if (path[i] == k) &#123; return false; &#125; // 2. 不能在主对角线上 if (t - i == k - path[i]) &#123; return false; &#125; // 3. 不能在副对角线上 if (t + k == i + path[i]) &#123; return false; &#125; printf(\"p[%d]=%d,p[%d]=%d\\n\", t, k, i, path[i]); &#125; return true;&#125; N皇后问题-052 返回有多少种解法 做了上面的题，那这个就很简单了，返回数量就行了。 1234567891011121314/* * 返回n皇后的解法 */int solveNQueens(int n) &#123; // 所有的结果 vector&lt;vector&lt;int&gt;&gt; locations; // 当前的位置 vector&lt;int&gt; path(n); // 初始化为0-n-1 std::iota(path.begin(), path.end(), 0); // dfs遍历搜索 dfs(0, n, path, locations); return locations.size();&#125; 查找第k大的数总结 给N个数，确定第k个最大值 1 排序 排好序，取出第k大的值。\\(O(n\\log n + k)\\) 2 简单选择排序 简单选择。第k次选择，就是第k大的数字。\\(O(n*k)\\) 3 快速排序思想 每次partition，会把x放到位置i上。注意partition要从大到小排列，左大右小，而不是普通排序的左小右大。 i == k， 则就是a[i] k &gt; i， 则在i的右边 k &lt; i， 则在i的左边 [关键代码] 123456789101112131415161718192021222324/* * 使用快排思想查找第k大的数字，从大到小排列！！ * Args: * a -- 数组 * l -- 范围的开始 * r -- 范围的结束 * k -- 该范围内第k大的数 * Returns: * 第k大的数 */int find_kth_num(vector&lt;int&gt; &amp;a, int l, int r, int k) &#123; // 1. 划分。左边大，中间a[l]，右边小 int i = partition(a, l, r); // 2. 通过i+1==k来判断是否是第k大的数 if (i + 1 == k) &#123; return a[i]; &#125; else if (i + 1 &gt; k) &#123; // 在左边 return find_kth_num(a, l, i - 1, k); &#125; else &#123; // 在右边 return find_kth_num(a, i + 1, r, k); &#125;&#125; 4 最大堆 \\(O(4*n)\\)的空间建立最大堆，pop k次即可。\\(O(4 \\times n + k\\times\\log n)\\) 5 最小堆 维护大小为k的最小堆，遍历数组 堆顶元素大，则不管 堆顶元素小，则把当前值插入堆中 最后的堆顶，就是第k大的元素 \\(O(n \\times \\log k)\\) 6 Hash法 查找最小的k个数的总结 给一个数组，找到最小的k个数。注意改变或不改变原数组 1 排序思路 对n个数字从小到大排好序，再取前k个数。O(nlogn + k)=O(nlogn) 。排序算法总结 2 快速排序 排好前k个即可，改变原数组。 3 最大堆 建立大小为k的最大堆。不改变原数组。遇到新的元素，小于堆顶，则加入 4 堆排序 对整个数组n进行堆排序，每次取堆顶，取k次。 数组中次数超过一半的数-169 Majority Element-169， easy 一个数组，有一个元素出现次数超过一半，找到它。 思路0 先排序再找 \\(O(n\\log n)\\) 思路1 快速排序查找第k大元素思想 快速排序笔记 。 如果排好序，则该重复的数字应该在数组中间\\(a_{\\frac{n}{2}}\\)。 也就是中位数，第n/2大的数字 问题就转化为查找数组中的K大的元素 查找第k大的元素 partition(a, l, r) 会把x=a[l]放到中间去，小于的在右边，大于的在左边。返回x的最终位置i i == k， 则x就是第k大的元素 k &lt; i， 则k在右边 k &gt; i， 则k在左边 继续查找，知道 i == k 思路2 count加加减减思想 遇见友军（相同的），就++ 遇见敌军（不同的），就-- 最后剩余的肯定就是人数最多的那个（数字） [关键代码] 1234567891011121314151617181920212223242526/* * 查找主元素，阵地攻守思想。相同加价，不同减减，为0重新赋值 */int majority_element(vector&lt;int&gt;&amp; a) &#123; if (a.size() == 0) &#123; return 0; &#125; // 当前数值与计数 int res = a[0]; int count = 1; for (int i = 1; i &lt; a.size(); i++) &#123; if (a[i] == res) &#123; // 相同++ count++; &#125; else &#123; // 不同--或者重置为1 if (count == 0) &#123; res = a[i]; count = 1; &#125; else &#123; count--; &#125; &#125; &#125; return res;&#125; 主元素2-229 给一个数组，找到所有出现次数超过n/3的数 Majority Element II, medium 。 思路 当然最终结果只有2个或1个。思路同阵地攻守。 用两个变量去记录两个主元素 有一个相同，对应加1 两个都不同，有一个count==0， 则重置 两个都不听，两个都有count，则都减减 遍历之后，得到两个数，两个count 返回count &gt; n/3的数 最后，一定要注意去重！n1 != n2 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/* * 查找出现次数超过n/3的元素 * Args: * nums -- 数组 * Returns: * res -- 超过n/3的元素 */vector&lt;int&gt; majority_element(const vector&lt;int&gt;&amp; nums) &#123; vector&lt;int&gt; res; if (nums.empty()) &#123; return res; &#125; // 1. 找到出现次数最多的两个数 int n1 = 0, c1 = 0; int n2 = 0, c2 = 0; for (int n : nums) &#123; if (n == n1) &#123; c1++; &#125; else if (n == n2) &#123; c2++; &#125; else if (c1 == 0) &#123; n1 = n; c1 = 1; &#125; else if (c2 == 0) &#123; n2 = n; c2 = 1; &#125; else &#123; c1--; c2--; &#125; &#125; c1 = 0, c2 = 0; // 2. 重新计算出现次数 for (auto n : nums) &#123; if (n1 == n) &#123; c1++; &#125; else if (n2 == n) &#123; c2++; &#125; &#125; // 3. 把出现次数超过n/3的数字放到res里面 if (c1 &gt; nums.size() / 3) &#123; res.push_back(n1); &#125; // 去重 if (n2 != n1 &amp;&amp; c2 &gt; nums.size() / 3) &#123; res.push_back(n2); &#125; return res;&#125; 最长不重复子字符串-003 Longest Substring Without Repeating Characters-003 给一个字符串，找到里面最长子字符串的长度。 1 DP思路 设l[i]=k， 是以s[i]结尾最长字符串的长度是k。从左到右进行计算。遍历到s[i] 1、 s[i]在前面没有出现过，l[i] = l[i-1] + 1 2 、s[i]在前面出现过，计算该字符现在与前面两次出现的距离d，比较d和s[i-1]的最大长度l[i-1] d &gt; l[i-1]， d很远（不在i-1的最长字符串里），l[i] = l[i-1] + 1 d &lt;= l[i-1]，d很近（在i-1的最长字符串里），l[i]=d 使用HashMap&lt;Char, Int&gt;去记录位置信息，保留最近时间的出现位置。 1234567891011121314151617181920212223242526272829303132333435363738394041/* * 字符串中，最长不重复子串的长度，是连续 * Args: * s -- 原字符串 * Returns: * len -- 最长子串长度 */int long_substr_len(const string&amp; s) &#123; if (s.empty()) &#123; return 0; &#125; // 长度，l[i]=k，以s[i]结尾的最大子串长度为k vector&lt;int&gt; l(s.length(), 0); // 位置，m['a']=i，最近，字符a出现的位置是i map&lt;char, int&gt; m; for (int i = 0; i &lt; s.length(); i++) &#123; char ch = s[i]; if (i == 0) &#123; l[i] = 1; m[ch] == i; continue; &#125; if (m.find(ch) == m.end()) &#123; // ch 没出现过 l[i] = l[i-1] + 1; &#125; else &#123; // ch 出现过，距离比较 int d = i - m[ch]; if (d &gt; l[i-1]) &#123; // 上一个ch距离很远 l[i] = l[i-1] + 1; &#125; else &#123; // 上一个ch距离很近 l[i] = d; &#125; &#125; // 更新出现位置 m[ch] = i; &#125; return *max_element(l.begin(), l.end());&#125; 丑数-263-264-313 263-判断是否是丑数 ，easy 只包含2、3、5作为因子的正整数是丑数，1也是。判断是否是丑数 1234567891011121314151617bool isUgly(int num) &#123; // 特殊情况 if (num &lt;= 0) &#123; return false; &#125; if (num == 1) &#123; return true; &#125; // 不断分解2,3,5 vector&lt;int&gt; a = &#123;2, 3, 5&#125;; for (auto i : a) &#123; while (num % i == 0) &#123; num /= i; &#125; &#125; return num == 1;&#125; 264-找到第n个丑数 ，medium 找到第n个丑数 如果依次找，所有的数都要分解求余，效率低。**丑数=丑数*{2,3,5}。 用数组去存放丑数，从1开始依次向前面递推计算。** 用3个索引，去分别保存乘以2,3,5的基数，每次选择最小的作为下一个丑数，同时更新对应的索引++。 1234567891011121314151617181920212223242526272829303132/* * 第n个丑数，从已有的丑数不断地向前乘得到新的丑数。 */int nthUglyNumber(int n) &#123; if (n &lt;= 0) &#123; return -1; &#125; if (n == 1) &#123; return 1; &#125; vector&lt;int&gt; u(n); u[0] = 1; int t2 = 0, t3 = 0, t5 = 0; // 计算丑数 for (int i = 1; i &lt; n; i++) &#123; // 计算新的丑数，选择最小的 int cur = min(min(u[t2] * 2, u[t3] * 3), u[t5]*5); u[i] = cur; // 更新基数索引 if (cur == u[t2] * 2) &#123; t2++; &#125; if (cur == u[t3] * 3) &#123; t3++; &#125; if (cur == u[t5] * 5) &#123; t5++; &#125; &#125; return u[n-1];&#125; 313-超级丑数， medium 超级丑数是，给一个素数列表去计算丑数，不再局限于2,3,5去计算丑数。 给一个素数列表和n，返回第n个丑数 和上一个思路一致，用数组保存。注意t数组的初始大小是primes.size()，而不是n。 1234567891011121314151617181920212223242526272829303132/* * 给定素数，返回第n个丑数 */int nthSuperUglyNumber(int n, vector&lt;int&gt;&amp; primes) &#123; if (n == 1) &#123; return 1; &#125; if (n &lt;= 0 || primes.empty()) &#123; return -1; &#125; vector&lt;int&gt; u(n); u[0] = 1; vector&lt;int&gt; t(primes.size(), 0); for (int i = 1; i &lt; n; ++i) &#123; // 计算新的丑数，选择最小的 int cur = INT_MAX; for (int j = 0; j &lt; primes.size(); ++j) &#123; cur = min(cur, u[t[j]] * primes[j]); &#125; u[i] = cur; // 更新索引，向前推进 for (int j = 0; j &lt; primes.size(); ++j) &#123; if (cur == u[t[j]] * primes[j]) &#123; t[j]++; &#125; &#125; &#125; return u[n-1];&#125;","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"},{"name":"树","slug":"树","permalink":"http://plmsmile.github.io/tags/树/"}]},{"title":"剑指offer(11-20)","date":"2017-12-29T03:57:48.000Z","path":"2017/12/29/aim2offer2/","text":"剑指offer算法题(11-20) 剑指offer算法题(03-10) 旋转数组中的最小值-11 排序算法 我的排序算法总结 旋转数组最小值 原有序数组：1,2,3,4,5,6,7，旋转一下，把一部分放到后面去：4,5,6,7, 1,2,3。 求：在\\(O(logn)\\)内找到数组中最小的元素 leetcode旋转数组 思路 特点：左边序列全部大于等于右边序列。 最小的值在中间，顺序查找肯定不行，那就只能二分查找了。 设两个指针，left从左边向中间靠拢，right从右边向中间靠拢。 循环条件：a[l] &gt;= a[r] 直到l+1==r 为止，那么a[r]就是我们要的最小值。 计算中间值 a[m] a[m] &gt;= a[l] ：m在左边序列，l = m， l向右走 a[m] &lt;= a[r] ： m在右边序列，r == m， r向左走 陷阱 可能一个数字都不旋转，即a为有序序列，直接返回a[0] 有重复的数字，a[l]==a[m] &amp;&amp; a[m]==[r]， 则只能顺序查找了 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* * 查找旋转数组中的最小值。两个指针指向前后两个递增序列，向中间靠拢 */int minum_rotate_array(const vector&lt;int&gt;&amp; a) &#123; if (a.size() == 0) &#123; return 0; &#125; int l = 0; int r = a.size() - 1; // 特殊情况，旋转0个，原数组，小于不能等于 if (a[l] &lt; a[r]) &#123; return a[l]; &#125; int res = -1; while (a[l] &gt;= a[r]) &#123; // 两个指针已经相邻 if (l + 1 == r) &#123; res = a[r]; break; &#125; // 中间指针 int m = (l + r) / 2; // 三个数相等，无法确定中间数在前后那个序列 if (a[l] == a[m] &amp;&amp; a[m] == a[r]) &#123; res = get_min(a, l, r); break; &#125; if (a[m] &gt;= a[l]) &#123; l = m; &#125; else if (a[m] &lt;= a[r]) &#123; r = m; &#125; &#125; return res;&#125;int get_min(const vector&lt;int&gt;&amp;a, int start, int end) &#123; int min = a[start]; for (int i = start + 1; i &lt;= end; i++) if (a[i] &lt; min) &#123; min = a[i]; &#125; return min;&#125; 矩阵中的路径-12 给一个字符数组，手动给行和列进行分割。再给一个字符串，判断该字符串是否在该字符矩阵中。使用回溯法进行搜索。 其实就是遍历所有的点开始，然后依次进行上下左右继续查找。用visited去记录点是否已经走过，用pathlen记录目标字符串的遍历长度。 [关键代码] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/* * 判断str在字符矩阵matrix中是否有路径 * * Args: * matrix -- 字符数组，由rows和cols切分为矩阵 * rows -- 行 * cols -- 列 * str -- 字符串 * Returns: * true -- 包含，false -- 不包含 */bool has_path(const char* matrix, int rows, int cols, const char* str) &#123; if (matrix== nullptr || rows &lt; 1 || cols &lt; 1 || str == nullptr) &#123; return false; &#125; bool *visited = new bool[rows * cols]; memset(visited, 0, rows * cols); int pathlen = 0; // 从每个点开始 for (int row = 0; row &lt; rows; row++) &#123; for (int col = 0; col &lt; cols; col++) &#123; bool has = go_find(matrix, rows, cols, row, col, str, pathlen, visited); if (has) &#123; delete[] visited; return true; &#125; &#125; &#125; delete[] visited; return false;&#125;/* * 上下左右去回溯查询 * Args: * matrix -- 字符矩阵 * rows -- 矩阵的行数 * rows -- 矩阵的列数 * row -- 当前要判断的行 * col -- 当前要判断的列 * str -- 原字符串 * pathlen -- 判断到第几个字符 * visited -- 位置访问与否 * Returns: * true -- * false -- */bool go_find(const char* matrix, int rows, int cols, int row, int col, const char* str, int &amp;pathlen, bool *visited) &#123; if (str[pathlen] == '\\0') &#123; return true; &#125; bool ok = false; int cur = row * cols + col; if (row &gt;= 0 &amp;&amp; row &lt; rows &amp;&amp; col &gt;= 0 &amp;&amp; col &lt; cols &amp;&amp; matrix[cur] == str[pathlen] &amp;&amp; visited[cur] == false) &#123; ++pathlen; visited[cur] = true; bool left = go_find(matrix, rows, cols, row, col - 1, str, pathlen, visited); bool right = go_find(matrix, rows, cols, row, col + 1, str, pathlen, visited); bool down = go_find(matrix, rows, cols, row + 1, col, str, pathlen, visited); bool up = go_find(matrix, rows, cols, row - 1, col, str, pathlen, visited); ok = left || right || down || up; if (!ok) &#123; --pathlen; visited[cur] = false; &#125; &#125; return ok;&#125; 机器人的运动范围-13 给一个\\(\\rm{rows \\times cols}\\)的矩阵，和一个阈值\\(k\\)。 机器人从\\((0, 0)\\)出发，进入矩阵的格子。 能进入格子：(35, 38)，因为3+5+3+8k 思路 回溯法， \\(\\rm{count = 1 + left + right + up + down}\\) [关键代码] 定义Solution 123456789101112131415161718192021class Solution &#123; private: // 阈值 int threshold; // 矩阵行 int rows; // 矩阵列 int cols; // 记录格子有没有被走过 bool *visited; public: // 外部接口 int movingCount(int threshold, int rows, int cols); // 回溯计算 int dfs(int row, int col); // 检查该点是否可以进入 bool check_point(int row, int col); // 把各个位的数字加起来 static int resolve_num(int n);&#125;; movingcount 12345678910111213141516171819202122232425/* * 在这个矩阵和阈值上，从(0, 0)进入统计能进入多少个格子 * * Args: * threshold -- 各个位的阈值 * rows -- 矩阵的行数 * cols -- 矩阵的列数 * Returns: * count -- 可以进入的总的格子数量 */int Solution::movingCount(int threshold, int rows, int cols) &#123; // 参数校验 if (threshold &lt; 0 || rows &lt; 1 || cols &lt; 1) &#123; return 0; &#125; // 变量初始化 this-&gt;threshold = threshold; this-&gt;rows = rows; this-&gt;cols = cols; this-&gt;visited = new bool[rows * cols]; memset(this-&gt;visited, 0, rows * cols); int count = dfs(0, 0); delete[] this-&gt;visited; return count;&#125; 回溯法上下左右格子累加 12345678910111213141516171819202122/* * 回溯查询 * Args: * row -- 当前行，索引 * col -- 当前列，索引 * Returns: * count -- 从当前点(row,col)开始向上下左右走能走的格子之和 */int Solution::dfs(int row, int col) &#123; // 可以进入该点 if (check_point(row, col) == false) &#123; return 0; &#125; int cur = row * cols + col; visited[cur] = true; int left = dfs(row, col - 1); int right = dfs(row, col + 1); int up = dfs(row - 1, col); int down = dfs(row + 1, col); int count = 1 + left + right + up + down; return count;&#125; 检查点是否可进入和分解数 1234567891011121314151617181920212223242526/* * 把n的各个位上的数加起来 */int Solution::resolve_num(int n) &#123; int sum = 0; while (n &gt; 0) &#123; sum += n % 10; n = n / 10; &#125; return sum;&#125;/* * 检查一个点是否可以进入。索引、访问状态、阈值 */bool Solution::check_point(int row, int col) &#123; int cur = row * cols + col; if (row &lt; 0 || row &gt;= rows || col &lt; 0 || col &gt;= cols || visited[cur] == true) &#123; return false; &#125; if (resolve_num(row) + resolve_num(col) &gt; threshold) &#123; return false; &#125; return true;&#125; 剪绳子-14 动态规划 求最优解（最大值or最小值），大问题分解成若干个小问题。使用动态规划四个特点，如下 求最优解 整体问题的最优解依赖于各个子问题的最优解 若干个子问题，之间有相互重叠的更小的子问题。如f(2)是f(4)和f(6)共同的子问题 从上往下分析问题，从下网上求解问题。用数组存下小问题的解嘛。 贪心算法 每一步做一个贪婪的选择，基于这个选择，可以得到最优解。需要数学证明。 剪绳子-动态规划 条件 长度为\\(n\\)的绳子，把绳子剪成\\(m, \\; (m \\ge 2)\\)段， 每段长度为\\(k[0], k[1], \\cdots, k[m]\\)。 m和n都是整数，都大于1。 问题 长度连乘积最大是多少？ 思路 定义： \\(f(n)\\) 为把绳子切成若干段后，各段长度乘积的最大值 第一刀的时候，在长度为\\(i\\)的地方剪，分成\\(i\\)和\\(n-i\\)两段。 递推公式如下： \\[ f(n) = \\max (f(i) \\cdot f(n-i)), \\quad 0 &lt; i &lt; n \\] 1234567891011// 自下而上计算f[i]// 计算单个f[i]int max = 0;for (int j = 1; j &lt;= i / 2; j++) &#123; int t = f[j] * f[i - j]; if (t &gt; max) &#123; max = t; &#125;&#125; f[i] = max; 关键代码 长度在3以内的特殊返回 构建f[i]，f[0,1,2,3]手动赋值处理，意义不同 自下而上计算f[4, ..., length] 单独计算f[i]，找到\\(\\max(f_j \\cdot f_{i-j})\\)， 其中\\(j \\in \\{1, \\cdots, i/2\\}\\) 1234567891011121314151617181920212223242526272829303132333435363738int max_product_dp(int length) &#123; // 1. 特殊处理，长度在3以内，自动返回 if (length &lt;= 1) &#123; return 0; &#125; else if (length == 2) &#123; return 1; &#125; else if (length == 3) &#123; return 2; &#125; // 2. f[i]=k，长度为i的绳子剪成若干段最大乘积为k，i&gt;=4 int * f = new int[length + 1]; // 3. f[i]特殊值处理。比如4切一刀：1-3和2-2，f[1]=1,f[3]=3, f[2]=2。因为2*2&gt;1*3，f[4]=4 f[0] = 0; f[1] = 1; f[2] = 2; f[3] = 3; int max = 0; // 4. 自下而上计算 for (int i = 4; i &lt;= length; i++) &#123; max = 0; // 找最大的分割 for (int j = 1; j &lt;= i / 2; j++) &#123; int t = f[j] * f[i - j]; if (t &gt; max) &#123; max = t; &#125; &#125; f[i] = max; &#125; max = f[length]; delete[] f; return max;&#125; 剪绳子-贪心 贪心策略 当\\(n \\ge 5\\)时， 尽可能多地剪成长度为3的绳子 当剩余长度为4的时候，要剪成2*2的绳子 关键代码 特殊处理，长度在3以内，自动返回 计算3的个数 计算2的个数 计算最终结果 pow(3, t3)*pow(2,t2) 12345678910111213141516171819202122232425int max_product_greedy(int length) &#123; // 1. 特殊处理，长度在3以内，自动返回 if (length &lt;= 1) &#123; return 0; &#125; else if (length == 2) &#123; return 1; &#125; else if (length == 3) &#123; return 2; &#125; else if (length == 4) &#123; return 4; &#125; // 2. 计算3的个数 int t3 = length / 3; // 最后剩1，则补成4 if (length - t3*3 == 1) &#123; t3--; &#125; // 3. 计算2的个数 int t2 = (length - t3*3) / 2; // 4. 计算最终结果 return (int) pow(3, t3) * (int) (pow(2, t2));&#125; 二进制中1的个数-15 基础知识 位运算 操作 意义 与&amp; 都为1，才为1 或| 有一个为1，就为1 异或^ 不同为1，相同为0 移位 操作 意义 注意 左移n位 丢弃左边n位，右边n位补0 右移n位 无符号数&amp;正数：右移n位，左边补n个0 负数：右移n位，左边补n个1 重要结论 操作 意义 n &amp; (n-1) 把n的二进制中，最右边的1变成0 n &amp; 1 检测n的二进制，末尾位是否是1 n &amp; 2 检测n的二进制，倒数第二位是否是1 二进制中1的个数 问题：给一个整数，判断它的二进制中，有多少个1 正数思路 n &amp; 1，n不停右移。每次判断 n &amp; 1，即最末尾位是否是1。 但是负数有问题，负数右移，左边会补1，陷入死循环。 正数负数思路 n不变，n &amp; flag，flag每次向左移。即从右到左依次判断n的各个位是否是1。一共循环n的二进制位数次。 最优思路 n = n &amp; (n - 1)，每次把n的最右边的1变成0。看看能执行多少次这样的操作，就有多少个1。 关键代码 123456789101112131415161718192021222324252627/* * flag不断左移 */int count_1_by_flag(int n) &#123; int count = 0; unsigned int flag = 1; while (flag != 0) &#123; if (n &amp; flag) &#123; count++; &#125; flag = flag &lt;&lt; 1; &#125; return count;&#125;/* * 把n的最左边1变成0，看看能变几次，则有几个1 */int count_1(int n) &#123; int count = 0; while (n) &#123; n = n &amp; (n - 1); count++; &#125; return count;&#125; 数值的整数次方-16 代码风格 代码的规范性：清晰的书写、清晰的布局、合理的命名 代码的完整性：功能测试、边界测试、负面测试 处理错误的方法 方法 优点 缺点 返回值 和系统API一致 不能方便使用计算结果 全局变量 能够方便使用计算结果 用户可能会忘记检查全局变量 异常 不同错误，抛出不同异常，可自定义。逻辑清晰 有的语言不支持；对性能负面影响 数值的整数次方 要求实现 1double power(double base, int exponent); 注意特殊情况条件 指数为0 指数为负数 指数为负数，底数不能为0或者约等于0。用fabs(a-b) &lt; theta 思路1 条件考虑全面，一个一个乘 思路2递归 使用这个公式 \\[ a^n = \\begin{cases} &amp; a^{n/2} \\cdot a^{n/2} &amp;\\text{n为偶数} \\\\ &amp; a^{(n-1)/2} \\cdot a^{(n-1)/2} &amp; \\text{n为奇数} \\\\ \\end{cases} \\] 思路3位运算 例如\\(a^{13}\\)， 13的二进制是1101。 从右向左，依次读1，flag &amp; 1, b = b &gt;&gt; 1 遇到1就累乘， res *= a 每读一位，倍数增加， a *= a 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869double power_normal_binary(double base, int exponent) &#123; if (exponent == 0) &#123; return 1; &#125; else if (exponent == 1) &#123; return base; &#125; double res = 1; while (exponent != 0) &#123; if ((exponent &amp; 1) == 1) &#123; res *= base; &#125; base *= base; exponent = exponent &gt;&gt; 1; &#125; return res;&#125;/* * 递归计算 */double power_normal_recur(double base, int exponent) &#123; if (exponent == 0) &#123; return 1; &#125; else if (exponent == 1) &#123; return base; &#125; double res = power_normal_recur(base, exponent &gt;&gt; 1); // 翻次方倍 res *= res; // 奇数 if ((exponent &amp; 1) == 1) &#123; res *= base; &#125; return res;&#125;double power_normal(double base, int exponent) &#123; double res = 1; for (int i = 0; i &lt; exponent; i++) &#123; res *= base; &#125; return res;&#125;double power(double base, int exponent) &#123; // 指数为0，返回1 if (exponent == 0) &#123; return 1; &#125; // 指数为负数，底数不能为0 if (exponent &lt; 0 &amp;&amp; equal(base, 0.0)) &#123; cout &lt;&lt; \"error. exponent &lt; 0, base should != 0\" &lt;&lt; endl; return 0; &#125; // 分正负计算 double res = 1; if (exponent &gt; 0) &#123; res = power_normal_binary(base, exponent); &#125; else &#123; res = power_normal_binary(base, -exponent); res = 1.0 / res; &#125; return res;&#125; 打印1到最大的N位数-17 问题：输出打印1-999。但是n不确定，非常大，要注意溢出的问题。 思路 开辟n+1位的字符串，来存储数字。末尾位是'' 字符串模拟数字的自增、进位和溢出 长度超过n+1，或者第0位产生进位，则溢出 输出的时候，要人性化点。比如0087，要打印87 关键代码 开始的时候，个位加1。要注意进位， 清楚当前数字存在哪些位置上面的。 12345678910111213141516171819202122232425262728293031323334353637383940/* * 当前数加1，字符串模拟加法、进位、溢出 */void Solution::increment() &#123; // 当前位的值 int now = -1; // 前一位的进位 int take = 0; int i = -1; // 当前位数是n-1~n-real_len for (i = n - 1; i &gt;= n - real_len; i--) &#123; int now = num[i] - '0' + take; if (i == n - 1) &#123; // 实现自增，末尾加1 now = now + 1; &#125; if (now &gt;= 10) &#123; num[i] = '0' + (now - 10); take = 1; &#125; else &#123; num[i] = '0' + now; take = 0; break; &#125; &#125; // 需要新增一位 if (take &gt; 0) &#123; if (i &lt; 0) &#123; cout &lt;&lt; \"num char* over flow\" &lt;&lt; endl; this-&gt;over_flow = true; // 注意释放空间 delete this-&gt;num; return; &#125; num[i] = '0' + 1; real_len++; &#125; return;&#125; 删除链表中的元素-18 删除一个节点 给一个链表和一个节点，删除这个节点i，要求\\(O(1)\\) 思路 从头到尾遍历找到i的前驱节点，时间复杂度为\\(O(n)\\)， 显然不行。 把i的后继节点的值拷贝到i上，然后删除i的后继节点。 但，要注意：头结点、中间节点、末尾节点（顺序遍历）。 关键代码 123456789101112131415161718192021222324252627282930void delete_node(ListNode ** phead, ListNode* pdeleted) &#123; if (!phead || !pdeleted) &#123; return; &#125; // 只有一个节点，删除头节点/尾节点 if (*phead == pdeleted &amp;&amp; pdeleted-&gt;next == nullptr) &#123; delete pdeleted; pdeleted = nullptr; *phead = nullptr; &#125; // 有多个节点，删除尾节点 else if (pdeleted-&gt;next == nullptr) &#123; ListNode* p = *phead; while (p-&gt;next != pdeleted) &#123; p = p-&gt;next; &#125; p-&gt;next = nullptr; delete pdeleted; pdeleted = nullptr; &#125; // 有多个节点，删除中间的节点 else &#123; // 直接把下一个节点的值赋值到当前节点，再删除下一个节点 ListNode* pnext = pdeleted-&gt;next; pdeleted-&gt;val = pnext-&gt;val; pdeleted-&gt;next = pnext-&gt;next; delete pnext; pnext = nullptr; &#125;&#125; 删除有序链表中的重复元素 Remove Duplicates from Sorted List 有序的链表。 留下一个 每个重复的元素，留下一个。 思路 从head开始循环，遍历每一个节点。 对于当前节点c，再进行遍历，直到下一个节点不等于它或者为空。中间删除掉重复的节点。 全部删除 把所有重复的元素，都删除，有序链表。 思路 当前节点cur， 需要保留前一个元素的指针pre。 我的思路 循环遍历到cur，head应该为第一个pre。 cur无重复cur!=next || next==null pre为空，则pre=cur ； pre不为空， 则pre.next=cur ，pre=pre.next 。实际上pre.next指向下一轮的cur cur重复 cur=next 遍历删除所有与cur相同的元素 删除cur 但由于原本pre.next指向当前cur，cur又被删除。所以pre.next=nullptr 其实最重要是：当前不重复，则续接到pre后面，pre后移；当前重复，则删除，pre后面设为空。 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041ListNode* del_dups_nosave(ListNode* head) &#123; if (head == nullptr) &#123; return head; &#125; ListNode* cur = head; ListNode* pre = nullptr; ListNode* next = nullptr; while (cur) &#123; // 1. cur无重复 if (cur-&gt;next == nullptr || cur-&gt;val != cur-&gt;next-&gt;val) &#123; if (pre == nullptr) &#123; pre = cur; head = pre; &#125; else &#123; pre-&gt;next = cur; pre = cur; &#125; cur = cur-&gt;next; &#125; // 2. cur重复，删掉重复的元素 else &#123; next = cur-&gt;next; // 2.1 删除与cur相同的 while (next &amp;&amp; cur-&gt;val == next-&gt;val) &#123; cur-&gt;next = next-&gt;next; delete next; next = cur-&gt;next; &#125; // 2.2 删除cur delete cur; cur = next; // 原本pre-&gt;next=cur，但是当前cur已经被删除，所以重新置为空 if (pre != nullptr) pre-&gt;next = nullptr; &#125; &#125; if (pre == nullptr) &#123; head = nullptr; &#125; return head;&#125; 正则表达式匹配-19 问题 正则表达式：普通字符、. 任意字符 、* 前面的字符出现0次或多次。 给定字符串，去判断该字符串是否符合正则表达式。 例如：aaa 与a.a和ab*ac*a匹配， 与aa.a和ab*a不匹配。 思路 递归去判断。用\\(s_1\\)和\\(p_1\\)去 代表第一个字符和第一个模式。 \\(s_1\\)与\\(p_1\\)匹配成功： s1==p1 || (p1 == '.' &amp;&amp; s1 != '\\0') 必须先判断\\(p2\\)是否是* 如果，第二个模式\\(p_2\\)是* \\(s_1\\)与\\(p_1\\)匹配成功 *代表出现1次，go(s2, p3) *代表出现0次，go(s1, p3) *代表出现多次，go(s2, p1) \\(s_1\\)与\\(p_1\\)匹配失败 *只能代表出现0次，go(s1, p3) 如果\\(s_1\\)与\\(p_1\\)匹配成功 字符串和模式都向后挪一次，go(s2, p2) 总结 p结束，s结束。返回true p结束，s还有。 返回false p未结束，\\(p_2\\) == *。\\(s_1\\)与\\(p_1\\)匹配成功， p2作为0、1、多次，合并返回。匹配失败，作为0次，返回。 p未结束，\\(p_2\\) != *，单独match成功。go(s2, p2) p未结束，\\(p_2\\) != *，单独match失败。返回false 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/* * 单个字符是否能匹配 */bool single_char_match(const string &amp;str, int s, const string &amp;pattern, int p) &#123; // 超出 if (s &gt;= str.size() || p &gt;= pattern.size()) &#123; return false; &#125; // 匹配成功 if (str[s] == pattern[p] || (pattern[p] == '.' &amp;&amp; s &lt; str.size())) &#123; return true; &#125; // 匹配失败 return false;&#125;/* * 递归判断str和pattern是否匹配，从snow和pbgin开始 * Args: * str -- 字符串 * snow -- 从字符串的第几个开始 * pattern -- 正则表达式 * pnow -- 模式的开始 * Returns: * true or false */bool match_core(const string &amp;str, int snow, const string &amp;pattern, int pnow) &#123; // 1. p结束，s结束 if (pnow &gt;= pattern.size() &amp;&amp; snow &gt;= str.size()) &#123; return true; &#125; // 2. p结束，s还有 if (pnow &gt;= pattern.size() &amp;&amp; snow &lt; str.size()) &#123; return false; &#125; // 3. p未结束，p2 == * int p2 = pnow + 1; if (p2 &lt; pattern.size() &amp;&amp; pattern[p2] == '*') &#123; // s1与p1匹配成功 if (single_char_match(str, snow, pattern, pnow) == true) &#123; bool t0 = match_core(str, snow, pattern, pnow + 2); bool t1 = match_core(str, snow+1, pattern, pnow + 2); bool t_many = match_core(str, snow+1, pattern, pnow); return t0 || t1 || t_many; &#125; // s1与p1匹配失败 else &#123; // *只能代表0 return match_core(str, snow, pattern, pnow + 2); &#125; &#125; // 4. p未结束，p2 != *，单独match成功 if (single_char_match(str, snow, pattern, pnow) == true) &#123; return match_core(str, snow+1, pattern, pnow+1); &#125; // 5. p未结束，p2 != *，单独match失败 return false;&#125; 表示数值的字符串-20 数值：+100、5e2、-123、3.1416、-1E-16 、1.5e2 非数值：12e、1a3.14、1.2.3、+-5、12e+5.4 判断字符串是否是一个正确的数值。A[.[B]][e|EC]或者.B[e|EC] （符号）、整数、小数点、（整数）、e、整数 小数点前后，必须有一个整数 e前面是一个数，后面是一个整数 关键代码 12345678910111213141516171819202122232425bool Solution::isNumeric(const string &amp;str) &#123; int start = 0; bool pre_int = scan_integer(str, start); bool numeric = pre_int; // 有小数点 if (start &lt; str.size() &amp;&amp; str[start] == '.') &#123; start++; bool dot_int = scan_unsigned_integer(str, start); // 小数点前面或者后面至少有一个整数 numeric = pre_int || dot_int; &#125; // 有指数符号 if (start &lt; str.size() &amp;&amp; (str[start] == 'E' ||str[start] == 'e')) &#123; start++; bool e_int = scan_integer(str, start); // e前面是一个数值，e后面是一个整数 numeric = numeric &amp;&amp; e_int; &#125; // 没有走完，还剩余字符 if (start &lt; str.size()) &#123; return false; &#125; return numeric;&#125;","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"}]},{"title":"排序算法总结","date":"2017-12-26T08:06:53.000Z","path":"2017/12/26/sort-algorithms/","text":"所有排序算法的介绍、实现和比较，值得一看 插入排序 直接插入 前面的已经有序，把后面的插入到前面有序的元素中。 步骤 找到待插入位置 给插入元素腾出空间，边比较边移动 如对4 5 1 2 6 3 12345678910# 5插入44 5 1 2 6 3# 1插入 4 51 4 5 2 6 3# 2插入 1 4 51 2 4 5 6 3# 6插入 1 2 4 51 2 4 5 6 3# 3插入到前面1 2 3 4 5 6 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 直接插入排序，先比较找位置，再移动 **/ void insert_sort(int a[], int n) &#123; for (int i = 1; i &lt; n; i++) &#123; // 最大，追加在末尾即可 if (a[i] &gt; a[i-1]) &#123; continue; &#125; // 找到待插入的位置 int k = -1; for (int j = 0; j &lt; i; j++) &#123; if (a[i] &lt; a[j]) &#123; k = j; break; &#125; &#125; int t = a[i]; // 先挪动元素，向后移动 for (int j = i; j &gt; k; j--) &#123; a[j] = a[j-1]; &#125; a[k] = t; &#125;&#125;/** * 直接插入排序，边比较边移动 **/void insert_sort2(int a[], int n) &#123; for (int i = 1; i &lt; n; i++) &#123; if (a[i] &lt; a[i-1]) &#123; int t = a[i]; int j = i - 1; while (a[j] &gt; t &amp;&amp; j &gt;= 0) &#123; a[j+1] = a[j]; j--; &#125; a[j+1] = t; &#125; &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 适用性 直接插入 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 顺序存储和链式存储的线性表 折半插入 先折半查找出位置，再统一的移动。 若m为1个数字，则\\(a[i]&gt;a[m]\\)，该插入位置为\\(l=m+1\\)。 仅仅减少了元素的比较次数，元素的移动次数依然没有改变。时间复杂度仍然为\\(\\mathrm{O}(n)\\)。 关键代码 123456789101112131415161718192021222324252627282930/** * 插入排序，折半查找出位置，再统一移动 **/ void insert_sort_bisearch(int a[], int n) &#123; for (int i = 1; i &lt; n; i++) &#123; if (a[i] &gt; a[i-1]) &#123; continue; &#125; // 折半查找，a[i]要插入的位置为l int l = 0, r = i - 1; while (l &lt;= r) &#123; int m = (l + r) / 2; if (a[i] &gt; a[m]) &#123; // 查找右边 l = m + 1; &#125; else if (a[i] &lt; a[m])&#123; // 查找左边 r = m - 1; &#125; else &#123; l = m + 1; break; &#125; &#125; int t = a[i]; for (int j = i; j &gt; l; j--) &#123; a[j] = a[j-1]; &#125; a[l] = t; &#125;&#125; 希尔排序 希尔排序又称为缩小增量排序， 把整个列表，分成多个\\(\\rm{L[i, i+d,i+2d,\\cdots, i+kd]}\\)这样的列表，每个进行直接插入排序。每一轮不断缩小d的值，直到全部有序。 实际例子 4 5 1 2 6 3 1234567891011121314151617# d = 34 _ _ 2 _ __ 5 _ _ 6 __ _ 1 _ _ 3# 化为3个列表，分别进行直接插入排序，得到2 5 1 4 6 3# d = 22 _ 1 _ 6 __ 5 _ 4 _ 3# 排序，得到1 3 2 4 6 5# d = 11 3 2 4 6 5# 直接插入排序，得到1 2 3 4 5 6 关键代码 12345678910111213141516171819202122232425262728293031323334/* * 希尔排序，按照步长，去划分为多个组。对这些组分别进行插入排序 */void shell_sort(vector&lt;int&gt;&amp; a) &#123; // 步长gap==组的个数 for (int gap = a.size() / 2; gap &gt; 0; gap = gap / 2) &#123; // 对各个组进行排序 for (int i = 0; i &lt; gap; i++) &#123; group_sort(a, i, gap); &#125; &#125;&#125;/* * 对希尔排序中的单个组进行排序，直接插入 * Args: * a -- 数组 * start -- 该组的起始地址 * gap -- 组的步长，也是组的个数 */void group_sort(vector&lt;int&gt; &amp;a, int start, int gap) &#123; for (int i = start + gap; i &lt; a.size(); i += gap) &#123; if (a[i] &lt; a[i - gap]) &#123; int t = a[i]; int j = i - gap; // 从后向前比较，边比较，边移动 while (a[j] &gt; t &amp;&amp; j &gt;= start) &#123; a[j + gap] = a[j]; j -= gap; &#125; a[j + gap] = t; &#125; &#125;&#125; 总结分析 最好的增量\\(d_1 = \\frac{n}{2}, d_{i+1} = \\lfloor \\frac{d_i}{2}\\rfloor\\) 名称 时间 最好 最差 空间 稳定 适用性 希尔排序 \\({O}(1)\\) 不稳定 线性存储的线性表 交换排序 冒泡排序 执行n-1轮，每一轮把\\(a[0, \\ldots, i]\\)的最大的向下沉。 123456789101112131415162 4 3 1 6 5# 第一趟2 4 _ _ _ _2 3 4 _ _ _ # 4-3 to 3-42 3 1 4 _ _ # 4-1 to 1-4 2 3 1 4 6 _ 2 3 1 4 5 6 # 6-5 to 5-6# 第二趟2 3 _ _ _ 6 2 1 3 _ _ 6 # 3-1 to 1-32 1 3 4 _ 6 2 1 3 4 5 6# 第三趟1 2 _ _ 5 6 # 2-1 to 1-21 2 3 _ 5 61 2 3 4 5 6 关键代码 12345678910111213141516171819202122232425262728293031void bubble_sort1(int* a, int n) &#123; for (int i = n-1; i &gt; 0; i--) &#123; // 每一轮把a[0,...,i]中最大的向下沉 for (int j = 0; j &lt; i; j++) &#123; if (a[j] &gt; a[j+1]) &#123; int t = a[j]; a[j] = a[j+1]; a[j+1] = t; &#125; &#125; &#125; &#125;// 若当前轮，已经没有发生交换，说明已经全部有序void bubble_sort2(int* a, int n) &#123; int swapped = 0; for (int i = n - 1; i &gt; 0; i--) &#123; swapped = 0; for (int j = 0; j &lt; i; j++) &#123; if (a[j] &gt; a[j+1]) &#123; int t = a[j]; a[j] = a[j+1]; a[j+1] = t; swapped = 1; &#125; &#125; if (swapped = 0) &#123; break; &#125; &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 冒泡排序 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 每一轮都有一个元素到最终位置上 快速排序 思想 把一个序列，选一个数（第一个数），进行划分。左边小于x，中间x，右边大于x。再依次递归划分左右两边。 关键代码 划分 123456789101112131415161718192021222324252627282930313233343536/** * 划分，左边小于x，中间x，右边大于x * Args: * a: * l: * r: * Returns: * i: x=a[l]的最终所在位置 **/int partition(int* a, int l, int r) &#123; int x = a[l]; int i = l; int j = r; // 划分 while (i &lt; j) &#123; // 从右到左，找到第一个小于x的a[j]，放到a[i]上 while (a[j] &gt;= x &amp;&amp; j &gt; i) &#123; j--; &#125; // 把a[j]放到左边i上 if (j &gt; i) &#123; a[i++] = a[j]; &#125; // 从左到右，找到一个大于x的[i] while (a[i] &lt;= x &amp;&amp; i &lt; j) &#123; i++; &#125; // 把a[i]放到右边j上 if (i &lt; j) &#123; a[j--] = a[i]; &#125; &#125; // x放在中间 a[i] = x; return i;&#125; 递归快排 1234567891011void quick_sort(int* a, int l, int r) &#123; // 1. 递归终止 if (l &gt;= r) &#123; return; &#125; // 2. 划分，左边小于x，中间x，右边大于x int k = partition(a, l, r); // 3. 递归快排左右两边 quick_sort(a, l, k - 1); quick_sort(a, k + 1, r);&#125; 非递归快排 123456789101112131415161718192021222324252627void quick_sort_stack(int* a, int l, int r) &#123; int i, j; stack&lt;int&gt; st; // 注意进栈和出栈的顺序 st.push(r); st.push(l); while (st.empty() == false) &#123; // 每次出栈一组 i = st.top(); st.pop(); j = st.top(); st.pop(); if (i &lt; j) &#123; int k = partition(a, i, j); // 左边的 if (k &gt; i) &#123; st.push(k - 1); st.push(i); &#125; // 右边的 if (k &lt; j) &#123; st.push(j); st.push(k + 1); &#125; &#125; &#125;&#125; 总结分析 第i趟完成后，最少有i个元素在最终位置。 名称 时间 最好 最差 空间 稳定 备注 快排 \\({O}(n\\log n)\\) \\(O(n\\log n)\\) \\({O}(n^2)\\) \\({O}(\\log_2n)\\)栈的深度 不稳定 基本有序或者逆序，效果最差 选择排序 简单选择 前面已经有序，从后面选择最小的与前面末尾最大的进行交换。 12345678910114 5 1 2 6 3# 选择1与4交换1 5 4 2 6 3# 选择2与5交换1 2 4 5 6 3# 选择3与4交换1 2 3 5 6 4# 选择4与5交换1 2 3 4 6 5# 选择5与6交换1 2 3 4 5 6 总结分析 名称 时间 最好 最差 空间 稳定 备注 简单选择 \\({O}(n^2)\\) \\({O}(n^2)\\) \\({O}(n^2)\\) \\(O(1)\\) 不稳定 关键代码 1234567891011/** * 简单选择排序，选择后面最小的来与当前有序的最后一个（最大的）交换 **/void select_sort(int *a, int n) &#123; for (int i = 1; i &lt; n; i++) &#123; int k = min_index(a, i, n-1); if (a[i-1] &gt; a[k]) &#123; swap(a, i-1, k); &#125; &#125;&#125; 堆 堆 堆是一颗完全二叉树。 有n个节点，堆的索引从0开始，节点i的 左孩子：\\(2\\cdot i+1\\) 右孩子：\\(2\\cdot i +2\\) 父亲：\\(\\lceil \\frac{i-1}{2}\\rceil\\) 最后一个节点是：下标为\\(\\lfloor n/2\\rfloor-1\\) 的节点的孩子，即第\\(\\lfloor n/2\\rfloor\\)个节点的孩子。 大根堆 大根堆：最大元素在根节点。小根堆：最小元素在根节点。 90 70 80 60 10 40 50 30 20 是一个大根堆，10 20 70 30 50 90 80 60 40 是一个小根堆，如下 堆添加 先把元素添加到末尾，再依次向上面调整，若大于父节点，就和父节点交换。 堆删除 删除堆顶元素：把末尾元素，放到堆顶，再依次向下调整，每次选择较大的子节点进行交换。 删除堆中元素：把末尾元素，放入空白处，再调整堆即可。 堆排序 思路 初始化堆。把\\(a_1, \\cdots, a_n\\)构造成为最大堆 取出最大值。每次出堆根最大元素。出\\(a_1\\)，把\\(a_n\\)放到\\(a_1\\)上，再把\\(a_1, \\cdots. a_{n-1}\\)调整为最大堆。再出元素 重复2 建立堆 n个元素，最后一个父亲节点\\(\\lfloor n/2\\rfloor\\)， 下标为\\(k=\\lfloor n/2\\rfloor-1\\)。 对这些节点\\(a_k, \\cdots, a_0\\)， 依次调整它们的子树，使子树成堆。即，若根节点小于左右节点的较大值，则交换。 建立堆实例 对于数据{20,30,90,40,70,110,60,10,100,50,80}，建立为最大堆{110,100,90,40,80,20,60,10,30,50,70} 取出最大值 把根节点（最大值）和当前堆的末尾值进行交换，最大值放到最后。再对剩余的数据进行成堆，再依次取最大值交换。 每一次取出最大值重新恢复堆，要\\(O(\\log n)\\)，有n个数，一共是\\(O(n \\log n)\\)。 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 保证以start为根节点的子树是一个最大堆，末尾元素为end * * Args: * a: 存放堆的数组 * start: 根节点 * end: 子树的末尾元素 * Returns: * None **/ void max_heap_down(vector&lt;int&gt;&amp; a, int start, int end) &#123; // 当前节点 int c = start; // 左孩子 int l = 2 * c + 1; // 当前父亲节点 int t = a[c]; for (; l &lt;= end; c = l, l = 2*c + 1) &#123; // 选择较大的孩子与父亲交换 if (l + 1 &lt;= end &amp;&amp; a[l] &lt; a[l + 1]) &#123; // 有右孩子，并且右孩子比左孩子大，则选择右孩子 l++; &#125; if (t &gt;= a[l]) &#123; // 父亲大于孩子 break; &#125; else &#123; // 交换 a[c] = a[l]; a[l] = t; &#125; &#125;&#125;/** * 堆排序，升序 **/void heap_sort_asc(vector&lt;int&gt;&amp; a) &#123; int n = a.size(); // 初始化一个最大堆 for (int i = n / 2 - 1; i &gt;= 0; i--) &#123; max_heap_down(a, i, n - 1); &#125; // 依次取堆顶元素放到末尾 for (int i = n - 1; i &gt;= 0; i--) &#123; // max放到a[i] int t = a[i]; a[i] = a[0]; a[0] = t; // 保证a[0...i-1]依然是个最大堆 max_heap_down(a, 0, i-1); &#125; return;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 堆排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}( \\log n)\\) \\(O(n)\\) 不稳定 归并排序 从上往下-递归 思路 分解 -- 将区间一分为二，求分裂点\\(\\rm{mid} = \\frac{\\rm{start +end}}{2}\\) 递归求解，sort， sort -- 递归对两个无序子区间 \\(a_s,\\cdots , a_m\\)和\\(a_{m+1}, \\cdots, a_{e}\\)进行归并排序。终结条件是子区间长度为1 合并，merge -- 把两个有序的子区间 \\(a_s,\\cdots , a_m\\)和\\(a_{m+1}, \\cdots, a_{e}\\) 合并为一个完整的有序区间\\(a_s, \\cdots, a_e\\) 分解 分解&amp;递归&amp;合并 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 归并排序，从上到下 **/void merge_sort_up2down(vector&lt;int&gt; &amp;a, int start, int end) &#123; if (start &gt;= end) &#123; return; &#125; int mid = (start + end) / 2; // 递归排序a[start...mid] merge_sort_up2down(a, start, mid); // 递归排序a[mid+1...end] merge_sort_up2down(a, mid + 1, end); // 两个有序序列merge在一起 merge(a, start, mid, end);&#125;/** * 将a中前后两个有序序列合并在一起 **/ void merge(vector&lt;int&gt; &amp;a, int start, int mid, int end) &#123; // 把有序序列临时存放到t中 int * t = new int [end - start + 1]; int i = start; int j = mid + 1; int k = 0; // 依次合并 while (i &lt;= mid &amp;&amp; j &lt;= end) &#123; if (a[i] &lt; a[j]) &#123; t[k++] = a[i++]; &#125; else &#123; t[k++] = a[j++]; &#125; &#125; while (i &lt;= mid) &#123; t[k++] = a[i++]; &#125; while (j &lt;= end) &#123; t[k++] = a[j++]; &#125; // 把新的有序列表复制回a中 for (int i = 0; i &lt; k; i++) &#123; a[start + i] = t[i]; &#125; delete [] t;&#125; 从下往上-非递归 思想 把数组分成若干个长度为1的子数组，再两两合并；得到长度为2的数组，再两两合并；依次反复，直到形成一个数组。 关键代码 123456789101112131415161718192021222324252627282930/** * 归并排序，从下到上 **/void merge_sort_down2up(vector&lt;int&gt; &amp;a) &#123; if (a.size() &lt;= 0) return; for (int i = 1; i &lt; a.size(); i = i * 2) merge_groups(a, i);&#125;/* * 对a做若干次合并，分为若干个gap。对每相邻的两个gap进行合并排序 * Args: * a: 数组 * gap: 一个子数组的长度 */void merge_groups(vector&lt;int&gt; &amp;a, int gap) &#123; int twolen = 2 * gap; int i; for (i = 0; i + twolen - 1 &lt; a.size(); i += twolen) &#123; int start = i; int mid = i + gap - 1; int end = i + twolen - 1; merge(a, start, mid, end); &#125; // 最后还有一个gap if (i + gap - 1 &lt; a.size() - 1) &#123; merge(a, i, i + gap - 1, a.size() - 1); &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 归并排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}(n \\log n)\\) \\(O(n)\\)， merge占用 稳定 归并排序的形式是一颗二叉树，遍历的次数就是二叉树的深度\\(O(\\log n)\\)， 而一共n个数。 桶排序 桶排序很简单。数组a有n个数，最大值为max。则，建立一个长度为max的数组b，初始化为0。 遍历a，遇到\\(a_i = k\\)， 则\\(b_k += 1\\) 。即在对应的桶里计数加1。 基数排序 基数排序分为最高位优先和最低位优先。 基数排序是桶排序的扩展。把所有的数，统一位数。然后，按照每一位进行，从低位到高位跑排序。 关键是找到output和buckets的对应关系。每个bucket存储前面累积的元素的数量。 123456buckets[2] = 1;// 说明排序数是2的元素有1个buckets[3] = 4;// 说明排序数是3的元素有 4-1=3个buckets[4] = 7;// 说明排序数是4的元素有 7-4=3个 后面在进行根据排序数找到当前数的最终所在位置的时候，就会利用这个关系。 1234567// 比如排序数是3的数字，会出现3个// 则id就为 buckets[3]-1// 每出现一个，则buckets[3]--// 举个例子// 初始buckets[2]=1，则这1个数字的最终序号是：0// 初始buckets[3]=4，则这3个数字的最终序号是：3,2,1 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/* * 基数排序 */void radix_sort(vector&lt;int&gt; &amp;a) &#123; if (a.size() &lt;= 1) &#123; return; &#125; int max = *max_element(a.begin(), a.end()); // exp=1, 10, 100, 1000... for (int exp = 1; max / exp &gt; 0; exp *= 10) &#123; count_sort(a, exp); &#125;&#125;/* * 对数组按照某个位数进行排序 * Args: * a -- 数组 * exp -- 指数，1, 10, 100... 分别按照个位、十位、百位排序 * Returns: * None */void count_sort(vector&lt;int&gt;&amp; a, int exp) &#123; // 存储被排序数据的临时数组 int output [a.size()]; // 桶 数据的出现次数 int buckets[10] = &#123;0&#125;; for (int i = 0; i &lt; a.size(); i++) &#123; int t = (a[i] / exp) % 10; buckets[t]++; &#125; // 根据前面的出现次数，推算出当前数字在原数组中的index for (int i = 1; i &lt; 10; i++) buckets[i] += buckets[i - 1]; // 将数据存储到output中 for (int i = a.size() - 1; i &gt;= 0; i--) &#123; int j = (a[i] / exp) % 10; int k = buckets[j]; output[k - 1] = a[i]; buckets[j]--; &#125; // 赋值给a for (int i = 0; i &lt; a.size(); i++) &#123; a[i] = output[i]; &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 基数排序 \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\(O(r)\\)， r个队列 稳定 总结比较 总结 思想总结 名称 一句话描述 直接插入 前面有序，为新来的，在前面找到合适的位置，进行插入 折半插入 前面有序，为新来的，使用折半查找到插入位置，进行插入 希尔排序 gap个间隔为gap的子序列，每个进行直接插入排序；减小gap，依次排序，直至为1 冒泡排序 交换n-1趟，\\(a_{i-1}&gt;a_i\\)，则进行交换，每一趟都有个最大的沉到末尾 快排 第一个数x，先划分，左边小于x，中间x，右边大于x。再依次递归排序左右两边 简单选择 前面有序，从后面选择最小的与前面末尾的（最大的）进行交换 堆排序 初始化大根堆，堆顶和末尾元素交换，再调整使剩下的元素成堆， 重复 归并排序 分解为左右两个序列，对左右两个序列进行递归归并排序，再合并。即sort,sort,merge 基数排序 位数一样，从低位到高位，分别按照每一位进行排序 时空复杂度总结 名称 时间 最好 最差 空间 稳定 直插 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 希尔 \\({O}(1)\\) 不稳定 冒泡 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 快排 \\({O}(n\\log n)\\) \\(O(n\\log n)\\) \\({O}(n^2)\\) \\({O}(\\log_2n)\\)，栈的深度 不稳定 简选 \\({O}(n^2)\\) \\({O}(n^2)\\) \\({O}(n^2)\\) \\(O(1)\\) 不稳定 堆排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}( \\log n)\\) \\(O(n)\\) 不稳定 归并排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}(n \\log n)\\) \\(O(n)\\)， merge占用 稳定 基数排序 \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\(O(r)\\)， r个队列 稳定 稳定的：插、冒、归、基 较快的：快、些、归、堆，插得好、冒得好 比较次数与初始状态无关：选择、归并 排序趟数与初始状态无关：选择、插入、基数 冒、选、堆：每趟都有1个在最终的位置，最大or最小 直接插入：第\\(i\\)趟，前\\(i+1\\)个有序，但不是最终序列 快排： \\(i\\)趟后， 至少有\\(i\\)个在最终位置 时间复杂度：快些归堆\\({O(n\\log n)}\\)，基数排序\\(O(d(n+r))\\) ， 其余\\(O(n^2)\\) ， 空间复杂度：归\\(O(n)\\)， 快\\(O(\\log n)\\)， 基\\(O(r)\\)， 其余\\(O(1)\\) 算法选择 条件 可选算法 n较小， \\(n \\le 50\\) 直接插入、简单选择 基本有序 直接插入、冒泡 n较大，要\\(O(n \\log n)\\) 快排、堆排（不稳定），归并排序（稳定） n较大，要快，要稳定 归并排序，与直插结合的改进的归并排序 n很大，位数很少，可以分解 基数排序 记录本身信息量太大 为了避免移动，可以使用链表作为存储结构","tags":[{"name":"排序","slug":"排序","permalink":"http://plmsmile.github.io/tags/排序/"},{"name":"插入","slug":"插入","permalink":"http://plmsmile.github.io/tags/插入/"},{"name":"快排，快速排序","slug":"快排，快速排序","permalink":"http://plmsmile.github.io/tags/快排，快速排序/"},{"name":"堆排序","slug":"堆排序","permalink":"http://plmsmile.github.io/tags/堆排序/"},{"name":"归并排序","slug":"归并排序","permalink":"http://plmsmile.github.io/tags/归并排序/"},{"name":"基数排序","slug":"基数排序","permalink":"http://plmsmile.github.io/tags/基数排序/"},{"name":"冒泡排序","slug":"冒泡排序","permalink":"http://plmsmile.github.io/tags/冒泡排序/"}]},{"title":"C++类对象和指针的区别","date":"2017-12-25T13:17:36.000Z","path":"2017/12/25/15-cpp-pointer-object-reference/","text":"类对象和指针的区别 类对象和指针 代码 类 1234567class Test&#123; public: int a; Test()&#123; a = 1; &#125;&#125;; 类指针 12345678910111213141516171819202122void test1() &#123; // 1. 两个类的指针，使用了new Test* t1 = new Test(); t1-&gt;a = 10; Test* t2 = new Test(); t2-&gt;a = 5; // 2. 两个指针的所指向的地址不一样 cout &lt;&lt; \"&amp;t1:\" &lt;&lt; t1 &lt;&lt; \" a = \" &lt;&lt; t1-&gt;a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; t2 &lt;&lt; \" a = \" &lt;&lt; t2-&gt;a &lt;&lt;endl; // 3. 指针t1的值赋值给了t2，两个指针指向的地址相同，都是t1的地址 t2 = t1; cout &lt;&lt; \"&amp;t1:\" &lt;&lt; t1 &lt;&lt; \" a = \" &lt;&lt; t1-&gt;a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; t2 &lt;&lt; \" a = \" &lt;&lt; t2-&gt;a &lt;&lt;endl; t1-&gt;a = 111; t2-&gt;a = 222; // 4. 修改了同样的地方，输出为222 cout &lt;&lt; \"&amp;t1:\" &lt;&lt; t1 &lt;&lt; \" a = \" &lt;&lt; t1-&gt;a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; t2 &lt;&lt; \" a = \" &lt;&lt; t2-&gt;a &lt;&lt;endl;&#125; 类对象 1234567891011121314151617181920212223void test2() &#123; // 1. 两个类对象 Test t1; t1.a = 10; Test t2; t2.a = 5; // 2. 对象的内容不一样 cout &lt;&lt; \"&amp;t1:\" &lt;&lt; &amp;t1 &lt;&lt; \" a = \" &lt;&lt; t1.a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; &amp;t2 &lt;&lt; \" a = \" &lt;&lt; t2.a &lt;&lt;endl; // 3. 把t1对象的内容赋值给t2。t2的内容和t1相同 t2 = t1; cout &lt;&lt; \"&amp;t1:\" &lt;&lt; &amp;t1 &lt;&lt; \" a = \" &lt;&lt; t1.a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; &amp;t2 &lt;&lt; \" a = \" &lt;&lt; t2.a &lt;&lt;endl; // 4. 再分别修改两个不同的对象，输出分别为111， 222 t1.a = 111; t2.a = 222; cout &lt;&lt; \"&amp;t1:\" &lt;&lt; &amp;t1 &lt;&lt; \" a = \" &lt;&lt; t1.a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; &amp;t2 &lt;&lt; \" a = \" &lt;&lt; t2.a &lt;&lt;endl; &#125; 不同点 类指针 类对象 内存分配 定义类指针，不分配内存 定义类对象，分配内存 关系 内存地址，指向类对象。利用构造函数分配一块内存 访问 间接访问 直接访问 多态 实现多态，父类指针调用子类对象 不能实现多态，声明即调用构造函数，已分配内存 堆栈 内存堆，永久变量，手动释放，new-delete搭配 内存栈，局部临时变量 new的对象在堆中 在栈中 使用 new： S* s = new S() , s-&gt;name 声明即可： S s;，s.name 生命期 需要delete 类的析构函数来释放空间 传参 指针4个字节 对象，参数传递资源占用太大 虚函数f 调用分配给它空间时那种类的func 调用自己的func 其它 父类指针指向子类对象 推荐使用const &amp;引用，安全系数高。","tags":[{"name":"cpp","slug":"cpp","permalink":"http://plmsmile.github.io/tags/cpp/"},{"name":"指针，对象，引用","slug":"指针，对象，引用","permalink":"http://plmsmile.github.io/tags/指针，对象，引用/"}]},{"title":"cs224n作业一","date":"2017-12-17T04:47:06.000Z","path":"2017/12/17/cs224n-assignment-1/","text":"cs224n的第一个作业，包括softmax、神经网络基础和词向量 Softmax Softmax常数不变性 \\[ \\rm{softmax}(\\mathbf{x})_i = \\frac{e^{\\mathbf x_i}}{\\sum_{j}e^{\\mathbf{x}_j}} \\] 一般在计算softmax的时候，避免太大的数，要加一个常数。 一般是减去最大的数。 \\[ \\rm{softmax}(x) = \\rm{softmax}(x+c) \\] 关键代码 12345678def softmax(x): exp_func = lambda x: np.exp(x - np.max(x)) sum_func = lambda x: 1.0 / np.sum(x) x = np.apply_along_axis(exp_func, -1, x) denom = np.apply_along_axis(sum_func, -1, x) denom = denom[..., np.newaxis] x = x * denom return x 神经网络基础 Sigmoid实现 我的sigmoid笔记 \\[ \\begin{align} &amp; \\sigma (z) = \\frac {1} {1 + \\exp(-z)}, \\; \\sigma(z) \\in (0,1) \\\\ \\\\ &amp; \\sigma^\\prime (z) = \\sigma(z) (1 - \\sigma(z)) \\\\ \\end{align} \\] 关键代码 12345678910def sigmoid(x): s = 1.0 / (1 + np.exp(-x)) return sdef sigmoid_grad(s): \"\"\" 对sigmoid的函数值，求梯度 \"\"\" ds = s * (1 - s) return ds Softmax求梯度 交叉熵和softmax如下，记softmax的输入为\\(\\theta\\) ，\\(y\\)是真实one-hot向量。 \\[ \\begin{align} &amp; \\rm{CE}(y, \\hat y) = - \\sum_{i} y_i \\times \\log (\\hat y_i) \\\\ \\\\ &amp; \\hat y = \\rm{softmax} (\\theta)\\\\ \\end{align} \\] softmax求导 引入记号： \\[ \\begin{align} &amp; f_i = e^{\\theta_i} &amp; \\text{分子} \\\\ &amp; g_i = \\sum_{k=1}^{K}e^{\\theta_k} &amp; \\text{分母，与i无关} \\\\ &amp; \\hat y_i = S_i = \\frac{f_i}{g_i} &amp; \\text{softmax}\\\\ \\end{align} \\] 则有\\(S_i​\\)对其中的一个数据\\(\\theta_j​\\) 求梯度： \\[ \\frac{\\partial S_i}{\\partial \\theta_j} = \\frac{f_i^{\\prime} g_i - f_i g_i^{\\prime}}{g_i^2} \\] 其中两个导数 \\[ f^{\\prime}_i(\\theta_j) = \\begin{cases} &amp; e^{\\theta_j}, &amp; i = j\\\\ &amp; 0, &amp; i \\ne j \\\\ \\end{cases} \\] \\[ g^{\\prime}_i(\\theta_j) = e^{\\theta_j} \\] \\(i=j\\)时 \\[ \\begin{align} \\frac{\\partial S_i}{\\partial \\theta_j} &amp; = \\frac{e^{\\theta_j} \\cdot \\sum_{k}e^{\\theta_k}- e^{\\theta_i} \\cdot e^{\\theta_j}}{\\left( \\sum_ke^{\\theta_k}\\right)^2} \\\\ \\\\ &amp; = \\frac{e^{\\theta_j}}{\\sum_ke^{\\theta_k}} \\cdot \\left( 1 - \\frac{e^{\\theta_j}}{\\sum_k e^{\\theta_k}} \\right) \\\\ \\\\ &amp; = S_i \\cdot (1 - S_i) \\end{align} \\] \\(i \\ne j\\)时 \\[ \\begin{align} \\frac{\\partial S_i}{\\partial \\theta_j} &amp; = \\frac{ - e^{\\theta_i} \\cdot e^{\\theta_j}}{\\left( \\sum_ke^{\\theta_k}\\right)^2} = - S_i \\cdot S_j \\end{align} \\] 交叉熵求梯度 \\[ \\begin{align} &amp; \\rm{CE}(y, \\hat y) = - \\sum_{i} y_i \\times \\log (\\hat y_i) \\\\ \\\\ &amp; \\hat y = \\rm{S} (\\theta)\\\\ \\end{align} \\] 只关注有关系的部分，带入\\(y_i =1\\) ： \\[ \\begin{align} \\frac{\\partial CE}{\\partial \\theta_i} &amp; = -\\frac{\\partial \\log \\hat y_i}{\\partial \\theta_i} = - \\frac{1}{\\hat y_i} \\cdot \\frac{\\partial \\hat y_i}{\\partial \\theta_i} \\\\ \\\\ &amp; = - \\frac{1}{S_i} \\cdot \\frac{\\partial S_i}{\\partial \\theta_i} = S_i - 1 \\\\ \\\\ &amp; = \\hat y_i - y_i \\end{align} \\] 不带入求导 \\[ \\begin{align} \\frac{\\partial CE}{\\partial \\theta_i} &amp; = - \\sum_{k}y_k \\times \\frac{\\partial \\log S_k}{\\partial \\theta_i} \\\\ &amp; = - \\sum_{k}y_k \\times \\frac{1}{S_k}\\times \\frac{\\partial S_k}{\\partial \\theta_i} \\\\ &amp; = - y_i (1 - S_i) - \\sum_{k \\ne i} y_k \\cdot \\frac{1}{S_k} \\cdot (- S_i \\cdot S_k) \\\\ &amp; = - y_i (1 - S_i) + \\sum_{k \\ne i} y_k \\cdot S_i \\\\ &amp; = S_i - y_i \\end{align} \\] 所以，交叉熵的导数是 \\[ \\frac{\\partial CE}{\\partial \\theta_i} = \\hat y_i - y_i, \\quad \\quad \\frac{\\partial CE(y, \\hat y)}{\\partial \\theta} = \\hat y - y \\] 即 \\[ \\frac{\\partial CE(y, \\hat y)}{\\partial \\theta_i} = \\begin{cases} &amp; \\hat y_i - 1, &amp; \\text{i是label} \\\\ &amp;\\hat y_i, &amp; \\text{其它}\\\\ \\end{cases} \\] 简单网络 前向计算 \\[ \\begin{align} &amp; z_1 = xW_1 + b_1 \\\\ \\\\ &amp; h = \\rm{sigmoid}(z1) \\\\ \\\\ &amp; z_2 = hW_2 + b_2 \\\\ \\\\ &amp; \\hat y = \\rm{softmax}(z_2) \\end{align} \\] 关键代码： 123def forward_backward_prop(data, labels, params, dimensions): h = sigmoid(np.dot(data, W1) + b1) yhat = softmax(np.dot(h, W2) + b2) loss函数 \\[ J = \\rm{CE}(y, \\hat y) \\] 关键代码： 123def forward_backward_prop(data, labels, params, dimensions): # yhat[labels==1]实际上是boolean索引，见我的numpy_api.ipynb cost = np.sum(-np.log(yhat[labels == 1])) / data.shape[0] 反向传播 \\[ \\begin {align} &amp; \\delta_2 = \\frac{\\partial J}{\\partial z_2} = \\hat y - y \\\\ \\\\ &amp; \\frac{\\partial J}{\\partial h} = \\delta_2 \\cdot \\frac{\\partial z_2}{\\partial h} = \\delta_2 W_2^T \\\\ \\\\ &amp; \\delta_1 = \\frac{\\partial J}{\\partial z_1} = \\frac{\\partial J}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z_1} = \\delta_2 W_2^T \\circ \\sigma^{\\prime}(z_1) \\\\ \\\\ &amp; \\frac{\\partial J}{\\partial x} = \\delta_1 W_1^T \\end{align} \\] 一共有\\((d_x + 1) \\cdot d_h + (d_h +1) \\cdot d_y\\) 个参数。 关键代码： 123456789101112131415def forward_backward_prop(data, labels, params, dimensions): # 前面推导的softmax梯度公式 gradyhat = (yhat - labels) / data.shape[0] # 链式法则 gradW2 = np.dot(h.T, gradyhat) # 本地导数是1，把第1维的所有加起来 gradb2 = np.sum(gradyhat, axis=0, keepdims=True) gradh = np.dot(gradyhat, W2.T) gradz1 = gradh * sigmoid_grad(h) gradW1 = np.dot(data.T, gradz1) gradb1 = np.sum(gradz1, axis=0, keepdims=True) grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten())) return cost, grad 梯度检查 我的梯度检查 123456789101112131415161718192021222324252627def gradcheck_naive(f, x): fx, grad = f(x) # Evaluate function value at original point h = 1e-4 # Do not change this! # Iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: ix = it.multi_index # 关键代码 x[ix] += h random.setstate(rndstate) new_f1 = f(x)[0] x[ix] -= 2 * h random.setstate(rndstate) new_f2 = f(x)[0] x[ix] += h numgrad = (new_f1 - new_f2) / (2 * h) # Compare gradients reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix])) if reldiff &gt; 1e-5: print (\"Gradient check failed.\") print (\"First gradient error found at index %s\" % str(ix)) print (\"Your gradient: %f \\t Numerical gradient: %f\" % ( grad[ix], numgrad)) return it.iternext() # Step to next dimension Word2Vec 我的word2vec笔记 词向量的梯度 符号定义 \\(v_c\\) 中心词向量，输入词向量，\\(V\\)， \\(\\mathbb{R}^{W\\times d}\\) \\(u_o\\) 上下文词向量，输出词向量，\\(U=[u_1, u_2, \\cdots, u_w]\\) , \\(\\mathbb{R}^{d\\times W}\\) 前向 预测o是c的上下文概率，o为正确单词 \\[ \\hat y_o = p(o \\mid c) = \\rm{softmax}(o) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w} \\exp(u_w^T v_c)} \\] 得分向量： \\[ z=U^T \\cdot v_c, \\quad [W,d] \\times[ d] \\in ,\\mathbb{R}^{W } \\] loss及梯度 \\[ J_{\\rm{softmax-CE}}(v_c, o, U) = CE(y, \\hat y), \\quad \\text{其中} \\; \\frac{\\partial CE(y, \\hat y)}{\\partial \\theta} = \\hat y - y \\] 梯度 中文 计算 维数 \\(\\frac{\\partial J}{\\partial z}\\) softmax \\(\\hat y - y\\) \\(W\\) \\(\\frac{\\partial J}{\\partial v_c}\\) 中心词 \\(\\frac{\\partial J}{\\partial z} \\cdot \\frac{\\partial z}{\\partial v_c} = (\\hat y - y) \\cdot U^T\\) \\(d\\) \\(\\frac{\\partial J}{\\partial U}\\) 上下文 \\(\\frac{\\partial J}{\\partial z} \\cdot \\frac{\\partial z}{\\partial U^T}= (\\hat y - y) \\cdot v_c\\) \\(d \\times W\\) 关键代码 123456789101112131415161718192021222324def softmaxCostAndGradient(predicted, target, outputVectors, dataset): \"\"\" Softmax cost function for word2vec models Args: predicted: 中心词vc target: 上下文uo, index outputVectors: 输出，上下文矩阵U，W*d，未转置 dataset: Returns: cost: 交叉熵loss gradv: 一维向量 gradU: W*d \"\"\" vhat = predicted z = np.dot(outputVectors,vhat) preds = softmax(z) # Calculate the cost: cost = -np.log(preds[target]) # Gradients gradz = preds.copy() gradz[target] -= 1.0 gradU = np.outer(z, vhat) gradv = np.dot(outputVectors.T, z) ### END YOUR CODE return cost, gradv, gradU","tags":[{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"},{"name":"cs224n","slug":"cs224n","permalink":"http://plmsmile.github.io/tags/cs224n/"},{"name":"assignment","slug":"assignment","permalink":"http://plmsmile.github.io/tags/assignment/"},{"name":"cbow","slug":"cbow","permalink":"http://plmsmile.github.io/tags/cbow/"},{"name":"skip-gram","slug":"skip-gram","permalink":"http://plmsmile.github.io/tags/skip-gram/"}]},{"title":"词性标注和句法依存的表示符号","date":"2017-12-03T07:31:23.000Z","path":"2017/12/03/11-nlp-labels/","text":"词性标注和句法依存的符号 词性标注 IBM英语和中文标注集 书上的 斯坦福Stanford coreNLP 中 宾州树库 中的汉语词性标注规范， 如下 词性标记 英文名称 中文名称 示例 AD adverbs 副词 还 AS aspect marker 体表词 了，着，过，的（我是去年来过的） BA in ba-const 把，将 把，将 CC coordinating conjunction 并列连词 和，与，或，或者 CD cardinal conj 数词，基数词 一百 CS subordinating conj 从数连词 若，如果，如 DEC for relative-clause etc 标句词，关系从句的 我买的书 DEG associative 所有格、连接作用的 我的书 DT determiner 限定词 这 ETC tag for words in coordination phrase 等，等等 科技文教等领域，等，等等 IJ Interjection 感叹词 啊 JJ noun-modifier other than nouns 其他名词修饰语 共同的/DEG目的/NN他/PN是/VC男的/DEG LB in long bei-construction 长被 被他打了 LC localizer 方位词 桌子上 M measure word（including classifiers） 量词 一块糖 MSP some particles 其他结构助词 他/PN 所 需要/VV 的/DEC 所，而，以 NN common nouns 普通名词 桌子 NR proper nouns 专有名词 天安门 NT temporal nouns 时间名词 清朝，一月 OD ordinal numbers 序数词 第一 ON onomatopoeia 拟声词 哗啦啦 P prepositions 介词 在 PN pronouns 代词 你，我，他 PU punctuations 标点 ， 。 SB In long bei-consturction 短被 他/PN 被/SB 训了/AS SP Sentence-final particle 句末助词 你好吧、SP吧 呢 啊 吗 VA Predicative adjective 谓词形容词 太阳 红彤彤/VA 雪白 丰富 VC Copula 系动词 是 为 非 VE as the main verb “有”作为主要动词 有，无 VV verbs 普通动词 喜欢，走 自己总结的 标记 英文 中文 NP noun phrase 名词短语 PP prepositional phrase 介词短语 VP verb phrase 动词短语 NNS 名词（复数） NNP 专有名词（单数） NNPS 专有名词（复数） PRP 人称代词 JJ 形容词 JJS 形容词（最高级） JJR 形容词（比较级） MD 情态动词 VB 动词 VBP 动词，现在时，非第三人称单数 VBZ 动词，现在时，第三人称单数 VBG 动词，动名词，现在分词 英语标记 复杂版 词性标记 描述 UNKNOW 未知词 DT 限定词 QT 量词 CD 基数 NN 名词，单数 NNS 名词，复数 NNP 专有名词，单数 NNPS 专有名词，复数 EX 存在性的There PRP 人称代词，PP PRP$ 物主代词，PP$ POS 所有格结束词 RB 副词 RBS 副词，最高级 RBR 副词，比较级 句法依存 中心语为谓语 符号 意义 英语 备注 subj 主语 subject nsubj 名词性主语 nominal subject 同步、建设 top 主题 topic 是，建筑 npsubj 被动型主语 nominal passive subject 被句子 中的主语 csubj 从句主语 clausal subject 中文里无 xsubj x主语 一般一个主语下含多个从句 中心语为谓语或介词 符号 意义 英语 备注 obj 宾语 object dobj 直接宾语 颁布，文件 iobj 间接宾语 indirect object 基本不存在 range 间接宾语为数量词，格 成交，元 pobj 介词宾语 根据，要求 lobj 时间介词 来，今年 中心语为谓词 符号 意义 英语 备注 comp 补语 ccomp 从句补语 一般由两个动词组成， xcomp x从句补语 xclausal complement 不存在 acomp 形容词补语 adjectival complement tcomp 时间补语 temporal complement 遇到，以前 lccomp 位置补语 localizer complement 占，以上 rscomp 结果补语 resultative complement 中心语为名词 符号 意义 英语 备注 mod 修饰语 modifier pass 被动修饰 passive tmod 时间修饰 temporal modifier remod 关系从句修饰 relative clause modifier 问题，遇到 numod 数量修饰 numeric modifier 规定，若干 ornmod 序数修饰 numeric modifier clf 类别修饰 classifier modifier 文件，件 nmod 符合名词修饰 noun compound modifier 浦东，上海 amod 形容词修饰 adjective modifier 情况，新 advmod 副词修饰 adverbial modifier 做到，基本 vmod 动词修饰 verb modifier, participle modifier prnmod 插入词修饰 parenthetical modifier neg 不定修饰 negative modifier 遇到，不 det 限定词修饰 determiner modifier 活动，这些 possm 所属标记 possessive maker NP poss 所属修饰 possessive modifier NP dvpm DVP标记 DVP maker DVP(简单，的) dvpmod DVP修饰 DVP modifier DVP(采取，简单) assm 关联标记 associative marker DNP(开发，的) assmod 关联修饰 associative modifier NP|QP (教训，特区) prep 介词修饰 prepositional modifier NP|VP|IP (采取，对) clmod 从句修饰 clause modifier 因为，开始 plmod 介词性地点修饰 prepositional localizer modifier 在，上 asp 时态修饰 aspect marker 做到，了 partmod 分词修饰 participial modifier 中文不存在 etc 等关系 办法，等 中心语为实词 符号 意义 英语 备注 conj 联合 conjunct cop 系动双指助动词 copula cc 连接 coordination 指中心词与连词 其他 符号 意义 英语 备注 attr 属性关系 是，过程 cordmod 并列联合动词 coordinated verb compound 颁布，实行 mmod 清潭洞次 modal verb 得到，能 ba 把字关系 tclaus 时间从句 以后，积累 补语化成分 complementizer 一般指","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://plmsmile.github.io/tags/词性标注/"},{"name":"句法依存","slug":"句法依存","permalink":"http://plmsmile.github.io/tags/句法依存/"}]},{"title":"cs231n线性分类器和损失函数","date":"2017-11-27T03:24:40.000Z","path":"2017/11/27/cs231n-linear-notes/","text":"线性分类器，svm和交叉熵损失函数 线性分类器 得分函数 图片是三维数组，元素在0-255的整数。宽度-高度-3 ，3代表RGB颜色通道。 对图像进行零中心化。 输入图片\\(D=32 \\times 32 \\times 3 = 3072\\)个像素，压缩成1维向量，一共有\\(K=10\\)个类别。 \\[ f(x_i, W, b) = Wx_i + b \\] 分析 \\(W\\)的每一行都是一个类别的分类器，一共10个 得到分为每个类的score 改变\\(W, b\\) ，使得分类准确，正确的score高，错误的score低 为x添加一维，\\(x \\in \\mathbb R^{D+1}\\) ， 写为： \\[ f (x_i, W) = Wx_i \\] 理解 权重 输入4个像素，函数会根据权重对某些位置的某些颜色表现出喜好或者厌恶（正负）。 比如船类别，一般周围有很多蓝色的水，那么蓝色通道的权值就会很大（正）。绿色和红色就比较低（负）。那么如果出现绿色和红色的像素，就会降低是船的概率。 权重解释2 \\(W\\)的每一行对应于一个分类的模板(原型)， 用图像和模板去比较，计算得分（点积），找到最相似的模板。 线性函数 实际上，每个输入\\(x_i\\)就是3072维空间中的一个点，线性函数就是对这些点进行边界决策分类。 与线的距离越大，得分越高。 损失函数 最常用两个分类器：SVM和Softmax。分别使用SVM loss和交叉熵loss。 SVM 多类支持向量积损失（Multiclass Support Vector Machine）。正确分类比错误分类的得分高出一个边界值\\(\\Delta (一般= 1)\\) 。 记\\(x_i\\)分为第j个类别的得分为\\(s_j = f(x_i, W)_j\\) ，单个折叶损失 hinge loss， 也称作max margin loss ，如下： \\[ \\begin {align} loss &amp; = \\max(0, s_j - s_{y_i}+ \\Delta)= \\begin {cases} &amp; 0, &amp; s_{y_i} - s_j &gt; \\Delta \\\\ &amp; s_j - s_{y_i}+ \\Delta, &amp; 其它 \\\\ \\end{cases} \\\\ \\\\ &amp; =\\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\\\ \\end{align} \\] 如果错误分类进入红色区域，就开始计算loss。 第\\(i\\)个数据的loss就是把所有错误类别的loss加起来： \\[ L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i}+ \\Delta) \\] 正则化 plmsmile的L2正则化 使用L2正则化，对\\(W\\)的元素进行平方惩罚， 抑制大数值的权值。正则化loss（正则化惩罚）如下： \\[ R(W) = \\sum_k\\sum_l W_{kl}^2 \\] 最终loss就是数据损失+正则化损失， \\(\\lambda\\) 是正则化强度。 \\[ L = \\underbrace {\\frac{1}{N}\\sum_{i}L_i}_{\\text{data loss}} + \\underbrace {\\lambda R(W)}_{\\text{正则化 loss}} \\] 引入正则化以后，SVM就有了最大边界max margin 这一个良好性质。（不是很懂，后面再解决） Softmax plmsmile的交叉熵 每个类别的得分 \\[ s_j = f(x_i, W)_j = f_j \\] Softmax函数求得\\(x_i\\)分为第\\(j\\) 类的概率，这样会求得所有类别的概率，即预测的结果。 \\[ p(j \\mid x_i) = \\frac {\\exp(f_j)} {\\sum_{k} \\exp(f_k)} \\] 单个数据的loss，就是取其概率的负对数 ： \\[ L_i = - \\log p(y_i \\mid x_i) = - \\log \\left( \\frac{e^{f_{y_i}}}{\\sum e^{f_k}}\\right) = -f_{y_i} + \\log \\sum_{k}e^{f_k} \\] 从直观上看，最小化loss就是要最大化正确的概率（最小化正确分类的负对数概率），最小化其它分类的概率。 交叉熵的体现 程序会预测一个所有类别的概率分布\\(q = (p(1 \\mid x_i), \\cdots, p(K \\mid x_i))\\) 。真实label概率\\(p = (0, \\cdots, 1, 0,\\cdots, 0)\\) ，交叉熵： \\[ \\begin{align} H(p, q) &amp; = - \\sum_{x} p(x) \\log q(x) \\\\ &amp; = - (p(y_i) \\cdot \\log q(y_i)) = - (1 \\cdot \\log p(y_i \\mid x_i) ) = - \\log p(y_i \\mid x_i) \\end{align} \\] 由于\\(H(p) = 0\\)， 唯一确定，熵为0。交叉熵就等于真实和预测的分布的KL距离 。也就是说想要两个概率分布一样，即预测的所有概率密度都在正确类别上面。 \\[ H(p, q) = H(p) + D_{KL}(p || q) = D_{KL}(p || q) \\] 结合正则化 结合正则化 \\[ L = \\underbrace {\\frac{1}{N}\\sum_{i}L_i}_{\\text{data loss}} + \\underbrace {\\lambda R(W)}_{\\text{正则化 loss}} \\] 最小化正确概率分类的负对数概率，就是在进行最大似然估计。正则化部分就是对W的高斯先验，这里进行的是最大后验估计。（不懂） SVM和Softmax比较 SVM loss：希望正确分类比其他分类的得分高出一个边界值。 Softmax 交叉熵loss：希望正确分类概率大，其它分类概率小。","tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://plmsmile.github.io/tags/cs231n/"},{"name":"线性分类","slug":"线性分类","permalink":"http://plmsmile.github.io/tags/线性分类/"},{"name":"svm","slug":"svm","permalink":"http://plmsmile.github.io/tags/svm/"}]},{"title":"神经网络-过拟合-预处理-BN","date":"2017-11-26T08:21:23.000Z","path":"2017/11/26/cs224n-notes3-neural-networks-2/","text":"cs224n的笔记，过拟合、预处理、初始化、Batch Normalization 过拟合 过拟合 训练数据很少，或者训练次数很多，会导致过拟合。避免过拟合的方法有如下几种： early stop 数据集扩增 正则化 （L1， L2权重衰减） Dropout 决策树剪枝（尽管不属于神经网络） 现在一般用L2正则化+Dropout。 过拟合时，拟合系数一般都很大。过拟合需要顾及到所有的数据点，意味着拟合函数波动很大。 看到，在某些很小的区间内里，函数值的变化很剧烈。意味着这些小区间的导数值（绝对值）非常大。由于自变量值可大可小，所以只有系数足够大，才能保证导数值足够大。 所以：过拟合时，参数一般都很大。参数较小时，意味着模型复杂度更低，对数据的拟合刚刚好， 这也是奥卡姆剃刀法则。 范数 向量范数 \\(x \\in \\mathbb {R}^d\\) 范数 定义 1-范数 \\(\\left \\| x\\right\\|_1 = \\sum_i^d \\|x_i\\|\\)， 绝对值之和 2-范数 \\(\\left \\| x\\right\\|_2 = \\left(\\sum_i^d \\vert x_i\\vert^2\\right)^{\\frac{1}{2}}\\)， 绝对值平方之和再开方 p-范数 \\(\\left \\| x\\right\\|_p = \\left(\\sum_i^d \\|x_i\\|^p\\right)^{\\frac{1}{p}}\\)， 绝对值的p次方之和的\\(\\frac{1}{p}​\\)次幂 \\(\\infty\\)-范数 \\(\\left \\| x\\right\\|_\\infty = \\max_\\limits i \\|x_i\\|\\) ，绝对值的最大值 -\\(\\infty\\)-范数 \\(\\left \\| x\\right\\|_{-\\infty} = \\min_\\limits i \\|x_i\\|\\) ，绝对值的最小值 矩阵范数 \\(A \\in \\mathbb R^{m \\times n}\\) 范数 定义 1-范数 \\(\\left \\| A\\right\\|_1 = \\max \\limits_{j}\\sum_i^m \\|a_{ij}\\|\\)，列和范数，矩阵列向量绝对值之和的最大值。 \\(\\infty\\)-范数 \\(\\left \\| A\\right\\|_\\infty = \\max_\\limits i \\sum_{j}^{n}\\|a_{ij}\\|\\) ，行和范数，所有行向量绝对值之和的最大值。 2-范数 \\(\\left \\| A\\right\\|_2 = \\sqrt{\\lambda_{m}}\\) ， 其中\\(\\lambda_m\\)是\\(A^TA\\)的最大特征值。 F-范数 \\(\\left \\| A\\right\\|_F = \\left(\\sum_i^m \\sum_j^n a_{ij}^2\\right)^{\\frac{1}{2}}\\)，所有元素的平方之和，再开方。或者不开方， L2正则化就直接平方，不开方。 L2正则化权重衰减 为了避免过拟合，使用L2正则化参数。\\(\\lambda\\)是正则项系数，用来权衡正则项和默认损失的比重。\\(\\lambda\\) 的选取很重要。 \\[ J_R = J + \\lambda \\sum_{i=1}^L \\left \\| W^{(i)}\\right \\|_F \\] L2惩罚更倾向于更小更分散的权重向量，鼓励使用所有维度的特征，而不是只依赖其中的几个，这也避免了过拟合。 标准L2正则化 \\(\\lambda\\) 是正则项系数，\\(n\\)是数据数量，\\(w\\)是模型的参数。 \\[ C = C_0 + \\frac {\\lambda} {2n} \\sum_w w^2 \\] \\(C\\)对参数\\(w\\)和\\(b\\)的偏导： \\[ \\begin {align} &amp; \\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\\\ &amp; \\frac{\\partial C}{\\partial b} = \\frac{\\partial C_0}{\\partial b} \\\\ \\end{align} \\] 更新参数 ：可以看出，正则化\\(C\\)对\\(w\\)有影响，对\\(b\\)无影响。 \\[ \\begin{align} w &amp;= w - \\alpha \\cdot \\frac{\\partial C}{\\partial w} \\\\ &amp;= (1 - \\frac{\\alpha \\lambda}{n})w - \\alpha \\frac{\\partial C_0}{\\partial w} \\\\ \\end{align} \\] 从上式可以看出： 不使用正则化时，\\(w\\)的系数是1 使用正则化时 \\(w\\)的系数是\\(1 - \\frac{\\alpha \\lambda}{n} &lt; 1\\) ，效果是减小\\(w\\)， 所以是权重衰减 weight decay 当然，\\(w\\)具体增大或减小，还取决于后面的导数项 mini-batch随机梯度下降 设\\(m\\) 是这个batch的样本个数，有更新参数如下，即求batch个C对w的平均偏导值 \\[ \\begin {align} &amp; w = (1 - \\frac{\\alpha \\lambda}{n})w - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}\\frac{\\partial C_i}{\\partial w} \\\\ &amp; b = b - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}\\frac{\\partial C_i}{\\partial b} \\\\ \\end{align} \\] 所以，权重衰减后一般可以减小过拟合。 L2正则化比L1正则化更加发散，权值也会被限制的更小。 一般使用L2正则化。 还有一种方法是最大范数限制：给范数一个上界\\(\\left \\| w \\right \\| &lt; c\\) ， 可以在学习率太高的时候网络不会爆炸，因为更新总是有界的。 实例说明 增加网络的层的数量和尺寸时，网络的容量上升，多个神经元一起合作，可以表达各种复杂的函数。 如下图，2分类问题，有噪声数据。 一个隐藏层。神经元数量分别是3、6、20。很明显20过拟合了，拟合了所有的数据。正则化就是处理过拟合的非常好的办法。 对20个神经元的网络，使用正则化，解决过拟合问题。正则化强度\\(\\lambda\\)很重要。 L1正则化 正则化loss如下： \\[ C = C_0 + \\frac {\\lambda} {n} \\sum_w |w| \\] 对\\(w\\)的偏导， 其中\\(\\rm{sgn}(w)\\)是符号函数： \\[ \\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} \\cdot \\rm{sgn}(w) \\] 更新参数： \\[ w = w - \\frac{\\alpha \\lambda}{n} \\cdot \\rm{sgn}(w) - \\alpha \\frac{\\partial C_0}{\\partial w} \\] 分析：\\(w\\)为正，减小；\\(w\\)为负，增大。所以L1正则化就是使参数向0靠近，是权重尽可能为0，减小网络复杂度，防止过拟合。 特别地：当\\(w=0\\)时，不可导，就不要正则化项了。L1正则化更加稀疏。 随机失活Dropout Dropout是非常有用的正则化的办法，它改变了网络结构。一般采用L2正则化+Dropout来防止过拟合。 训练的时候，输出不变，随机以概率\\(p\\)保留神经元，\\(1-p\\)删除神经元（置位0）。每次迭代删除的神经元都不一样。 BP的时候，置位0的神经元的参数就不再更新， 只更新前向时alive的神经元。 预测的时候，要保留所有的神经元，即不使用Dropout。 相当于训练了很多个（指数级数量）小网络（半数网络），在预测的时候综合它们的结果。随着训练的进行，大部分的半数网络都可以给出正确的分类结果。 数据预处理 用的很多的是0中心化。CNN中很少用PCA和白化。 应该：线划分训练、验证、测试集，只是从训练集中求平均值！然后各个集再减去这个平均值。 中心化 也称作均值减法， 把数据所有维度变成0均值，其实就是减去均值。就是将数据迁移到原点。 \\[ x = x - \\rm{avg}(x) = x - \\bar x \\] 标准化 也称作归一化， 数据所有维度都归一化，使其数值变化范围都近似相等。 除以标准差 最大值和最小值按照比例缩放到\\((-1 ,1)\\) 之间 方差\\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar x)^2\\) ，标准差就是\\(s\\) 。数据除以标准差，接近标准高斯分布。 \\[ x = \\frac{x}{s} \\] PCA 斯坦福PCA ，CSDNPCA和SVD的区别和联系 协方差 协方差就是乘积的期望-期望的乘积。 \\[ \\rm{Cov}(X, Y) = E(XY) - E(X)E(Y) \\] 协方差的性质如下： \\[ \\begin{align} &amp; \\rm{Cov}(X, Y) = \\rm{Conv}(Y, X) \\\\ \\\\ &amp; \\rm{Cov}(aX, bY) = ab \\cdot \\rm{Conv}(Y, X) \\\\ \\\\ &amp; \\rm{Cov}(X, X) = E(X^2) - E^2(X) = D(X) , \\quad \\text{三方公式}\\\\ \\\\ &amp; \\rm{Cov}(X, C) = 0 \\\\ \\\\ &amp; \\rm{Cov}(X, Y) = 0 \\leftrightarrow X与Y独立 \\end{align} \\] 还有别的性质就看考研笔记吧。 奇异值分解 \\[ A_{m \\times n} = U_{m \\times m} \\Sigma_{m \\times n} V^T_{n \\times n} \\] \\(V_{n \\times n}\\) ：\\(V\\)的列，一组对A正交输入或分析的基向量（线性无关）。这些向量是\\(M^TM\\) 的特征向量。 \\(U_{m \\times m}\\) ：\\(U\\)的列，一组对A正交输出的基向量 。是\\(MM^T\\)的特征向量。 \\(\\Sigma_{m \\times n}\\)：对角矩阵。对角元素按照从小到大排列，这些对角元素称为奇异值。 是\\(M^TM, MM^T\\) 的特征值的非负平方根，并且与U和V的行向量对应。 记\\(r\\)是非0奇异值的个数，则A中仅有\\(r\\)个重要特征，其余特征都是噪声和冗余特征。 奇异值的物理意义 利用SVD进行PCA 先将数据中心化。输入是\\(X \\in \\mathbb R^ {N \\times D}\\) ，则协方差矩阵 如下： \\[ \\mathrm{Cov}(X) = \\frac{X^TX}{N} \\; \\in \\mathbb R^{D \\times D} \\] 比如X有a和b两维，均值均是0。那么\\(\\rm{Cov}(ab)=E(ab)-0=(a_0b_0+a_1b_1+\\cdots + a_nb_n) /n\\) ，就得到了协方差值。 中心化 计算\\(x\\)的协方差矩阵cov 对协方差矩阵cov进行svd分解，得到u, s, v 去除x的相关性，旋转，\\(xrot = x \\cdot u\\) ，此时xrot的协方差矩阵只有对角线才有值，其余均为0 选出大于0的奇异值 数据降维 12345678910111213141516171819202122def test_pca(): x = np.random.randn(5, 10) # 中心化 x -= np.mean(x, axis=0) print (x.shape) # 协方差 conv = np.dot(x.T, x) / x.shape[0] print (conv.shape) print (conv) u, s, v = np.linalg.svd(conv) print (s) print (u.shape, s.shape, v.shape) # 大于0的奇异值 n_sv = np.where(s &gt; 1e-5)[0].shape[0] print(n_sv) # 对数据去除相关性 xrot = np.dot(x, u) print (xrot.shape) # 数据降维 xrot_reduced = np.dot(x, u[:, :n_sv]) # 降到了4维 print (xrot_reduced.shape) 白化 斯坦福白化 白化希望特征之间的相关性较低，所有特征具有相同的协方差。白化后，得到均值为0，协方差相等的矩阵。对\\(xrot\\)除以特征值。 \\[ x_{white} = \\frac{x_{rot}}{\\sqrt{\\lambda + \\epsilon}} \\] 1x_white = xrot / np.sqrt(s + 1e-5) 缺陷是：可能会夸大数据中的早上，因为把所有维度都拉伸到了相同的数值范围。可能有一些极少差异性（方差小）但大多数是噪声的维度。可以使用平滑来解决。 权重初始化 如果数据恰当归一化以后，可以假设所有权重数值中大约一半为正数，一半为负数。所以期望参数值是0。 千万不能够全零初始化。因为每个神经元的输出相同，BP时梯度也相同，参数更新也相同。神经元之间就失去了不对称性的源头。 小随机数初始化 如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并成为网络的不同部分。 参数接近于0单不等于0。使用零均值和标准差的高斯分布来生成随机数初始化参数，这样就打破了对称性。 1W = 0.01 * np.random.randn(D,H) 注意：不是参数值初始越小就一定好。参数小，意味着会减小BP中的梯度信号，在深度网络中，就会有问题。 校准方差 随着数据集的增长，随机初始化的神经元的输出数据分布的方差也会增大。可以使用\\(\\frac{1}{\\sqrt{n}}\\) 校准方差。n是数据的数量。这样就能保证网络中所有神经元起始时有近似同样的输出分布。这样也能够提高收敛的速度。 感觉实际上就是做了一个归一化。 数学详细推导见cs231n ， \\(s = \\sum_{i}^nw_ix_i\\) ，假设w和x都服从同样的分布。想要输出s和输入x有同样的方差。 \\[ \\begin {align} &amp; \\because D(s) = n \\cdot D(w)D(x), \\; D(s) = D(x) \\\\ &amp; \\therefore D(w) = \\frac{1}{n} \\\\ &amp; \\because D(w_{old}) = 1, \\; D(aX) = a^2 D(X) \\\\ &amp; \\therefore D(w) = \\frac{1}{n}D(w_{old}) = D(\\frac{1}{\\sqrt n} w_{old}) \\\\ &amp; \\therefore w = \\frac{1}{\\sqrt n} w_{old} \\end{align} \\] 所以要使用\\(\\frac{1}{\\sqrt{n}}\\)来标准化参数： 1W = 0.01 * np.random.randn(D,H)/ sqrt(n) 经验公式 对于某一层的方差，应该取决于两层的输入和输出神经元的数量，如下： \\[ \\rm{D}(w) = \\frac{2}{n_{in} + n_{out}} \\] ReLU来说，方差应该是\\(\\frac{2}{n}\\) 1W = 0.01 * np.random.randn(D,H) * sqrt(2.0 / n) 稀疏和偏置初始化 一般稀疏初始化用的比较少。一般偏置都初始化为0。 Batch Normalization 莫凡python BN讲解 和 CSDN-BN论文介绍 。Batch Normalization和普通数据标准化类似，是将分散的数据标准化。 Batch Normalization在神经网络非常流行，已经成为一个标准了。 训练速度分析 网络训练的时候，每一层网络参数更新，会导致下一层输入数据分布的变化。这个称为Internal Convariate Shift。 需要对数据归一化的原因 ： 神经网络的本质是学习数据分布。如果训练数据与测试数据的分布不同，那么泛化能力也大大降低 如果每个batch数据分布不同（batch 梯度下降），每次迭代都要去学习适应不同的分布，会大大降低训练速度 深度网络，前几层数据微小变化，后面几层数据差距会积累放大。 一旦某一层网络输入数据发生改变，这层网络就需要去适应学习这个新的数据分布。如果训练数据的分布一直变化，那么就会影响网络的训练速度。 敏感度问题 神经网络中，如果使用tanh激活函数，初始权值是0.1。 输入\\(x=1\\)， 正常更新： \\[ z = wx = 0.1, \\quad a(z_1) = 0.1 \\quad \\to \\quad a^\\prime(z) = 0.99 \\] 但是如果一开始输入 \\(x=20\\) ，会导致梯度消失，不更新参数。 \\[ z = wx = 2 ,\\quad a(z) \\approx 1 \\quad \\to \\quad a^\\prime(z) = 0 \\] 同样地，如果再输入\\(x=100\\) ，神经元的输出依然是接近于1，不更新参数。 \\[ z = wx = 10 ,\\quad a(z) \\approx 1 \\quad \\to \\quad a^\\prime(z) = 0 \\] 对于一个变化范围比较大特征维度，神经网络在初始阶段对它已经不敏感没有区分度了！ 这样的问题，在神经网络的输入层和中间层都存在。 BN算法 BN算法在每一次迭代中，对每一层的输入都进行归一化。把数据转换为均值为0、方差为1的高斯分布。 \\[ \\hat x = \\frac{x - E(x)} {\\sqrt{D(x) + \\epsilon}} \\] 非常大的缺陷：强行归一化会破坏掉刚刚学习到的特征。 把每层的数据分布都固定了，但不一定是前面一层学习到的数据分布。 牛逼的地方 ：设置两个可以学习的变量扩展参数\\(\\gamma\\) ，和平移参数 \\(\\beta\\) ，用这两个变量去还原上一层应该学习到的数据分布。（但是芳芳说，这一步其实可能没那么重要，要不要都行，CNN的本身会处理得更好）。 \\[ y = \\gamma \\hat x+ \\beta \\] 这样理解：用这两个参数，让神经网络自己去学习琢磨是前面的标准化是否有优化作用，如果没有优化效果，就用\\(\\gamma, \\beta\\)来抵消标准化的操作。 这样，BN就把原来不固定的数据分布，全部转换为固定的数据分布，而这种数据分布恰恰就是要学习到的分布。从而加速了网络的训练。 对一个mini-batch进行更新， 输入一个\\(batchsize=m\\)的数据，学习两个参数，输出\\(y\\) \\[ \\begin{align} &amp; \\mu = \\frac{1}{m} \\sum_{i=1}^m x_i &amp; \\text{求均值} \\\\ &amp; \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2 &amp; \\text{求方差} \\\\ &amp; \\hat x = \\frac{x - E(x)} {\\sqrt{\\sigma^2 + \\epsilon}} &amp; \\text{标准化} \\\\ &amp; y = \\gamma \\hat x+ \\beta &amp; \\text{scale and shfit} \\end{align} \\] 其实就是对输入数据做个归一化： \\[ z = wx+b \\to z = \\rm{BN}(wx + b) \\to a = f(z) \\] 一般在全连接层和激活函数之间添加BN层。 在测试的时候，由于是没有batch，所以使用固定的均值和标准差，也就是对训练的各个batch的均值和标准差做批处理。 \\[ E(x) = E(\\mu), \\quad D(x) = \\frac{b}{b-1} E(\\sigma^2) \\] BN的优点 1 训练速度快 2 选择大的初始学习率 初始大学习率，学习率的衰减也很快。快速训练收敛。小的学习率也可以。 3 不再需要Dropout BN本身就可以提高网络泛化能力，可以不需要Dropout和L2正则化。源神说，现在主流的网络都没有dropout了。但是会使用L2正则化，比较小的正则化。 4 不再需要局部相应归一化 5 可以把训练数据彻底打乱 效果图片展示 对所有数据标准化到一个范围，这样大部分的激活值都不会饱和，都不是-1或者1。 大部分的激活值在各个分布区间都有值。再传递到后面，数据更有价值。","tags":[{"name":"数据预处理","slug":"数据预处理","permalink":"http://plmsmile.github.io/tags/数据预处理/"},{"name":"正则化","slug":"正则化","permalink":"http://plmsmile.github.io/tags/正则化/"},{"name":"范数","slug":"范数","permalink":"http://plmsmile.github.io/tags/范数/"},{"name":"Dropout","slug":"Dropout","permalink":"http://plmsmile.github.io/tags/Dropout/"},{"name":"PCA","slug":"PCA","permalink":"http://plmsmile.github.io/tags/PCA/"},{"name":"白化","slug":"白化","permalink":"http://plmsmile.github.io/tags/白化/"},{"name":"BN","slug":"BN","permalink":"http://plmsmile.github.io/tags/BN/"}]},{"title":"神经网络基础-反向传播-激活函数","date":"2017-11-23T04:01:08.000Z","path":"2017/11/23/cs224n-notes3-neural-networks/","text":"cs224n神经网络基础，前向反向传播，激活函数等 神经网络基础 很多数据都是非线性分割的，所以需要一种非线性non-linear决策边界 来分类。神经网络包含很多这样的非线性的决策函数。 神经元 神经元其实就是一个计算单元。 输入向量 \\(x \\in \\mathbb R^n\\) \\(z = w^T x + b\\) \\(a = f(z)\\) 激活函数，sigmoid, relu等，后文有讲。 Sigmoid神经元 传统用sigmoid多，但是现在一定不要使用啦。大多使用Relu作为激活函数。 \\[ z = \\mathbf{w}^T \\mathbf{x} + b , \\; a = \\frac {1}{1 + \\exp (-z)} \\] 网络层 一个网络层有很多个神经元。输入\\(\\mathbf x\\)向量，会传递到多个神经元。如 输入是\\(n\\)维，隐层是\\(m\\)维，有\\(m\\)个神经元。则有 \\[ \\begin{align} &amp; z = W x + b , &amp; W \\in \\mathbb{R}^{m \\times n}, x \\in \\mathbb R^n, b \\in \\mathbb R^m\\\\ &amp; a = f (z) &amp; a \\in \\mathbb R^m\\\\ &amp; s = U^T a &amp; 一般会对a进行变换得到最终结果s\\\\ \\end{align} \\] 激活函数的意义 每个神经元 输入\\(z = w^Tx+b\\) ：对特征进行加权组合的结果 激活\\(a = f(z)\\)： 对\\(z\\)是否继续保留 最后会把所有的神经元的所有\\(z\\)的激活信息\\(a\\)综合起来，得到最终的分类结果。比如\\(s = U^T a\\)。 前向计算 输入\\(x \\in \\mathbb R^n\\)， 激活信息\\(a \\in \\mathbb R^m\\)。一般前向计算如下： \\[ \\begin{align} &amp; z = W x + b , &amp; W \\in \\mathbb{R}^{m \\times n}, x \\in \\mathbb R^n, b \\in \\mathbb R^m\\\\\\\\ &amp; a = f (z) &amp; a \\in \\mathbb R^m\\\\\\\\ &amp; s = U^T a &amp; 一般会对a进行变换得到最终结果s\\\\ \\end{align} \\] 下面是一个简单的全连接，最后的圆圈里的1代表等价输出。 NER例子 NER(named-entity recognition)，命名实体识别。对于一个句子Museums in Paris are amazing。 要判断中心单词Paris是否是个命名实体。 既要看window里的所有词向量，也要看这些词的交互关系。比如：Paris出现在in的后面。 因为可能有Paris和Paris Hilton。这就需要non-linear decisions。 如果直接把input给到softmax，是很难获取到非线性决策的。所以需要添加中间层使用神经网络。如上图所示。 维数分析 每个单词4维，输入整个窗口就是20维。在隐层使用8个神经元。计算过程如下，最终得到一个分类的得分。 \\[ \\begin {align} &amp; z = Wx + b \\\\ &amp; a = f(z) \\\\ &amp; s = U^T a \\\\ \\end{align} \\] 维数如下： \\[ x \\in \\mathbb R^{20}, \\; W \\in \\mathbb R^{8\\times20}, \\; U \\in \\mathbb R^{8\\times1}, s \\in R \\] Max magin目标函数 正样本\\(s\\) ：Museums in Paris are amazing ，负样本\\(s_c\\)： Not all museums in Paris 。 只关心：正样本的得分高于负样本的得分， 其它的不关注。即要\\(s - s_c &gt; 0\\)： \\[ \\mathrm{maxmize}(s -s_c) \\leftrightarrow \\mathrm{minmize}(s_c - s) \\] 优化目标函数如下： \\[ \\rm{minimize} \\; J = \\max(s_c - s, 0) \\; = \\begin{cases} &amp; s_c - s, &amp; s &lt; s_c \\\\ &amp; 0, &amp; s \\ge s_c \\end{cases} \\] 上式其实有风险，更需要\\(s - s_c &gt; \\Delta\\)， 即\\(s\\)比\\(s_c\\)得分大于\\(\\Delta\\)，来保证一个安全的间距。 \\[ \\rm{minimize} \\; J = \\max(\\Delta + s_c - s, 0) \\] 给具体间距\\(\\Delta=1\\)， 所以优化目标函数：详情见SVM。 \\[ \\rm{minimize} \\; J = \\max(1 + s_c - s, 0) \\] 其中\\(s_c = U^T f(Wx_c + b), \\; s = U^T f(Wx+b)\\) 。 反向传播训练 梯度下降 ，或者SGD： \\[ \\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\cdot \\Delta_{\\theta^{(t)}} J \\] 反向传播 使用链式法则 来计算前向计算中用到的参数的梯度。 符号定义 如下图，一个简单的网络： 网络在输入层和输出层是等价输入和等价输出，只有中间层会使用激活函数进行非线性变换。 符号 意义 \\(x\\) 网络输入，这里是4维 \\(s\\) 网络输出，这里是1维，即一个数字 \\(W^{(k)}\\) 第\\(k \\to k+1\\)层的转移矩阵。\\(W \\in \\mathbb R^{n \\times m}\\)。 k层m个神经元，k+1层n个神经元 \\(W_{ij}^{(k)}\\) k+1层的\\(i\\) 神经元 到 到\\(k\\)层\\(j\\)神经元的 的权值 \\(b_i^{(k)}\\) \\(k \\to k+1\\) 转移， k+1层的\\(i\\) 神经元的接收偏置 \\(z^{(k)}_j\\) 第\\(k\\)层的第\\(j\\)个神经元的输入 计算输入 \\(z_j^{(k+1)} = \\sum_i W_{ji}^{(k)} \\cdot a^{(k)}_i + b^{(k)}_j\\) \\(a_j^{(k)}\\) 第\\(k\\)层的第\\(j\\)个神经元的输入。\\(a = f(z)\\) \\(\\delta_j^{(k)}\\) BP时，在\\(z_j^{(k)}\\)处的梯度。即\\(f^\\prime(z_j^{(k)}) \\cdot g\\) ，\\(g\\)是传递来的梯度 W梯度推导 误差函数\\(J = \\max (1 + s_c - s, 0)\\) ，当\\(J &gt; 0\\)的时候，\\(J = 1 + s_c - s\\)要去更新参数W和b。 \\[ \\frac{\\partial J} {\\partial s} = - \\frac{\\partial J} {\\partial s_c} = -1 \\] 反向传播时，必须知道参数在前向时所贡献所关联的对象，即知道路径。 这里是等价输出： \\[ s = a_1^{(3)} = z_1^{(3)} = W_1^{(2)}a_1^{(2) } + W_2^{(2)}a_2^{(2) } \\] 这里对\\(W_{ij}^{(1)}\\)的偏导进行反向传播推导： \\[ \\begin{align} \\frac{\\partial s}{\\partial W_{ij}^{(1)}} &amp; = \\frac{\\partial W^{(2)} a^{(2)}}{\\partial W_{ij}^{(1)}} \\\\ &amp;= \\frac{ \\color{blue} {\\partial W_i^{(2)} a_i^{(2)}}} {\\partial W_{ij}^{(1)}} = \\color{blue}{W_i^{(2)}} \\cdot \\frac{\\partial a_i^{(2)}}{\\partial W_{ij}^{(1)}} \\\\ &amp; = W_i^{(2)} \\cdot \\color{blue} {\\frac{\\partial a_i^{(2)}}{\\partial z_i^{(2)}} \\cdot \\frac{\\partial z_i^{(2)}}{\\partial W_{ij}^{(1)}}} \\\\ &amp; = W_i^{(2)} \\cdot \\color{blue}{f^\\prime(z_i^{(2)})} \\cdot \\frac{\\partial }{\\partial W_{ij}^{(1)}} \\left(\\color{blue}{b_i^{(2)} + \\sum_k^4 a_k^{(1)}W_{ik}^{(1)}}\\right) \\\\ &amp; = W_i^{(2)}f^\\prime(z_i^{(2)}) \\color{blue}{a_j^{(1)}} \\\\ &amp; = \\color{blue}{\\delta^{(2)}_i} \\cdot a_j^{(1)} \\end{align} \\] 结果分析 我们知道\\(z_i^{(2)} = \\sum_k^4 a_k^{(1)}W_{ik}^{(1)} + b_i^{(2)}\\)。 单纯\\(z_i^{(2)}\\)对\\(W_{ij}^{(2)}\\)的导数是\\(a_j^{(1)}\\)。反向时，在\\(z_i^{(2)}\\)处的梯度是\\(\\delta_i^{(2)}\\)。 反向时，\\(\\frac{\\partial s}{\\partial W_{ij}^{(1)}} = \\delta^{(2)}_i \\cdot a_j^{(1)}\\)，是传来的梯度和当前梯度的乘积。这正好应证了反向传播。 传来的梯度也作error signal。 反向过程也是error sharing/distribution。 W元素实例 \\(W_{14}^{(1)}\\) 只直接贡献于\\(z_1^{(2)}\\)和\\(a_1^{(2)}\\) 步骤 梯度 \\(s \\to a_1^{(3)}\\) 梯度\\(g=1\\)。开始为1。 \\(a_1^{(3)} \\to z_1^{(3)}\\) 在\\(z_1^{(3)}\\)处的梯度\\(g = 1 \\cdot 1 = \\delta_1^{(3)}\\) 。\\(local \\; g= 1\\) ，等价变换 \\(z_1^{(3)} \\to a_1^{(2)}\\) \\(g = \\delta_1^{(3)} \\cdot W_1^{(2)} = W_1^{(2)}\\) 。\\(lg = w\\), \\(z=wa+b\\) \\(a_1^{(2)} \\to z_1^{(2)}\\) \\(g = W_1^{(2)} \\cdot f^\\prime(z_1^{(2)}) = \\delta_1^{(2)}\\)。 \\(lg=f^\\prime(z_1^{(2)})\\) \\(z_1^{(2)} \\to W_{14}^{(1)}\\) \\(g =W_1^{(2)} \\cdot f^\\prime(z_1^{(2)}) \\cdot a_4^{(1)} = \\delta_1^{(2)} \\cdot a_4^{(1)}\\)。 \\(lg = a_4^{(1)}\\) ， 因为\\(z =wa+b\\) \\(z_1^{(2)} \\to b_1^{(1)}\\) \\(g = W_1^{(2)} \\cdot f^\\prime(z_1^{(2)}) \\cdot 1 = \\delta_1^{(2)} \\cdot a_4^{(1)}\\)。 \\(lg = 1\\) ， 因为\\(z =wa+b\\) 对于上式的梯度计算，有两种理解方法，通过这两种思路去思考能更深入了解。 链式法则 error sharing and distributed flow approach 梯度反向传播 \\(\\delta_i^{(k)} \\to \\delta_j^{(k-1)}\\) 传播图如下： 但是更多时候，当前层的某个神经元的信息会传播到下一层的多个节点上，如下图： 梯度推导公式如下： \\[ \\begin{align} &amp; g_w = \\delta_i^{(k)} \\cdot a_j^{(k-1)} &amp; W_{ij}^{(k-1)}的梯度\\\\\\\\ &amp; g_a = \\sum_i \\delta_i^{(k)}W_{ij}^{(k-1)} &amp; a_j^{(k-1)}的梯度 \\\\\\\\ &amp; g_z = \\delta_j^{(k-1)} = f^\\prime(z_j^{(k-1)}) \\cdot \\sum_i \\delta_i^{(k)}W_{ij}^{(k-1)} &amp; z_j^{(k-1)}的梯度 \\\\\\\\ \\end{align} \\] BP向量化 很明显，不能一个一个参数地去更新element-wise。所以需要用矩阵和向量去表达，去一次性全部更新matrix-vector level。 梯度计算， \\(W_{ij}^{(k)}\\)的梯度是\\(\\delta_i^{(k+1)} \\cdot a_j^{(k)}\\) 。向量表达如下： \\[ \\Delta _{W^{(k)}} = \\begin{bmatrix} \\delta_1^{(k+1)} \\cdot a_1^{(k)} &amp; \\delta_1^{(k+1)} \\cdot a_2^{(k)} &amp; \\cdots\\\\ \\delta_2^{(k+1)} \\cdot a_1^{(k)} &amp; \\delta_2^{(k+1)} \\cdot a_2^{(k)} &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix} = \\delta^{(k+1)} a^{(k)T} \\] 梯度传播，\\(\\delta_j^{(k)} = f^\\prime(z_j^{(k)}) \\cdot \\sum_i \\delta_i^{(k+1)}W_{ij}^{(k)}\\)。向量表达如下： \\[ \\delta^{(k)} = f^\\prime(z^{(k)}) \\circ (\\delta^{(k+1)}W^{(k)}) \\] 其中\\(\\circ\\)是叉积向量积element-wise，是各个位置相乘， 即\\(\\mathbb R^N \\times \\mathbb R^N \\to \\mathbb R^N\\)。 点积和数量积是各个位置相乘求和。 计算效率 很明显，在计算的时候要把上一层的\\(\\delta^{(k+1)}\\)存起来，去计算\\(\\delta^{(k)}\\) ，这样可以减少大量的多余的计算。 神经网络常识 梯度检查 使用导数的定义来估计导数，去和BP算出来的梯度做对比。 \\[ f ^\\prime (\\theta) \\approx \\frac{J(\\theta^{(i+)}) - J(\\theta^{(i-)})} {2 \\epsilon } \\] 由于这样计算非常，效率特别低，所以只用这种办法来检查梯度。具体实现代码见原notes。 激活函数 激活函数有很多，现在主要用ReLu，不要用sigmoid。 用ReLU学习率一定不要设置太大！同一个网络中都使用同一种类型的激活函数。 Sigmoid 数学形式和导数如下： \\[ \\begin{align} &amp; \\sigma (z) = \\frac {1} {1 + \\exp(-z)}, \\; \\sigma(z) \\in (0,1) \\\\ \\\\ &amp; \\sigma^\\prime (z) = \\sigma(z) (1 - \\sigma(z)) \\\\ \\end{align} \\] 图像 优点是具有好的解释性，将实数挤压到\\((0,1)\\)中，很大的负数变成0，很大的正数变成1 。但现在用的已经越来越少了。有下面2个缺点。 Sigmoid会造成梯度消失 靠近0和1两端时，梯度会变成0。 BP链式法则，\\(0 \\times g_{from} = 0\\) ，后面的梯度接近0， 将没有信息去更新参数。 初始化权重过大，大部分神经元会饱和，无法更新参数。因为输入值很大，靠近1了。\\(f^\\prime(z) = 0\\)， 没法传播了。 Sigmoid输出不是以0为均值 如果输出\\(x\\)全是正的，\\(z=wx+b\\)， 那么\\(\\frac{\\partial z}{\\partial w} = x\\) 梯度就全是正的 不过一般是batch训练，其实问题也还好 Sigmoid梯度消失的问题最严重。 Tanh 数学公式和导数如下： \\[ \\begin{align} &amp; \\tanh (z) = \\frac{\\exp(z) - \\exp(-z)}{\\exp(z) + \\exp(-z)} = 2 \\sigma(2z) - 1, \\; \\tanh(z) \\in(-1, 1) \\\\ \\\\ &amp; \\tanh^\\prime (z) = 1 - \\tanh^2 (z) \\end{align} \\] 图像： Tanh是Sigmoid的代替，它是0均值的，但是依然存在梯度消失的问题。 ReLU ReLURectified Linear Unit 最近越来越流行，不会对于大值\\(z\\)就导致神经元饱和的问题。在CV取得了很大的成功。 \\[ \\begin{align} &amp; \\rm{rect}(z) = \\max(z, 0) \\\\ \\\\ &amp; \\rm{rect}^\\prime (z) = \\begin{cases} &amp;1, &amp;z &gt; 0 \\\\&amp; 0, &amp; z \\le 0 \\end{cases} \\\\ \\end{align} \\] 其实ReLU是一个关于0的阈值，现在一般都用ReLU： ReLU的优点 加速收敛（6倍）。线性的，不存在梯度消失的问题。一直是1。 计算简单 ReLU的缺点 训练的时候很脆弱 BP时，如果有大梯度经过ReLU，当前在z处的梯度\\(\\delta^{(k+1)} = 1 \\times g_m\\) 就很大 对参数\\(w\\)的梯度 \\(\\Delta_{W^{(k)}} =\\delta^{(k)} a^{(k)T}\\) 也就很大 参数\\(w\\)会更新的特别小 \\(W^{(k)} = W^{(k)} - \\alpha \\cdot \\Delta_{W^{(k)}}\\) 前向时，\\(z =wx+b \\le 0\\) 也就特别小，激活函数就不会激活 不激活，梯度就为0。 再BP的时候，就无法更新参数了 总结也就是：大梯度\\(\\to\\)小参数\\(w\\) ，新小$z = wx+b $ ReLU不激活， 不激活梯度为0 \\(\\to\\) 不更新参数w了。 当然可以使用比较小的学习率来解决这个问题。 Maxout maxout 有ReLU的优点，同时避免了它的缺点。但是maxout加倍了模型的参数，导致了模型的存储变大。 \\[ \\begin{align} &amp; \\rm{mo}(x) = \\max(w_1x+b_1, w_2x+b_2) \\\\ \\\\ &amp; \\rm{mo}^\\prime (x) = \\begin{cases} &amp;w_1, &amp;w_1x+b_1 大 \\\\&amp; w_2, &amp; 其它 \\\\\\end{cases} \\\\ \\end{align} \\]","tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://plmsmile.github.io/tags/神经网络/"},{"name":"反向传播","slug":"反向传播","permalink":"http://plmsmile.github.io/tags/反向传播/"},{"name":"激活函数","slug":"激活函数","permalink":"http://plmsmile.github.io/tags/激活函数/"}]},{"title":"Word2vec之总体介绍","date":"2017-11-12T12:54:37.000Z","path":"2017/11/12/cs224n-notes1-word2vec/","text":"cs224n笔记，word2vec总体介绍，包括CBOW和Skip-gram，负采样训练 Word2vec 简介 把词汇变成词向量。 类别1 类别2 算法 CBOW，上下文预测中心词汇 Skip-gram，中心词汇预测上下文 训练方法 负采样 哈夫曼树 语言模型 两种句子： 正常的句子：The cat jumped over the puddle。 概率高，有意义。 没意义的句子：stock boil fish is toy 。概率低，没意义。 二元模型 一个句子，有\\(n\\)个单词。每个词出现的概率由上一个词语来决定。则整体句子的概率如下表示： \\[ P(w_1, w_2, \\cdots, w_n) = \\prod_{i=2}^n P(w_i \\mid w_{i-1}) \\] 缺点 只考虑单词相邻传递概率，而忽略句子整体的可能性。 context size=1，只学了相邻单词对的概率 会计算整个大数据集的全局信息 CBOW 给上下文The cat _ over the puddle，预测jump 。对于每个单词，学习两个向量： \\(v\\) ：输入向量 ，（上下文单词） \\(u\\)： 输出向量 ， （中心单词） 符号说明 \\(V\\) ：词汇表，后面用\\(V\\)代替词汇表单词个数 \\(w_i\\) ：词汇表中第\\(i\\)个单词 \\(d\\) ：向量的维数 \\(\\mathcal V_{d \\times |V|}\\)：输入矩阵，也可以用\\(W\\)来表达 \\(v_i\\) ：\\(\\mathcal{V}\\)的第\\(i\\)列，\\(w_i\\)的输入向量表达 \\(\\mathcal {U}_{|V| \\times d}\\) ：输出矩阵，可以用\\(W ^ \\prime\\)来表达 \\(u_i\\) ：\\(\\mathcal U\\)的第i行， \\(w_i\\)的输出向量表达 输入与输出 \\(x^{(c)}\\)， 输入\\(2m\\)个上下文单词，上下文词汇的one-hot向量 \\(y_c\\)： 真实标签 \\(\\hat y^{(c)}\\)， 输出一个中心单词，中心词汇的one-hot向量 步骤 1 上下文单词onehot向量 one-hot向量的表达：\\((x^{(c-m)}, \\cdots, x^{(c-1)}, x^{(c+1)}, x^{(c+m)} \\in \\mathbb R^V)\\) 2 上下文单词向量 \\((v_{c-m}, v_{c-m+1}, \\cdots. v_{c+m} \\in \\mathbb{R}^d)\\)， 其中，\\(v_{c-m}=\\mathcal V x^{(c-m)}\\)， 即输入矩阵乘以one-hot向量就找到所在的列 3 平均上下文词向量 \\(\\hat v = \\frac {v_{c-m} + \\cdots + v_{c+m}}{2m} \\in \\mathbb R^d\\) 4 输出单词与上下文计算得分向量 \\(z = \\mathcal U \\hat v \\in \\mathbb R ^V\\) 。点积，单词越相似，得分越高 5 得分向量转为概率 $y = (z) R^V $ 6 真实预测概率对比 预测的概率向量\\(\\hat y\\)与唯一真实中心单词one-hot向量\\(y\\)，进行交叉熵比较算出loss。 目标函数 使用交叉熵计算loss，损失函数如下： \\[ H(\\hat y, y) = - \\sum_{j=1}^{|V|} y_j \\log (\\hat y_j) \\] 由于中心单词\\(y\\)是one-hot编码，只有正确位置才为1，其余均为0，所以只需计算中心单词对应的位置概率的loss即可： \\[ H(\\hat y, y) = - y_c \\log (\\hat y_c) = - \\log (\\hat y_c) \\] 交叉熵很好是因为 \\(-1 \\cdot \\log (1) = 0\\)，预测得好 \\(-1 \\cdot \\log (0.01) = 4.605​\\)， 预测得不好 最终损失函数： \\[ \\begin{align} \\rm{minimize} \\; J &amp; = - \\log P(w_c \\mid w_{c-m}, \\ldots, w_{c-1}, w_{c+1}, \\ldots, w_{c+m}) \\\\ &amp; = - \\log P(u_c \\mid \\hat v) \\\\ &amp; = - \\log \\frac {\\exp(u_c^T \\hat v)}{\\sum_{j=1}^{|V|} \\exp(u_j^T \\hat v)} \\\\ &amp; = -u_c^T \\hat v + \\log \\sum_{j=1}^{|V|} \\exp(u_j^T \\hat v) \\end{align} \\] 再使用SGD方法去更新相关的两种向量\\(u_c, v_j\\) 。 Skip-gram 给中心单词jump，预测上下文The cat _ over the puddle 。 输入中心单词\\(x\\)， 输出上下文单词\\(y\\) 。与CBOW正好输入输出相反，但同样有两个矩阵\\(\\mathcal {U, V}\\) 。符号说明同CBOW。 步骤 1 中心单词onehot向量 \\(x \\in \\mathbb {R}^{|V|}\\) 2 中心单词词向量 \\(v_c = \\mathcal V x \\in \\mathbb R^d\\) 3 中心词与其他词的得分向量 \\(z = \\mathcal U v_c \\in \\mathbb R ^{|V|}\\) 4 得分向量转为概率 概率 \\(\\hat y = \\rm {softmax} (z)\\)， \\(\\hat y_{c-m}, \\ldots, \\hat y_{c+m}\\) 是目标上下文单词是中心单词的上下文的预测概率。 5 预测真实概率对比 预测概率\\(\\hat y\\) 与\\(2m\\) 个真实上下文onehot向量\\(y_{c-m}, \\ldots, y_{c+m}\\)进行交叉熵对比，算出loss 目标函数 与CBOW不同的是，Skip-gram做了一个朴素贝叶斯条件假设，所有的输出上下文单词都是独立的。 \\[ \\begin {align} \\rm{minimize} \\; J &amp; = - \\log P(w_{c-m}, \\ldots, w_{c-1}, w_{c+1}, \\ldots, w_{c+m} \\mid w_c) \\\\ &amp; = - \\log \\prod_{j=0, j \\neq m}^{2m} P(w_{c-m+j} \\mid w_c) \\\\ &amp; = -\\log \\prod_{j=0, j \\neq m}^{2m} \\frac {\\exp (u_{c-m+j}^T \\cdot v_c)} {\\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c)} \\\\ &amp; = - \\sum_{j=0, j \\neq m}^{2m} \\left ( \\log \\exp (u_{c-m+j}^T \\cdot v_c) - \\log \\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c) \\right) \\\\ &amp; = - \\sum_{j=0, j \\neq m}^{2m} u_{c-m+j}^T v_c + 2m \\cdot \\log \\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c) \\end {align} \\] 一样，使用SGD去优化U和V。 损失函数实际上是\\(2m\\)个交叉熵求和，求出的向量\\(\\hat y\\)与\\(2m\\)个onehot向量\\(y_{c-m+j}\\) 计算交叉熵： \\[ \\begin {align} J &amp; = - \\sum_{j=0, j \\neq m}^{2m} \\log P(u_{c-m+j} \\mid v_c) \\\\ &amp; = \\sum_{j=0, j \\neq m}^{2m} H(\\hat y, y_{c-m+j}) \\\\ \\end{align} \\] 负采样训练 每次计算都会算整个\\(|V|\\)词表，太耗时了。 可以从噪声分布\\(P_n(w)\\)中进行负采样，来代替整个词表。当然单词采样概率与其词频相关。只需关心：目标函数、梯度、更新规则。 标签函数 对于一对中心词和上下文单词\\((w, c)\\) ，设标签如下： \\(P(l = 1 \\mid w, c)\\)， \\((w, c)\\) 来自于真实语料 \\(P(l = 0 \\mid w, c)\\) ，\\((w, c)\\)来自于负样本，即不在语料中 用sigmoid表示标签函数： \\[ \\begin {align} &amp; P(l = 1 \\mid w, c; \\theta) = \\sigma (u^T_w v_c) = \\frac {1}{ 1 + e^{-u^T_w v_c}} \\\\ &amp; P(l = 0 \\mid w, c; \\theta) = 1 - \\sigma (u^T_w v_c) = \\frac {1}{ 1 + e^{u^T_w v_c} } \\\\ \\end {align} \\] 目标函数 选取合适的\\(\\theta= \\mathcal {U, V}\\) ，去增大正样本的概率，减小负样本的概率。设\\(D\\)为正样本集合，\\(\\bar D\\)为负样本集合。 \\[ \\begin {align} \\theta &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\prod_{(w, c) \\in D} P(l=1 \\mid w, c, \\theta) \\prod_{(w, c) \\in \\bar D} P(l=0 \\mid w, c, \\theta) \\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\prod_{(w, c) \\in D} P(l=1 \\mid w, c, \\theta) \\prod_{(w, c) \\in \\bar D} (1 - P(l=1 \\mid w, c, \\theta) )\\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\sum_{(w, c) \\in D} \\log P(l=1 \\mid w, c, \\theta) + \\sum_{(w, c) \\in \\bar D} \\log (1 - P(l=1 \\mid w, c, \\theta) )\\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\sum_{(w, c) \\in D} \\log \\frac {1}{ 1 + \\exp (-u^T_w v_c)}+ \\sum_{(w, c) \\in \\bar D} \\log \\frac {1}{ 1 + \\exp (u^T_w v_c) } \\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\sum_{(w, c) \\in D} \\log \\sigma(u^T_w v_c) + \\sum_{(w, c) \\in \\bar D} \\log \\sigma (-u^T_w v_c) \\end {align} \\] 最大化概率也就是最小化负对数似然 \\[ J = - \\sum_{(w, c) \\in D} \\log \\sigma(u^T_w v_c) - \\sum_{(w, c) \\in \\bar D} \\log \\sigma (-u^T_w v_c) \\] 负采样集合选择 为中心单词\\(w_c\\) 从\\(P_n(w)\\) 采样\\(K\\)个假的上下文单词。表示为\\(\\{ \\bar u_k \\mid k=1\\ldots K\\}\\) CBOW 给上下文向量\\(\\hat{v}=\\frac {v_{c-m} + \\cdots + v_{c+m}}{2m}\\) 和真实中心词\\(u_c\\) 原始loss \\[ J = -u_c^T \\hat v + \\log \\sum_{j=1}^{|V|} \\exp(u_j^T \\hat v) \\] 负采样loss \\[ J = - \\log \\sigma (u_c^T \\cdot \\hat v) - \\sum_{k=1}^K \\log \\sigma (- \\bar u_k^T \\cdot \\hat v) \\] Skip-gram 给中心单词\\(v_c\\)， 和\\(2m\\)个真实上下文单词\\(u_{c-m+j}\\) 原始loss \\[ J = - \\sum_{j=0, j \\neq m}^{2m} u_{c-m+j}^T v_c + 2m \\cdot \\log \\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c) \\] 负采样loss \\[ J = - \\sum_{j=0, j \\neq m}^{2m} \\log \\sigma (u_{c-m+j}^T \\cdot v_c) - \\sum_{k=1}^K \\log \\sigma (-\\bar u_{k}^T \\cdot v_c) \\]","tags":[{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"},{"name":"cbow","slug":"cbow","permalink":"http://plmsmile.github.io/tags/cbow/"},{"name":"skip-gram","slug":"skip-gram","permalink":"http://plmsmile.github.io/tags/skip-gram/"},{"name":"负采样","slug":"负采样","permalink":"http://plmsmile.github.io/tags/负采样/"}]},{"title":"Word2vec之数学模型","date":"2017-11-02T13:53:49.000Z","path":"2017/11/02/word2vec-math/","text":"Word2vec有两种模型：CBOW和Skip-gram，有两种训练方法：Hierarchical Softmax和Negative Sampling。偏数学公式推导 背景介绍 符号 \\(C\\) ：语料Corpus，所有的文本内容，包含重复的词。 D：词典，D是从C中取出来的，不重复。 \\(w\\)：一个词语 \\(m\\)：窗口大小，词语\\(w\\)的前后\\(m\\)个词语 \\(\\rm{Context(w)} = C_w\\)： 词\\(w\\)的上下文词汇，取决于\\(m\\) \\(v(w)\\)： 词典\\(D\\)中单词\\(w\\)的词向量 \\(k\\)：词向量的长度 \\(i_w\\)：词语\\(w\\)在词典\\(D\\)中的下标 \\(NEG(w)\\) ： 词\\(w\\)的负样本子集 常用公式： \\[ \\begin {align} &amp; \\log (a^n b^m) = \\log a^n + \\log b^m = n \\log a + m \\log b \\\\ \\end{align} \\] \\[ \\log \\prod_{i=1}a^i b^{1-i} = \\sum_{i=1} \\log a^i + \\log b^{1-i} = \\sum_{i=1} i \\cdot \\log a + (i-1) \\cdot \\log b \\] 目标函数 n-gram模型。当然，我们使用神经概率语言模型。 \\(P(w \\mid C_w)\\) 表示上下文词汇推出中心单词\\(w\\)的概率。 对于统计语言模型来说，一般利用最大似然，把目标函数设为： \\[ \\prod_{w \\in C} p(w \\mid C_w) \\] 一般使用最大对数似然，则目标函数为： \\[ L = \\sum_{w \\in C} \\log p(w \\mid C_w) \\] 其实概率\\(P(w \\mid C_w)\\)是关于\\(w\\)和\\(C_w\\)的函数，其中\\(\\theta\\)是待定参数集，就是要求最优 \\(\\theta^*\\)，来确定函数\\(F\\)： \\[ p(w \\mid C_w) = F(w, C_w; \\; \\theta) \\] 有了函数\\(F\\)以后，就能够直接算出所需要的概率。 而F的构造，就是通过神经网络去实现的。 神经概率语言模型 一个二元对\\((C_w, w)\\)就是一个训练样本。神经网络结构如下，\\(W, U\\)是权值矩阵，\\(p, q\\)是对应的偏置。 但是一般会减少一层，如下图：（其实是去掉了隐藏层，保留了投影层，是一样的） 窗口大小是\\(m\\)，\\(\\rm{Context}(w)\\)包含\\(2m\\)个词汇，词向量长度是\\(k\\)。可以做拼接或者求和（下文是）。拼接得到长向量\\(2mk\\)， 在投影层得到\\(\\mathbf{x_w}\\)，然后给到隐藏层和输出层进行计算。 \\[ \\mathbf{z}_w = \\rm{tanh}(W\\mathbf{z}_w + \\mathbf{p}) \\;\\to \\;\\mathbf{y}_w = U \\mathbf{z}_w + \\mathbf{q} \\] 再对\\(\\mathbf{y}_w = (y_1, y_2, \\cdots, y_K)\\) 向量进行softmax即可得到所求得中心词汇的概率： \\[ p(w \\mid C_w) = \\frac{e^{y_{i_w}}}{\\sum_{i=1}^K e^{y_i}} \\] 优点 词语的相似性可以通过词向量来体现 自带平滑功能。N-Gram需要自己进行平滑。 词向量的理解 有两种词向量，一种是one-hot representation，另一种是Distributed Representation。one-hot太长了，所以DR中把词映射成为相对短的向量。不再是只有1个1（孤注一掷），而是向量分布于每一维中（风险平摊）。再利用欧式距离就可以算出词向量之间的相似度。 传统可以通过LSA（Latent Semantic Analysis）和LDA（Latent Dirichlet Allocation）来获得词向量，现在也可以用神经网络算法来获得。 可以把一个词向量空间向另一个词向量空间进行映射，就可以实现翻译。 Hierarchical Softmax 两种模型都是基于下面三层模式（无隐藏层），输入层、投影层和输出层。没有hidden的原因是据说是因为计算太多了。 CBOW和Skip-gram模型： CBOW模型 一共有\\(\\left| C \\right|\\)个单词。CBOW是基于上下文\\(context(w) = c_w\\)去预测目标单词\\(w\\)，求条件概率\\(p(w \\mid c_w)\\)，语言模型一般取目标函数为对数似然函数： \\[ L = \\sum_{w \\in C} \\log p(w \\mid c_w) \\] 窗口大小设为\\(m\\)，则\\(c_w\\)是\\(w\\)的前后m个单词。 输入层 是上下文单词的词向量。（初始随机，训练过程中逐渐更新） 投影层 就是对上下文词向量进行求和，向量加法。得到单词\\(w\\)的所有上下文词\\(c_w\\)的词向量的和\\(\\mathbf{x}_w\\)，待会儿参数更新的时候再依次更新回来。 输出层 从\\(C\\)中选择一个词语，实际上是多分类。这里是哈夫曼树层次softmax。 因为词语太多，用softmax太慢了。多分类实际上是多个二分类组成的，比如SVM二叉树分类： 这是一种二叉树结构，应用到word2vec中，被称为Hierarchical Softmax。CBOW完整结构如下： 每个叶子节点代表一个词语\\(w\\)，每个词语被01唯一编码。 哈夫曼编码 哈夫曼树很简单。每次从许多节点中，选择权值最小的两个合并，根节点为合并值；依次循环，直到只剩一棵树。 比如“我 喜欢 看 巴西 足球 世界杯”，这6个词语，出现的次数依次是15, 8, 6, 5, 3, 1。建立得到哈夫曼树，并且得到哈夫曼编码，如下： CBOW足球例子 引入一些符号： \\(p^w\\) ：从根节点到达\\(w\\)叶子节点的路径 \\(l^w\\) ： 路径\\(p^w\\)中节点的个数 \\(p^w_1, \\cdots, p^w_{l_w}\\) ：依次代表路径中的节点，根节点-中间节点-叶子节点 \\(d^w_2, \\cdots, d^w_{l^w} \\in \\{0, 1\\}\\)：词\\(w\\)的哈夫曼编码，由\\(l^w-1\\)位构成， 根节点无需编码 \\(\\theta_1^w, \\cdots, \\theta^w_{l^w -1}\\)：路径中非叶子节点对应的向量， 用于辅助计算。 单词\\(w\\)是足球，对应的所有上下文词汇是\\(c_w\\)， 上下文词向量的和是\\(\\mathbf{x}_w\\) 看一个例子： 约定编码为1是负类，为0是正类。即左边是负类，右边是正类。 每一个节点就是一个二分类器，是逻辑回归(sigmoid)。其中\\(\\theta\\)是对应的非叶子节点的向量，一个节点被分为正类和负类的概率分别如下： \\[ \\sigma(\\mathbf{x}_w^T \\theta) = \\frac {1}{ 1 + e^{-\\mathbf{x}_w^T \\theta}}, \\quad 1 - \\sigma(\\mathbf{x}_w^T \\theta) \\] 那么从根节点到达足球的概率是： \\[ p (足球 \\mid c_{足球}) = \\prod_{j=2}^5 p(d_j^w \\mid \\mathbf{x}_w, \\theta_{j-1}^w) \\] CBOW总结 目标函数 从根节点到每一个单词\\(w\\)都存在一条路径\\(p^w\\)，路径上有\\(l^w-1\\)个分支节点，每个节点就是一个二分类，每次产生一个概率 \\(p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1})\\)， 把这些概率乘起来就得到了\\(p(w \\mid c_w)\\)。 其中每个节点的概率是，与各个节点的参数和传入的上下文向量和\\(\\mathbf{x}_w\\)相关。 \\[ p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1}) = \\begin{cases} &amp; \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}), &amp; d_j^w = 0 \\\\ &amp; 1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}), &amp; d_j^w = 1\\\\ \\end{cases} \\] 写成指数形式是 \\[ p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1}) = [\\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{1-d_j^w} \\cdot [1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{d_j^w} \\] 则上下文推中间单词的概率，即目标函数： \\[ p(w \\mid c_w) = \\prod_{j=2}^{l^w} p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1}) \\] 对数似然函数 对目标函数取对数似然函数是： \\[ \\begin{align} L &amp; = \\sum_{w \\in C} \\log p(w \\mid c_w) = \\sum_{w \\in C} \\log \\prod_{j=2}^{l^w} [\\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{1-d_j^w} \\cdot[1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{d_j^w} \\\\ &amp; = \\sum_{w \\in C} \\sum_{j=2}^{l^w} \\left( (1-d_j^w) \\cdot \\log \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}) + d_j^w \\cdot \\log (1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})) \\right) \\\\ &amp; = \\sum_{w \\in C} \\sum_{j=2}^{l^w} \\left( (1-d_j^w) \\cdot \\log A + d_j^w \\cdot \\log (1 -A)) \\right) \\end{align} \\] 简写： \\[ \\begin{align} &amp; L(w, j) = (1-d_j^w) \\cdot \\log \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}) + d_j^w \\cdot \\log (1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})) \\\\ &amp; L = \\sum_{w,j} L(w, j) \\end{align} \\] 怎样最大化对数似然函数呢，可以最大化每一项，或者使整体最大化。尽管最大化每一项不一定使整体最大化，但是这里还是使用最大化每一项\\(L(w, j)\\)。 sigmoid函数的求导： \\[ \\sigma ^{\\prime}(x) = \\sigma(x)(1 - \\sigma(x)) \\] \\(L(w, j)\\)有两个参数：输入层的\\(\\mathbf{x}_w\\) 和 每个节点的参数向量\\(\\theta_{j-1}^w\\) 。 分别求偏导并且进行更新参数： \\[ \\begin{align} &amp; \\frac{\\partial}{\\theta_{j-1}^w} L(w, j) = [1 - d_j^w - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})] \\cdot \\mathbf{x}_w \\quad \\to \\quad \\theta_{j-1}^w = \\theta_{j-1}^w + \\alpha \\cdot \\frac{\\partial}{\\theta_{j-1}^w} L(w, j) \\\\ &amp; \\frac{\\partial}{\\mathbf{x}_w} L(w, j) = [1 - d_j^w - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})] \\cdot \\theta_{j-1}^w \\quad \\to \\quad v(\\hat w)+= v(\\hat w) + \\alpha \\cdot \\sum_{j=2}^{l^w} \\frac{\\partial}{\\mathbf{x}_w} L(w, j), \\hat w \\in c_w \\\\ \\end{align} \\] 注意：\\(\\mathbf{x}_w\\)是所有上下文词向量的和，应该把它的更新平均更新到每个上下文词汇中去。\\(\\hat w\\) 代表\\(c_w\\)中的一个词汇。 Skip-Gram模型 Skip-gram模型是根据当前词语，预测上下文。网络结构依然是输入层、投影层(其实无用)、输出层。如下： 输入一个中心单词的词向量\\(v(w)\\)，简记为\\(v_w\\)，输出是一个哈夫曼树。单词\\(u\\)是\\(w\\)的上下文单词\\(c_w\\)中的一个。这是一个词袋模型，每个\\(u\\)是互相独立的。 目标函数 所以\\(c_w\\)是\\(w\\)的上下文词汇的概率是： \\[ p(c_w \\mid w) = \\prod_{u \\in c_w} p(u \\mid w) \\] 与上面同理，\\(p(u \\mid w)\\) 与传入的中心单词向量\\(v(w)\\)和路径上的各个节点相关： \\[ \\begin{align} &amp; p(u \\mid w) = \\prod_{j=2}^{l^w} p(d_j^u \\mid v_w,\\; \\theta^u_{j-1}) \\\\ &amp; p(d_j^u \\mid v_w ,\\; \\theta^u_{j-1} ) = [\\sigma(v_w^T \\theta^u_{j-1})]^{1-d_j^u} \\cdot [1 - \\sigma(v_w^T \\theta^u_{j-1})]^{d_j^u} \\\\ \\end{align} \\] 下文\\(v_w^T \\theta^w_{j-1}\\)简记为\\(v_w \\theta_{j-1}^w\\)，要记得转置向量相乘就可以了。 对数似然函数 \\[ \\begin{align} L &amp; = \\sum_{w \\in C} \\log p(c_w \\mid w) \\\\ &amp; = \\sum_{w \\in C} \\log \\prod_{u \\in c_w} \\prod _{j=2}^{l^w} [\\sigma(v_w^T \\theta^u_{j-1})]^{1-d_j^u} \\cdot [1 - \\sigma(v_w^T \\theta^u_{j-1})]^{d_j^u} \\\\ &amp; = \\sum_{w \\in C} \\sum_{u \\in c_w} \\sum_{j=2}^{l^w} \\left( (1-d_j^u) \\cdot \\log \\sigma(v_w^T \\theta^u_{j-1}) + d_j^u \\cdot \\log (1 - \\sigma(v_w^T \\theta^u_{j-1})) \\right) \\\\ \\end{align} \\] 同样，简写每一项为\\(L(w, u, j)\\) \\[ L(w, u, j) = (1-d_j^u) \\cdot \\log \\sigma(v_w^T \\theta^u_{j-1}) + d_j^u \\cdot \\log (1 - \\sigma(v_w^T \\theta^u_{j-1})) \\] 然后就是，分别对\\(v_w\\)和\\(\\theta_{j-1}^u\\)求梯度更新即可，同上面的类似。得到下面的更新公式 \\[ \\begin{align} &amp; \\theta_{j-1}^u = \\theta_{j-1}^u + \\alpha \\cdot [1 - d_j^u - \\sigma(v_w^t \\cdot \\theta_{j-1}^u)] \\cdot v(w) \\\\ &amp; v_w = v_w + \\alpha \\cdot \\sum_{u \\in c_w} \\sum_{j=2}^{l^w} \\frac{\\partial L(w, u, j)}{\\partial v_w} \\\\ \\end{align} \\] Negative Sampling 背景知识介绍 Negative Sampling简称NEG，是Noise Contrastive Estimation(NCE)的一个简化版本，目的是用来提高训练速度和改善所得词向量的质量。 NEG不使用复杂的哈夫曼树，而是使用随机负采样，大幅度提高性能，是Hierarchical Softmax的一个替代。 NCE 细节有点复杂，本质上是利用已知的概率密度函数来估计未知的概率密度函数。简单来说，如果已知概率密度X，未知Y，如果知道X和Y的关系，Y也就求出来了。 在训练的时候，需要给正例和负例。Hierarchical Softmax是把负例放在二叉树的根节点上，而NEG，是随机挑选一些负例。 CBOW 对于一个单词\\(w\\)，输入上下文\\(\\rm{Context}(w) = C_w\\)，输出单词\\(w\\)。那么词\\(w\\)是正样本，其他词都是负样本。 负样本很多，该怎么选择呢？后面再说。 定义\\(\\rm{Context}(w)\\)的负样本子集\\(\\rm{NEG}(w)\\)。对于样本\\((C_w, w)\\)，\\(\\mathbf{x}_w\\)依然是\\(C_w\\)的词向量之和。\\(\\theta_u\\)为词\\(u\\)的一个（辅助）向量，待训练参数。 设集合\\(S_w = w \\bigcup NEG(w)\\) ，对所有的单词\\(u \\in S_w\\)，有标签函数： \\[ b^w(u) = \\begin{cases} &amp; 1, &amp; u = w \\\\ &amp; 0, &amp; u \\neq w \\\\ \\end{cases} \\] 单词\\(u\\)是\\(C_w\\) 的中心词的概率是： \\[ p(u \\mid C_w) = \\begin{cases} &amp; \\sigma(\\mathbf x_w^T \\theta^u), &amp; u=w \\; \\text{正样本} \\\\ &amp; 1 - \\sigma(\\mathbf x_w^T \\theta^u), &amp; u \\neq w \\; \\text{负样本} \\\\ \\end{cases} \\] 简写为： \\[ \\color{blue} {p(u \\mid C_w)} = [ \\sigma(\\mathbf x_w^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(\\mathbf x_w^T \\theta^u)]^{1 - b^w(u)} \\] 要最大化目标函数\\(g(w) = \\sum_{u \\in S_w} p(u \\mid C_w)\\)： \\[ \\begin{align} \\color{blue}{g(w) } &amp; = \\prod_{u \\in S_w} [ \\sigma(\\mathbf x_w^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(\\mathbf x_w^T \\theta^u)]^{1 - b^w(u)} \\\\ &amp;= \\color{blue} {\\sigma(\\mathbf x_w^T \\theta^u) \\prod_{u \\in NEG(w)} (1 - \\sigma(\\mathbf x_w^T \\theta^u)) } \\\\ \\end{align} \\] 观察\\(g(w)\\)可知，最大化就是要：增大正样本概率和减小化负样本概率。 每个词都是这样，对于整个语料库的所有词汇，将\\(g\\)累计得到优化目标，目标函数如下： \\[ \\begin {align} L &amp; = \\log \\prod_{w \\in C}g(w) = \\sum_{w \\in C} \\log g(w) \\\\ &amp; = \\sum_{w \\in C} \\log \\left( \\prod_{u \\in S_w} [ \\sigma(\\mathbf x_w^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(\\mathbf x_w^T \\theta^u)]^{1 - b^w(u)} \\right) \\\\ &amp;= \\sum_{w \\in C} \\sum_{u \\in S_w} \\left[ b^w_u \\cdot \\sigma(\\mathbf x_w^T \\theta^u) + (1-b^w_u) \\cdot (1 - \\sigma(\\mathbf x_w^T \\theta^u)) \\right] \\end {align} \\] 简写每一步\\(L(w, u)\\)： \\[ L(w, u) = b^w_u \\cdot \\sigma(\\mathbf x_w^T \\theta^u) + (1-b^w_u) \\cdot (1 - \\sigma(\\mathbf x_w^T \\theta^u)) \\] 计算\\(L(w, u)\\)对\\(\\theta^u\\)和\\(\\mathbf{x}_w\\)的梯度进行更新，得到梯度(对称性)： \\[ \\frac{\\partial L(w, u) }{ \\partial \\theta^u} = [b^w(u) - \\sigma(\\mathbf x_w^T \\theta^u)] \\cdot \\mathbf{x}_w, \\quad \\frac{\\partial L(w, u) }{ \\partial \\mathbf{x}_w} = [b^w(u) - \\sigma(\\mathbf x_w^T \\theta^u)] \\cdot \\theta^u \\] 更新每个单词的训练参数\\(\\theta^u\\) ： \\[ \\theta^u = \\theta^u + \\alpha \\cdot \\frac{\\partial L(w, u) }{ \\partial \\theta^u} \\] 对每个单词更新词向量\\(v(u)\\) ： \\[ v(u) = v(u) + \\alpha \\cdot \\sum_{u \\in S_w} \\frac{\\partial L(w, u) }{ \\partial \\mathbf{x}_w} \\] Skip-gram H给单词\\(w\\)，预测上下文向量\\(\\rm{Context}(w) = C_w\\)。 输入样本\\((w, C_w)\\)。 中心单词是\\(w\\)，遍历样本中的上下文单词\\(w_o \\in C_w\\)，为每个上下文单词\\(w_o\\)生成一个包含负采样的集合\\(S_o = w \\bigcup \\rm{NEG}(o)\\) 。即\\(S_o\\)里面只有\\(w\\)才是\\(o\\)的中心单词。 下面\\(w_o\\)简写为\\(o\\)，要注意实际上是当前中心单词\\(w\\)的上下文单词。 \\(S_o\\)中的\\(u\\)是实际的w就为1，否则为0。标签函数如下： \\[ b^w(u) = \\begin{cases} &amp; 1, &amp; u = w \\\\ &amp; 0, &amp; u \\neq w \\\\ \\end{cases} \\] \\(S_o​\\)中的\\(u​\\)是\\(o​\\)的中心词的概率是 \\[ p(u \\mid o) = \\begin{cases} &amp; \\sigma (v_o^T \\theta^u ), &amp; u=w \\; \\leftrightarrow \\; b^w(u) = 1 \\\\ &amp; 1 - \\sigma (v_o^T \\theta^u ), &amp;u \\neq w \\; \\leftrightarrow \\; b^w(u) = 0 \\\\ \\end{cases} \\] 简写为 \\[ \\color{blue} {p(u \\mid o)} = [ \\sigma(v_o^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(v_o^T \\theta^u)]^{1 - b^w(u)} \\] 对于\\(w\\)的一个上下文单词\\(o\\)来说，要最大化这个概率： \\[ \\prod_{u \\in S_o} p(u \\mid o ) \\] 对于\\(w\\)的所有上下文单词\\(C_w\\)来说，要最大化： \\[ g(w) = \\prod_{o \\in C_w} \\prod_{u \\in S_o} p(u \\mid o ) \\] 那么，对于整个预料，要最大化： \\[ G = \\prod_{w \\in C} g(w) =\\prod_{w \\in C} \\prod_{o \\in C_w} \\prod_{u \\in S_o} p(u \\mid o ) \\] 对G取对数，最终的目标函数就是： \\[ \\begin {align} L &amp; = \\log G = \\sum_{w \\in C} \\sum_{o \\in C_w} \\log \\prod_{u \\in S_o} p(u \\mid o ) \\\\ &amp;= \\sum_{w \\in C} \\sum_{o \\in C_w} \\log \\prod_{u \\in S_o} [ \\sigma(v_o^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(v_o^T \\theta^u)]^{1 - b^w(u)} \\\\ &amp; = \\sum_{w \\in C} \\sum_{o \\in C_w} \\sum_{u \\in S_o} \\left ( b^w_u \\cdot \\sigma(v_o^T \\theta^u) + (1 - b^w_u) \\cdot (1-\\sigma(v_o^T \\theta^u)) \\right) \\end{align} \\] 取\\(w, o, u\\)简写L(w, o, u)： \\[ L(w, o, u) = b^w_u \\cdot \\sigma(v_o^T \\theta^u) + (1 - b^w_u) \\cdot (1-\\sigma(v_o^T \\theta^u)) \\] 分别对\\(\\theta^u、v_o\\)求梯度 \\[ \\frac{\\partial L(w, o, u) }{ \\partial \\theta^u} = [b^w_u - \\sigma(v_o^T \\theta^u)] \\cdot v_o, \\quad \\frac{\\partial L(w,o, u) }{ \\partial v_o} = [b^w_u - \\sigma(v_o^T \\theta^u)] \\cdot \\theta^u \\] 更新每个单词的训练参数\\(\\theta^u\\) ： \\[ \\theta^u = \\theta^u + \\alpha \\cdot \\frac{\\partial L(w, o,u) }{ \\partial \\theta^u} \\] 对每个单词更新词向量\\(v(o)\\) ： \\[ v(o) = v(o) + \\alpha \\cdot \\sum_{u \\in S_o} \\frac{\\partial L(w, u) }{ \\partial v_o} \\]","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"}]},{"title":"Word2vec之公式推导笔记","date":"2017-11-02T01:33:45.000Z","path":"2017/11/02/cs224n-lecture2-word2vec/","text":"cs224n word2vec 简介和公式推导 Word meaning 词意 词的意思就是idea，如下： 词汇本身表达的意义 人通过词汇传达的想法 在写作、艺术中表达的意思 signifier - signified(idea or thing) - denotation 传统离散表达 传统使用分类学去建立一个WordNet，其中包含许多上位词is-a和同义词集等。如下： 上义词 同义词 entity, physical_entity,object, organism, animal full, good; estimable, good, honorable, respectable 离散表达的问题： 丢失了细微差别，比如同义词：adept, expert, good, practiced, proficient, skillful 不能处理新词汇 分类太主观 需要人力去构建和修改 很难去计算词汇相似度 每个单词使用one-hot编码，比如hotel=\\([0, 1, 0, 0, 0]\\)，motel=\\([0, 0, 1, 0, 0]\\)。 当我搜索settle hotel的时候也应该去匹配包含settle motel的文章。 但是我们的查询hotel向量和文章里面的motel向量却是正交的，算不出相似度。 分布相似表达 通过一个单词的上下文去表达这个单词。 You shall know a word by the company it keeps. --- JR. Firth 例如，下面用周围的单词去表达banking ： government debt problems turning into banking crises as has happened in ​ saying that Europe needs unified banking regulation to replace the hodgepodge 稠密词向量 一个单词的意义应该是由它本身的词向量来决定的。这个词向量可以预测出的上下文单词。 比如lingustics的词向量是\\([0.286, 0.792, -0.177, -0.107, 0.109, -0.542, 0.349]\\) 词嵌入思想 构建一个模型，根据中心单词\\(w_t\\)，通过自身词向量，去预测出它的上下文单词。 \\[ p (context \\mid w_t) = \\cdots \\] 损失函数如下，\\(w_{-t}\\)表示\\(w_t\\)的上下文（负号通常表示除了某某之外），如果完美预测，损失函数为0。 \\[ J = 1 - p(w_{-t} \\mid w_t) \\] Word2Vec 在每个单词和其上下文之间进行预测。 有两种算法： Skip-grams(SG)： 给目标单词，预测上下文 Continuous Bag of Words(CBOW)：给上下文，预测目标单词 两个稍微高效的训练方法： 分层softmax 负采样 课上只是Naive softmax。两个模型，两种方法，一共有4种实现。这里是word2vec详细信息。 Skip-gram 对于每个单词\\(w_t\\)，会选择一个上下文窗口\\(m\\)。 然后要预测出范围内的上下文单词，使概率\\(P(w_{t+i} \\mid w_t)\\)最大。 目标函数 \\(\\theta\\)是我们要训练的参数，目标函数就是所有位置预测结果的乘积，最大化目标函数： \\[ J^\\prime (\\theta) = \\prod_{t=1}^T \\prod_{-m\\le j \\le m} p(w_{t+j} \\mid w_t ;\\; \\theta), \\quad t \\neq j \\] 一般使用negative log likelihood ：负采样教程。 要最大化目标函数，就得得到损失函数。对于对数似然函数，取其负对数就可以得到损失函数，再最小化损失函数，其中\\(T\\)是文本长度，\\(m\\)是窗口大小： \\[ J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^T \\sum_{-m\\le j \\le m} \\log P(w_{t+j} \\mid w_t) \\] Loss 函数 = Cost 函数 = Objective 函数 对于softmax概率分布，一般使用交叉熵作为损失函数 单词\\(w_{t+j}\\)是one-hot编码 negative log probability Word2vec细节 词汇和词向量符号说明： \\(u\\) 上下文词向量，向量是\\(d\\)维的 \\(v\\) 词向量 中心词汇\\(t\\)，对应的向量是\\(v_t\\) 上下文词汇\\(j\\) ，对应的词向量是\\(u_j\\) 一共有\\(V\\)个词汇 计算\\(p(w_{t+j} \\mid w_t)\\)， 即： \\[ p(w_{j} \\mid w_t) = \\mathrm{softmax} (u_j^T \\cdot v_t) = \\frac{\\exp(u_j^T \\cdot v_t)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_t)} \\] 两个单词越相似，点积越大，向量点积如下： \\[ u^T \\cdot v = \\sum_{i=1}^M u_i \\times v_i \\] softmax之所以叫softmax，是因为指数会让大的数越大，小的数越小。类似于max函数。下面是计算的详细信息： 一些理解和解释： \\(w_t\\)是one-hot编码的中心词汇，维数是\\((V, 1)\\) \\(W\\)是词汇表达矩阵，维数是\\((d, V)\\)，一列就是一个单词 \\(Ww_t = v_t\\) 相乘得到词向量\\(v_t\\) ，\\((d, V) \\cdot (V, 1) \\to (d, 1)\\)， 用\\(d\\)维向量去表达了词汇t \\(W^\\prime\\)， \\(W^{\\prime}\\cdot v_t = s\\)，\\((V, d) \\cdot (d, 1) \\to (V, 1)\\) ， 得到 语义相似度向量\\(s\\) 再对\\(s\\)进行softmax即可求得上下文词汇 每个单词有两个向量，作为center单词向量和context单词向量 偏导计算 设\\(o\\)是上下文单词，\\(c\\)是中心单词，条件概率如下： \\[ P(o \\mid c) = \\frac{\\exp(u_o^T \\cdot v_c)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} \\] 这里只计算\\(\\log P\\)对\\(v_c\\)向量的偏导。 用\\(\\mathbf{\\theta}\\)向量表示所有的参数，有\\(V\\)个单词，\\(d\\)维向量。每个单词有2个向量。参数个数一共是\\(2dV\\)个。 向量偏导计算公式，\\(\\mathbf{x, a}\\) 均是向量 \\[ \\frac {\\partial \\mathbf{x}^T \\mathbf{a}} { \\partial \\mathbf{x}} = \\frac {\\partial \\mathbf{a}^T \\mathbf{x}} { \\partial \\mathbf{x}} = \\mathbf{a} \\] 函数偏导计算，链式法则，\\(y=f(u), u=g(x)\\) \\[ \\frac{\\mathrm{d}y}{\\mathrm{d} x} = \\frac{\\mathrm{d}y}{\\mathrm{d} u} \\frac{\\mathrm{d}u}{\\mathrm{d} x} \\] 最小化损失函数： \\[ J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^T \\sum_{-m\\le j \\le m} \\log P(w_{t+j} \\mid w_t), \\quad j \\neq m \\] 这里只计算\\(v_c\\)的偏导，先进行分解原式为2个部分： \\[ \\frac { \\partial} {\\partial v_c} \\log P(o \\mid c) = \\frac { \\partial} {\\partial v_c} \\log \\frac{\\exp(u_o^T \\cdot v_c)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} = \\underbrace { \\frac { \\partial} {\\partial v_c} \\log \\exp (u_o^T \\cdot v_c) }_{1} - \\underbrace { \\frac { \\partial} {\\partial v_c} \\log \\sum_{i=1}^V \\exp(u_i^T \\cdot v_c) }_{2} \\] 部分1推导 \\[ \\begin{align} \\frac { \\partial} {\\partial v_c} \\color{red}{\\log \\exp (u_o^T \\cdot v_c) } &amp; = \\frac { \\partial} {\\partial v_c} \\color{red}{u_o^T \\cdot v_c} = \\mathbf{u_o} \\end{align} \\] 部分2推导 \\[ \\begin{align} \\frac { \\partial} {\\partial v_c} \\log \\sum_{i=1}^V \\exp(u_i^T \\cdot v_c) &amp; = \\frac{1}{\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} \\cdot \\color{red}{ \\frac { \\partial} {\\partial v_c} \\sum_{x=1}^V \\exp(u_x^T \\cdot v_c)} \\\\ &amp; = \\frac{1}{A} \\cdot \\sum_{x=1}^V \\color{red} {\\frac { \\partial} {\\partial v_c} \\exp(u_x^T \\cdot v_c)} \\\\ &amp; = \\frac{1}{A} \\cdot \\sum_{x=1}^V \\exp (u_x^T \\cdot v_c) \\color{red} {\\frac { \\partial} {\\partial v_c} u_x^T \\cdot v_c} \\\\ &amp; = \\frac{1}{\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} \\cdot \\sum_{x=1}^V \\exp (u_x^T \\cdot v_c) \\color{red} {u_x} \\\\ &amp; = \\sum_{x=1}^V \\color{red} { \\frac{\\exp (u_x^T \\cdot v_c)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)}} \\cdot u_x \\\\ &amp; = \\sum_{x=1}^V \\color{red} {P(x \\mid c) }\\cdot u_x \\end{align} \\] 所以，综合起来可以求得，单词o是单词c的上下文概率\\(\\log P(o \\mid c)\\) 对center向量\\(v_c\\)的偏导： \\[ \\frac { \\partial} {\\partial v_c} \\log P(o \\mid c) = u_o -\\sum_{x=1}^V P(x \\mid c) \\cdot u_x = \\color{blue} {\\text{观察到的} - \\text{期望的}} \\] 实际上偏导是，单词\\(o\\)的上下文词向量，减去，所有单词\\(x\\)的上下文向量乘以x作为\\(c\\)的上下文向量的概率。 总体梯度计算 在一个window里面，对中间词汇\\(v_c\\)求了梯度， 然后再对各个上下文词汇\\(u_o\\)求梯度。 然后更新这个window里面用到的参数。 比如句子We like learning NLP。设\\(m=1\\)： 中间词汇求梯度 \\(v_{like}\\) 上下文词汇求梯度 \\(u_{we}\\) 和 \\(u_{learning}\\) 更新参数 梯度下降 有了梯度之后，参数减去梯度，就可以朝着最小的方向走了。机器学习梯度下降 \\[ \\theta^{new} = \\theta^{old} - \\alpha \\frac{\\partial}{\\partial \\theta^{old}} J(\\theta), \\quad \\quad \\theta^{new} = \\theta^{old} - \\alpha \\Delta_{\\theta} J(\\theta) \\] 随机梯度下降 预料会有很多个window，因此每次不能更新所有的。只更新每个window的，对于window t： \\[ \\theta^{new} = \\theta^{old} - \\alpha \\Delta_{\\theta} J_t(\\theta) \\]","tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"}]},{"title":"subword-units","date":"2017-10-19T14:16:07.000Z","path":"2017/10/19/subword-units/","text":"Subword策略，很有用的论文。BERT也使用到了，有对应的tokenization.py Neural Machine Translation of Rare Words with Subword Units 背景 摘要 NMT处理的词汇表是定长的，但是实际翻译却是OOV(out of vocabulary)的。以前是把 新词汇加到词典里去。本文是提出一种subword单元的策略，会把稀有和未知词汇以subword units序列来进行编码，更简单更有效。 会介绍不同分词技术的适用性，包括简单的字符n元模型和基于字节对编码压缩算法的分词技术。也会以经验说明subword模型比传统back-off词典的方法好。 简介 NMT模型的词汇一般是30000-5000，但是翻译却是open-vocabulary的问题。很多语言富有信息创造力，比如凝聚组合等等，翻译系统就需要一种低于word-level的机制。 Word-level NMT的缺点 对于word-level的NMT模型，翻译out-of-vocabulary的单词会回退到dictionary里面去查找。有下面几个缺点 种技术在实际上使这种假设并不成立。比如源单词和目标单词并不是一对一的，你怎么找呢 不能够翻译或者产生未见单词 把unknown单词直接copy到目标句子中，对于人名有时候可以。但是有时候却需要改变形态或者直译。 Subword-level NMT 我们的目标是建立open-vocabulary的翻译模型，不用针对稀有词汇去查字典。事实证明，subword模型效果比传统大词汇表方法更好、更精确。Subword神经网络模型可以从subword表达中学习到组合和直译等能力，也可以有效的产生不在训练数据集中的词汇。本文主要有下面两个贡献 open-vocabulary的问题可以通过对稀有词汇使用subword units单元来编码解决 采用Byte pair encoding (BPE) 算法来进行分割。BPE通过一个固定大小的词汇表来表示开放词汇，这个词汇表里面的是变长的字符串序列。这是一种对于神经网络模型非常合适的词分割策略。 神经机器翻译 NMT是使用的Bahdanau的Attention模型。Encoder是双向RNN，输入\\(X=(x_1, x_2, \\cdots, x_m)\\)，会把两个方向的隐状态串联起来得到annotation向量\\(\\mathbf x\\)。实际上是一个矩阵，对于单个\\(x_j\\)来说，对应的注释向量是\\(\\mathbf{x}_j\\)。 Decoder是一个单向的RNN，预测\\(Y=(y_1, y_2, \\cdots, y_n)\\)。 预测\\(y_i\\) 时，需要： 当前的隐状态\\(s_i\\) 上一时刻的输出\\(y_{i-1}\\)作为当前的输入 语义向量\\(\\mathbf c_i\\) 。语义向量是由所有的注释向量\\(x_j\\) 加权求和得到的。权就是对齐概率\\(\\alpha_{ij}\\)。 即\\(\\mathbf c_i = \\sum_{j=1} ^ m \\alpha_{ij} \\mathbf x_j\\) 详情请看谷歌论文里面的介绍或者Bahdanau的论文。 Subword 翻译 下面词汇的翻译是透明的(transparent，明显的) 命名实体。如果两个语言的字母表相同，可以直接copy到目标句子中去，也可以抄写音译直译等。 同源词和外来词。有着共同的起源，但是不同的语言表达形式不同，所以character-level翻译规则就可以了。 形态复杂的词语。包含多个语素的单词，可以通过单独翻译语素来翻译。 总之，通过subword单元表示稀有词汇对于NMT来说可以学到transparent翻译，并且可以翻译和产生未见词汇。 相关工作 对于SMT(Statistical Machine Translation)来说，翻译未见单词一直是研究的主题。 很多未见单词都是人名，如果两种语言的字母表一样，那么可以直接复制过去。如果不一样，那么就得音译过去。基于字符（character-based）的翻译是比较成功的。 形态上很复杂的单词往往需要分割，这里有很多的分割算法。基于短语的SMT的分割算法是比较保守的。而我们需要积极的细分，让网络可以处理open-vocabulary，而不是去求助于背字典。 怎么选择subword units要看具体的任务。 提出了很多这样的技术：生成基于字符或者基于语素的定长的连续的词向量。于此同时，word-based的方法并没有重大发现。现在的注意力机制还是基于word-level的。我们希望，注意力机制能从我们变长表达中收益：网络可以把注意力放在不同的subword units中。 这可以突破定长表达的信息传达瓶颈。 NMT减少词汇表可以大大节省时间和增加空间效率。我们也想要对一个句子更紧凑的表达。因为文本长度增加了，会减少效率，也会增加模型传递信息的距离。(hidden size？) 权衡词汇表大小和文本长度，可以用未分割单词列表，subword 单元表达的稀有词汇。作为一个代替，Byte pair encoding就是这样的一种分割算法，可以学到一个词汇表，同时对文本有很好的压缩率。 Byte Pair Encoding Byte pair encoding是一种简单的数据压缩技术，它把句子中经常出现的字节pairs用一个没有出现的字节去替代。我们使用这种算法去分割单词，但我们合并字符或者字符序列。 算法步骤 算法步骤如下： 初始化符号词表。用所有的字符加入到符号词表中。对所有单词的末尾加入特殊标记，如-。翻译后恢复原始的标记。 迭代对所有符号进行计数，找出次数最多的(A, B)，用AB代替。 每次合并，会产生一个新的符号，代表着n-gram字符 常见的n-grams字符(或者whole words)，最终会被合并到一个符号 最终符号词表大小=初始大小+合并操作次数。操作次数是算法唯一的超参数。 不用考虑不在训练集里面的pair，为每个word根据出现频率设置权重。 和传统的压缩算法(哈夫曼编码)相比，我们的以subword 单元堆积的符号序列依然是可以解释的，网络也可以翻译和产生新的词汇（训练集没有见过的）。 下面是代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def process_raw_words(words, endtag='-'): '''把单词分割成最小的符号，并且加上结尾符号''' vocabs = &#123;&#125; for word, count in words.items(): # 加上空格 word = re.sub(r'([a-zA-Z])', r' \\1', word) word += ' ' + endtag vocabs[word] = count return vocabsdef get_symbol_pairs(vocabs): ''' 获得词汇中所有的字符pair，连续长度为2，并统计出现次数 Args: vocabs: 单词dict，(word, count)单词的出现次数。单词已经分割为最小的字符 Returns: pairs: ((符号1, 符号2), count) ''' #pairs = collections.defaultdict(int) pairs = dict() for word, freq in vocabs.items(): # 单词里的符号 symbols = word.split() for i in range(len(symbols) - 1): p = (symbols[i], symbols[i + 1]) pairs[p] = pairs.get(p, 0) + freq return pairsdef merge_symbols(symbol_pair, vocabs): '''把vocabs中的所有单词中的'a b'字符串用'ab'替换 Args: symbol_pair: (a, b) 两个符号 vocabs: 用subword(symbol)表示的单词，(word, count)。其中word使用subword空格分割 Returns: vocabs_new: 替换'a b'为'ab'的新词汇表 ''' vocabs_new = &#123;&#125; raw = ' '.join(symbol_pair) merged = ''.join(symbol_pair) # 非字母和数字字符做转义 bigram = re.escape(raw) p = re.compile(r'(?&lt;!\\S)' + bigram + r'(?!\\S)') for word, count in vocabs.items(): word_new = p.sub(merged, word) vocabs_new[word_new] = count return vocabs_newraw_words = &#123;\"low\":5, \"lower\":2, \"newest\":6, \"widest\":3&#125;vocabs = process_raw_words(raw_words)num_merges = 10print (vocabs)for i in range(num_merges): pairs = get_symbol_pairs(vocabs) # 选择出现频率最高的pair symbol_pair = max(pairs, key=pairs.get) vocabs = merge_symbols(symbol_pair, vocabs)print (vocabs) 单独BPE 为目标语言和原语言分别使用BPE去计算词典。从文本和词汇表大小来说更加紧凑，能保证每个subword单元在各自的训练数据上都有。同样的名字在不同的语言中可能切割的不一样，神经网络很难去学习subword units之间的映射。 Joint BPE 为目标语言和原语言一起使用BPE，即联合两种语言的词典去做BPE。提高了源语言和目标语言的分割一致性。训练中一般concat两种语言。 评估 有两个重要问题 subword units表达稀有词汇，是否真的对翻译有效果？ 根据词汇表大小，文本长度，翻译质量，怎样分割才是最好的？ 我们的使用WMT2015的数据，使用BLEU来评判结果。英语-德语： 420万句子对 1亿个token 英语-俄罗斯语： 260万句子对 5000万个token minibatch-size是80，每个epoch都会reshuffle训练数据。训练了7天，每12个小时存一次模型，取最后4个模型再单独训练。分别选择clip 梯度是5.0和1.0，1.0效果好一些。最终是融合了8个模型。 Beam search的大小是12，使用双语词典进行快速对齐，类似于对稀有词汇查找词典，也会用词典去加速训练。 Subword统计 我们的目标是通过一个紧凑的固定大小的subword词典去代表一个open-vocabulary，并且能够有效的训练和解码。 一个简单的基准就是把单词分割成字符n-grams 。n的选择很重要，可以在序列长度(tokens)和词汇表大小(size)之间做一个权衡。序列的长度会增加许多，一个比较好得减少长度的方法就是使用k个最常见的未被分割的词列表。只有unigram(n=1，一元模型)表达才能真正实现open-vocabulary，但是实际上效果却并不好。Bigram效果好，但是不能产生测试集中的tokens。 BPE符合open-vocabulary的目标，并且合并操作可以应用于测试集，去发现未知符号的分割。与字符集模型的主要区别在于，BPE更紧凑的表示较短序列，注意力模型可以应对变长的单元。 分割方法 tokens types unk merge次数 BPE 112 m 63000 0 59500 BPE(joint) 111 m 82000 32 89500 实际上，NMT词汇表中，并不会包含不常见的subword单元，因为里面有很多噪声。 name seg shotlist s-v t-v S-BLEU BLEU CHAR-F3 CHAR-F3 F1 F1 F1 BPE-60k BPE 无 60000 60000 21.5 24.5 52.0 53.9 58.4 40.9 29.3 BPE-J60k BPE(joint) 无 90000 90000 22.8 24.7 51.7 54.1 58.5 41.8 33.6 翻译评估 所有的subword系统都不会去查字典。使用UNK表示模型词典以外的单词，OOV表示训练集里面没有的单词。","tags":[{"name":"NMT","slug":"NMT","permalink":"http://plmsmile.github.io/tags/NMT/"},{"name":"subword","slug":"subword","permalink":"http://plmsmile.github.io/tags/subword/"}]},{"title":"Wordpiece模型","date":"2017-10-19T06:41:00.000Z","path":"2017/10/19/26-wordpieacemodel/","text":"WordPiece模型，BERT也有用到。Japanese and Korean Voice Search 看了半天才发现不稳啊。 背景知识 摘要 这篇文章主要讲了构建基于日语和法语的语音搜索系统遇到的困难，并且提出了一些解决的方法。主要是下面几个方面： 处理无限词汇表的技术 在语言模型和词典的书面语中，完全建模并且避免系统复杂度 如何去构建词典、语言和声学模型 展示了由于模糊不清，多个script语言的打分结果的困难性。这些语言语音搜索的发展，大大简化了构建一门新的语言的语音搜索系统的最初的处理过程，这些很多都成为了语言搜索国际化的默认过程。 简介 语音搜索通过手机就可以访问到互联网，这对于一些不好输入字符的语言来说，非常有用。尽管从基础技术来讲，语音识别的技术是在不同的语言之间是非常相似的，但是许多亚洲语言面临的问题，如果只是用传统的英语的方法去对待，这根本很难解决嘛。许多亚洲语言都有非常大的字符库。这让发音词典就很复杂。在解码的时候，由于很多同音异义词汇，解码也会很复杂。基本字符集里面的很多字符都会以多种形式存在，还要数字也会有多种形式，在某些情况下，这都需要适当的标准化。 很多亚洲语言句子中没有空格去分割单词。需要使用segmenters去产生一些词单元。 这些词单元会在词典和语言模型中使用，词单元之间可能需要添加或者删除空白字符。我们开发了一个纯数据驱动的sementers，可以使用任何语言，不需要修改。 还有就是如何去处理英文中的许多词汇，比如URL、数字、日期、姓名、邮件、缩写词汇、标点符号和其它特殊词汇等等。 语音数据收集 公告开放的数据集很难用作商用，有很多限制，所以自己收集数据集。通过手机，从不同的地区、年龄、方言等等，收集数据。一般是尽可能使用这些原始的数据并且建模，而不是转化为书面的数据或者有利于英语的数据。 分词和词库 提出一种WordPieceModel去解决OOV(out-of-vocabulary)的问题。WordPieaceModel通过一种贪心算法，自动地、增量地从大量文本中学得单词单元（word units），一般数量是200k。算法可以，不关注语义，而去最大化训练数据语言模型的可能性，这也是解码过程中的度量标准。该算法可以有效地自动学习词库。 WordPieceModel算法步骤 1 初始化词库 给词库添加基本的所有的unicode字符和ascii字符。日语是22000，韩语是11000。 2 建立模型 基于训练数据，建立模型，使用初始化好的词库。 3 生成新单元 从词库中选择两个词单元组成新的词单元，加入到词库中。组成的新词要使模型的似然函数likelyhood最大。 4 继续加或者停止 如果达到词库数量的上限，或者似然函数增加很小，那么就停止，否则就继续2步，继续合并添加。 算法优化 你也发现了，计算所有可能的Pair这样会非常非常耗费时间。如果当前词库数量是\\(K\\)，那么每次迭代计算的复杂度是\\(O(K^2)\\) 。有下面3个步骤可以进行优化 选择组合新的单元时，只测试训练数据中有的单元。 只测试有很大机会成为最好的Pair，例如high priors 把一些不会影响到彼此的group pairs组合到一起，作为一个单一的迭代过程 only modify the language model counts for the affected entries （不懂什么意思） 使用这些加速算法，我们可以在一个机器上，几个小时以内，从频率加权查询列表中，构建一个200k的词库。 得到wordpiece词库之后，可以用来语言建模，做词典和解码。分割算法，构建了以基础字符开始的Pairs的逆二叉树。本身已经不需要动态规划或者其他的搜索方法。因此在计算上非常有效。分开基本的字符，基于树从上到下，会在线性时间给出一个确定的分割信息，线性时间取决于句子的长度。大约只有4%的单词具有多个发音。如果添加太多的发音会影响性能，可能是因为在训练和解码时对齐过程期间的可能数太多了 继续说明 一般是句子没有空格的，但是有的时候却有空格，比如韩文，搜索关键字。线上系统没有办法去把这些有空格的word pieces组合在一起。这对于常见的词汇和短查询是没有影响的，因为它们已经组合成一个完整的word unit。但是对于一些例如空格出现在不该出现的地方等不常见的查询，就很烦恼了。 在解码的时候，加空格效率更高，采用下面的技术： 1 原始语言模型数据被用来&quot;as written&quot;，表示一些有空格一些没有空格。 2 WPM模型分割LM数据时，每个单元在前面或者后面遇到一个空格，那么就添加一个空格标记。单元有4种情况：两边都有空格，左边有，右边有，两边都没有。使用下划线标记 3 基于这个新词库构建LM和词典 4 解码时，根据模型会选择一个最佳路径，之前在哪些地方放了空格或者没有。为了输出显示，需要把空格全部移除。有3种情况，移除所有空格；移除两个空格用一个空格表示；移除一个空格。","tags":[{"name":"WordPiece","slug":"WordPiece","permalink":"http://plmsmile.github.io/tags/WordPiece/"},{"name":"语音搜索","slug":"语音搜索","permalink":"http://plmsmile.github.io/tags/语音搜索/"},{"name":"语音识别","slug":"语音识别","permalink":"http://plmsmile.github.io/tags/语音识别/"}]},{"title":"循环神经网络","date":"2017-10-18T12:35:29.000Z","path":"2017/10/18/rnn/","text":"RNN, LSTM, GRU图文介绍，RNN梯度消失问题等 LSTM经典描述 经典RNN模型 模型 人类在思考的时候，会从上下文、从过去推断出现在的结果。传统的神经网络无法记住过去的历史信息。 循环神经网络是指随着时间推移，重复发生的结构。它可以记住之前发生的事情，并且推断出后面发生的事情。用于处理时间序列很好。所有的神经元共享权值。如下图所示。 记住短期信息 比如预测“天空中有__”，如果过去的信息“鸟”离当前位置比较近，则RNN可以利用这个信息预测出下一个词为“鸟” 不能长期依赖 如果需要的历史信息距离当前位置很远，则RNN无法学习到过去的信息。这就是不能长期依赖的问题。 LSTM总览与核心结构 LSTM可以记住一些记忆，捕获长依赖问题 也可以让ERROR根据输入，依照不同强度流动 见后面GRU解决梯度消失 总览 所有的RNN有着重复的结构，如下图，比如内部是一个简单的tanh 层。 LSTM也是一样的，只不过内部复杂一些。 单元状态 单元状态像一个传送带，通过整个链向下运行，只有一些小的线性作用。信息就沿着箭头方向流动。 LSTM的门结构 LSTM的门结构 可以添加或者删除单元状态的信息，去有选择地让信息通过。它由sigmoid网络层 和 点乘操作组成。输出属于\\([0, 1]\\)之间，代表着信息通过的比例。 LSTM细节解剖 一些符号说明，都是\\(t\\)时刻的信息 ： \\(C_{t-1}\\) : 的单元状态 \\(h_{t}\\) : 隐状态信息 （也作单个神经元的输出信息） \\(x_t\\) : 输入信息 \\(o_t\\) ：输出信息 （输出特别的信息） 1 遗忘旧信息 对于\\(C_{t-1}\\)中的每一个数字， \\(h_{t-1}\\)和\\(x_t\\)会输出0-1之间的数来决定遗忘\\(C_{t-1}\\)中的多少信息。 2 生成候选状态和它的更新比例 生成新的状态：tanh层创建新的候选状态\\(\\hat{C}_t\\) 输入门：决定新的状态哪些信息会被更新\\(i_t\\)，即候选状态\\(\\hat{C}_t\\)的保留比例。 3 新旧状态合并更新 生成新状态\\(C_t\\)：旧状态\\(C_{t-1}\\) + 候选状态\\(\\hat{C}_t\\)。 旧状态\\(C_{t-1}\\)遗忘不需要的， 候选状态\\(\\hat{C}_{t-1}\\)保留需要更新的，都是以乘积比例形式去遗忘或者更新。 4 输出特别的值 sigmoid：决定单元状态\\(C_t\\)的哪些信息要输出。 tanh: 把单元状态\\(C_t\\)的值变到\\([-1, 1]\\)之间。 LSTM总结 核心结构如下图所示 要忘掉部分旧信息，旧信息\\(C_{t-1}\\)的遗忘比例\\(f_t\\) \\[ f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f) \\] 新的信息来了，生成一个新的候选\\(\\hat{C}_t\\) \\[ \\hat{C}_t = \\tanh (W_C \\cdot [h_{t-1}, x_t] + b_C) \\] 新信息留多少呢，新候选\\(\\hat C_t\\)的保留比例\\(i_t\\) \\[ i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i) \\] 合并旧信息和新信息，生成新的状态信息\\(C_t\\) \\[ C_t = f_t * C_{t-1} + i_t * \\hat C_t \\] 输出多少呢，单元状态\\(C_t\\)的输出比例\\(o_t\\) \\[ o_t = \\sigma (W_o \\cdot [h_{t-1}, x_t] + b_o) \\] 把\\(C_t\\)化到\\([-1, 1]\\)再根据比例输出 \\[ h_t = o_t * \\tanh(C_t) \\] 图文简介描述LSTM 总体架构 单元架构 流水线架构 数据流动 圆圈叉叉代表着遗忘\\(C_{t-1}\\)的信息。乘以向量来实现，向量各个值在\\([0, 1]\\)之间。 靠近0就代表着遗忘很多，靠近1就代表着保留很多。 框框加号代表着数据的合并。旧信息\\(C_{t-1}\\)和新候选信息\\(\\hat C_t\\)的合并。 合并之后就得到新信息\\(C_t\\)。 遗忘门 上一个LSTM的输出\\(h_{t-1}\\) 和 当前的输入\\(x_t\\)，一起作为遗忘门的输入。 0是偏置\\(b_0\\)， 一起做个合并，再经过sigmoid生成遗忘权值\\(f_t\\)信息， 去遗忘\\(C_{t-1}\\)。 新信息门 新信息门决定着新信息对旧信息的影响力。和遗忘门一样\\(h_{t-1}\\)和\\(x_t\\)作为输入。 sigmoid：生成新信息的保留比例。tanh：生成新的信息。 新旧信息合并 旧信息\\(C_{t-1}\\)和新信息\\(\\hat{C}_t\\)合并，当然分别先过遗忘阀门和更新阀门。 输出特别的值 把新生成的状态信息\\(C_t\\)使用tanh变成\\((-1, 1)\\)之间，然后经过输出阀门进行输出。 LSTM变体 观察口连接 传统LSTM阀门值比例的计算，即更新、遗忘、输出的比例只和\\(h_{t-1}, x_t\\)有关。 观察口连接，把观察到的单元状态也连接sigmoid上，来计算。即遗忘、更新比例和\\(C_{t-1}, h_{t-1}, x_t\\)有关，输出的比例和\\(C_t, h_{t-1}, x_t\\)有关。 组队遗忘 如下图所示，计算好\\(C_{t-1}\\)的遗忘概率\\(i_t\\)后，就不再单独计算新候选\\(\\hat C_t\\)的保留概率\\(i_t\\)。而是直接由1减去遗忘概率得到更新概率。即\\(i_t = 1 - f_t\\)，再去更新。 GRU LSTM有隐状态\\(h_t\\)和输出状态\\(o_t\\)，而GRU只有\\(h_t\\)，即GRU的隐状态和输出状态是一样的，都用\\(h_t\\)表示。 更新门\\(z_t\\)负责候选隐层\\(\\hat h_t\\)保留的比例， \\(1-z_t\\)负责遗忘旧状态信息\\(h_{t-1}\\)的比例 \\[ z_t = \\sigma (W_z \\cdot [h_{t-1}, x_t]) \\] 候选隐藏层\\(\\hat h_t\\)的计算由\\(h_{t-1}\\)和\\(x_t\\)一起计算得到。所以计算\\(\\hat h_t\\)之前，要先计算\\(h_{t-1}\\)的重置比例。 重置门\\(r_t\\)负责\\(h_{t-1}\\)对于生成新的候选\\(\\hat h_t\\)的作用比例 \\[ r_t = \\sigma (W_r \\cdot [h_{t-1}, x_t]) \\] 新记忆\\(\\hat h_t\\)的计算 \\[ \\hat h_t = \\tanh (W \\cdot [r_t * h_{t-1}, x_t]) \\] 最终记忆\\(h_t\\)由\\(h_{t-1}\\)和\\(\\hat h_t\\)计算得到，分别的保留比例是\\(1-z_t\\)和\\(z_t\\) \\[ h_t = (1 - z_t) * h_{t-1} + z_t * \\hat h_t \\] 更新门 \\(z_t\\)：过去的信息有多重要。 \\(z=1\\)， 则过去信息非常重要，完全保留下来 重置门\\(r_t\\)： 旧记忆对新记忆的贡献程度。\\(r=0\\)， 则当前新记忆和旧记忆不想关。 RNN梯度问题 RNN梯度推导 简单点 \\[ \\begin {align} &amp; h_t = Wh_{t-1} + W^{(hx)} x_t \\\\ \\\\ &amp; \\hat y_t =W^{(s)} f(h_t) \\\\ \\\\ \\end{align} \\] 总的误差是之前每个时刻的误差之和 \\[ \\frac{\\partial E}{\\partial W} = \\sum_{t=1}^T \\frac{\\partial E_t}{\\partial W} \\] 每一时刻的误差又是之前每个时刻的误差之和，应用链式法则 \\[ \\frac{\\partial E_t}{\\partial W} = \\frac{\\partial E_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\sum_{k=1}^t \\frac{\\partial h_t}{\\partial h_k} \\frac{\\partial h_k}{\\partial W} \\] \\[ \\frac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t \\frac{\\partial E_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\color{blue} {\\frac{\\partial h_t}{\\partial h_k}} \\frac{\\partial h_k}{\\partial W} \\] \\[ \\frac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t \\frac{\\partial E_k}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k}\\frac{\\partial h_k}{\\partial h_{k-1}} \\frac{\\partial h_{k-1}}{\\partial W} \\] 而\\(\\frac{\\partial h_t}{\\partial h_k}\\)会变得非常大或者非常小！！ \\[ \\frac{\\partial h_t}{\\partial h_k} = \\prod_{j=k+1}^t \\frac{\\partial h_j}{\\partial h_{j-1}} = \\prod_{j=k+1}^t W^T \\times \\rm{diag}[f^{\\prime}(j_{j-1})] \\] 而导数矩阵雅克比矩阵 \\[ \\frac{\\partial h_j}{\\partial h_{j-1}} = [ \\frac{\\partial h_{j}}{\\partial h_{j-1,1}}, \\cdots , \\frac{\\partial h_{j}}{\\partial h_{j-1,d_h}}] = \\begin{bmatrix} \\frac{\\partial h_{j,1}}{\\partial h_{j-1,1}} &amp; \\cdots &amp; \\frac{\\partial h_{j,1}}{\\partial h_{j-1,d_h}} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial h_{j,d_h}}{\\partial h_{j-1,1}} &amp; \\cdots &amp; \\frac{\\partial h_{j,d_h}}{\\partial h_{j-1,d_h}} \\\\ \\end{bmatrix} \\] 合并起来，得到最终的 \\[ \\frac{\\partial E}{\\partial W} = \\sum_{t=1}^T\\sum_{k=1}^t \\frac{\\partial E_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} (\\prod_{j=k+1}^t \\frac{\\partial h_j}{\\partial h_{j-1}}) \\frac{\\partial h_k}{\\partial W} \\] 两个不等式 \\[ \\| \\frac{\\partial h_j}{\\partial h_{j-1}}\\| \\le \\| W^T\\| \\cdot \\|\\rm{diag}[f^{\\prime}(h_{j-1})] \\| \\le \\beta_W \\beta_h \\] 所以有，会变得非常大或者非常小。会产生梯度消失或者梯度爆炸问题。 \\[ \\| \\frac{\\partial h_t}{\\partial h_k} \\| =\\| \\prod_{j=k+1}^t \\frac{\\partial h_j}{\\partial h_{j-1}} \\| \\le \\color{blue}{ (\\beta_W \\beta_h)^{t-k}} \\] 梯度 是过去对未来影响力的一个度量方法。如果梯度消失了不确定\\(t\\)和\\(k\\)之间是否有关系，或者是因为参数错误 。 解决梯度爆炸 原始梯度 \\[ \\mathbf {\\hat g} =\\frac{\\partial E}{\\partial W} \\\\ \\\\ \\\\ \\] 如果\\(\\mathbf {\\hat g} &gt; 阈值\\)， 则更新 \\[ \\mathbf {\\hat g} = \\frac{\\text{threshold}}{\\|\\mathbf {\\hat g}\\|} \\mathbf {\\hat g} \\] GRU解决梯度消失 LSTM可以记住一些记忆，捕获长依赖问题 也可以让ERROR根据输入，依照不同强度流动 RNN的前向和反向传播，都会经过每一个节点 GRU可以自动地去创建一些短连接，也可以自动地删除一些不必要的连接。（门的功能） RNN会读取之前所有信息，并且更新所有信息。 GRU 选择可读部分，读取 选择可写部分，更新","tags":[{"name":"RNN","slug":"RNN","permalink":"http://plmsmile.github.io/tags/RNN/"},{"name":"LSTM","slug":"LSTM","permalink":"http://plmsmile.github.io/tags/LSTM/"},{"name":"GRU","slug":"GRU","permalink":"http://plmsmile.github.io/tags/GRU/"}]},{"title":"谷歌RNN翻译模型","date":"2017-10-17T05:25:38.000Z","path":"2017/10/17/25-google-nmt/","text":"谷歌神经机器翻译系统，Transformer之前最强的基于RNN的翻译模型 简介 神经机器翻译是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点 训练和翻译都太慢了，花费代价很大 缺乏鲁棒性，特别是输入句子包含生僻词汇 精确度和速度也不行 传统NMT缺点 神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。 NMT避开了传统基于短语的翻译模型的很多缺点。但是，在实际中，NMT的准确度要比基于短语的翻译模型更差些。 NMT有3个主要的缺点：训练和推理速度太慢，不能有效处理稀有词汇，有时不能完全翻译原句子。 训练和推理速度太慢 训练大数据集，需要大量时间和资源；反馈太慢周期太长。加了一个小技巧，看结果要等很长时间。推理翻译的时候，要使用大量的参数去计算，也很慢。 不能有效处理稀有词汇 有两个方法去复制稀有单词： 模仿传统对齐模型去训练1个copy model 使用注意力机制去复制 但是效果都不是很好，都不可靠，不同语言的对齐效果差；在网络很深的时候，注意力机制的对齐向量也不稳定。而且，简单的复制过去也不是最好的办法，比如需要直译的时候。 不能完整翻译整个句子 不能覆盖整个输入句子的内容，然后会导致一些奇怪的翻译结果。 GNMT的模型优点 采用的模型：深层LSTM 、Encoder8层、Decoder8层 。我的LSTM笔记。 各层之间使用残差连接促进梯度流，顶层Enocder到底层Decoder使用注意力连接，提高并行性。 进行翻译推断的时候，使用低精度算法，去加速翻译。 处理稀有词汇：使用sub-word单元，也称作wordpieces方法。把单词划分到有限的sub-word (wordpieces)单元集合，输入输出都这样。sub-word结合了字符分割模型的弹性和单词分割模型的效率。 Beam Search 使用长度规范化和覆盖惩罚。覆盖惩罚就是说，希望，翻译的结果句子，尽量多地包含输入句子中的所有词汇。 使用强化学习去优化模型， 优化翻译的BLEU 分数。 先进技术 有很多先进的技术来提高NMT，下面这些都有论文的。 利用attention去处理稀有词汇 建立翻译覆盖的机制 多任务和半监督训练，去合并使用更多数据 字符分割的encoder和decoder 使用subword单元处理稀疏的输出 系统架构 系统总览 架构 有3个模块：Encoder，Decoder，Attention。 Encdoer：把句子转换成一系列的向量，每一个向量代表一个输入词汇（符号）。 Decoder：根据这些向量，每一时刻会生成一个目标词汇，直到EOS。 Attention：连接Encoder和Decoder，在解码的过程中，可以让Decoder有权重的有选择的关注输入句子的部分区域。 符号说明 加粗小写代表向量，如\\(\\boldsymbol {v, o_i}\\) 加粗大写，矩阵，如\\(\\boldsymbol {U, W}\\) 和\\(\\mathbf{U, W}\\) 手写体，集合，如\\(\\mathcal{V, F}\\) 大写字母，句子，如\\(X, Y\\) 小写字母，单个符号，如\\(x_1, x_2\\) Encoder 输入句子和目标句子组成一个Pair \\((X, Y)\\)，其中输入句子\\(X = x_1, x_2, \\cdots, x_M\\) ，\\(M\\) 个单词，翻译的输出目标句子\\(Y = y_1, y_2, \\cdots, y_N\\) ，有\\(N\\)个单词。 Encoder其实就是一个转换函数，得到\\(M\\)个长度固定的向量，也就是其中Encoder对各个\\(x_i\\)的编码向量 \\(\\mathbf{x_i}\\) ： \\[ \\mathbf {x_1, x_2, \\cdots, x_M} = \\mathit{EncoderRNN} (x_1, x_2, \\cdots, x_n) \\] 使用链式条件概率可得到翻译概率\\(\\color{blue} {P (Y \\mid X)}\\) ，其中\\(y_0\\)是起始符号\\(SOS\\) 。 \\[ \\begin{align} P(Y \\mid X) &amp; = P(Y \\mid \\mathbf{x_1, x_2, \\cdots, x_M}) \\\\ &amp; =\\prod_{i=1}^N P(y_i \\mid y_0, \\cdots, y_{i-1}; \\mathbf{x_1, x_2, \\cdots, x_M}) \\\\ \\end{align} \\] Decoder 在翻译\\(y_i\\)的时候， 利用Encoder得到的编码向量\\(\\mathbf{x_i}\\) 和 \\(y_0 \\sim y_{i-1}\\) 来进行计算概率翻译 \\[ P(y_i \\mid y_0, \\cdots, y_{i-1}; \\mathbf{x_1, x_2, \\cdots, x_M}) \\] Decoder是由RNN+Softmax构成的。会得到一个隐状态\\(\\mathbf{y_i}\\) 向量，有2个作用： 作为下一个RNN的输入 \\(\\mathbf{y_i}\\)经过softmax得到概率分布， 选出\\(y_i\\) 输出符号 Attention 在之前的文章里有介绍论文 和 通俗理解，其实就是影响力模型。原句子的各个单词对翻译当前单词分别有多少的影响力，也叫作对齐概率吧。使用decoder-RNN的输出\\(\\mathbf{y_{t-1}}\\) 向量作为时刻\\(t\\)的输入。 时刻\\(t\\)，给定\\(\\mathbf{y_{t-1}}\\) 有3个符号定义： \\(s_i\\) ： \\(y_t\\)与\\(x_i\\)的得分，在luong论文里面有3种计算方式，分别是dot, general和concat。 \\(p_i\\) ：\\(y_t\\)与\\(x_i\\)的对齐概率，\\((p_1, p_2, \\cdots, p_M)\\) 联合起来就是\\(y_t\\)与\\(X\\)的对齐向量。其实就是对得分softmax。 \\(\\mathbf{a_t}\\) ：带注意力的语义向量。对于所有的\\(x_i\\)，使用\\(y_t\\)与它的对齐概率\\(p_i\\)乘以本身的编码向量\\(\\mathbf{x_i}\\)，得到\\(x_i\\)传达的语义，再对所有的语义求和，即得到总体的带有注意力的语义。 整体详细计算的流程，如下面的公式： \\[ \\begin {align} &amp; s_i = \\mathit{AttentionFunction} (\\mathbf{y_{t-1}}, \\mathbf{x_i}), \\quad i \\in [1, M] \\\\ &amp; p_i = \\frac {\\exp (s_i)}{\\sum_{j=1}^M \\exp(s_j)} \\quad i \\in [1, M] \\\\ &amp; \\mathbf{a_t} = \\sum_{i=1}^M p_i \\cdot \\mathbf{x_i} \\quad \\color{blue}{对所有带注意力的x_i的语义求和得总体的语义} \\end{align} \\] 计算打分的函数即\\(\\mathit{AttentionFunction}\\)是一个有隐藏层的前馈网络！实现是Badh这个人的，不是Luong的。 系统架构图说明 架构图如下 Encoder是8层的LSTM：最底层是双向的LSTM，得到两个方向的信息；上面7层都是单向的。Encoder和Decoder的残差连接都是从第3层开始的。 训练时，会让Encoder最底层的双向的LSTM开始训练，完成之后，再训练别的层，每层都用单独的GPU。 为了提高并行性，Decoder最底层，只是为了用来计算Attention Context。带注意力的语义计算好之后，会单独发给其它的各个层。 经验说明 实验结果得到，要想NMT有好效果，Encoder和Decoder的网络层数一定要够深，才能发现2种语言之间的细微异常规则。和这个同理，深层LSTM比浅层LSTM明显效果好。每加一层，会大约减少10%的perplexity。所以使用deep stacked LSTM。 残差连接 残差网络讲解 。 虽然深层LSTM比浅层LSTM效果好，但是如果只是简单堆积的话，只在几个少数层效果才可以。经过试验，4层的话估计效果还可以，6层大部分都不好，8层的话，效果就相当差了。这是因为网络会变得很慢和很难训练，很大程度是因为梯度爆炸和梯度消失的问题。 根据在中间层和目标之间建立差别的思想，引入残差连接，如下图右边所示。其实就是把之前层的输入和当前的输出合并起来，作为下一层的输入。 一些参数和符号说明，一下均是时刻\\(t\\) \\(\\mathbf{x^i_t}\\) : 第\\(i+1\\)层 \\(\\mathit{LSTM}_{i+1}\\)的输入。 即上标代表LSTM的层数，下标代表时间。 \\(\\mathbf{W} ^i\\) : 第\\(i\\)层LSTM的参数 \\(\\mathbf {h} _t^i\\) : 第\\(i\\)层输出隐状态 \\(\\mathbf {c} ^i_t\\) : 第\\(i\\)层输出单元状态 那么\\(LSTM_i\\)和\\(LSTM_{i+1}\\)是这样交互的。即层层纵向传递输入，时间横向传递隐状态和单元状态。 \\[ \\begin{align} &amp; \\mathbf{c}_t^i, \\mathbf{h}_t^i = LSTM_i(\\mathbf{c}_{t-1}^i, \\mathbf{h}_{t-1}^i, \\mathbf x_{t}^{i-1} ; \\; \\mathbf W^i ) \\\\ &amp; \\mathbf x_t^i = \\mathbf h_t^i \\quad\\quad\\quad\\quad \\color{blue}{普通连接：i+1层输入=i层隐层输出} \\\\ &amp; \\mathbf{x}_t^i = \\mathbf h_t^i + \\mathbf{x}_t^{i-1} \\quad \\color{blue} {残差连接：第i+1层的输入=i层输入+i层隐层输出} \\\\ &amp; \\mathbf{c}_t^{i+1}, \\mathbf{h}_t^{i+1} = LSTM_{i+1} (\\mathbf c_{t-1}^{i+1}, \\mathbf {h} _{t-1}^{i+1}, x_{t}^i ; \\; \\mathbf W ^{i+1}) \\\\ \\end{align} \\] 残差连接可以在反向传播的时候大幅度提升梯度流，这样就可以训练很深的网络。 双向Encoder 一般输入的句子是从左到右，输出也是。但是由于语言的复杂性，有助于翻译的关键信息可能在原句子的不同地方。为了在Encoder中的每一个点都有最好的上下文语义，所以需要使用双向LSTM。 这里只在Encoder的最底层使用双向LSTM，其余各层均使用单向的LSTM。双向LSTM训练完成之后，再训练别的层。 \\(LSTM_f\\)从左到右处理句子，\\(LSTM_b\\)从右到左处理句子。把两个方向的信息\\(\\mathbf{x^f_i}\\)和\\(\\mathbf{x}^b_i\\)concat起来，传递给下一层。 模型并行性 模型很复杂，所以使用模型并行和数据并行，来加速。 数据并行 数据并行很简单，使用大规模分布式深度网络(Downpour SGD) 同时训练\\(n\\)个模型副本，它们都使用相同的模型参数，但是每个副本会使用Adam和SGD去异步地更新参数。每个模型副本一次处理m个句子。一般实验中，\\(n=10, m=128\\)。 模型并行 除了数据并行以外，模型并行也会加速每个副本的梯度计算。Encoder和Decoder会进行深度去划分，一般每一层会放在一个单独的GPU上。除了第一层的Encoder之外，所有的层都是单向的，所以第\\(i+1\\)层可以提前运行，不必等到第\\(i\\)层完全训练好了才进行训练。Softmax也会进行划分，每个处理一部分的单词。 并行带来的约束 由于要并行计算，所以我们不能够在Encoder的所有层上使用双向LSTM。因为如果使用了双向的，上面层必须等到下面层前向后向完全训练好之后才能开始训练，就不能并行计算。在Attention上，我们也只能使用最顶层的Encoder和最底层的Decoder进行对齐计算。如果使用顶层Encoder和顶层Decoder，那么整个Decoder将没有任何并行性，也就享受不到多个GPU的快乐了。 分割技巧 一般NMT都是的词汇表都是定长的，但是实际上词汇表却是开放的。比如人名、地名和日期等等。一般有两种方法去处理OOV(out-of-vocabulary)单词，复制策略和sub-word单元策略。GNMT是使用sub-word单元策略，也称为wordpiece模型。 复制策略 有下面几种复制策略 把稀有词汇直接复制到目标句子中，因为大部分都是人名和地名 使用注意力模型，添加特别的注意力 使用一个外部的对齐模型，去处理稀有词汇 使用一个复杂的带有特殊目的的指出网络，去指出稀有词汇 sub-word单元 比如字符，混合单词和字符，更加智能的sub-words。 Wordpiece 模型","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"RNN","slug":"RNN","permalink":"http://plmsmile.github.io/tags/RNN/"},{"name":"神经机器翻译","slug":"神经机器翻译","permalink":"http://plmsmile.github.io/tags/神经机器翻译/"}]},{"title":"博客搭建及相关问题","date":"2017-10-16T14:47:46.000Z","path":"2017/10/16/24-hexo-problems/","text":"搭建hexo博客，数学公式问题，Indigo主题 搭建博客 搭建博客 12345678910111213141516171819202122232425mkdir PLMBlogscd PLMBlogs# install hexonpm install hexo-cli -g# inithexo init npm installhexo server# install pluginsnpm install hexo-deployer-git --savenpm install hexo-renderer-scss --save# 在默认_config.yml中添加需要的插件plugins: hexo-generator-feed #RSS订阅插件 hexo-generator-sitemap #sitemap插件git clone https://github.com/ahonn/hexo-theme-even themes/even# 替换配置文件 or 一步一步地去配置# 生成，再替换文件hexo new page tagshexo new page categories indigo主题 123456789101112131415161718192021222324hexo init# 配置.yml文件，复制旧的过来即可npm install hexo-deployer-git --savegit clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigogit checkout -b card origin/cardnpm install hexo-renderer-less --savenpm install hexo-generator-feed --savenpm install hexo-generator-json-content --savenpm install hexo-helper-qrcode --savehexo new page tagshexo new page categorieshexo new page about# 再去配置各个目录下的index文件，也可以直接copy# 配置主题中的yml，直接copy# 修改图片等# 后续 修改宽度# source/css/_partial/variable.css 中第28行，修改为80%的宽度contentWidth: 80%# 主题配置文件中 cdn改为falsecdn: false 搭建indigo博客 安装博客 1234567891011121314hexo initnpm install hexo-deployer-git --save''' 安装 '''git clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigonpm install hexo-renderer-less --savenpm install hexo-generator-feed --savenpm install hexo-generator-json-content --savenpm install hexo-helper-qrcode --save''' 配置标签和类别页面，去配置index.md中的数据 '''hexo new page tagshexo new page categories# 主要是配置 layout: tags layout: categories comment: false# 新建关于我的页面，并填上相应的信息 layout: abouthexo new page about 配置hexo/_config.yml中的主题是indigo 12# 配置indigo主题theme: indigo 配置数学公式 请务必使用pandoc进行渲染，这是最好的没有之一！别的会有各种问题，比如默认的多行数学公式渲染不了，这个问题困扰了好久。 但是pandoc也有坑啊，测试了2.5不行。我这里成功的是pandoc-2.2.3.2-windows-x86_64 + hexo-renderer-pandoc@0.2.0。最新的都不行。 12345678910111213141516171819# 在主题_config.yml 中配置 mathjax: true# 要先卸载已有的渲染器npm uninstall hexo-renderer-marked --save# 潜在的npm uninstall hexo-renderer-kramed --savenpm uninstall hexo-math --save# 只需要安装pandoc就可以了# 先在本地下载pandoc，安装好，再执行如下命令。最新的pandoc测试有问题。推荐2.2和0.2.npm install hexo-renderer-pandoc --save# 其他命令npm view hexo-renderer-pandoc versionsnpm ls hexo-renderer-pandocnpm uninstall hexo-renderer-pandoc@0.2.0 --save# 1. 安装旧版pandoc-2.2.3.2-windows-x86_64.msi在win10上# 2. 安装hexo-renderer-pandoc@0.2.0在博客中npm install hexo-renderer-pandoc@0.2.0 --save 做到这里，要先启动，放两篇带数学公式的文档，去测试一下。 博客配置 编辑hexo/_config.yml ，添加如下项目 1234567891011121314151617181920212223242526272829303132333435363738# Sitetitle: PLM's Blogsubtitle: 好好学习，天天向上description: 菜鸟程序员author: 蒲黎明language: zh-CN# url url: https://plmsmile.github.io/# Deploymentdeploy: type: git repo: git@github.com:plmsmile/plmsmile.github.io.git branch: master# indigo的配置项feed: type: atom path: atom.xml limit: 0jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true indigo-config配置 参考官方配置说明 编辑themes/indigo/_config.yml 配置左侧菜单 12345678910111213141516171819menu: home: text: 主页 url: / archives: text: 归档 url: /archives tags: text: 标签 url: /tags th-list: text: 类别 url: /categories user: url: /about text: 关于我 github: url: https://github.com/plmsmile target: _blank 配置自我介绍 1about: 自然语言处理，机器学习，深度学习，Spark，Leetcode，Java，C++，数据结构。都不会呢，赶紧快学吧！ 设置图片，需要配置站点图片、头像图片，也可以换头像背景图片。 1234567891011# 你的头像urlavatar: /img/avatar.jpg# avatar linkavatar_link: https://plmsmile.github.io/about# 头像背景图brand: /img/brand.jpg# faviconfavicon: /img/favicon.png# emailemail: plmsmile@126.com 配置页面宽度 修改source/css/_partial/variable.css 中的28行，设置为80%。设置主题配置文件中，cdn: false。 、 配置支付宝和微信图片，默认就有打赏功能。可以关掉。 12345''' 是否开启打赏，关闭 reward: false'''reward: title: 谢谢大爷~ wechat: /img/wechat.png '微信，关闭设为 false alipay: /img/alipay.png '支付宝，关闭设为 false 每张文章最后的备注 1postMessage: &lt;br&gt;原始链接：&lt;a href=\"&lt;%- url_for(page.path).replace(/index\\.html$/, '') %&gt;\" target=\"_blank\" rel=\"external\"&gt;&lt;%- page.permalink.replace(/index\\.html$/, '') %&gt;&lt;/a&gt; 关闭动态title 1234# 动态定义title# title_change:# normal: (つェ⊂)咦!又好了!# leave: 死鬼去哪里了！ 配置valine评论 12345678910valine: enable: true # 如果你想使用valine，请将值设置为 true appId: xxxx # your leancloud appId appKey: xxxx # your leancloud appKey notify: true # Mail notify verify: false # Verify code avatar: mm # Gravatar style : mm/identicon/monsterid/wavatar/retro/hide placeholder: Just go go # Comment Box placeholder guest_info: nick,mail,link # Comment header info pageSize: 10 # comment list page size 配置几个页面的标题 1234# 页面标题tags_title: 标签archives_title: 归档categories_title: 类别 语言栏 indigo其他3个文件配置 languages/zh-CN.yml配置 12345678910global: search_input_hint: \"输入感兴趣的关键字\"post: continue_reading: \"点击阅读全文\" last_updated: \"最后更新时间：\"footer: license: '博客内容遵循 &lt;a rel=\"license\" href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh\"&gt;知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议&lt;/a&gt;'tag: all: \"全部\" tags: \"标签\" layout/_partial/footer.ejs 1234567891011&lt;footer class=&quot;footer&quot;&gt; &lt;div class=&quot;bottom&quot;&gt; &lt;%- partial(&apos;plugins/site-visit&apos;) %&gt; &lt;p&gt; &lt;span&gt; PLM&apos;s Notes &amp;nbsp; &amp;copy; &amp;nbsp &lt;/span&gt; &lt;%if (theme.since_year &amp;&amp; theme.since_year &lt; date(new Date(), &apos;YYYY&apos;)) &#123;%&gt;&lt;%- theme.since_year %&gt; - &lt;%&#125;%&gt;&lt;%- date(new Date(), &apos;YYYY&apos;) %&gt; &lt;/p&gt; &lt;/div&gt;&lt;/footer&gt; layout/_partial/script.ejs 更改卜算子的域名，解决统计访问量的问题 12345678910111213141516171819202122&lt;script src=&quot;//cdn.bootcss.com/node-waves/0.7.4/waves.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;var BLOG = &#123; ROOT: &apos;&lt;%= config.root %&gt;&apos;, SHARE: &lt;%- theme.share %&gt;, REWARD: &lt;%- Boolean(page.reward) %&gt; &#125;;&lt;% if (theme.cnzz)&#123; %&gt;lazyScripts.push(&apos;//s95.cnzz.com/z_stat.php?id=&lt;%-theme.cnzz %&gt;&amp;web_id=&lt;%-theme.cnzz %&gt;&apos;)&lt;% &#125; %&gt;&lt;/script&gt;&lt;script src=&quot;&lt;%- url_for(theme_js(&apos;/js/main&apos;, cache)) %&gt;&quot;&gt;&lt;/script&gt;&lt;% if (theme.search)&#123; %&gt;&lt;%- partial(&apos;search&apos;) %&gt;&lt;script src=&quot;&lt;%- url_for(theme_js(&apos;/js/search&apos;, cache)) %&gt;&quot; async&gt;&lt;/script&gt;&lt;% &#125; %&gt;&lt;%- partial(&apos;plugins/mathjax&apos;) %&gt;&lt;% if (theme.visit_counter) &#123; %&gt;&lt;script async src=&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;&lt;% &#125; %&gt;&lt;%- partial(&apos;plugins/dynamic-title&apos;) %&gt; indigo-config一览 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144menu: home: text: 主页 url: / archives: text: 归档 url: /archives tags: text: 标签 url: /tags th-list: text: 类别 url: /categories user: url: /about text: 关于我 github: url: https://github.com/plmsmile target: _blank# 你的头像urlavatar: /img/avatar.jpg# avatar linkavatar_link: /# 头像背景图brand: /img/brand.jpg# faviconfavicon: /img/favicon.png# emailemail: plmsmile@126.com# 设置 Android L Chrome 浏览器状态栏颜色color: '#3F51B5'# 页面标题tags_title: 标签archives_title: 归档categories_title: 类别# 文章截断excerpt_render: falseexcerpt_length: 200excerpt_link: 点击阅读全文mathjax: truearchive_yearly: true# 是否显示文章最后更新时间show_last_updated: true# 是否开启分享share: true# 是否开启打赏，关闭 reward: falsereward: title: 谢谢大爷~ wechat: /img/wechat.png #微信，关闭设为 false alipay: /img/alipay.png #支付宝，关闭设为 false# 是否开启搜索search: true# 是否大屏幕下文章页隐藏导航hideMenu: true# 是否开启toc# toc: falsetoc: list_number: true # 是否显示数字排序# 文章页留言内容，hexo中所有变量及辅助函数等均可调用，具体请查阅 hexo.iopostMessage: &lt;br&gt;原始链接：&lt;a href=\"&lt;%- url_for(page.path).replace(/index\\.html$/, '') %&gt;\" target=\"_blank\" rel=\"external\"&gt;&lt;%- page.permalink.replace(/index\\.html$/, '') %&gt;&lt;/a&gt;# 站长统计，如要开启，输入CNZZ站点id，如 cnzz: 1255152447cnzz: false# 百度统计，如要开启，改为你的 keybaidu_tongji: false# 腾讯分析，如要开启，输入站点idtajs: false# googlegoogle_analytics: falsegoogle_site_verification: false# sogou站长验证 http://zhanzhang.sogou.com/sogou_site_verification: false# lessless: compress: true paths: - source/css/style.less# 以下评论插件开启一个即可# 是否开启 disqusdisqus_shortname: false# 是否开启友言评论, 填写友言用户iduyan_uid: false# 是否使用 gitment，https://github.com/imsun/gitmentgitment: false# Valine Comment system. https://valine.js.orgvaline: enable: true # 如果你想使用valine，请将值设置为 true appId: xxxx # your leancloud appId appKey: xxxx # your leancloud appKey notify: true # Mail notify verify: false # Verify code avatar: mm # Gravatar style : mm/identicon/monsterid/wavatar/retro/hide placeholder: Just go go # Comment Box placeholder guest_info: nick,mail,link # Comment header info pageSize: 10 # comment list page sizehyper_id: falsecanonical: false# 版权起始年份since_year: 2016# 用户页面中作者相关的描述性文字，如不需要设为 falseabout: NLP &amp;&amp; DL student# “不蒜子”访问量统计，详见 http://ibruce.info/2015/04/04/busuanzi/visit_counter: site_uv: 总访客数： site_pv: 总访问量：# 动态定义title# title_change:# normal: (つェ⊂)咦!又好了!# leave: 死鬼去哪里了！# 设置为 true 发布后将使用 unpkg cdn 最新的主题样式# 如果想让你的自定义样式生效，把此项设为 falsecdn: false# 设置为 true 将使用 lightbox render 图片lightbox: true# icp备案号 ICP_license: 京ICP备1234556号-1ICP_license: false hexo-config一览 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: PLM's Notessubtitle: 好好学习，天天笔记description: NLP, DL, MRC.keywords:author: PLMlanguage: zh-CNtimezone:# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://plmsmile.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: index_generator: path: '' per_page: 15 order_by: -date # Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensionstheme: indigo# Deploymentdeploy: type: git repo: git@github.com:plmsmile/plmsmile.github.io.git branch: master## addedjsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 总结 123456789101112131415161718hexo init# 1. 运行下面的脚本npm install hexo-deployer-git --savegit clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigonpm install hexo-renderer-less --savenpm install hexo-generator-feed --savenpm install hexo-generator-json-content --savenpm install hexo-helper-qrcode --savehexo new page tagshexo new page categorieshexo new page about# 2. 替换source/中3个文件夹，about, categories, tags# 3. 替换hexo和主题中的配置文件 _config.yml# 4. 替换indigo/source/img 文件夹# 5. 修改source/css/_partial/variable.css的28行 宽度为80%# 6. 替换indigo其他3个文件配置，footer.ejs, scipt.ejs, zh-CN.yml# 7. 数学公式，请使用pandoc渲染！！！# 8. 把博客文件复制过来，运行查看。 PyPlot使用中文 参考文档 12345678910111213# 下载字体放到下面的目录# 下载simhei.tff/home/plm/app/anaconda2/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/ttf# 编辑文件/home/plm/app/anaconda2/lib/python2.7/site-packages/matplotlib/mpl-data/matplotlibrc# 打开下面的注释# font.family : sans-serif# 打开注释，加上SimHei# font.sans-serif : SimHei,Bitstream Vera Sans, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif# 删除缓存rm -rf ~/.cache/matplotlib 简单测试例子 123456789101112131415import matplotlibmatplotlib.matplotlib_fname()import matplotlib.pyplot as pltplt.rcParams['font.sans-serif']=['simhei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号matplotlib.rcParams['axes.unicode_minus'] = False #-号为方块问题plt.plot((1,2,3),(4,3,-1))s = \"横坐标\"plt.xlabel(unicode(s))plt.ylabel(u'纵坐标')plt.show()print (s) 有时候依然不好使，那么就 123import sys reload(sys) sys.setdefaultencoding('utf8') 应该就可以了。","tags":[{"name":"心得","slug":"心得","permalink":"http://plmsmile.github.io/tags/心得/"},{"name":"hexo","slug":"hexo","permalink":"http://plmsmile.github.io/tags/hexo/"},{"name":"pandoc","slug":"pandoc","permalink":"http://plmsmile.github.io/tags/pandoc/"},{"name":"数学公式","slug":"数学公式","permalink":"http://plmsmile.github.io/tags/数学公式/"},{"name":"indigo","slug":"indigo","permalink":"http://plmsmile.github.io/tags/indigo/"},{"name":"中文","slug":"中文","permalink":"http://plmsmile.github.io/tags/中文/"},{"name":"pyplot","slug":"pyplot","permalink":"http://plmsmile.github.io/tags/pyplot/"}]},{"title":"机器翻译注意力机制及其PyTorch实现","date":"2017-10-12T08:12:39.000Z","path":"2017/10/12/Attention-based-NMT/","text":"前面阐述注意力理论知识，后面简单描述PyTorch利用注意力实现机器翻译 Effective Approaches to Attention-based Neural Machine Translation 简介 Attention介绍 在翻译的时候，选择性的选择一些重要信息。详情看这篇文章 。 本着简单和有效的原则，本论文提出了两种注意力机制。 Global 每次翻译时，都选择关注所有的单词。和Bahdanau的方式 有点相似，但是更简单些。简单原理介绍。 Local 每次翻译时，只选择关注一部分的单词。介于soft和hard注意力之间。(soft和hard见别的论文)。 优点有下面几个 比Global和Soft更好计算 局部注意力 随处可见、可微，更好实现和训练。 应用范围 在训练神经网络的时候，注意力机制应用十分广泛。让模型在不同的形式之间，学习对齐等等。有下面一些领域： 机器翻译 语音识别 图片描述 between image objects and agent actions in the dynamic control problem (不懂，以后再说吧) 神经机器翻译 思想 输入句子\\(x = (x_1, x_2, \\cdots, x_n)\\) ，输出目标句子\\(y = (y_1, y_2, \\cdots, y_m)\\) 。 神经机器翻译(Neural machine translation, NMT)，利用神经网络，直接对\\(\\color{blue} {p(y \\mid x)}\\) 进行建模。一般由Encoder和Decoder构成。Encoder-Decoder介绍文章链接 。 Encoder把输入句子\\(x\\) 编码成一个语义向量\\(s\\) (c表示也可以)，然后由Decoder 一个一个产生目标单词 \\(y_i\\) \\[ \\log p(y \\mid x) = \\sum_{j=1}^m \\log \\color{red} {p(y_j \\mid y _{&lt;j}, s) } = \\sum_{j=1}^m \\log p(y_j \\mid y_1, \\cdots, y_{j-1}, s) \\] 但是怎么选择Encoder和Decoder（RNN, CNN, GRU, LSTM），怎么去生成语义\\(s\\)却有很多选择。 概率计算 结合Decoder上一时刻的隐状态\\(\\color{blue}{h_{j-1}}\\)和encoder给的语义向量\\(\\color{blue}{s}\\)，通过函数\\(\\color{blue}{f}\\) ，就可以计算出当前的隐状态\\(\\color{blue}{h_j}\\) ： \\[ h_j = f(h_{j-1}, s) \\] 通过函数\\(\\color{blue}{g}\\)对当前隐状态\\(h_j\\)进行转换，再softmax，就可以得到翻译的目标单词\\(y_i\\)了（选概率最大的那个）。 \\(g\\)一般是线性变换，维数变化是\\([1, h] \\to [1, vocab\\_size]\\)。 \\[ p(y_j \\mid y _{&lt;j}, s) = \\mathrm{softmax} \\; g(h_j) \\] 语义向量\\(s​\\) 会贯穿整个翻译的过程，每一步翻译都会使用到语义向量的内容，这就是注意力机制。 本论文的模型 本论文采用stack LSTM的构建NMT系统。如下所示： 训练目标是 \\[ J_t = \\sum_{(x, y)} - \\log p(y \\mid x) \\] 注意力模型 注意力模型广义上分为global和local。Global的attention来自于整个序列，而local的只来自于序列的一部分。 解码总体流程 Decoder时，在时刻\\(t\\)，要翻译出单词\\(y_t\\) ，如下步骤： 最顶层LSTM的隐状态 \\(h_t\\) 计算带有原句子信息语义向量\\(c_t\\)。Global和Local的区别在于\\(c_t\\)的计算方式不同 串联\\(h_t, c_t\\)，计算得到带有注意力的隐状态 \\(\\hat {h}_t = \\tanh (W_c [c_t; h_t])\\) 通过注意力隐状态得到预测概率 \\(p(y_t \\mid y_{&lt;t}, x) = \\rm {softmax} (W_s \\hat h _t)\\) Global Attention 总体思路 在计算\\(c_t\\) 的时候，会考虑整个encoder的隐状态。Decoder当前隐状态\\(h_t\\)， Encoder时刻s的隐状态\\(\\bar h _s\\)。 对齐向量\\(\\color{blue}{\\alpha_t}\\)代表时刻\\(t\\) 输入序列中的单词对当前单词\\(y_t\\) 的对齐概率，长度是\\(T_x\\)， 随着输入句子的长度而改变 。\\(x_s\\)与\\(y_t\\) 的对齐概率如下： \\[ \\alpha_t(s) = \\mathrm {align} (h_t, \\bar h_s) = \\frac {score(h_t, \\bar h_s)}{ \\sum_{i=1}^{T_x} score(h_t, \\bar h_i)}, \\quad 实际上\\mathrm{softmax} \\] 结合上面的解码总体流程，有下面的流程 \\[ all (\\bar h_s) , h_t \\to \\alpha_t \\to c_t . \\quad c_t , h_t \\to \\hat h_t .\\quad \\hat h_t \\to y_t \\quad \\] 简单来说是\\(h_t \\to \\alpha_t \\to c_t \\to \\hat h_t \\to y_t\\) 。 score计算 \\(score(h_t, \\bar h_s)\\) 是一种基于内容content-based的函数，有3种实现方式 \\[ \\color{blue}{score(h_t, \\bar h_s)} = \\begin{cases} h_t^T \\bar h_s &amp; dot \\\\ h_t^T W_a \\bar h_s &amp; general \\\\ v_a^T \\tanh (W_a [h_t; \\bar h_s]) &amp; concat \\\\ \\end{cases} \\] 缺点 生成每个目标单词的时候，都必须注意所有的原单词， 这样计算量很大，翻译长序列可能很难，比如段落或者文章。 Local Attention 在生成目标单词的时候，Local会选择性地关注一小部分原单词去计算\\(\\alpha_t, c_t\\)，这样就解决了Global的问题。如下图 Soft和Hard注意 Soft 注意 ：类似global注意，权值会放在图片的所有patches中。计算复杂。 Hard 注意： 不同时刻，会选择不同的patch。虽然计算少，但是non-differentiable，并且需要复杂的技术去训练模型，比如方差减少和强化学习。 Local注意 类似于滑动窗口，计算一个对齐位置\\(\\color{blue}{p_t}\\)，根据经验设置窗口大小\\(D\\)，那么需要注意的源单词序列是 ： \\[ [p_t -D, p_t + D] \\] \\(\\alpha_t\\) 的长度就是\\(2D\\)，只需要选择这\\(2D\\)个单词进行注意力计算，而不是Global的整个序列。 对齐位置选择 对齐位置的选择就很重要，主要有两种办法。 local-m (monotonic) 设置位置， 即以当前单词位置作为对齐位置 \\[ p_t = t \\] local-p (predictive) 预测位置 \\(S\\) 是输入句子的长度，预测对齐位置如下 \\[ p_t = S \\cdot \\mathrm{sigmoid} \\left(v_p^T \\tanh (W_p h_t) \\right), \\quad p_t \\in [0, S] \\] 对齐向量计算 \\(\\alpha_t\\)的长度就是\\(2D\\)，对于每一个\\(s \\in [p_t -D, p_t + D]\\)， 为了更好地对齐，添加一个正态分布\\(N(\\mu, \\sigma ^2)\\)，其中 \\(\\mu = p_t, \\sigma = \\frac{D}{2}\\)。 计算对齐概率： \\[ \\alpha_t(s) = \\mathrm{align} (h_t, \\bar h_s) \\exp \\left( - \\frac{(s - \\mu)^2}{2\\sigma^2}\\right) = \\mathrm{align} (h_t, \\bar h_s) \\exp \\left( - \\frac{2(s - p_t)^2}{D^2}\\right) \\] Input-feeding 前面的Global和Local两种方式中，在每一步的时候，计算每一个attention (实际上是指 \\(\\hat h_t\\))，都是独立的，这样只是次最优的。 在每一步的计算中，这些attention应该有所关联，当前知道之前的attention才对。实际是应该有个coverage set去追踪之前的信息。 我们会把当前的注意\\(\\hat h_t\\) 作为下一次的输入，并且做一个串联，来计算新的attention，如下图所示 这样有两重意义： 模型会知道之前的对齐选择 会建立一个水平和垂直都很深的网络 PyTorch实现机器翻译 机器翻译github源代码 计算输入语义 比较简单，使用GRU进行编码，使用outputs作为哥哥句子的编码语义。PyTorch RNN处理变长序列 1234567891011121314151617181920def forward(self, input_seqs, input_lengths, hidden=None): ''' 对输入的多个句子经过GRU计算出语义信息 1. input_seqs &gt; embeded 2. embeded - packed &gt; GRU &gt; outputs - pad -output Args: input_seqs: [s, b] input_lengths: list[int]，每个batch句子的真实长度 Returns: outputs: [s, b, h] hidden: [n_layer, b, h] ''' # 一次运行，多个batch，多个序列 embedded = self.embedding(input_seqs) packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) outputs, hidden = self.gru(packed, hidden) outputs, output_length = nn.utils.rnn.pad_packed_sequence(outputs) # 双向，两个outputs求和 if self.bidir is True: outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:] return outputs, hidden 计算对齐向量 实际上就是attn_weights， 也就是输入序列对当前要预测的单词的一个注意力分配。 输入输出定义 Encoder的输出，所有语义\\(c\\)，encoder_outputs， [is, b, h]。 is=input_seq_len是输入句子的长度 当前时刻Decoder的\\(h_t\\)， decoder_rnn_output， [ts, b, h] 。实际上ts=1， 因为每次解码一个单词 12345678def forward(self, rnn_outputs, encoder_outputs): '''ts个时刻，计算ts个与is的对齐向量，也是注意力权值 Args: rnn_outputs -- Decoder中GRU的输出[ts, b, h] encoder_outputs -- Encoder的最后的输出, [is, b, h] Returns: attn_weights -- Yt与所有Xs的注意力权值，[b, ts, is] ''' 计算得分 使用gerneral的方式，先过神经网络(线性层)，再乘法计算得分 1234# 过Linear层 (b, h, is)encoder_outputs = self.attn(encoder_outputs).transpose(1, 2)# [b,ts,is] &lt; [b,ts,h] * [b,h,is]attn_energies = rnn_outputs.bmm(encoder_outputs) softmax计算对齐向量 每一行都是原语义对于某个单词的注意力分配权值向量。对齐向量实际例子 123# [b,ts,is]attn_weights = my_log_softmax(attn_energies)return attn_weights 计算新的语义 新的语义也就是，对于翻译单词\\(w_t\\)所需要的带注意力的语义。 输入输出 1234567891011121314151617def forward(self, input_seqs, last_hidden, encoder_outputs): ''' 一次输入(ts, b)，b个句子, ts=target_seq_len 1. input &gt; embedded 2. embedded, last_hidden --GRU-- rnn_output, hidden 3. rnn_output, encoder_outpus --Attn-- attn_weights 4. attn_weights, encoder_outputs --相乘-- context 5. rnn_output, context --变换,tanh,变换-- output Args: input_seqs: [ts, b] batch个上一时刻的输出的单词，id表示。每个batch1个单词 last_hidden: [n_layers, b, h] encoder_outputs: [is, b, h] Returns: output: 最终的输出，[ts, b, o] hidden: GRU的隐状态，[nl, b, h] attn_weights: 对齐向量，[b, ts, is] ''' 当前时刻Decoder的隐状态 输入上一时刻的单词和隐状态，通过GRU，计算当前的隐状态。实际上ts=1 12# (ts, b, h), (nl, b, h)rnn_output, hidden = self.gru(embedded, last_hidden) 计算对齐向量 当前时刻的隐状态 rnn_output 和源句子的语义encoder_outputs，计算对齐向量。对齐向量 每一行都是原句子对当前单词(只有一行)的注意力分配。 1234# 对齐向量 [b,ts,is]attn_weights = self.attn(rnn_output, encoder_outputs)# 如[0.1, 0.2, 0.7] 计算新的语义 原语义和原语义对当前单词分配的注意力，计算当前需要的新语义。 123# 新的语义 # [b,ts,h] &lt; [b,ts,is] * [b,is,h]context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) 预测当前单词 结合新语义和当前隐状态预测新单词 123456789# 语义和当前隐状态结合 [ts, b, 2h] &lt; [ts, b, h], [ts, b, h]output_context = torch.cat((rnn_output, context), 2)# [ts, b, h] 线性层2h-houtput_context = self.concat(output_context)concat_output = F.tanh(output_context)# [ts, b, o] 线性层h-ooutput = self.out(concat_output)output = my_log_softmax(output)return output 总结 1234567891011121314151617# 1. 对齐向量# 过Linear层 (b, h, is)encoder_outputs = self.attn(encoder_outputs).transpose(1, 2)# 关联矩阵 [b,ts,is] &lt; [b,ts,h] * [b,h,is]attn_energies = rnn_outputs.bmm(encoder_outputs)# 每一行求softmax [b,ts,is] '''每一行都是原语义对当前单词的注意力分配向量'''attn_weights = my_log_softmax(attn_energies)# 2. 新语义# 新的语义 [b,ts,h] &lt; [b,ts,is] * [b,is,h]context = attn_weights.bmm(encoder_outputs.transpose(0, 1))# 3. 新语义和当前隐状态结合，输出# 语义和输出 [ts, b, 2h] &lt; [ts, b, h], [ts, b, h]output_context = torch.cat((rnn_output, context), 2)","tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"注意力","slug":"注意力","permalink":"http://plmsmile.github.io/tags/注意力/"},{"name":"机器翻译","slug":"机器翻译","permalink":"http://plmsmile.github.io/tags/机器翻译/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"Attention","slug":"Attention","permalink":"http://plmsmile.github.io/tags/Attention/"}]},{"title":"图文介绍RNN注意力机制","date":"2017-10-10T03:40:01.000Z","path":"2017/10/10/attention-model/","text":"用图文简单介绍基于RNN的Encoder-Decoder中注意力机制 Encoder-Decoder 基本介绍 举个翻译的例子，原始句子\\(X = (x_1, x_2, \\cdots, x_m)\\) ，翻译成目标句子\\(Y = (y_1, y_2, \\cdots, y_n)\\) 。 现在采用Encoder-Decoder架构模型，如下图 Encoder会利用整个原始句子生成一个语义向量，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句子的意思、句法结构、性别信息等等。具体框架可以参考Encoder-Decoder框架。 Encoder对\\(X\\) 进行非线性变换得到中间语义向量c ： \\[ c = G(x_1, x_2, \\cdots, x_n) \\] Decoder根据语义\\(c\\) 和生成的历史单词\\((y_1, y_2, \\cdots, y_{i-1})\\) 来生成第\\(i\\) 个单词 \\(y_i\\)： \\[ y_i = f(c, y_1, y_2, \\cdots, y_{i-1}) \\] Encoder-Decoder是个创新大杀器，是个通用的计算框架。Encoder和Decoder具体使用什么模型，都可以自己选择。通常有CNN，RNN，BiRNN，GRU，LSTM， Deep LSTM。上面的内容任意组合，只要得到的效果好，就是一个创新，就可以毕业了。（当然别人没有提出过） 缺点 在生成目标句子\\(Y\\)的单词时，所有的单词\\(y_i\\)使用的语义编码\\(c\\) 都是一样的。而语义编码\\(c\\)是由句子\\(X\\) 的每个单词经过Encoder编码产生，也就是说每个\\(x_i\\)对所有\\(y_j\\)的影响力都是相同的，没有任何区别的。所以上面的是注意力不集中的分心模型。 句子较短时问题不大，但是较长时，所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，会丢失更多的细节信息。 例子 比如输入\\(X\\)是Tom chase Jerry，模型翻译出 汤姆 追逐 杰瑞。在翻译“杰瑞”的时候，“Jerry”对“杰瑞”的贡献更重要。但是显然普通的Encoder-Decoder模型中，三个单词对于翻译“Jerry-杰瑞”的贡献是一样的。 解决方案应该是，每个单词对于翻译“杰瑞”的贡献应该不一样，如翻译“杰瑞”时： \\[ (Tom, 0.3), \\; (Chase, 0.2), \\; (Jerry, 0.5) \\] Attention Model 基本架构 Attention Model的架构如下： 如图所示，生成每个单词\\(y_i\\)时，都有各自的语义向量\\(C_i\\)，不再是统一的\\(C\\) 。 \\[ y_i = f(C_i, y_1, \\cdots, y_{i-1}) \\] 例如，前3个单词的生成： \\[ \\begin{align} &amp; y_1 = f(C_1) \\\\ &amp; y_2 = f(C_2, y_1) \\\\ &amp; y_3 = f(C_3, y_1, y_2) \\\\ \\end{align} \\] 语义向量的计算 注意力分配概率 \\(a_{ij}\\) 表示 \\(y_i\\)收到\\(x_j\\) 的注意力概率。 例如\\(X=(Tom, Chase, Jerry)\\)，\\(Y = (汤姆, 追逐, 杰瑞)\\) 。\\(a_{12}=0.2\\)表示汤姆 收到来自Chase的注意力概率是0.2。 有下面的注意力分配矩阵： \\[ A = [a_{ij}] = \\begin {bmatrix} 0.6 &amp; 0.2 &amp; 0.2 \\\\ 0.2 &amp; 0.7 &amp; 0.1 \\\\ 0.3 &amp; 0.1 &amp; 0.5 \\\\ \\end {bmatrix} \\] 第\\(i\\)行表示\\(y_i\\) 收到的所有来自输入单词的注意力分配概率。\\(y_i\\) 的语义向量\\(C_i\\) 由这些注意力分配概率和Encoder对单词\\(x_j\\)的转换函数相乘，计算而成，例如： \\[ \\begin {align} &amp; C_1 = C_{汤姆} = g(0.6 \\cdot h(Tom),\\; 0.2 \\cdot h(Chase),\\; 0.2 \\cdot h(Jerry)) \\\\ &amp; C_2 = C_{追逐} = g(0.2 \\cdot h(Tom) ,\\;0.7 \\cdot h(Chase) ,\\;0.1 \\cdot h(Jerry)) \\\\ &amp; C_3 = C_{汤姆} = g(0.3 \\cdot h(Tom),\\; 0.2 \\cdot h(Chase) ,\\;0.5 \\cdot h(Jerry)) \\\\ \\end {align} \\] \\(\\color{blue}{h(x_j)}\\) 就表示Encoder对输入英文单词的某种变换函数。比如Encoder使用RNN的话，\\(h(x_j)\\)往往都是某个时刻输入\\(x_j\\) 后隐层节点的状态值。 g函数 表示注意力分配后的整个句子的语义转换信息，一般都是加权求和，则有语义向量计算公式： \\[ C_i = \\sum_{j=1}^{T_x} a_{ij} \\cdot h_j, \\quad h_j = h(x_j) \\] 其中\\(\\color{blue}{T_x}\\) 代表输入句子的长度。形象来看计算过程如下图： 注意力分配概率计算 语义向量需要注意力分配概率和Encoder输入单词变换函数来共同计算得到。 但是比如汤姆收到的分配概率\\(a_1 = (0.6, 0.2, 0.2)\\)是怎么计算得到的呢？ 这里采用RNN作为Encoder和Decoder来说明。 注意力分配概率如下图计算 对于\\(a_{ij}\\) 其实是通过一个对齐函数F来进行计算的，两个参数：输入节点\\(j\\)，和输出节点\\(i\\)，当然一般是取隐层状态。 \\[ a_i = F(i, j), \\quad j \\in [1, T_x], \\quad h(j)\\,Encoder, \\; H(i)\\,Decoder \\] \\(\\color{blue}{F(i, j)}\\)代表\\(y_i\\)和\\(x_j\\)的对齐可能性。一般F输出后，再经过softmax就得到了注意力分配概率。 AM模型的意义 一般地，会把AM模型看成单词对齐模型，输入句子单词和这个目标生句子成单词的对齐概率。 其实，理解为影响力模型也是合理的。就是在生成目标单词的时候，输入句子中的每个单词，对于生成当前目标单词有多大的影响程度。 AM模型有很多的应用，思想大都如此。 文本摘要例子 比如文本摘要的例子，输入一个长句，提取出重要的信息。 输入&quot;russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism&quot;。 输出&quot;russia calls for joint front against terrorism&quot;。 下图代表着输入单词对输出单词的影响力，颜色越深，影响力越大，注意力分配概率也越大。 PyTorch翻译AM实现 思想 参考这篇论文 。 生成目标单词\\(y_i\\) 的计算概率是 \\[ p(y_i \\mid (y_1,\\cdots, y_{i-1}), x) = g(y_{i-1}, s_i, c_i) \\] 符号意义说明 \\(y_i\\) 当前应该生成的目标单词，\\(y_{i-1}\\) 上一个节点的输出单词 \\(s_i\\) 当前节点的隐藏状态 \\(c_i\\) 生成当前单词应该有的语义向量 \\(g\\) 全连接层的函数 隐层状态\\(s_i\\) 求当前Decoder隐层状态\\(s_i\\)：由上一层的隐状态\\(s_{i-1}\\)，输出单词\\(y_{i-1}\\) ，语义向量\\(c_i\\) \\[ s_i = f(s_{i-1}, y_{i-1}, c_i) \\] 语义向量\\(c_i\\) 语义向量：分配权值\\(a_{ij}\\)，Encoder的输出 \\[ c_i = \\sum_{j=1}^{T_x} a_{ij} \\cdot h_j, \\quad h_j = h(x_j) \\] 分配概率\\(a_{ij}\\) 注意力分配概率\\(a_{ij} ，\\) \\(y_i\\) 收到\\(x_j\\) 的注意力：分配能量\\(e_{ij}\\) \\[ a_{ij} = \\frac{\\exp(e_{ij})} {\\sum_{k=1}^{T_x} \\exp (e_{ik})} \\] 分配能量\\(e_{ij}\\) \\(x_j\\) 注意\\(y_i\\) 的能量，由encoder的隐状态\\(h_j\\) 和 decoder的上一层的隐状态\\(s_{i-1}\\) 计算而成。a函数就是一个线性层。也就是上面的F函数。 \\[ e_{ij} = a(s_{i-1}, h_j) \\] 实现 Decoder由4层组成 embedding : word2vec attention layer: 为每个encoder的output计算Attention RNN layer: output layer: Decoder输入 \\(s_{i-1}\\) , \\(y_{i-1}\\) 和encoder的所有outputs \\(h_*\\) Embedding Layer 输入\\(y_{i-1}\\)，对其进行编码 12# y(i-1)embedded = embedding(last_rnn_output) Attention Layer 输入\\(s_{i-1}, h_j\\)，输出分配能量\\(e_{ij}\\)， 计算出\\(a_{ij}\\) 12attn_weights[j] = attn_layer(last_hidden, encoder_outputs[j])attn_weights = normalize(attn_weights) 计算语义向量 求语义向量\\(c_i\\)， 一般是加权求和 1context = sum(attn_weights * encoder_outputs) RNN Layer 输入\\(s_{i-1}, y_{i-1}, c_i\\) ，内部隐层状态，输出\\(s_i\\) 12rnn_input = concat(embeded, context)rnn_output, rnn_hidden = rnn(rnn_input, last_hidden) 输出层 输入\\(y_{i-1}, s_i, c_i\\) ，输出\\(y_i\\) 1output = out(embedded, rnn_output, context)","tags":[{"name":"注意力","slug":"注意力","permalink":"http://plmsmile.github.io/tags/注意力/"},{"name":"机器翻译","slug":"机器翻译","permalink":"http://plmsmile.github.io/tags/机器翻译/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"Attention Model","slug":"Attention-Model","permalink":"http://plmsmile.github.io/tags/Attention-Model/"},{"name":"Encoder-Decoder","slug":"Encoder-Decoder","permalink":"http://plmsmile.github.io/tags/Encoder-Decoder/"}]},{"title":"PyTorch快速上手","date":"2017-10-05T05:30:54.000Z","path":"2017/10/05/23-pytorch-start/","text":"PyTorch上手及其常用API总结 PyTorch介绍 PyTorch Torch 是一个使用Lua 语言的神经网络库，而PyTorch是Torch在Python的衍生。 PyTorch是一个基于python的科学计算包。本质上是Numpy的代替者，支持GPU、带有高级功能，可以用来搭建和训练深度神经网络；是一个深度学习框架，速度更快，弹性更好。 PyTorch和Tensorflow Tensorflow 类似一个嵌入Python的编程语言。写的Tensorflow代码会被Python编译成一张计算图，然后由TensorFlow执行引擎运行。Tensorflow有一些额外的概念需要学习，上手时间慢。 对比参考这篇文章PyTorch还是Tensorflow 。下面是结论。后续再补充详细内容。 PyTorch更有利于研究人员、爱好者、小规模项目等快速搞出原型，易于理解。而TensorFlow更适合大规模部署，特别是需要跨平台和嵌入式部署时。 PyTorch和Tensorflow对比如下 PyTORCH Tensorflow 动静态 建立的神经网络是动态的 建立静态计算图 代码难度 易于理解，好看一些。有弹性 底层代码难以看懂 工业化 好上手 高度工业化 数据操作 Tensor 创建Tensor Tensor实际上是一个数据矩阵，PyTorch处理的单位就是一个一个的Tensor。下面是一些创建方法 12345678910111213import torch# 1. 未初始化，都是0x = torch.Tensor(5, 3)# 2. 随机初始化x = torch.rand(5, 3)# 3. 传递参数初始化x = torch.Tensor([1, 2])# 4. 通过Numpy初始化a = np.ones(5)b = torch.from_numpy(a)# 5. 获取size，返回一个tuple [5, 3]print x.size() Tensor也可以通过Numpy来进行创建，或者从Tensor得到一个Numpy。 1234import numpy as npa = np.ones(5)b = torch.from_numpy(a)c = b.numpy() Tensor操作 Tensor的运算也很简单，一般的四则运算都是支持的。 123456789101112x = torch.rand(5, 3)y = torch.rand(5, 3)# 1. 直接相加z = x + y# 2. torch相加z = torch.add(x, y)# 3. 传递参数返回结果result = torch.rand(5, 3)torch.add(x, y, out = result)# 4. 加到自身去，自身y会改变y.add_(x) 其中所有类似于x.add_(y)的操作都会改变自己x，如x.copy_(y) 、x.t_() 。 对于Tensor可以像Numpy那样索引和切片。 改变Tensor和Numpy 改变Tensor后，对应的Numpy也会发生改变 1234567a = torch.ones(5)b = a.numpy()# 改变aa.add_(1)# b也会改变print a # 22222print b # 22222 CUDA Tensors 使用GPU很简单，只需使用.cuda就可以了 1234if torch.cuda.is_available(): x = x.cuda() y = y.cuda() x + y Variable 在神经网络中，最重要的是torch.autograd这个包，而其中最重要的一个类就是Variable。 本质上Variable和Tensor没有区别，不过Variable会放入一个计算图，然后进行前向传播，反向传播和自动求导。这也是PyTorch和Numpy不同的地方。 Variable由data, grad, creator 三部分组成。 data: 包装的Tensor，即数据 grad: 方向传播的梯度缓冲区 creator: 得到这个Variable的操作，如乘法加法等等。 用一个Variable进行计算，返回的也是一个同类型的Variable。 梯度计算例子 线性计算\\(z= 2 \\cdot x + 3 \\cdot y + 4\\) ，求\\(\\frac{ \\partial z}{\\partial x}\\) 和\\(\\frac{ \\partial z}{\\partial y}\\) 。 123456789101112import torchfrom torch.autograd import Variable# 1. 准备式子# 默认求导是falsex = Variable(torch.Tensor([2]), requires_grad = True)y = Variable(torch.Tensor([3]), requires_grad = True)z = 2 * x + 3 * y + 4# 2. z对x和y进行求导z.backward()# 3. 获得z对x和y的导数print x.grad.data # 2print y.grad.data # 3 复杂计算$ y = x + 2$， \\(z = y * y * 3\\)， \\(o = avg(z)\\) ，求\\(\\frac{dz}{dx}\\) 1234567x = Variable(torch.ones(2, 2), requires_grad = True)y = x + 2z = y * y * 3out = z.mean()out.backward()# d(out)/dxprint x.grad 传递梯度 12345x = Variable(torch.Tensor([2]), requires_grad = True)y = x + 2gradients = torch.FloatTensor([0.01, 0.1, 1])y.backward(gradients)print x.grad.data 一些常用的API总结 12345678910111213x = torch.Tensor([2, 2])# 随机创建数字x = torch.randn(3)x = torch.randn(3, 3)# 求平均值x.mean()# 范数x.norm()# torch view。数据相同，改变形状。得到一个Tensorx = torch.randn(4, 4)y = x.view(16)z = x.view(2, 2, 4)z = x.view(2, 2, -1) # 最后-1，会自己适配 神经网络","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://plmsmile.github.io/tags/PyTorch/"},{"name":"神经网络","slug":"神经网络","permalink":"http://plmsmile.github.io/tags/神经网络/"},{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"}]},{"title":"最初RNN神经翻译简略笔记","date":"2017-10-02T02:03:31.000Z","path":"2017/10/02/NMT/","text":"只是一个Seq2seq的在机器翻译中的简略笔记 Basic 背景知识 传统翻译是以词为核心一词一词翻译的，这样会切断句子本身的意思，翻译出来也很死板，不像我们人类说的话。 Encoder-Decoder 现在采用Encoder-Decoder架构模型。如下图 Encoder会利用整个原始句子生成一个语义向量，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句子的意思、句法结构、性别信息等等。具体框架可以参考Encoder-Decoder框架。 举个翻译的例子，原始句子\\(X = (x_1, x_2, \\cdots, x_m)\\) ，翻译成目标句子\\(Y = (y_1, y_2, \\cdots, y_m)\\) 。 Encoder对\\(X\\) 进行非线性变换得到中间语义\\(C\\) \\[ C = \\Gamma(x_1, x_2, \\cdots, x_n) \\] Decoder根据语义\\(C\\) 和生成的历史信息\\(y_1, y_2, \\cdots, y_{i-1}\\) 来生成第\\(i\\) 个单词 \\(y_i\\) \\[ y_i = \\Psi(C, y_1, y_2, \\cdots, y_{i-1}) \\] 当然，在Attention Model 中，Decoder生成Y的时候每个单词对应的\\(C\\)不一样，记作\\(C_j, j \\in [1, n]\\) 。\\(C_j\\) 就是体现了源语句子中不同的单词对目标句子中不同的单词的注意力概率分布。即各个单词的对齐的概率，也就是student对&quot;学生&quot;更重要，而对&quot;我&quot;不那么重要。这个在后续会用到。 Encoder-Decoder是个创新大杀器，是个通用的计算框架。Encoder和Decoder具体使用什么模型，都可以自己选择。通常有CNN,RNN,BiRNN,GRU,LSTM, Deep LSTM。比如编码CNN-解码RNN, 编码BiRNN-解码Deep LSTM等等。 上面的内容任意组合，只要得到的效果好，就是一个创新，就可以毕业了。（当然别人没有提出过） NMT模型选择 有3个维度需要选择。 方向性。是单向还是双向 深度：是一层还是多层 网络选择：encoder和decoder具体分别选什么 在本文的实现中，我们选择单向的、多层的、LSTM。基于这篇论文。如下图。","tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"机器翻译","slug":"机器翻译","permalink":"http://plmsmile.github.io/tags/机器翻译/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"}]},{"title":"条件随机场","date":"2017-09-28T03:17:59.000Z","path":"2017/09/28/crf/","text":"条件随机场(Conditional Random Field, CRF)是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型。常常用于标注问题。隐马尔科夫模型和条件随机场是自然语言处理中最重要的算法。CRF最重要的就是根据观测序列，把标记序列给推测出来。 概率无向图模型 概率无向图模型又称为马尔科夫随机场，是一个可以由无向图表示的联合概率分布。一些类似内容。 有一组随机变量\\(Y \\in \\Gamma\\)，联合概率分布为\\(P(Y)\\)，由图\\(G=(V,E)\\)表示。节点v代表变量\\(Y_v\\)，节点之间的边代表两个变量的概率依赖关系。 定义 马尔可夫性就是说，给定一些条件下，没有连接的节点之间是条件独立的。 成对马尔可夫性 设\\(u\\)和\\(v\\)是两个没有边连接的节点，其它所有节点为\\(O\\)。成对马尔可夫性是说，给定随机变量组\\(Y_O\\)的条件下，随机变量\\(Y_u\\)和\\(Y_v\\)是独立的。即有如下： \\[ P(Y_u, Y_v \\mid Y_O) = P(Y_u \\mid Y_O)P(Y_v \\mid Y_O) \\] 局部马尔可夫性 节点\\(v\\)，\\(W\\)是与\\(v\\)连接的所有节点，\\(O\\)是与\\(v\\)没有连接的节点。局部马尔可夫性认为，给定\\(Y_w\\)的条件下，\\(Y_v\\)和\\(Y_O\\)独立。即有： \\[ P(Y_v, Y_O \\mid Y_W) = P(Y_v \\mid Y_W) P(Y_O \\mid Y_W) \\] 全局马尔可夫性 节点集合\\(A\\)，\\(B\\)被中间节点集合\\(C\\)分隔开，即不相连。全局马尔可夫性认为，给定\\(Y_C\\)的条件下，\\(Y_A\\)和\\(Y_B\\)是独立的。即有： \\[ P(Y_A, Y_B \\mid Y_C) = P(Y_A \\mid Y_C) P(Y_B \\mid Y_C) \\] 上面的3个马尔可夫性的定义是等价的。 概率无向图模型 设有联合概率密度\\(P(Y)\\)，由无向图\\(G=(V,E)\\)表示。节点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率密度\\(P(Y)\\)满足马尔可夫性，那么就称此联合概率分布为概率图模型，或马尔可夫随机场。 实际上我们更关心怎么求联合概率密度，一般是把整体的联合概率写成若干个子联合概率的乘积，即进行因子分解。概率无向图模型最大的优点就是易于因子分解。 概率无向图因子分解 团与最大团 团：无向图中的一个子集，任何两个节点均有边连接。 最大团：无向图中的一个子集，任何两个节点均有边连接。不能再加入一个节点组成更大的团了。 如\\(\\{Y_1, Y_2\\}\\)，\\(\\{Y_1, Y_2, Y_3\\}\\) 都是团，其中后者是最大团。而\\(\\{Y_1, Y_2, Y_3, Y_4\\}\\) 不是团，因为\\(Y_1\\)和\\(Y_4\\)没有边连接。 因子分解 有无向图模型\\(G\\), \\(C\\) 是 \\(G\\) 上的最大团，有很多个。\\(Y_C\\) 是\\(C\\) 对应的随机变量。则联合概率分布\\(P(Y)\\) 可以写成多个最大团\\(C\\) 上的势函数的乘积。 \\[ \\color{blue}{P(Y)} = \\frac {1} {Z} \\prod_C \\Psi_C(Y_C), \\quad Z = \\sum_Y \\prod_C \\Psi_C(Y_C) \\] 其中\\(Z\\)是规范化因子。\\(\\Psi_C(Y_C)\\)是 势函数，是一个严格正函数。等式左右两端都取条件概率也是可以的。下文就是。 \\[ \\color{blue}{\\Psi_C(Y_C)} = \\exp \\left(-E(Y_C) \\right) \\] 其中\\(\\color{blue}{E(Y_C) }\\) 是能量函数。 条件随机场的定义与形式 HMM的问题 这里是HMM的讲解 。HMM有下面几个问题 需要给出隐状态和观察符号的联合概率分布，即发射概率 \\(b_j(k)\\)，是生成式模型，也是它们的通病。 观察符号需要是离散的，可以枚举的，要遍历所有观察符号。如果是一个连续序列，则不行。 观察符号是独立的，没有观察相互之间的依赖关系。如一个句子的前后，都有关联才是。即输出独立性假设问题。 无法考虑除了字词顺序以外的其它特征。比如字母为大小写，包含数字等。 标注偏置问题。 标注偏置问题，举例，是说有两个单词&quot;rib-123&quot;和&quot;rob-456&quot;，&quot;ri&quot;应该标记为&quot;12&quot;，&quot;ro&quot;应该标记为&quot;45&quot;。 \\[ \\begin {align} &amp; P(12 \\mid ri) = P(1 \\mid r)P(2 \\mid i, r=1) = P(1 \\mid r) \\cdot 1 = P(1 \\mid r) \\\\ &amp; P(45 \\mid ro) = P(4 \\mid r)P(5 \\mid o, r=4) = P(4 \\mid r) \\cdot 1 = P(4 \\mid r) \\\\ \\end {align} \\] 由上面计算概率可知，ri标为12和 ro标为45的概率最终变成r标为1和4的概率。但是由于语料库中&quot;rob&quot;的出现次数很多，所以\\(P(4 \\mid r) &gt; P(1 \\mid r)\\) ，所以可能会一直把&quot;rib&quot;中的&quot;i&quot;标记为1，会导致标记出错。这就是标记偏置问题。 定义 条件随机场是给定随机变量\\(X\\)条件下，随机变量\\(Y\\) 的马尔可夫随机场。我们主要关心线性链随机场，它可以用于标注问题。 条件随机场 \\(X\\)与\\(Y\\)是随机变量，条件概率分布\\(P(Y \\mid X)\\)。随机变量\\(Y\\)可以构成一个无向图表示的马尔可夫随机场。任意一节点\\(Y_v\\)，\\(Y_A\\)是与\\(v\\)相连接的节点，\\(Y_B\\)是除了\\(v\\)以外的所有节点。若都有 \\[ P(Y_v \\mid X, Y_B) = P(Y_v \\mid X, Y_A) \\] 则称\\(P(Y \\mid X)\\) 为条件随机场。并不要求\\(X\\)和\\(Y\\) 具有相同的结构。 线性链条件随机场 \\(X\\)和\\(Y\\) 有相同的线性结构。设\\(X = (X_1, X_2, \\cdots, X_n)\\)，\\(Y = (Y_1, Y_2, \\cdots, Y_n)\\)均为线性链表示的随机变量序列。每个最大团包含2个节点。 \\(P(Y \\mid X)\\) 构成条件随机场，即满足马尔可夫性 \\[ P(Y_i \\mid X, Y_1, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_n) = P(Y_i \\mid X, Y_{i-1}, Y_{i+1}), \\quad i=1,\\cdots, n。 \\; (1和n时只考虑单边) \\] 则称\\(P(Y \\mid X)\\)为线性链条件随机场。 HMM，每个观察状态只与当前的隐状态有关系，分离了关系。就像1个字1个字地向后讲。输出观察符号还需要条件独立。 线性链条件随机场， 每个状态都与整个序列有关系。即先想好了整句话，再依照相应的次序去说出来。更加直击语言模型的核心。\\(X_1, X_2\\) 不需要条件独立。 基本形式 两种特征函数 状态转移特征函数t，只依赖与当前和前一个位置，即\\(y_i\\)和\\(y_{i-1}\\)。一般是01函数。 \\[ t(y_{i-1}, y_i, x, i) = \\begin {cases} 1, \\quad &amp; 满足某种条件, i \\in [2, n]. \\;例如y_{i-1}+y_{i}=3 \\\\ 0, \\quad &amp; 其他 \\end {cases} \\] 状态特征函数s，只依赖与当前位置\\(y_i\\) \\[ s(y_i, x, i) = \\begin {cases} 1, \\quad &amp; 满足某种条件, i \\in [1, n]. \\;例如y_{i}是偶数 \\\\ 0, \\quad &amp; 其他 \\end {cases} \\] 基础形式 设有\\(K_1\\)个状态特征转移函数，\\(K_2\\)个状态特征函数。分别对应的权值是\\(\\lambda_{k_1}\\)和\\(\\mu_{k_2}\\)。则线性链条件随机场参数化形式\\(P(y \\mid x)\\) 如下： \\[ P(y \\mid x) = \\frac {1}{Z(x)} \\exp \\left( \\sum_k^{K_1}\\lambda_k \\sum_{i=2}^n t_k(y_{i-1}, y_i, x, i) + \\sum_k^{K_2}\\mu_k \\sum_{i=1}^n s_k(y_i, x, i) \\right) \\] 其中\\(Z(x)\\)是规范化因子，如下 \\[ Z(x) = \\sum_x \\exp \\left( \\sum_k^{K_1}\\lambda_k \\sum_{i=2}^nt_k(y_{i-1}, y_i, x, i) + \\sum_k^{K_2}\\mu_k \\sum_{i=1}^n s_k(y_i, x, i) \\right) \\] 条件随机场完全由特征函数\\(t_{k_1}\\) 、\\(s_{k_2}\\)，和对应的权值\\(\\lambda_{k_1}\\) 和\\(\\mu_{k_2}\\) 决定的。 特征函数实际上也是势函数。 简化形式 有\\(K=K_1 + K_2\\)个特征，特征函数如下： \\[ f_k(y_{i-1}, y_i, i) = \\begin {cases} t_k(y_{i-1}, y_i, x, i) \\quad &amp; k = 1, \\cdots, K_1 \\\\ s_k(y_i, x, i) \\quad &amp; k = K_1 + l; \\; l = 1, \\cdots, K_2 \\\\ \\end {cases} \\] 同一个特征函数，要在整个\\(Y​\\)序列的各个位置进行计算，可以进行求和，即转化为全局特征函数， 新的特征函数\\(f_k (y, x)​\\)如下： \\[ \\color{blue} {f_k (y, x)} = \\sum _{i=1} ^n f_k(y_{i-1}, y_i, i), \\quad k = 1, \\cdots, K \\] \\(f_k (y, x)\\) 对应的新的权值\\(w_k\\)如下 \\[ \\color{blue} {w_k} = \\begin {cases} \\lambda_{k}, \\quad &amp; k = 1, \\cdots, K_1 \\\\ \\mu_{k - K_1}, \\quad &amp; k = K_1 + 1, K_1 + 2, \\cdots, K \\\\ \\end {cases} \\] 所以新的条件随机场形式如下： \\[ P(y \\mid x) = \\frac {1} {Z(x)} \\exp \\sum_{k=1} ^K w_k f_k(y, x) , \\quad Z(x) = \\sum_y \\exp \\sum_{k=1} ^K w_k f_k(y, x) \\] 可以看出，格式和最大熵模型很像。条件随机场最重要的就是，根据观察序列，把标记序列给推测出来。 向量形式 向量化特征函数和权值 \\[ F(y, x) = (f_1(y, x), \\cdots, f_K(y, x))^T, \\quad w = (w_1, \\cdots, w_K)^T \\] 可以写成向量内积的形式 \\[ P_w (y \\mid x) = \\frac{1}{Z(x)} \\exp (w \\cdot F(y, x)) , \\quad Z(x) = \\sum_y \\exp (w \\cdot F(y, x)) \\] 矩阵形式 为状态序列\\(Y\\)设置起点和终点标记，\\(y_0 = start\\) 和\\(y_{n+1} = stop\\)。从\\(0 \\to n+1\\)中，有\\(n+1\\)次的状态转移。我们可以用\\(n+1\\) 个状态转移矩阵来表示状态转移的概率。 设\\(\\color{blue}{M_i(x)}\\) 是\\(i-1 \\to i\\)的转移矩阵，是\\(m\\)阶，\\(m\\) 是\\(y_i\\) 取值的个数。表示了各种取值情况互相转化的概率。 \\[ M_1(x) = \\begin{bmatrix} a_{01} &amp; a_{02} \\\\ 0 &amp; 0 \\\\ \\end{bmatrix} , \\; M_2(x) = \\begin{bmatrix} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\\\ \\end{bmatrix} , \\; M_3(x) = \\begin{bmatrix} c_{11} &amp; c_{12} \\\\ c_{21} &amp; c_{22} \\\\ \\end{bmatrix} , \\; M_4(x) = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\] 要求什么样的路径概率，则相乘相应的数值概率即可。那么这些矩阵里面的数值是怎么来的呢。有下面的矩阵定义 ： \\[ \\color{blue} {g(y_{i-1}, y_i, x, i)} = M_i(y_{i-1}, y_i \\mid x) = \\exp \\sum_{k=1}^K w_kf_k(y_{i-1}, y_i, i, x) ,\\quad \\color{blue} {M_i(x)} = [g(y_{i-1}, y_i, x, i) ] \\] 每一步转移的时候，由于\\((y_{i-1}, y_i)\\) 有\\(m^2\\) 种情况，计算\\(g\\)时会得到多个值，即可得到一个矩阵。 其中\\(g(y_{i-1}, y_i, x, i)\\) 在计算的时候，会根据\\(i\\) 的不同，而选择不同的特征函数进行计算。不要忘记了\\(f_k\\)函数的定义。 \\[ \\color{blue} {f_k(y_{i-1}, y_i, i)} = \\begin {cases} t_k(y_{i-1}, y_i, x, i) \\quad &amp; k = 1, \\cdots, K_1 \\\\ s_k(y_i, x, i) \\quad &amp; k = K_1 + l; \\; l = 1, \\cdots, K_2 \\\\ \\end {cases} \\] 所以非规范化条件概率可以通过矩阵的某些元素的乘积表示，有 \\[ P_w( y \\mid x) = \\frac {1}{Z_w(x)} \\prod_{i=1}^{n+1} M_i(y_{i-1}, y_i \\mid x) \\] 其中规范化因子 是\\(n+1\\)的矩阵相乘的结果矩阵中，第\\((start, stop)\\) 元素。例如第\\((0, 0)\\)。其中是\\((start, end)\\) 是矩阵下标对应。 条件随机场的概率计算问题 主要问题是给定条件随机场\\(P(Y \\mid X)\\) ，给定输入序列\\(x\\) 和输出序列\\(y\\)，求\\(P(Y_i = y_i \\mid x)\\) 和\\(P(Y_{i-1} = y_{i-1}, Y_i = y_{i} \\mid X)\\)， 以及相应的期望问题。 关键是求这些特征函数期望值，当模型训练好之后，去验证我们的模型。 前向后向算法 \\(y_i\\) 确定后， \\(\\color{blue} {\\alpha_i(y_i \\mid x) }\\) 表示，从\\(start \\to i\\)，就是$y = (start, y_1, , y_i) $ 的概率，也就是从前面到位置\\(y_i\\) 的概率。特别地 \\[ \\alpha_0(y \\mid x) = \\begin{cases} 1, \\quad &amp; y=start \\\\ 0, \\quad &amp; 其他 \\\\ \\end{cases} \\] 而\\(y_i​\\) 的取值有\\(m​\\) 种， 所以前向变量 \\(\\color{blue}{\\alpha_i(x)}​\\) 到\\(y​\\) 到第 \\(i​\\) 个位置 的所有概率取值向量。 \\[ \\color{blue} {\\alpha_i^T(x)} = \\alpha_{i-1}^T(x) \\cdot M_i(x) ,\\quad i = 1,2,\\cdots, n+1 \\] \\(y_i\\) 确定后， \\(\\color{blue} {\\beta_i (y_i \\mid x) }\\) 表示，位置\\(i\\)的标记为\\(y_i\\) ，并且后面为\\(y_{i+1}, \\cdots, y_n, stop\\) 的概率。同理一个是概率值。特别地 \\[ \\beta_{n+1}(y \\mid x) = \\begin{cases} 1, \\quad &amp; y=stop \\\\ 0, \\quad &amp; 其他 \\\\ \\end{cases} \\] 后向变量 \\(\\color{blue} {\\beta_i (y \\mid x) }​\\)，是一个m维向量 \\[ \\color{blue} {\\beta_i (x) } = M_{i+1}(x) \\cdot \\beta_{i+1}(x) \\] 可以得到\\(Z(x)\\)： \\[ Z(x) = \\alpha_n^T(x) \\cdot 1 = 1^T \\cdot \\beta_{i+1}(x) \\] 条件概率计算 位置是\\(i\\) 标记\\(y_i\\) 的条件概率\\(P(Y_i = y_i \\mid x)\\)是 \\[ P(Y_i = y_i \\mid x) = \\frac {1}{Z(x)} \\cdot \\alpha_i(y_i \\mid x) \\beta_i(y_i \\mid x) \\] 位置\\(i-1, i\\) 分别标记为\\(y_{i-1}, y_i\\) 的概率是 \\[ P(Y_{i-1} = y_{i-1}, Y_i = y_i \\mid x) = \\frac{1}{Z(x)} \\cdot \\alpha_{i-1}(y_{i-1} \\mid x) M_i(y_{i-1}, y_i \\mid x) \\beta_i (y_i \\mid x) \\] 特征期望值计算 两个期望值和最大熵模型的约束条件等式 有点像。 特征函数\\(f_k\\) 关于条件概率分布\\(P(Y \\mid X)\\) 的概率 \\[ E_{P(Y \\mid X)}(f_k) = \\sum_y P(y \\mid x) f_k(y, x) = \\sum_{i=1}^{n+1}\\sum_{y_{i-1}y_i} f_k(y_{i-1}, y_i, x, i) P(y_{i-1}, y_i \\mid x) \\] 特征函数\\(f_k\\) 关于条件概率分布\\(P(X, Y)\\) 的概率，\\(\\hat P(x)\\) 是经验分布 \\[ E_{P(X, Y)}(f_k) = \\sum_{x, y} P(x, y) \\sum_{i=1}^{n+1} f_k(y_{i-1}, y_i, x, i) = \\sum_{x} \\hat P(x) \\sum_y P(y \\mid x) \\sum_{i=1}^{n+1} f_k(y_{i-1}, y_i, x, i) \\] 通过前向和后向向量可以计算出两个概率，然后可以计算出相应的期望值。就可以与我们训练出的模型进行比较。 学习算法 条件随机场模型实际上是定义在时序数据上的对数线性模型，学习方法有极大似然估计和正则化的极大似然估计。具体的优化实现算法有：改进的迭代尺度法IIS、梯度下降法和拟牛顿法。 改进的迭代尺度法 这里是最大熵模型中的改进的迭代尺度算法。每次更新一个\\(\\delta_i\\) 使得似然函数的该变量的下界增大，即似然函数增大。 已知经验分布\\(\\hat P(X, Y)\\)，和模型如下 \\[ P(y \\mid x) = \\frac {1} {Z(x)} \\exp \\sum_{k=1} ^K w_k f_k(y, x) , \\quad Z(x) = \\sum_y \\exp \\sum_{k=1} ^K w_k f_k(y, x) \\] 对数似然函数和最大熵算法的极大似然函数很相似，如下： \\[ L(w) = L_{\\hat P}(P_w) = \\log \\prod_{x,y} P_w(y \\mid x) ^ {\\widetilde P(x, y)} = \\sum_{x,y} \\widetilde P(x, y) \\log P_w(y \\mid x) \\] 对数似然函数\\(L(w)\\) \\[ L(w) = \\sum_{j=1}^{N} \\sum_{k=1}^K w_k f_k(y_j, x_j) - \\sum_{j=1}^N \\log Z_w(x_j) \\] 数据\\((x, y)\\) 中出现的特征总数\\(T(x, y)\\) ： \\[ T(x, y) = \\sum_k f_k(y, x) = \\sum_{k=1}^K \\sum_{i=1}^{n+1}f_k(y_{i-1}, y_i, x) \\] 输入：特征函数\\(t_1, t_2, \\cdots, t_{K_1}\\)和\\(s_1, s_2, \\cdots, s_{K_2}\\) ；经验分布\\(\\hat P(x, y)\\) 输出：模型参数\\(\\hat w\\)，模型\\(P_{\\hat w}\\) 步骤： 1 赋初值 \\(w_k = 0\\) 2 对所有\\(k\\)，求解方程，解为\\(\\delta_k\\) \\[ \\begin{align} &amp; \\sum_{x, y} \\hat P(x) P(y \\mid x) \\sum_{i=1}^{n+1} t_k(y_{i-1}, y_i, x, i) \\exp (\\delta_k T(x, y)) = E_{\\hat p}[t_k] , \\quad k = 1, 2, \\cdots, K_1 时 \\\\ &amp; \\sum_{x, y} \\hat P(x) P(y \\mid x) \\sum_{i=1}^{n+1} s_l(y_{i-1}, y_i, x, i) \\exp (\\delta_k T(x, y)) = E_{\\hat p}[s_l] , \\quad k = K_1 + l, l = 1, 2, \\cdots, K_2 时 \\\\ \\end{align} \\] 3 更新\\(w_k + \\delta_k \\to w_k\\) ，如果还有\\(w_k\\)未收敛，则继续2 算法S 对于不同的数据\\((x, y)\\)的特征出现次数\\(T(x, y)\\) 可能不同，可以选取一个尽量大的数\\(S\\)作为特征总数，使得所有松弛特征\\(s(x, y) \\ge 0\\) ： \\[ s(x, y) = S - \\sum_{i=1}^{n+1}\\sum_{k=1}^K f_k(y_{i-1}, y_i, x, i) \\] 所以可以直接解得\\(\\delta_k\\) ，当然\\(f_k\\) 要分为\\(t_k\\)和\\(s_k\\)，对应的期望值计算也不一样。具体见书上。 \\[ \\delta_k = \\frac{1}{S} \\log \\frac{E_{ \\hat P}[f_k] } {E_P[f_k]} \\] 算法T 算法S中\\(S\\)会选择很大，导致每一步的迭代增量会加大，算法收敛会变慢，算法T重新选择一个特征总数 T(x) \\[ T(x) = \\max \\limits_y T(x, y) \\] 使用前后向递推公式，可以算得\\(T(x)=t\\) 。 对于\\(k \\in [1, K_1]\\)的\\(t_k\\)关于经验分布的期望： \\[ E_{\\hat P}[t_k] = \\sum_{t=0}^{T_{max}} a_{k,t} \\beta_{k}^t \\] 其中，\\(a_{k,t}\\)是\\(t_k\\)的期待值， \\(\\delta_k = \\log \\beta_k\\) 对于\\(k \\in [1+K_1, K]\\)的\\(s_k\\)关于经验分布的期望： \\[ E_{\\hat P}[s_k] = \\sum_{t=0}^{T_{max}} b_{k,t} \\gamma_{k}^t \\] 其中\\(\\gamma_k^t\\)是特征\\(s_k\\)的期望值，\\(\\delta_k = \\log \\gamma_k\\)。当然，求根也可以使用牛顿法去求解。 拟牛顿法 预测算法 预测问题 给定条件随机场\\(P(Y \\mid X)\\)和输入序列\\(x\\)，求条件概率最大的输出序列（标记序列）\\(y^*\\)，即对观测序列进行标注。 \\[ \\begin{align} y^* &amp; = \\arg \\max \\limits_y P_w(y \\mid x) = \\arg \\max \\limits_y \\frac{\\exp (w \\cdot F(y, x))}{Z_w(x)} \\\\ &amp; = \\arg \\max \\limits_y ( w \\cdot F(y, x)) \\\\ \\end {align} \\] 其中路径\\(y\\)表示标记序列，下面是参数说明 \\[ \\begin {align} &amp; w = (w_1, w_2, \\cdots, w_k)^T \\\\ &amp; F(y, x) = (f_1(y, x), \\cdots, f_K(y, x))^T, \\quad w = (w_1, \\cdots, w_K)^T \\\\ &amp; f_k (y, x) = \\sum _{i=1} ^n f_k(y_{i-1}, y_i, x, i) \\\\ &amp; F_i(y_{i-1}, y_i, x) = \\left(f_1(y_{i-1}, y_i, x, i), f_2(y_{i-1}, y_i, x, i),\\cdots, f_k(y_{i-1}, y_i, x, i) \\right)^T \\end {align} \\] 所以，为了求解最优路径，只需计算非规范化概率，即转换为下面的问题： \\[ \\max \\limits_y \\quad \\sum_{i=1}^n w \\cdot F_i(y_{i-1}, y_i, x) \\] 维特比算法 HMM的维特比算法。 维特比变量\\(\\delta_i(l)\\)，到达位置\\(i\\)， 标记为\\(l \\in [1, m]\\) 的概率 \\[ \\delta_i(l) = \\max \\limits_{1 \\le j \\le m} \\{ \\delta_{i-1}(j) + w \\cdot F_i(y_{i-1} = j, y_i = l, x) \\}, \\quad j = 1, 2, \\cdots, m \\] 记忆路径\\(\\psi_i(l) = a\\) 当前时刻\\(t\\)标记为l, \\(t-1\\)时刻标记为a \\[ \\psi_i(l) = = \\arg \\max \\limits_{1 \\le j \\le m} \\{ \\delta_{i-1}(j) + w \\cdot F_i(y_{i-1} = j, y_i = l, x) \\} \\] 算法主体 输入：特征向量\\(F(y, x)\\)和权值向量\\(\\mathbf{w}\\)，观测向量\\(x = (x_1, x_2, \\cdots. x_n)\\) 输出：最优路径\\(y^* = (y_1^*, y_2^*, \\cdots, y_n^*)\\) 步骤如下 初始化 \\[ \\delta_1(j) = w \\cdot F_1(y_0 = start, y_1 = j, x), \\quad j = 1, \\cdots, m \\] 递推 \\[ \\begin{align} &amp; \\delta_i(l) = \\max \\limits_{1 \\le j \\le m} \\{ \\delta_{i-1}(j) + w \\cdot F_i(y_{i-1} = j, y_i = l, x) \\}, \\quad j = 1, 2, \\cdots, m \\\\ &amp; \\psi_i(l) = \\arg \\max \\limits_{1 \\le j \\le m} \\delta_i(j), \\quad \\text{即上式的参数j} \\\\ \\end{align} \\] 终止 \\[ \\begin{align} &amp; \\max \\limits_y (w \\cdot F(y, x)) = \\max \\limits_{1 \\le j \\le m} \\delta_n(j) \\\\ &amp; y_n^* = \\arg \\max \\limits_{1 \\le j \\le m} \\delta_n(j) \\\\ \\end{align} \\] 返回路径 \\[ y_i^* = \\psi_{i+1} (y_{i+1}^*), \\quad i = n-1, n-2, \\cdots, 1 \\] 求得最优路径\\(y^* = (y_1^*, y_2^*, \\cdots, y_n^*)\\)","tags":[{"name":"条件随机场","slug":"条件随机场","permalink":"http://plmsmile.github.io/tags/条件随机场/"},{"name":"概率无向图","slug":"概率无向图","permalink":"http://plmsmile.github.io/tags/概率无向图/"},{"name":"概率计算问题","slug":"概率计算问题","permalink":"http://plmsmile.github.io/tags/概率计算问题/"},{"name":"学习算法","slug":"学习算法","permalink":"http://plmsmile.github.io/tags/学习算法/"},{"name":"预测问题","slug":"预测问题","permalink":"http://plmsmile.github.io/tags/预测问题/"},{"name":"维特比","slug":"维特比","permalink":"http://plmsmile.github.io/tags/维特比/"},{"name":"前向后向","slug":"前向后向","permalink":"http://plmsmile.github.io/tags/前向后向/"},{"name":"迭代尺度法","slug":"迭代尺度法","permalink":"http://plmsmile.github.io/tags/迭代尺度法/"}]},{"title":"最大熵模型","date":"2017-09-20T09:39:12.000Z","path":"2017/09/20/maxentmodel/","text":"机器学习中最大熵模型的介绍，包括模型思想、学习问题、学习算法等 最大熵原理 预备知识 离散型变量\\(X\\)的概率分布是\\(\\color{blue}{P(X)}\\)。它的熵\\(\\color{blue}{H(X) \\; or \\; H(P)}\\)越大，代表越均匀、越混乱、越不确定。各种熵点这里 \\[ \\color{blue}{H(P)} = \\color{red} {- \\sum_{x \\in X}P(x) \\log P(x)} \\] 熵满足下面不等式 \\[ 0 \\le H(P) \\le \\log |X|, \\quad 其中|X|是X的取值个数 \\] 当前仅当\\(X\\)的分布是均匀分布的时候等号成立。当\\(X\\)服从均匀分布时，熵最大。 最大熵的思想 最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。 事情分为两个部分：确定的部分（约束条件）和不确定的部分。选择模型时要 要满足所有的约束条件，即满足已有的确定的事实 要均分不确定的部分 \\(X\\)有5个取值\\(\\{A, B,C,D,E\\}\\)，取值概率分别为\\(P(A), P(B), P(C), P(D), P(E)\\)。满足以下约束条件 \\[ P(A)+ P(B)+ P(C)+ P(D)+ P(E) = 1 \\] 满足这个条件的模型有很多。再加一个约束条件 \\[ P(A) + P(B) = \\frac{3}{10} \\] 则，满足约束条件，不确定的平分（熵最大）：这样的模型是最好的模型 \\[ P(A) = P(B) = \\frac{3}{20}, \\quad P(C)=P(D)=P(E) = \\frac{7}{30} \\] 即：约束条件，熵最大 最大熵模型 假设分类模型是一个条件概率分布\\(\\color{blue}{P(Y \\mid X)}\\)。(有的不是选择条件模型，如论文里面)。训练数据集\\(N\\)个样本 \\(T = \\{(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}\\) 基本概念 联合分布：\\(\\color{blue}{P(X, Y)}\\) 边缘分布：\\(\\color{blue}{P(X)}\\) 联合经验分布：\\(\\color{blue}{\\widetilde{P}(X, Y)} = \\color{red}{\\frac {v(X=x, Y=y)}{N}}\\)，其中\\(v(x, y)\\)为频数 联合边缘分布：\\(\\color{blue}{\\widetilde P(X) = \\color{red} {\\frac{v(X=x)}{N}}}\\) 特征函数\\(\\color{blue}{f(x, y)}\\)用来描述\\(x\\)和\\(y\\)满足的一个事实约束条件： \\[ f(x, y) = \\begin{cases} 1, \\quad &amp; x与y满足一个事实，即约束条件 \\\\ 0, \\quad &amp; 否则 \\end{cases} \\] 如果有\\(n\\)个特征函数\\(\\color{blue}{f_i(x, y), i = 1, 2, \\cdots, n}\\), 就有\\(n\\)个约束条件。 概率期望的计算 \\(X\\)的期望 \\(X\\) 是随机变量，概率分布是\\(P(X)\\) ，或概率密度函数是\\(f(x)\\) \\[ E(X) = \\begin{cases} &amp;\\sum_{i} x_i P(x_i), \\quad &amp; \\text{离散} \\\\ &amp; \\int_{-\\infty}^{+\\infty} {x \\cdot f(x)} \\, {\\rm d}x , \\quad &amp; \\text{连续} \\\\ \\end{cases} \\] 下面只考虑离散型的期望，连续型同理，求积分即可。 一元函数的期望 \\(Y = g(X)\\)，期望是 \\[ E[Y] = E[g(X)] = \\sum_{i}^{\\infty} g(x_i) \\cdot P(x_i) \\] 二元函数的期望 \\(Z = g(X, Y)\\) ，期望是 \\[ E(Z) = \\sum_{x, y} g(x, y) \\cdot p(x, y) = \\sum_{i=1} \\sum_{j=1} g(x_i, y_j) p(x_i, y_j) \\] 期望其实就是\\(E 狗 = \\sum 狗 \\cdot 老概率\\) 。可离散，可连续。 约束条件等式 实际分布期望 特征函数\\(f(x, y)\\)关于经验分布\\(\\color{blue}{\\widetilde P(x, y)}\\)的期望\\(\\color{blue}{E_{\\widetilde P}(f)}\\)，即实际应该有的特征 ，也就是一个给模型加的约束条件 ： \\[ \\color{blue}{E_{\\widetilde P}(f)} = \\sum_{x, y} \\color{red} {\\widetilde P(x,y)} f(x, y) \\] 理论模型期望 特征函数\\(f(x, y)\\) 关于模型\\(\\color{blue}{P(Y\\mid X)}\\)和经验分布\\(\\color{blue}{\\widetilde P(X)}\\)的期望\\(\\color{blue}{E_{P}(f)}\\) ，即理论上模型学得后的期望： \\[ \\color{blue}{E_{ P}(f)} =\\sum_{x, y} \\color{red} { \\widetilde{P}(x) P(y \\mid x)} f(x, y) \\] 要从训练数据中获取信息，特征函数关于实际经验分布和理论模型的两个期望就得相等，即理论模型要满足实际约束条件 \\[ E_{\\widetilde P}(f) = E_{ P}(f) \\] 最大熵模型思想 条件概率分布\\(P(Y \\mid X)\\)的条件熵为\\(\\color{blue}{H(P)}\\)如下，条件熵： \\[ \\color{blue}{H(P)} = \\color{red} {- \\sum_{x, y} \\widetilde P(x) P(y \\mid x) \\log P(y \\mid x)} \\] 则满足约束条件\\(\\color{blue}{E_{\\widetilde P}(f) = E_{ P}(f)}\\)的模型中，条件熵\\(\\color{blue}{H(P)}\\)最大的模型就是最大熵模型。 最大熵模型的学习 学习问题 最大熵模型的学习等价于约束最优化问题，这类问题可以用拉格朗日对偶性去求解。 给定数据集\\(T = \\{(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}\\)和特征函数\\(\\color{blue}{f_i(x, y), i = 1, 2, \\cdots, n}\\)。 要满足2个约束条件 \\[ \\color{red} {E_{\\widetilde P}(f) = E_{ P}(f), \\quad \\sum_{x, y}P(y \\mid x) = 1 } \\] 要得到最大化熵 \\[ \\max \\limits_{P \\in C} \\; \\color{blue}{H(P)} = \\color{red} {- \\sum_{x, y} \\widetilde P(x) P(y \\mid x) \\log P(y \\mid x)} \\] 按照最优化问题的习惯，将求最大值问题改写为等价的求最小值问题 ，如下 \\[ \\min \\limits_{P \\in C} \\; \\color{blue}{ - H(P)} = \\color{red} { \\sum_{x, y} \\widetilde P(x) P(y \\mid x) \\log P(y \\mid x)} \\] 推导最大熵模型 一般使用拉格朗日对偶性去求解，可以见李航书附录。 引入拉格朗日乘子\\(\\color{blue}{w = (w_0, w_1, \\cdots, w_n)}\\)，即参数向量，构造拉格朗日函数\\(\\color{blue}{L(P, w)}\\) ： \\[ \\color{blue}{L(P, w)} = \\color{red}{ -H(P) + w_0 \\cdot \\left( 1 - \\sum_{x, y} P(y \\mid x)\\right) + \\sum_{i=1}^n w_i \\cdot \\left(E_{\\widetilde P}(f) - E_{ P}(f) \\right) } \\] 由于是凸函数，根据相关性质，所以原始问题和对偶问题同解，原始问题如下： \\[ \\min \\limits_{P \\in C} \\max \\limits_{w} L(P, w) \\] 对应的对偶问题如下： \\[ \\max \\limits_{w} \\min \\limits_{P \\in C} L(P, w) \\] 主要思路是：先固定\\(\\color{blue}{w}\\)，去计算\\(\\color{blue} {\\min \\limits_{P \\in C} L(P, w)}\\)，即去找到一个合适的\\(\\color{blue}{P(Y \\mid X)}\\)。再去找到一个合适的\\(\\color{blue}{w}\\)。 第一步：求解\\(\\color{blue}{P}\\) 。设对偶函数\\(\\color{blue} { \\Psi (w)}\\)如下： \\[ \\color{blue} { \\Psi (w)} = \\color{red} {\\min \\limits_{P \\in C} L(P, w) = L(P_w, w)} \\] 对偶函数的解，即我们找到的\\(P(Y \\mid X)\\)，记作\\(\\color{blue}{P_w}\\)，如下： \\[ \\color{blue}{P_w} = \\arg \\min \\limits_{P \\in C} L(P, w) =\\color{red} {P_w(y \\mid x)} \\] 用\\({L(P, w)}\\)对\\(P\\)进行求偏导，令偏导为0，可以解得\\({P_w}\\)，即最大熵模型 如下： \\[ \\color{blue}{P_w(y \\mid x)} = \\color{red} {\\frac{1}{Z_w(x)} \\cdot \\exp \\left({\\sum_{i=1}^nw_if_i(x, y)}\\right) }, \\; \\color{blue}{Z_w(x)} = \\color{red} {\\sum_{y} \\exp \\left( \\sum_{i=1}^{n} w_i f_i(x, y)\\right) } \\] 其中\\(\\color{blue}{Z_w(x)}\\)是归一化因子，\\(\\color{blue} {f_i(x, y)}\\)是特征函数，\\(\\color{blue}{w_i}\\)是特征的权值，\\(\\color{blue}{P_w(y \\mid x)}\\) 就是最大熵模型， \\(\\color{blue}{w}\\)是最大熵模型中的参数向量。 第二步：求解\\(\\color{blue}{w}\\)。即求\\(w\\)去最大化对偶函数，设解为\\(\\color{blue} {w^*}\\) 。可以使用最优化算法去求极大化。 \\[ \\color{blue}{w^*} = \\color{red} {\\arg \\max \\limits_{w} \\Psi(w)} \\] 最终，求到的\\(\\color{blue} {P^* = P_{w^*} = P_{w^*}(y \\mid x)}\\)就是学习得到的最大熵模型。 最大熵模型 最大熵模型如下，其中\\(\\color{blue}{Z_w(x)}\\)是归一化因子，\\(\\color{blue} {f_i(x, y)}\\)是特征函数，\\(\\color{blue}{w_i}\\)是特征的权值 。 \\[ \\color{blue}{P_w(y \\mid x)} = \\color{red} {\\frac{1}{Z_w(x)} \\cdot \\exp \\left({\\sum_{i=1}^nw_if_i(x, y)}\\right) }, \\; \\color{blue}{Z_w(x)} = \\color{red} {\\sum_{y} \\exp \\left( \\sum_{i=1}^{n} w_i f_i(x, y)\\right) } \\] 极大似然估计 其实对偶函数\\(\\color{blue}{\\Psi(w)}\\)的极大化等价于最大熵模型的极大似然估计。 已知训练数据的经验分布\\(\\widetilde{P}(X, Y)\\)，条件概率分布的\\(P(Y \\mid X)\\)的对数似然函数是： \\[ \\begin{align*} \\color{blue} {L_{\\widetilde P}(P_w)} &amp; = \\log \\prod_{x, y} P(y \\mid x) ^ {\\widetilde{P}(X, Y)} = \\sum_{x, y} \\widetilde P(x, y) \\log P(y \\mid x) \\\\ &amp; = \\color{red}{\\sum_{x, y}\\widetilde{P}(X, Y) \\sum_{i=1}^{n} w_i f_i(x, y) - \\sum_{x} \\widetilde{P}(X)\\log Z_w(x) } \\end{align*} \\] 可以证明得到，$L_{P}(P_w) = (w) $，极大似然函数等于对偶函数。 模型学习的最优化算法 逻辑回归、最大熵模型的学习都是以似然函数为目标函数的最优化问题，可以通过迭代算法求解。这个目标函数是个光滑的凸函数。通过很多方法都可以保证找到全局最优解，常用的有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法，其中牛顿法和拟牛顿法一般收敛速度更快。 改进的迭代尺度法 改进的迭代尺度法(improved iterative scaling, IIS)是一种最大熵模型学习的最优化算法。 已知最大熵模型如下： \\[ \\color{blue}{P_w(y \\mid x)} = \\color{red} {\\frac{1}{Z_w(x)} \\cdot \\exp \\left({\\sum_{i=1}^nw_if_i(x, y)}\\right) }, \\; \\color{blue}{Z_w(x)} = \\color{red} {\\sum_{y} \\exp \\left( \\sum_{i=1}^{n} w_i f_i(x, y)\\right) } \\] 对数似然函数如下： \\[ \\color{blue} {L_{\\widetilde P}(P_w)} = \\color{red}{\\sum_{x, y}\\widetilde{P}(X, Y) \\sum_{i=1}^{n} w_i f_i(x, y) - \\sum_{x} \\widetilde{P}(X)\\log Z_w(x) } \\] 目标是：通过极大似然估计学习模型参数，即求对数似然函数的极大值\\(\\color{blue} {\\hat w}\\)。 基本思想 当前参数向量\\(w = (w_1, w_2, \\cdots, w_n)^T\\)，找到一个新的参数向量\\(w+\\delta = (w_1 + \\delta_1, w_2 + \\delta_2, \\cdots, w_n + \\delta_n)\\)，使得每次更新都使似然函数值增大。 由于\\(\\delta\\)是一个向量，含有多个变量，不易同时优化。所以IIS 每次只优化其中一个变量\\(\\delta_i\\)，而固定其他变量\\(\\delta_j\\)。 设所有特征在\\((x, y)\\)中的出现次数\\(f^\\#(x, y) = M\\) ： \\[ \\color{blue}{f^\\# (x, y)} = \\sum_i f_i(x, y) \\] 计算每次的改变量： \\[ L(w+\\delta) - L(w) \\ge \\color{blue}{B(\\delta \\mid w)}, \\; 改变量的下界限 \\] 如果找到适当的\\(\\delta\\)使得改变量的下界\\(B(\\delta \\mid w)\\)提高，则对数似然函数也能提高。 计算\\(B(\\delta \\mid w)\\)对\\(\\delta_i\\)求偏导，令偏导等于0，得如下方程： \\[ \\color{red} { \\sum_{x, y} \\widetilde P(x) P_w(y \\mid x) f_i(x, y) \\exp \\left(\\delta_i f^\\#(x, y)\\right) = E_{\\widetilde p}(f_i) }, \\quad 其中\\, E_{\\widetilde p}(f_i) = \\sum_{x, y} \\widetilde P(x, y)f_i(x, y) \\] 然后，依次对\\(\\delta_i​\\)求解该方程，就可以求得\\(\\delta​\\)，也就能够更新\\(w​\\)，即\\(w \\to w+\\delta​\\) 算法步骤 输入：特征函数\\(f_1, \\cdots, f_n\\)；经验分布\\(\\widetilde P(x, y)\\)，模型\\(P_w(y \\mid x)\\) 输出：最优参数值\\(w_i^*\\)；最优模型\\(P_{w^*}\\) 初始化参数，取初值 \\(w_i = 0\\) 求解方程 \\(\\delta_i\\) \\[ \\sum_{x, y} \\widetilde P(x) P_w(y \\mid x) f_i(x, y) \\exp \\left(\\delta_i f^\\#(x, y)\\right) = E_{\\widetilde p}(f_i), \\] 更新参数 \\(w_i + \\delta_i \\to w_i\\) 其中解方程的时候，如果特征出现次数\\(f^\\#(x, y)\\) 是常数\\(M\\)，则可以直接计算\\(\\delta_i\\) ： \\[ \\color{blue}{\\delta_i } = \\color{red} {\\frac{1}{M} \\log \\frac{E_{\\widetilde p}(f_i)}{E_{p}(f_i)} } \\] 如果\\(f^\\#(x, y)\\)不是常数，则必须通过数值计算\\(\\delta_i\\)。最简单就是通过牛顿迭代法去迭代求解\\(\\delta_i^*\\)。以\\(g(\\delta_i) = 0\\) 表示该方程，进行如下迭代： \\[ \\color{blue} {\\delta_i^{(k+1)}} = \\color{red} {\\delta_i^{(k)} - \\frac{g(\\delta_i^{(k)})}{g^\\prime (\\delta_i^{(k)})} } \\]","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"各种熵","slug":"各种熵","permalink":"http://plmsmile.github.io/tags/各种熵/"},{"name":"最大熵模型","slug":"最大熵模型","permalink":"http://plmsmile.github.io/tags/最大熵模型/"},{"name":"IIS","slug":"IIS","permalink":"http://plmsmile.github.io/tags/IIS/"},{"name":"期望","slug":"期望","permalink":"http://plmsmile.github.io/tags/期望/"}]},{"title":"线性回归和逻辑回归","date":"2017-08-20T13:38:54.000Z","path":"2017/08/20/21-lr/","text":"吴恩达线性回归、逻辑回归、梯度下降笔记 线性回归 有\\(m\\)个样本\\((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots, (x^{(m)}, y^{(m)})\\)，假设函数有2个参数\\(\\theta_0, \\theta_1\\)，形式如下： \\[ h_\\theta(x) = \\theta_0 + \\theta_1x \\] 代价函数 代价函数 \\[ \\color{red} {J(\\theta_0, \\theta_1) = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2} \\] 目标是要找到合适的参数，去最小化代价函数\\(min\\, J(\\theta_0, \\theta_1)\\)。 假设\\(\\theta_0 = 0\\)，去描绘出\\(J(\\theta_1)\\)和\\(\\theta_1\\)的关系，如下面右图所示。 假设有3个样本\\((1,1), (2,2), (3,3)\\)，图中选取了3个\\(\\theta_1 = 1, 0.5, 0\\)，其中\\(J(\\theta_1)\\)在\\(\\theta_1=1\\)时最小。 那么回到最初的两个参数\\(h_\\theta(x) = \\theta_0 + \\theta_1x\\)，如何去找\\(min\\, J(\\theta_0, \\theta_1)\\)呢？这里绘制一个等高图去表示代价函数，如下面右图所示，其中中间点是代价最小的。 梯度下降 基础说明 上文已经定义了代价函数\\(J(\\theta_0, \\theta_1)\\)，这里要使用梯度下降算法去最小化\\(J(\\theta_0, \\theta_1)\\)，自动寻找出最合适的\\(\\theta\\)。梯度下降算法应用很广泛，很重要。大体步骤如下： 设置初始值\\(\\theta_0, \\theta_1\\) 不停改变\\(\\theta_0, \\theta_1\\)去减少\\(J(\\theta_0, \\theta_1)\\) 当然选择不同的初始值，可能会得到不同的结果，得到局部最优解。 对于所有的参数\\(\\theta_j\\)进行同步更新，式子如下 \\[ \\color{red}{\\theta_j = \\theta_j - \\underbrace{\\alpha \\cdot \\frac{\\partial}{\\partial_{\\theta_j}} J(\\theta_0, \\theta_1)}_{学习率 \\times 偏导}} \\] 上面公式中\\(\\color{blue}{\\alpha}\\)是学习率(learning rate)，是指一次迈多大的步子，一次更新的幅度大小。 例如上面的两个参数，对于一次同步更新(梯度下降) \\[ t_0 = \\theta_0 - \\alpha \\frac{\\partial}{\\partial_{\\theta_0}} J(\\theta_0, \\theta_1), t_1 = \\theta_1 - \\alpha \\frac{\\partial}{\\partial_{\\theta_1}} J(\\theta_0, \\theta_1) \\quad \\to\\quad \\theta_0 = t_0, \\theta_1 = t_1 \\] 也有异步更新(一般指别的算法) \\[ t_0 = \\theta_0 - \\alpha \\frac{\\partial}{\\partial_{\\theta_0}} J(\\theta_0, \\theta_1),\\theta_0 = t_0 \\quad \\to\\quad t_1 = \\theta_1 - \\alpha \\frac{\\partial}{\\partial_{\\theta_1}} J(\\theta_0, \\theta_1), \\theta_1 = t_1 \\] 偏导和学习率 这里先看一个参数的例子，即\\(J(\\theta_1)\\)。\\(\\theta_1 = \\theta_1 - \\alpha \\frac{d}{dx}J(\\theta_1)\\)。当\\(\\theta\\)从左右靠近中间值，导数值(偏导/斜率)分别是负、正，所以从左右两端都会靠近中间值。 当学习率\\(\\alpha\\)太小，梯度下降会很缓慢；\\(\\alpha\\)太大，可能会错过最低点，导致无法收敛。 当已经处于局部最优的时候，导数为0，并不会改变参数的值，如下图 当逐渐靠近局部最优的时候，梯度下降会自动采取小步子到达局部最优点。是因为越接近，导数会越来越小。 在线性回归上使用梯度下降 代价函数 \\[ \\color{blue} {J(\\theta_0, \\theta_1)} = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2 = \\color{red}{\\frac{1}{2m} \\sum_{i=1}^m (\\theta_0 + \\theta_1x^{(i)} -y^{(i)})^2} \\] 分别对\\(\\theta_0\\)和\\(\\theta_1\\)求偏导有 \\[ \\color{blue}{\\frac{\\partial}{\\partial_{\\theta_0}}J(\\theta_0, \\theta_1) }= \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right), \\quad \\color{blue}{\\frac{\\partial}{\\partial_{\\theta_1}} J(\\theta_0, \\theta_1)}= \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)\\cdot x^{(i)} \\] 那么使用梯度下降对\\(\\theta_0 和 \\theta_1\\)进行更新，如下 \\[ \\theta_0 = \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right), \\quad \\theta_1 = \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)\\cdot x^{(i)} \\] 当前代价函数实际上是一个凸函数，如下图所示。它只有全局最优，没有局部最优。 通过不断地改变参数减小代价函数\\(\\color{blue} {J(\\theta_0, \\theta_1)}\\)，逼近最优解，最终会得到一组比较好的参数，正好拟合了我们的训练数据，就可以进行新的值预测。 梯度下降技巧 特征缩放 不同的特征的单位的数值变化范围不一样，比如\\(x_1 \\in (0,2000), x_2 \\in (1,5)\\)，这样会导致代价函数\\(J(\\theta)\\)特别的偏，椭圆。这样来进行梯度下降会特别的慢，会来回震荡。 所以特征缩放是把所有的特征缩放到相同的规模上。得到的\\(J(\\theta)\\)就会比较圆，梯度下降能很快地找到一条通往全局最小的捷径。 特征缩放的数据规模不能太小或者太大，如下面可以的规模是 \\[ [-1, 1], [0, 3], [-2, 0.5], [-3, 3], [-\\frac{1}{3}, \\frac{1}{3}] 都是可以的。而[-100, 100], [-0.0001, 0.0001]是不可以的 \\] 有一些常见的缩放方法 \\(x_i = \\frac{x_i - \\mu}{max - min}\\), \\(x_i = \\frac{x_i - \\mu}{s}\\)，其中\\(\\mu\\)是均值，\\(s\\)是标准差 \\(x_i = \\frac{x_i - min} {max - min}\\) \\(x_i = \\frac{x_i}{max}\\) 学习率的选择 当梯度下降正确运行的时候，每一次迭代\\(J(\\theta)\\)都会减少，但是减少到什么时候合适呢？当然最好的办法就是画图去观察，当然也可以设定减小的最小值来判断。下图中， 迭代次数到达400的时候就已经收敛。不同的算法，收敛次数不一样。 当图像呈现如下的形状，就需要使用更小的学习率。理论上讲，只要使用足够小的学习率，\\(J(\\theta)\\)每次都会减少。但是太小的话，梯度下降会太慢，难以收敛。 学习率总结 学习率太小，慢收敛 学习率太大，\\(J(\\theta)\\)可能不会每次迭代都减小，甚至不会收敛 这样去选择学习率调试： \\(\\ldots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, \\ldots\\) 多变量线性回归 数据有\\(n\\)个特征，如\\(x^{(i)} = (1, x_1, x_2, \\cdots, x_n)\\)，其中\\(x_0 = 1\\)。则假设函数有\\(n+1\\)个参数，形式如下 \\[ \\color{blue}{h_\\theta(x)} = \\theta^Tx = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n \\] 代价函数 \\[ \\color{blue}{J(\\theta)} = \\frac{1}{2m} \\sum_{i=1}^m\\left( h_\\theta(x^{(i)}) - y^{(i)}\\right) ^ 2 \\] 梯度下降，更新每个参数\\(\\theta_j\\) \\[ \\theta_j = \\theta_j - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial_{\\theta_j}} =\\color{red}{ \\theta_j -\\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left(h_\\theta(x^{(i)})-y^{(i)} \\right) \\cdot x_j^{(i)}} \\] 多项式回归 有时候，线性回归并不能很好地拟合数据，所以我们需要曲线来适应我们的数据。比如一个二次方模型 \\[ h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x^2_2 \\] 当然可以用\\(x_2 = x^2_2, x_3 = x_3^3\\)来转化为多变量线性回归。如果使用多项式回归，那么在梯度下降之前，就必须要使用特征缩放。 正规方程 对于一些线性回归问题，使用正规方程方法求解参数\\(\\theta\\)，比用梯度下降更好些。代价函数如下 \\[ \\color{red} {J(\\theta_0, \\theta_1) = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2} \\] 正规方程的思想是函数\\(J(\\theta)\\)对每个\\(\\theta_j\\)求偏导令其等于0，就能得到所有的参数。即\\(\\frac{\\partial J}{\\partial \\theta_j} = 0\\)。 那么设\\(X_{m\\times(n+1)}\\)为数据矩阵（其中包括\\(x_0=1\\)），\\(y\\)为标签向量。则通过如下方程可以求得\\(\\theta\\) \\[ \\theta = (X^TX)^{-1}X^Ty \\] 正规方程和梯度下降的比较 梯度下降 正规方程 需要特征缩放 不需要特征缩放 需要选择学习率\\(\\alpha\\) 不虚选择学习率 需要多次迭代计算 一次运算出结果 特征数量\\(n\\)很大时，依然适用 \\(n\\)太大，求矩阵逆运算代价太大，复杂度为\\(O(n^3)\\)。\\(n\\leq10000\\)可以接受 适用于各种模型 只适用于线性模型，不适合逻辑回归和其他模型 逻辑回归 线性回归有2个不好的问题：直线难以拟合很多数据；数据标签一般是\\(0, 1\\)，但是\\(h_\\theta(x)\\)却可能远大于1或者远小于0。如下图。 基本模型 逻辑回归是一种分类算法，使得输出预测值永远在0和1之间，是使用最广泛的分类算法。模型如下 \\[ h_\\theta(x) = g(\\theta^Tx), \\quad g(z) = \\frac{1}{1+e^{-z}} \\] \\(g(z)\\)的图像如下，也称作Sigmoid函数或者Logistic函数，是S形函数。 将上面的公式整理后得到逻辑回归的模型 \\[ \\color{red}{h_\\theta(x) = \\frac {1}{1+e^{-\\theta^Tx}}}, \\quad 其中\\; \\color{red}{0 \\le h_\\theta(x) \\le 1} \\] 模型的意义是给出分类为1的概率，即\\(h_\\theta(x) = P(y=1\\mid x; \\theta)\\)。例如\\(h_\\theta(x)=0.7\\)，则分类为1的概率是0.7，分类为0的概率是\\(1-0.7=0.3\\)。 \\[ x的分类预测, y = \\begin{cases} 1, \\; &amp; h_\\theta(x) \\ge 0.5, \\;即\\; \\theta^Tx \\ge 0\\\\ 0, \\; &amp; h_\\theta(x) &lt; 0.5, \\; 即 \\; \\theta^Tx &lt; 0 \\\\ \\end{cases} \\] 逻辑回归就是要学到合适的\\(\\theta\\)，使得正例的特征远大于0，负例的特征远小于0。 决策边界 线性边界 假设我们有一个模型\\(h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2)\\)，已经确定参数\\(\\theta = (-3, 1, 1)\\)，即模型\\(h_\\theta(x) = g(-3+x_1+x_2)\\)，数据和模型如下图所示 由上可知，分类结果如下 \\[ y = \\begin{cases} 1, \\, &amp; x_1+ x_2 \\ge 3 \\\\ 0, \\, &amp; x_1 + x_2 &lt; 3 \\\\ \\end{cases} \\] 那么直线\\(x_1+x_2=3\\)就称作模型的决策边界，将预测为1的区域和预测为0的区域分隔开来。gg 非线性边界 先看下面的数据 使用这样的模型去拟合数据 \\[ h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_1^2 + \\theta_4x_2^2) , \\; \\theta = (-1, 0, 0, 1, 1), \\; 即\\, \\color{red}{h_\\theta(x) = g(-1+x_1^2 + x_2^2)} \\] 对于更复杂的情况，可以用更复杂的模型去拟合，如\\(x_1x_2, x_1x_2^2\\)等 代价函数和梯度下降 我们知道线性回归中的代价函数是\\(J(\\theta_0, \\theta_1) = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2\\)，但是由于逻辑回归的模型是\\(\\color{red}{h_\\theta(x) = \\frac {1}{1+e^{-\\theta^Tx}}}\\)，所以代价函数关于\\(\\theta\\)的图像就是一个非凸函数，容易达到局部收敛，如下图左边所示。而右边，则是一个凸函数，有全局最小值。 代价函数 逻辑回归的代价函数 \\[ \\rm{Cost}(h_\\theta(x), y) = \\color{red}{ \\begin{cases} -\\log(h_\\theta(x)),\\; &amp; y=1 \\\\ -\\log(1-h_\\theta(x)), \\; &amp; y=0 \\\\ \\end{cases} } \\] 当实际上\\(y=1\\)时，若预测为0，则代价会无穷大。当实际上\\(y=0\\)时，若预测为1，则代价会无穷大。 整理代价函数如下 \\[ \\rm{Cost}(h_\\theta(x), y) =\\color{red}{ -y \\cdot \\log(h_\\theta(x)) - (1-y) \\cdot \\log(1- h_\\theta(x))} \\] 得到所有的\\(\\color{red}{J(\\theta)}\\) \\[ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)}\\log h_\\theta(x^{(i)}) + (1 - y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\right) \\] 梯度下降 逻辑回归的假设函数估计\\(y=1\\)的概率 \\(\\color{red}{h_\\theta(x) = \\frac {1}{1+e^{-\\theta^Tx}}}\\)。 代价函数\\(\\color{red}{J(\\theta)}\\)，求参数\\(\\theta\\)去\\(\\color{red}{\\min \\limits_{\\theta} J(\\theta)}\\) 对每个参数\\(\\theta_j\\)，依次更新参数 \\[ \\color{red} {\\theta_j = \\theta_j -\\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left(h_\\theta(x^{(i)})-y^{(i)} \\right) \\cdot x_j^{(i)}} \\] 逻辑回归虽然梯度下降的式子和线性回归看起来一样，但是实际上\\(h_\\theta(x)\\)和\\(J(\\theta)\\)都不一样，所以是不一样的。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"线性回归","slug":"线性回归","permalink":"http://plmsmile.github.io/tags/线性回归/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://plmsmile.github.io/tags/逻辑回归/"},{"name":"梯度下降","slug":"梯度下降","permalink":"http://plmsmile.github.io/tags/梯度下降/"}]},{"title":"最大期望算法","date":"2017-08-13T10:37:48.000Z","path":"2017/08/13/14-em/","text":"本文简单记录了EM算法的思想和Jensen不等式 EM算法定义 背景 如果概率模型的变量都是观测变量，那么可以直接使用极大似然估计法或贝叶斯估计法去估计模型参数。 如果模型既有观测变量又有隐变量，就不能简单使用上述方法。 EM算法，期望极大算法，就是含有隐变量的概率模型参数的极大似然估计法或极大后验概率估计法，是一种迭代算法。每次迭代分为如下两步 E步：求期望(expectation) M步：求极大(maximization) 三硬币模型 有3枚硬币，记做ABC，每次出现正面的概率分别是\\(\\pi, p, q\\)。先掷A，正面选B，反面选C。再掷B/C，得到正面1或反面0作为一次的结果。问：去估计参数\\(\\theta = (\\pi, p, q)\\) 观察变量：一次 实验得到的结果1或0，记做\\(y\\) 隐变量：A的结果，即掷的是B还是C，记做\\(z\\) 对于一次实验，求出\\(y\\)的概率分布 \\[ \\begin{align*} \\color{blue}{P(y \\mid \\theta)} &amp;= \\underbrace{\\sum_zP(y, z \\mid \\theta)}_{\\color{red}{把所有z的y加起来}} = \\sum_z \\underbrace{P(z \\mid \\theta)P(y \\mid z, \\theta)}_{\\color{red}{贝叶斯公式}} = \\underbrace{\\pi p^y(1-p)^{1-y}}_{\\color{red}{z=1时}}+\\underbrace{(1-\\pi)q^y(1-q)^{1-y}}_{\\color{red}{z=0时}} \\end{align*} \\] 设观察序列\\(Y=(y_1, y_2, \\cdots, y_n)^T\\)，隐藏数据\\(Z=(z_1, z_2, \\cdots, z_n)^T\\)，则观测数据的似然函数为 \\[ \\begin{align*} \\color{blue}{P(Y \\mid \\theta)} &amp;= \\sum_Z \\color{red}{P(Z \\mid \\theta)P(Y \\mid Z, \\theta)} \\\\ &amp;= \\prod_{i=1}^n[\\pi p^{y_i}(1-p)^{1-y_i} + (1-\\pi)q^{y_i}(1-q)^{1-y_i}] \\end{align*} \\] 求模型参数\\(\\theta = (\\pi, p, q)\\)的最大似然估计，即 \\[ \\hat\\theta = arg max_\\theta \\log P(Y \\mid \\theta) \\] 这个问题不能直接求解，只有通过迭代的方法求解。EM算法就是解决这种问题的一种迭代算法。先给\\(\\theta^{(0)}\\) 选择初始值，然后去迭代。每次迭代分为E步和M步。 EM算法 基本概念 一些概念如下 \\(Y\\) 观测变量，\\(Z\\) 隐变量 不完全数据：\\(Y\\)；概率分布：\\(P(Y \\mid \\theta)\\)；对数似然函数：\\(\\color{red} {L(\\theta) = \\log P(Y \\mid \\theta)}\\) 完全数据：\\(Y\\)和\\(Z\\)合在一起；概率分布：\\(P(Y, Z \\mid \\theta)\\)；对数似然函数：\\(\\log P(Y, Z \\mid \\theta)\\) EM算法通过迭代求\\(L(\\theta) = \\log P(Y \\mid \\theta)\\)的极大似然估计。 概率论函数的期望 设\\(Y\\)是随机变量\\(X\\)的函数，\\(Y = g(X)\\)，\\(g\\)是连续函数，那么 \\(X\\)是离散型变量，\\(X\\)的分布律为\\(P(X = x_i) = p_i, \\; i=1,2,3\\cdots\\)，则有 \\[ E(Y) = E(g(X)) = \\sum_{i=1}^{\\infty}g(x_i)p_i, \\quad 左式收敛时成立 \\] \\(X\\)是 连续型变量，\\(X\\)的概率密度为\\(f(x)\\)，则有 \\[ E(Y) = E(g(X)) = \\int_{-\\infty}^{+\\infty} {g(x)f(x)} \\, {\\rm d}x, \\quad 左式绝对收敛成立 \\] Q函数 \\(\\color{blue}{Q(\\theta, \\theta^{(i)})}\\)是EM算法的核心。它是完全数据的对数似然函数\\(\\log P(Y,Z \\mid \\theta)\\)的期望，是关于未观测数据\\(Z\\)的条件概率分布\\(P(Z \\mid Y, \\theta^{(i)})\\)，而\\(Z\\)的条件是在给定观测数据\\(Y\\)和当前参数\\(\\theta^{(i)}\\)。（都是后置定语，不通顺） \\[ \\begin {align*} \\color{blue}{Q(\\theta, \\theta^{(i)})} &amp;= E_Z[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}] = \\sum_Z \\color{red}{\\log P(Y, Z \\mid \\theta)P(Z \\mid Y, \\theta^{(i)})} \\end {align*} \\] 下面是我具体的理解 \\(\\color{blue}{\\theta^{(i)}}\\)是第\\(i\\)次迭代参数\\(\\theta\\)的估计值 \\(P(Z \\mid Y, \\theta^{(i)})\\)是以\\(Y\\)和当前参数\\(\\theta^{(i)}\\)的条件下的分布律，简写为\\(P(Z)\\)。类似于上面的\\(X\\) \\(P(Y, Z \\mid \\theta)\\) 是在以\\(\\theta\\)为参数的分布的联合概率密度，简写为\\(P(Y, Z)\\)。类似于上面的\\(Y=g(X)\\) 求对数似然函数\\(\\log P(Y, Z)\\)的期望，转移到隐变量\\(Z\\)上 把目标函数映射到\\(Z\\)上，\\(g(z) = \\log P(Y, Z)\\)，\\(E(g(z)) = \\sum_z \\log P(Y, Z) P(Z)\\) \\(\\color{blue}{Q(\\theta, \\theta^{(i)})}\\) 是因为要找到一个新的\\(\\theta\\)优于之前的\\(\\theta^{(i)}\\)，是代表的分布优于 EM算法步骤 输入：\\(\\color{blue}{Y}\\) 观测变量，\\(\\color{blue}{Z}\\) 隐藏变量，\\(\\color{blue}{P(Y, Z \\mid \\theta)}\\) 联合分布，条件分布 \\(\\color{blue}{P(Z \\mid Y, \\theta)}\\) 输出：模型参数\\(\\color{blue}{\\theta}\\) 步骤 选择参数初始值\\(\\color{blue}{\\theta^{(0)}}\\)，开始迭代 E步：第\\(i+1\\)次迭代， 求 \\(\\color{blue}{Q(\\theta, \\theta^{(i)})} = \\sum_Z \\color{red}{\\log P(Y, Z \\mid \\theta)P(Z \\mid Y, \\theta^{(i)})}\\) M步：求使\\(Q(\\theta, \\theta^{(i)})\\)极大化的\\(\\theta\\)，得到\\(i+1\\)次迭代新的估计值\\(\\color{blue}{\\theta^{(i+1)}} = \\color{red}{arg \\, max_\\theta (Q(\\theta, \\theta^{(i)}))}\\) 重复E和M步，直到收敛 Jensen不等式 凸函数与凹函数 从图像上讲，在函数上两点连接一条直线。直线完全在图像上面，就是凸函数 convex；完全在下面，就是凹函数 concave。 \\(f^{\\prime\\prime}(x) \\ge 0 \\implies f(x) 是凸函数; \\quad f^{\\prime\\prime}(x) \\le 0 \\implies f(x)是凹函数\\) Jensen不等式 函数\\(f(x)​\\)，上有两点\\(x_1, x_2​\\)，对于任意\\(\\lambda \\in [0,1]​\\) 如果\\(f(x)\\)是凸函数，\\(f(\\lambda x_1+(1-\\lambda)x_2) \\le \\lambda f(x_1) + (1-\\lambda)f(x_2)\\) 如果\\(f(x)\\)是凹函数，\\(f(\\lambda x_1+(1-\\lambda)x_2) \\ge \\lambda f(x_1) + (1-\\lambda)f(x_2)\\) 一般地，\\(n\\)个点\\(x_1, x_2, \\cdots, x_n\\)和参数\\(\\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n = 1\\)，对于凸函数，则有 \\[ f(\\underbrace{\\lambda_1x_1 + \\lambda_2x_2 + \\cdots + \\lambda_nx_n}_{\\color{red}{E[X], 总体是f(E[X])}}) \\le \\underbrace{f(\\lambda_1x_1) + f(\\lambda_2x_2) + \\cdots + f(\\lambda_nx_n)}_{\\color{red}{E[f(X)]}}, \\;即\\color{red}{f(\\sum_{i=1}^n\\lambda_ix_i) \\leq \\sum_{i=1}^nf(\\lambda_ix_i)} \\] 琴声不等式 凸函数 \\(f(E[X]) \\le E[f(X)]\\)，即\\(\\color{red}{f(\\sum_{i=1}^n\\lambda_ix_i) \\leq \\sum_{i=1}^nf(\\lambda_ix_i)}\\) 凹函数 \\(f(E[X]) \\ge E[f(X)]\\)，即\\(\\color{red}{f(\\sum_{i=1}^n\\lambda_ix_i) \\geq \\sum_{i=1}^nf(\\lambda_ix_i)}\\)","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"最大期望算法","slug":"最大期望算法","permalink":"http://plmsmile.github.io/tags/最大期望算法/"},{"name":"Jensen不等式","slug":"Jensen不等式","permalink":"http://plmsmile.github.io/tags/Jensen不等式/"}]},{"title":"马尔可夫模型","date":"2017-08-04T03:06:41.000Z","path":"2017/08/04/pgm-01/","text":"本文包括概率图模型、马尔科夫模型和隐马尔可夫模型。重点是HMM的前后向算法、维特比算法和BW算法 概述 产生式和判别式 判别方法 由数据直接去学习决策函数\\(Y=f(X)\\) 或者\\(P(Y \\mid X)\\)作为预测模型 ，即判别模型 生成方法 先求出联合概率密度\\(P(X, Y)\\)，然后求出条件概率密度\\(P(Y \\mid X)\\)。即生成模型\\(P(Y \\mid X) = \\frac {P(X, Y)} {P(X)}\\) 判别式 生成式 原理 直接求\\(Y=f(X)\\) 或\\(P(Y \\mid X)\\) 先求\\(P(X,Y)\\)，然后 \\(P(Y \\mid X) = \\frac {P(X, Y)} {P(X)}\\) 差别 只关心差别，根据差别分类 关心数据怎么生成的，然后进行分类 应用 k近邻、感知机、决策树、LR、SVM 朴素贝叶斯、隐马尔可夫模型 概率图模型 概率图模型(probabilistic graphical models) 在概率模型的基础上，使用了基于图的方法来表示概率分布。节点表示变量，边表示变量之间的概率关系 概率图模型便于理解、降低参数、简化计算，在下文的贝叶斯网络中会进行说明。 贝叶斯网络 贝叶斯网络 又称为信度网络或者信念网络（belief networks），实际上就是一个有向无环图。 节点表示随机变量；边表示条件依存关系。没有边说明两个变量在某些情况下条件独立或者说是计算独立，有边说明任何条件下都不条件独立。 如上图所示，要表示上述情况的概率只需要求出\\(4*2*2*2*2-1=63\\)个参数的联合概率密度就行了，实际上这个太难以求得。我们可以考虑一下独立关系\\((F \\perp H \\mid S) \\,\\,\\, 表示在S确定的情况下，F和H独立\\)，所以有以下独立关系： \\[ (F \\perp H \\mid S)、\\, (C \\perp S \\mid F,H)、\\, (M \\perp H, C \\mid F)、 \\, (M \\perp C | F) \\] 所以我们得到如下的计算独立假设： \\[ P(C \\mid FHS) = P(C \\mid FH)，即假设C只与FH有关，而与S无关 \\] 又由\\(P(AB)=P(A|B)P(B)\\)，所以得到联合概率分布： \\[ \\begin{align*} P(SFHMC) &amp;= P(M \\mid SHFC) \\cdot P(SHFC) = \\underbrace {P(M \\mid F)}_{\\color {red}{计算独立性}} \\cdot \\underbrace {P(C \\mid SHF) \\cdot P(SHF)}_{\\color{red}{继续分解}} \\\\ &amp;= P(M \\mid F) \\cdot P(C \\mid FH) \\cdot P(F \\mid S) \\cdot P(H \\mid S) \\cdot P(S) \\end{align*} \\] \\(P(S)\\) 4个季节，需要3个参数；\\(P(H \\mid S)\\)时，\\(P(Y \\mid Spring)\\) 和 \\(P(N \\mid Spring)\\)只需要一个参数，所以\\(P(H \\mid S)\\)只需要4个参数即可，其他同理。 所以联合概率密度就转化成了上述公式中的5个乘积项，其中每一项需要的参数个数分别是2、4、4、4、3，所以一共只需要17个参数，这就大大降低了参数的个数。 马尔可夫模型 简介 马尔可夫模型(Markov Model) 描述了一类重要的随机过程，未来只依赖于现在，不依赖于过去。这样的特性的称为马尔可夫性，具有这样特性的过程称为马尔可夫过程。 时间和状态都是离散的马尔可夫过程称为马尔可夫链，简称马氏链，关键定义如下 系统有\\(N\\)个状态\\(S = \\{ s_1, s_2, \\cdots, s_N\\}\\)，随着时间的推移，系统将从某一状态转移到另一状态 设\\(q_t \\in S\\)是系统在\\(t\\)时刻的状态，\\(Q = \\{q_q, q_2, \\cdots, q_T \\}\\)系统时间的随机变量序列 一般地，系统在时间\\(t\\)时的状态\\(s_j\\)取决于\\([1, t-1]\\)的所有状态\\(\\{q_1, q_2, \\cdots, q_{t-1}\\}\\)，则当前时间的概率是 \\[ P(q_t = s_j \\mid q_{t-1} = s_i, q_{t-2} = s_k, \\cdots) \\] 在时刻\\(m\\)处于\\(s_i\\)状态，那么在时刻\\(m+n\\)转移到状态\\(s_j\\)的概率称为转移概率，即从时刻\\(m \\to m+n\\)： \\[ \\color{blue} {P_{ij}(m, m+n)} = P(q_{m+n} = s_j \\mid q_m = s_i) \\] 如果\\(P_{ij}(m, m+n)\\)只与状态\\(i, j\\)和步长\\(n\\)有关，而与起始时间\\(m\\)无关，则记为\\(\\color {blue} {P_{ij}(n)}\\),称为n步转移概率。 并且称此转移概率具有平稳性，且称此链是齐次的，称为齐次马氏链，我们重点研究齐次马氏链。\\(P(n) = [P_{ij}(n)]\\)称为n步转移矩阵。 \\[ P_{ij}(m, m+n) =\\color {blue} {P_{ij}(n)} = P(q_{m+n} = s_j \\mid q_m = s_i) \\] 特别地，\\(n = 1\\)时，有一步转移概率如下 \\[ p_{ij} = P_{ij}(1) = P(q_{m+1} \\mid q_{m}) = a_{ij} \\] 一阶马尔可夫 特别地，如果\\(t\\)时刻状态只与\\(t-1\\)时刻状态有关，那么下有离散的一阶马尔可夫链如下： \\[ P(q_t = s_j \\mid q_{t-1} = s_i, q_{t-2} = s_k, \\cdots) = P(q_t = s_j\\mid q_{t-1} = s_i) \\] 其中\\(t-1​\\)的状态\\(s_i​\\)转移到\\(t​\\)的状态\\(s_j​\\)的概率定义如下： \\[ P(q_t = s_j\\mid q_{t-1} = s_i) = \\color{blue} {a_{ij}}，其中i, j \\in [1, N]，a_{ij} \\ge 0，\\sum_{j=1}^Na_{ij} = 1 \\] 显然，\\(N​\\)个状态的一阶马尔可夫链有\\(N^2​\\)次状态转移，这些概率\\(a_{ij}​\\)构成了状态转移矩阵。 \\[ A = [a_{ij}] = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\\\ \\end{bmatrix} \\] 设系统在初始状态的概率向量是 \\(\\color{blue} {\\pi_i} \\ge 0\\) ，其中，\\(\\sum_{i=1}^{N}\\pi_i = 1\\) 那么时间序列\\(Q = \\{q_1, q_2, \\cdots, q_T \\}\\)出现的概率是 \\[ \\color{blue} {P(q_1, q_2, \\cdots, q_T) } = P(q_1) P(q2 \\mid q_1) P(q_3 \\mid q_2) \\cdots P(q_T \\mid q_{T-1}) = \\color{red} {\\underbrace {\\pi_{q_1}}_{初态概率} \\prod_{t=1}^{T-1} a_{q_tq_{t+1}}} \\] 下图是一个例子 多步转移概率 对于齐次马氏链，多步转移概率就是\\(u+v\\)时间段的状态转移，可以分解为先转移\\(u\\)步，再转移\\(v\\)步。则有CK方程的矩阵形式 \\[ P(u+v) = P(u)P(v) \\] 由此得到\\(n\\)步转移概率矩阵是一次转移概率矩阵的\\(n\\)次方 \\[ P(n) = P(1) P(n-1) = PP(n-1) \\implies P(n) = P^n \\] 对于求矩阵的幂\\(A^n\\)，则最好使用相似对角化来进行矩阵连乘。 存在一个可逆矩阵P，使得\\(P^{-1}AP = \\Lambda，A = P \\Lambda P^{-1}\\)，其中\\(\\Lambda\\)是矩阵\\(A\\)的特征值矩阵 \\[ \\Lambda = \\begin{bmatrix} \\lambda_1 &amp; &amp; &amp; \\\\ &amp;\\lambda_2 &amp; &amp; \\\\ &amp;&amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\lambda_n \\\\ \\end{bmatrix} ，其中\\lambda是矩阵A的特征值 \\] 则有\\(A^n = P\\Lambda ^ {n}P^{-1}\\) 遍历性 齐次马氏链，状态\\(i\\)向状态\\(j\\)转移，经过无穷步，任何状态\\(s_i\\)经过无穷步转移到状态\\(s_j\\)的概率收敛于一个定值\\(\\pi_j\\)，即\\(\\lim_{n \\to \\infty} P_{ij}(n) = \\pi_j \\; (与i无关)\\) 则称此链具有遍历性。若\\(\\sum_{j=1}^N \\pi_j = 1\\)，则称\\(\\vec{\\pi} = (\\pi_1, \\pi_2, \\cdots)\\)为链的极限分布。 遍历性的充分条件：如果存在正整数\\(m\\)(步数)，使得对于任意的，都有如下（转移概率大于0），则该马氏链具有遍历性 \\[ P_{ij}(m) &gt; 0, \\quad i, j =1, 2, \\cdots, N, \\quad s_i,s_j \\in S \\; \\] 那么它的极限分布\\(\\vec{\\pi} = (\\pi_1, \\pi_2, \\cdots, \\pi_N)​\\)，它是下面方程组的唯一解 \\[ \\pi = \\pi P, \\quad 即\\pi_j = \\sum_{i=1}^{N} \\pi_i p_{ij}, \\quad 其中\\pi_j &gt; 0, \\sum_{j=1}^N \\pi_j = 1 \\] PageRank应用 有很多应用，压缩算法、排队论等统计建模、语音识别、基因预测、搜索引擎鉴别网页质量-PR值。 Page Rank算法 这是Google最核心的算法，用于给每个网页价值评分，是Google“在垃圾中找黄金”的关键算法。 大致思想是要为搜索引擎返回最相关的页面。页面相关度是由和当前网页相关的一些页面决定的。 当前页面会把自己的importance平均传递给它所指向的页面，若有\\(k\\)个，则为每个传递\\(\\frac 1 k\\) 如果有很多页面都指向当前页面，则当前页面很重要，相关度高 当前页面有一些来自官方页面的backlink，当前页面很重要 例如有4个页面，分别如下 矩阵\\(\\color {blue }A\\)是页面跳转的一次转移矩阵，\\(\\color {blue }q\\)是当前时间每个页面的相关度向量，即PageRank vector。 \\[ A = \\begin{bmatrix} 0 &amp; 0 &amp;1 &amp; \\frac {1}{2} \\\\ \\frac {1}{3}&amp; 0 &amp; 0 &amp; 0 \\\\ \\frac {1}{3}&amp; \\frac {1}{2} &amp; 0 &amp; \\frac {1}{2} \\\\ \\frac {1}{3} &amp; \\frac {1}{2} &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\quad 初始时刻，q = \\begin {bmatrix} \\frac1 4 \\\\ \\frac1 4 \\\\ \\frac1 4 \\\\ \\frac1 4 \\\\ \\end {bmatrix} \\] \\(A\\)的一列是当前页面出去的所有页面，一行是进入当前页面的所有页面。设\\(u\\)表示第\\(A\\)的第\\(i\\)行，那么\\(u*q\\)就表示当页面\\(i\\)接受当前\\(q\\)的更新后的rank值。 定义矩阵\\(\\color {blue} {G} = \\color{red} {\\alpha A + (1-\\alpha) \\frac {1} {n} U}\\)，对\\(A\\)进行修正，\\(G\\)所有元素大于0，具有遍历性 \\(\\alpha \\in[0, 1] \\; (\\alpha = 0.85)\\) 阻尼因子 \\(A\\) 一步转移矩阵 \\(n\\) 页面数量 \\(U\\) 元素全为\\(1\\)的矩阵 使用\\(G\\)进行迭代的好处 解决了很多\\(A\\)元素为0导致的问题，如没有超链接的节点，不连接的图等 \\(A\\)所有元素大于0，具有遍历性，具有极限分布，即它的极限分布\\(q\\)会收敛 那么通过迭代就可以求出PR向量\\(\\color {red} {q^{next} = G q^{cur}}\\)，实际上\\(q\\)是\\(G\\)的特征值为1的特征向量。 迭代具体计算如下图(下图没有使用G，是使用A去算的，这是网上找的图[捂脸]) 随着迭代，\\(q\\)会收敛，那么称为\\(q\\)就是PageRank vector。 我们知道节点1有2个backlink，3有3个backlink。但是节点1却比3更加相关，这是为什么呢？因为节点3虽然有3个backlink，但是却只有1个outgoing，只指向了页面1。这样的话它就把它所有的importance都传递给了1，所以页面1也就比页面3的相关度高。 隐马尔可夫模型 定义 隐马尔可夫模型（Hidden Markov Model， HMM）是统计模型，它用来描述含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数，然后利用这些参数来做进一步的分析。大概形状如下 一个HMM由以下5个部分构成。 隐藏状态 模型的状态，隐蔽不可观察 有\\(N\\)种，隐状态种类集合\\(\\color {blue} {S = \\{s_1, s_2, \\cdots, s_N\\}}\\)会相 隐藏状态互相互转换，一步转移。\\(s_i\\)转移到\\(s_j\\)的概率 \\(\\color {red} {a_{ij} = P(q_t= s_j \\mid q_{t-1}=s_i)}\\) \\(q_t = s_i\\) 代表在\\(t\\)时刻，系统隐藏状态\\(q_t\\)是\\(s_i\\) 隐状态时间序列 \\(\\color{blue}{Q = \\{q_1, q_2, \\cdots, q_t, q_{t+1}\\cdots \\}}\\) 观察状态 模型可以显示观察到的状态 有\\(M\\)种，显状态种类集合\\(\\color{blue} {K = \\{v_1, v_2, \\cdots, v_M\\}}\\)。不能相互转换，只能由隐状态产生(发射) \\(o_t = v_k​\\) 代表在\\(t​\\)时刻，系统的观察状态\\(o_t​\\)是\\(v_k​\\) 每一个隐藏状态会发射一个观察状态。\\(s_j\\)发射符号\\(v_k\\)的概率\\(\\color {red} {b_j (k) = P(o_t = v_k \\mid s_j)}\\) 显状态时间序列 \\(\\color{blue} {O = \\{o_1, o_2, \\cdots, o_ t\\}}\\) 状态转移矩阵A (隐--隐) 从一个隐状\\(s_i\\)转移到另一个隐状\\(s_j\\)的概率。\\(A = \\{a_{ij}\\}\\) \\(\\color {red} {a_{ij} = P(q_t= s_j \\mid q_{t-1}=s_i)}\\)，其中 \\(1 \\leq i, j \\leq N, \\; a_{ij} \\geq 0, \\; \\sum_{j=1}^N a_{ij}=1\\) 发射概率矩阵B (隐--显) 一个隐状\\(s_j\\)发射出一个显状\\(v_k\\)的概率。\\(B = \\{b_j(k)\\}\\) \\(\\color {red} {b_j(k) = P(o_t = v_k \\mid s_j)}\\)，其中\\(1 \\leq j \\leq N; \\; 1 \\leq k \\leq M; \\; b_{jk} \\ge 0; \\; \\sum_{k=1}^Mb_{jk}=1\\) 初始状态概率分布 \\(\\pi\\) 最初的隐状态\\(q_1=s_i\\)的概率是\\(\\pi_i = P(q_1 = s_i)\\) 其中\\(1 \\leq i \\leq N, \\; \\pi_i \\ge 0, \\; \\sum_{i=1}^N \\pi_i = 1\\) 一般地，一个HMM记作一个五元组\\(\\mu = (S, K, A, B, \\pi)\\)，有时也简单记作\\(\\mu = (A, B, \\pi)\\)。一般，当考虑潜在事件随机生成表面事件的时候，HMM是非常有用的。 HMM中的三个问题 观察序列概率 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)和模型\\(\\mu = (A, B, \\pi)\\)，求当前观察序列\\(O\\)的出现概率\\(P(O \\mid \\mu)\\) 状态序列概率 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)和模型\\(\\mu = (A, B, \\pi)\\)，求一个最优的状态序列\\(Q=\\{q_1, q_2, \\cdots, q_T\\}\\)的出现概率，使得最好解释当前观察序列\\(O\\) 训练问题或参数估计问题 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)，调节模型\\(\\mu = (A, B, \\pi)\\)参数，使得\\(P(O \\mid u)\\)最大 前后向算法 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)和模型\\(\\mu = (A, B, \\pi)\\)，求给定模型\\(\\mu\\)的情况下观察序列\\(O\\)的出现概率。这是解码问题。如果直接去求，计算量会出现指数爆炸，那么会很不好求。我们这里使用前向算法和后向算法进行求解。 前向算法 前向变量\\(\\color {blue} {\\alpha_t(i)}\\)是系统在\\(t\\)时刻，观察序列为\\(O=o_1o_2\\cdots o_t\\)并且隐状态为\\(q_t = s_i\\)的概率，即 \\[ \\color {red} {\\alpha_t(i) = P(o_1o_2\\cdots o_t, q_t = s_i \\mid \\mu)} \\] \\(\\color {blue} {P(O \\mid \\mu)}\\) 是在\\(t\\)时刻，状态\\(q_t=\\) 所有隐状态的情况下，输出序列\\(O\\)的概率之和 \\[ \\color {blue} {P(O \\mid \\mu)} = \\sum_{i=1}^N P(O, q_t = s_i \\mid \\mu) = \\color {red} {\\sum_{i=1}^{N}\\alpha_t(i)} \\] 接下来就是计算\\(\\color {blue} {\\alpha_t(i)}\\)，其实是有动态规划的思想，有如下递推公式 \\[ \\color {blue} {\\alpha_{t+1}(j)} = \\color{red}{\\underbrace{\\left( \\sum_{i=1}^N \\alpha_t(i)a_{ij} \\right)}_{所有状态i转为j的概率} \\underbrace {b_j(o_{ t+1})}_{状态j发射o_{t+1}}} \\] 上述计算，其实是分为了下面3步 从1到达时间\\(t\\)，状态为\\(s_i\\)，输出\\(o_1o_2 \\cdots o_t\\)。\\(\\color{blue}{\\alpha_t(i)}\\) 从\\(t\\)到达\\(t+1\\)，状态变化\\(s_i \\to s_j \\text{。} \\quad\\color{blue}{a_{ij}}\\) 在\\(t+1\\)时刻，输出\\(o_{t+1}\\)。\\(\\color{blue}{b_j(o_{ t+1})}\\) 算法的步骤如下 初始化 \\(\\color {blue} {\\alpha_1(i)} = \\color {red} {\\pi_ib_i(o_1)}, \\; 1 \\leq i \\leq N\\) 归纳计算 \\(\\color {blue} {\\alpha_{t+1}(j)} = \\color{red} {\\left( \\sum_{i=1}^N \\alpha_t(i)a_{ij} \\right) b_j(o_{ t+1})}, \\; 1 \\leq t \\leq T-1\\) 求和终结 \\(\\color {blue} {P(O \\mid \\mu)} = \\color {red} {\\sum_{i=1}^{N}\\alpha_T(i)}\\) 在每个时刻\\(t\\)，需要考虑\\(N\\)个状态转移到\\(s_{j}\\)的可能性，同时也需要计算\\(\\alpha_t(1), \\cdots , \\alpha_t(N)\\)，所以时间复杂度为\\(O(N^2)\\)。同时在系统中有\\(T\\)个时间，所以总的复杂度为\\(O(N^2T)\\)。 后向算法 后向变量 \\(\\color {blue} {\\beta_{t}(i)}\\) 是系统在\\(t\\)时刻，状态为\\(s_i\\)的条件下，输出为\\(o_{t+1}o_{t+2}\\cdots o_T\\)的概率，即 \\[ \\color {red} {\\beta_t(i) = P(o_{t+1}o_{t+2}\\cdots o_T \\mid q_t = s_i , \\mu)} \\] 递推 \\(\\color {blue} {\\beta_{t}(i)}\\)的思路及公式如下 从\\(t \\to t+1\\)，状态变化\\(s_i \\to s_j\\)，并从\\(s_j \\implies o_{t+1}\\)，发射\\(o_{t+1}\\) 在\\(q_{t+1}=s_j\\)的条件下，输出序列\\(o_{t+2}\\cdots o_T\\) \\[ \\color {blue} {\\beta_{t}(i)} = \\sum_{j=1}^N\\color{red}{\\underbrace {a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}_{s_i转s_j \\; s_j发o_{t+1} \\; t+1时s_j后面\\{o_{t+2}, \\cdots\\}} } \\] 上面的公式个人的思路解释如下(不明白公式再看) 其实要从\\(\\beta_{t+1}(j) \\to \\beta_{t}(i)\\) \\(\\beta_{t+1}(j)\\)是\\(t+1\\)时刻状态为\\(s_j\\)，后面的观察序列为\\(o_{t+2}, \\cdots, o_{T}\\) \\(\\beta_{t}(i)\\)是\\(t\\)时刻状态为\\(s_i\\)，后面的观察序列为\\(\\color{red}{o_{t+1}}, o_{t+2}, \\cdots, o_{T}\\) \\(t \\to t+1\\) \\(s_i\\)会变成各种\\(s_j\\)，\\(\\beta_t(i)\\)只关心t+1时刻的显示状态为\\(o_{t+1}\\)，而不关心隐状态，所以是所有隐状态发射\\(o_{t+1}\\)的概率和 \\(\\color{red} {a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}\\)，\\(s_i\\)转为\\(s_j\\)的概率，在t+1时刻\\(s_j\\)发射\\(o_{t+1}\\)的概率，t+1时刻状态为\\(s_j\\) 观察序列为\\(o_{t+2}, \\cdots, o_{T}\\)的概率 把上述概率加起来，就得到了t时刻为\\(s_i\\),后面的观察为\\(o_{t+1}, o_{t+2}, \\cdots, o_{T}\\)的概率\\(\\beta_{t}(i)\\) 上式是把所有从\\(t+1 \\to t\\)的概率加起来，得到\\(t\\)的概率。算法步骤如下 初始化 \\(\\color {blue} {\\beta_T(i) = 1}, \\; 1 \\leq i \\leq N\\) 归纳计算 \\(\\color {blue} {\\beta_{t}(i)} = \\sum_{j=1}^N\\color{red}{a_{ij}b_j(o_{t+1})\\beta_{t+1}(j) }, \\quad 1 \\leq t \\leq T-1; \\; 1 \\leq i \\leq N\\) 求和终结 \\(\\color {blue} {P(O \\mid \\mu)} = \\sum_{i=1}^{N} \\color{red} {\\pi_i b_i(o_1)\\beta_1(i)}\\) 前后向算法结合 模型\\(\\mu\\)，观察序列\\(O=\\{o_1, o_2, \\cdots, o_t, o_{t+1}\\cdots, o_T\\}\\)，\\(t\\)时刻状态为\\(q_t=s_i\\)的概率如下 \\[ \\color {blue} {P(O, q_t = s_i \\mid \\mu)} = \\color{red} {\\alpha_t(i) \\times \\beta_t(i)} \\] 推导过程如下 \\[ \\begin{align*} P(O, q_t = s_i \\mid \\mu) &amp;= P(o_1\\cdots o_T, q_t=s_i \\mid \\mu) =P(o_1 \\cdots o_t, q_t=s_i, o_{t+1} \\cdots o_T \\mid \\mu) \\\\ &amp;= P(o_1 \\cdots o_t, q_t=s_i \\mid \\mu) \\times P(o_{t+1} \\cdots o_T \\mid o_1 \\cdots o_t, q_t=s_i, \\mu) \\\\ &amp;= \\alpha_t(i) \\times P((o_{t+1} \\cdots o_T \\mid q_t=s_i, \\mu) \\quad (显然o_1 \\cdots o_t是显然成立的，概率为1，条件忽略)\\\\ &amp;= \\alpha_t(i) \\times \\beta_t(i) \\end{align*} \\] 所以，把\\(q_t\\)等于所有\\(s_i\\)的概率加起来就可以得到观察概率\\(\\color{blue} {P(O \\mid \\mu)}\\) \\[ \\color{blue} {P(O \\mid \\mu)} = \\sum_{i=1}^N\\ \\color{red} {\\alpha_t(i) \\times \\beta_t(i)}, \\quad 1 \\leq t \\leq T \\] 维特比算法 维特比(Viterbi)算法用于求解HMM的第二个问题状态序列问题。即给定观察序列\\(O=o_1o_2\\cdots o_T\\)和模型\\(\\mu = (A, B, \\pi)\\)，求一个最优的状态序列\\(Q=q_1q_2 \\cdots q_T\\)。 有两种理解最优的思路。 使该状态序列中每一个状态都单独地具有最大概率，即\\(\\gamma_t(i) = P(q_t = s_i \\mid O,\\mu)\\)最大。但可能出现\\(a_{q_tq_{t+1}}=0\\)的情况 另一种是，使整个状态序列概率最大，即\\(P(Q \\mid O, \\mu)\\)最大。\\(\\hat{Q} = arg \\max \\limits_Q P(Q \\mid O, \\mu)\\) 维特比变量 \\(\\color{blue}{\\delta_t(i)}\\)是，在\\(t\\)时刻，\\(q_t = s_i\\) ，HMM沿着某一条路径到达状态\\(s_i\\)，并输出观察序列\\(o_1o_2 \\cdots o_t\\)的概率。 \\[ \\color{blue}{\\delta_t(i)} = \\arg \\max \\limits_{q_1\\cdots q_{t-1}} P(q_1 \\cdots q_{t-1}, q_t = s_i, o_1 \\cdots o_t \\mid \\mu) \\] 递推关系 \\[ \\color{blue}{\\delta_{t+1}(i)} = \\max \\limits_j [\\delta_t(j) \\cdot a_{ji}] \\cdot b_i(o_{t+1}) \\] 路径记忆变量 \\(\\color{blue}{\\psi_t(i) = k}\\) 表示\\(q_t = s_i, q_{t-1} = s_k\\)，即表示在该路径上状态\\(q_t=s_i\\)的前一个状态\\(q_{t-1} = s_k\\)。 维特比算法步骤 初始化 \\(\\delta_1(i) = \\pi_ib_i(o_1), \\; 1 \\le i \\le N\\)，路径变量\\(\\psi_1(i) = 0\\) 归纳计算 维特比变量 \\(\\delta_t(j) = \\max \\limits_{1 \\le i \\le N} [\\delta_{t-1}(i) \\cdot a_{ij}] \\cdot b_j(o_t), \\quad 2 \\le t \\le T; 1 \\le j \\le N\\) 记忆路径(记住参数\\(i\\)就行) \\(\\psi_t(j) = \\arg \\max \\limits_{1 \\le i \\le N} [\\delta_{t-1}(i) \\cdot a_{ij}] \\cdot b_j(o_t), \\quad 2 \\le t \\le T; 1 \\le j \\le N\\) 终结 \\[ \\hat{Q_T} = \\arg \\max \\limits_{1 \\le i \\le N} [\\delta_T(i)], \\quad \\hat P(\\hat{Q_T}) = \\max \\limits_{1 \\le i \\le N} [\\delta_T(i)] \\] 路径（状态序列）回溯 \\(\\hat{q_t} = \\psi_{t+1}(\\hat{q}_{t+1}), \\quad t = T-1, T-2, \\cdots, 1\\) Baum-Welch算法 Baum-Welch算法用于解决HMM的第3个问题，参数估计问题，给定一个观察序列\\(O= o_1 o_2 \\cdots o_T\\)，去调节模型\\(\\mu = (A, B, \\pi)\\)的参数使得\\(P(O\\mid \\mu)\\)最大化，即\\(\\mathop{argmax} \\limits_{\\mu} P(O_{training} \\mid \\mu)\\)。模型参数主要是\\(a_{ij}, b_j(k) \\text{和}\\pi_i\\)，详细信息见上文。 有完整语料库 如果我们知道观察序列\\(\\color{blue}{O= o_1 o_2 \\cdots o_T}\\)和状态序列\\(\\color{blue}{Q = q_1 q_2 \\cdots q_T}\\)，那么我们可以根据最大似然估计去计算HMM的参数。 设\\(\\delta(x, y)\\)是克罗耐克函数，当\\(x==y\\)时为1，否则为0。计算步骤如下 \\[ \\begin{align*} &amp; 初始概率\\quad \\color{blue}{\\bar\\pi_i} = \\delta(q_1, s_1) \\\\ &amp; 转移概率\\quad \\color{blue}{\\bar {a}_{ij}} = \\frac{s_i \\to s_j的次数}{s_i \\to all的次数} = \\frac {\\sum_{t=1}^{T-1} \\delta(q_t, s_i) \\times \\delta(q_{t+1}, s_j)} { \\sum_{t=1}^{T-1}\\delta(q_t, s_i)} \\\\ &amp; 发射概率 \\quad \\color{blue}{\\bar{b}_j(k)} = \\frac{s_j \\to v_k 的次数}{Q到达q_j的次数} = \\frac {\\sum_{t=1}^T\\delta(q_t, s_i) \\times \\delta(o_t, v_k)}{ \\sum_{t=1}^{T}\\delta(q_t, s_j)} \\end{align*} \\] 但是一般情况下是不知道隐藏状态序列\\(Q​\\)的，还好我们可以使用期望最大算法去进行含有隐变量的参数估计。主要思路如下。 我们可以给定初始值模型\\(\\mu_0\\)，然后通过EM算法去估计隐变量\\(Q\\)的期望来代替实际出现的次数，再通过上式去进行计算新的参数得到新的模型\\(\\mu_1\\)，再如此迭代直到参数收敛。 这种迭代爬山算法可以局部地使\\(P(O \\mid \\mu)\\)最大化，BW算法就是具体实现这种EM算法。 Baum-Welch算法 给定HMM的参数\\(\\mu\\)和观察序列\\(O= o_1 o_2 \\cdots o_T\\)。 定义t时刻状态为\\(s_i\\)和t+1时刻状态为\\(s_j\\)的概率是\\(\\color{blue}{\\xi_t(i, j)} = P(q_t =s_i, q_{t+1}=s_j \\mid O, \\mu)\\) \\[ \\begin{align} \\color{blue}{\\xi_t(i, j)} &amp;= \\frac{P(q_t =s_i, q_{t+1}=s_j, O \\mid \\mu)}{P(O \\mid \\mu)} = \\color{red}{\\frac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{P(O \\mid \\mu)}} = \\frac{\\overbrace{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}^{o_1\\cdots o_t, \\; o_{t+1}, \\; o_{t+2}\\cdots o_T}} {\\underbrace{\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}_{\\xi_t(i, j)对ij求和，只留下P(O\\mid \\mu)}} \\\\ \\end{align} \\] 定义\\(t\\)时刻状态为\\(s_i\\)的概率是\\(\\color{blue}{\\gamma_t(i)} = P(q_t = s_i \\mid O, \\mu)\\) \\[ \\color{blue}{\\gamma_t(i)} = \\color{red}{\\sum_{j=1}^N \\xi_t(i, j)} \\] 那么有算法步骤如下（也称作前向后向算法） 1初始化 随机地给参数\\(\\color{blue}{a_{ij}, b_j(k), \\pi_i}\\)赋值，当然要满足一些基本条件，各个概率和为1。得到模型\\(\\mu_0\\)，令\\(i=0\\)，执行下面步骤 2EM步骤 2.1E步骤 使用模型\\(\\mu_i\\)计算\\(\\color{blue}{\\xi_t(i, j)}和\\color{blue}{\\gamma_t(i)}\\) \\[ \\color{blue}{\\xi_t(i, j)} = \\color{red}{\\frac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}} , \\; \\color{blue}{\\gamma_t(i)} = \\color{red}{\\sum_{j=1}^N \\xi_t(i, j)} \\] 2.2M步骤 用上面算得的期望去估计参数 \\[ \\begin{align*} &amp; 初始概率\\quad \\color{blue}{\\bar\\pi_i} = P(q_1=s_i \\mid O, \\mu) = \\gamma_1(i) \\\\ &amp; 转移概率\\quad \\color{blue}{\\bar {a}_{ij}} = \\frac{\\sum_{t=1}^{T-1}\\xi_t(i, j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)} \\\\ &amp; 发射概率 \\quad \\color{blue}{\\bar{b}_j(k)} = \\frac{\\sum_{t=1}^T \\gamma_t(j) \\times \\delta(o_t, v_k)}{\\sum_{t=1}^T \\gamma_t(j)} \\end{align*} \\] 3循环计算 令\\(i=i+1\\)，直到参数收敛","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"概率图模型","slug":"概率图模型","permalink":"http://plmsmile.github.io/tags/概率图模型/"},{"name":"马尔可夫链","slug":"马尔可夫链","permalink":"http://plmsmile.github.io/tags/马尔可夫链/"},{"name":"隐马尔科夫模型","slug":"隐马尔科夫模型","permalink":"http://plmsmile.github.io/tags/隐马尔科夫模型/"},{"name":"维特比算法","slug":"维特比算法","permalink":"http://plmsmile.github.io/tags/维特比算法/"},{"name":"前向算法","slug":"前向算法","permalink":"http://plmsmile.github.io/tags/前向算法/"},{"name":"后向算法，BW算法","slug":"后向算法，BW算法","permalink":"http://plmsmile.github.io/tags/后向算法，BW算法/"}]},{"title":"语言模型和平滑方法","date":"2017-07-31T00:57:52.000Z","path":"2017/07/31/nlp-notes/","text":"传统的语言模型，模型评估参数（信息、各种熵和困惑度），最后介绍了一些数据平滑的方法 语言模型 二元语法$ $ 对于一个句子\\(s=w_1 \\cdots w_n\\)，近似认为一个词的概率只依赖于它前面的1个词。即一个状态只跟上一个状态有关，也称为一阶马尔科夫链。 \\[ \\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \\cdots p(w_n|w_{l-1})= \\color {red} {\\prod_{i=1}^l {p(w_i|w_{i-1})}} \\] 设\\(\\color {blue} {c(w_{i-1}w_i)}\\) 表示二元语法\\(\\color {green} {w_{i-1}w_i}\\)在给定文本中的出现次数，则上一个词是\\(w_{i-1}\\)下一个词是\\(w_i\\)的概率\\(\\color {blue} {p(w_i \\mid w_{i-1})}\\)是当前语法\\(\\color {green} {w_{i-1}w_i}\\)出现的次数比上所有形似\\(\\color {green} {w_{i-1}}w\\)的二元语法的出现次数 \\[ \\color {blue} {p(w_i \\mid w_{i-1})} =\\color {red} {\\frac {c(w_{i-1}w_i)} {\\sum_{w} {c(w_{i-1}w)}}}，w是变量 \\] \\(n\\)元语法 认为一个词出现的概率和它前面的n个词有关系。则对于句子\\(s=w_1w_2 \\cdots w_l\\)，其概率计算公式为如下： \\[ \\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_1w_2) \\cdots p(w_n|w_1w_2 \\cdots w_{l-1})=\\color {red} {\\prod_{i=1}^n{p(w_i|w_1 \\cdots w_{i-1})}} \\] 上述公式需要大量的概率计算，太理想了。一般取\\(n=2\\)或者\\(n=3\\)。 对于\\(n&gt;2\\)的\\(n\\)元语法模型，条件概率要考虑前面\\(n-1\\)个词的概率，设\\(w_i^j\\)表示\\(w_i\\cdots w_j\\)，则有 \\[ \\color{blue} {p(s)} = \\prod_{i=1}^{l+1}p(w_i \\mid w_{i-n+1}^{i})，\\color {blue} {p(w_i \\mid w_{i-n+1}^{i})}=\\frac { \\overbrace {c(w_{i-n+1}^i)}^{\\color{red}{具体以w_i结尾的词串w[i-n+1, i]}}} { \\underbrace{\\sum_{w_i}{c(w_{i-n+1}^i)}}_{\\color{red}{所有以w_i结尾的词串w[i-n+1, i]}}} \\] 实际例子 假设语料库\\(S\\)是由下面3个句子组成，所求的句子t在其后： 12s = ['brown read holy bible', 'plm see a text book', 'he read a book by david']t = 'brown read a book' 那么求句子\\(t\\)出现的概率是 \\[\\color{blue}{p(t)}=p(\\color{green}{brown\\,read\\, a\\, book})=p(brown|BOS)p(read|brown)p(a|read)p(book|a)p(eos|book)\\approx0.06\\] \\(n\\)元文法的一些应用如下 语音识别歧义消除 如给了一个拼音 \\(\\color{green}{ta\\,shi \\,yan \\,jiu \\,sheng\\, wu\\, de}\\)，得到了很多可能的汉字串：踏实研究生物的，他实验救生物的，他是研究生物的 ，那么求出\\(arg_{str}maxP(str|pinyin)\\)，即返回最大概率的句子 汉语分词问题 给定汉字串他是研究生物的。可能的汉字串 他 是 研究生 物 的和他 是 研究 生物 的，这也是求最大句子的概率 开发自然语言处理的统计方法的一般步骤 收集大量语料（基础工作，工作量最大，很重要） 对语料进行统计分析，得出知识（如n元文法，一堆概率） 针对场景建立算法，如计算概率可能也用很多复杂的算法或者直接标注 解释或者应用结果 模型评估参数 基础 评价目标：语言模型计 算出的概率分布与“真实的”理想模型是否接近 难点：无法知道“真实的”理想模型的分布 常用指标：交叉熵，困惑度 信息量和信息熵 \\(X\\)是一个离散随机变量，取值空间为\\(R\\)，其概率分布是\\(p(x)=P(X=x), x \\in R\\)。 信息量 概率是对事件确定性的度量，那么信息就是对不确定性的度量。信息量 \\(\\color {blue} {I(X)}\\)代表特征的不确定性，定义如下 \\[ \\color {blue} {I(X)}= \\color {red} {-\\log {p(x)}} \\] 信息熵 信息熵\\(\\color{blue}{H(x)}\\)是特征不确定性的平均值，用表示，定义如下 \\[ \\color{blue}{H(X)}=\\sum_{x \\in R}{p(x)log\\frac 1 {p(x)}}=\\color {red} {-\\sum_{x \\in R} {p(x) \\log p(x)}} \\] 一般是\\(log_2{p(x)}\\)，单位是比特。若是\\(\\ln {p(x)}\\)，单位是奈特。 信息熵的本质是信息量的期望 信息熵是对不确定性的度量 随机变量\\(X\\)的熵越大，说明不确定性也大；若\\(X\\)为定值，则熵为0 平均分布是&quot;最不确定”的分布 联合熵和条件熵 \\(X, Y\\)是一对离散型随机变量，并且\\(\\color{blue}{X,Y \\sim p(x,y)}\\)。 联合熵 联合熵实际上描述的是一对随机变量平均所需要的信息量，定义如下。 \\[ \\color{blue}{H(X, Y)} = \\color{red} {- \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(x, y)} \\] 条件熵 给定\\(X\\)的情况下，\\(Y\\)的条件熵为 \\[ \\color{blue}{H(Y \\mid X)} = \\color{red}{ - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(y \\mid x)} \\] 其中可以推导出：\\(H(X, Y) = H(X) + H(Y \\mid X)\\)。 相对熵和交叉熵 相对熵 随机变量\\(X\\)的状态空间\\(\\Omega {x}\\)上有两个概率分布\\(p(x)\\)和\\(q(x)\\)。一般p是真实分布，q是预测分布。 相对熵也称为KL距离，用来衡量相同事件空间里两个概率分布的差异。 \\(p\\)和\\(q\\)的相对熵\\(\\color{blue}{D(p\\mid\\mid q)}\\)用来度量它们之间的差异，如下 \\[ \\color{blue}{D(p\\mid\\mid q)} =\\color{red}{\\sum_{x\\in X} {p(x)\\log{\\frac {p(x)}{q(x)}}}} = E_p(\\log \\frac{p(X)}{q(X)}) \\; (期望) \\] 特别地，若\\(p==q\\)，则相对熵为0；若差别增加，相对熵的值也增加。简单理解“相对”如下： \\[ D(p \\mid\\mid q)=\\sum_{x \\in X}{p(x)(\\log p(x) - \\log q(x))} = \\underbrace{\\left(-\\sum_{x \\in X}{ \\color{red}{p(x)\\log q(x)}}\\right)}_{\\color {red}{以q去近似p的熵=交叉熵}} - \\underbrace{\\left(-\\sum_{x \\in X} {\\color{red}{p(x)\\log p(x)}}\\right)}_{\\color{red} {p本身的熵}} \\] 交叉熵 交叉熵用来衡量估计模型与真实概率分布之间的差异。 随机变量\\(X \\sim p(x)\\)，\\(q(x)\\)近似于\\(p(x)\\)。 则随机变量\\(X\\)和模型\\(q\\)之间的交叉熵\\(\\color {blue} {H(X, q)}\\)如下：以\\(q\\)去近似\\(p\\)的熵 \\[ \\color {blue} {H(p, q)} = H(X) + D(p \\mid\\mid q) = \\color {red} {-\\sum_{x \\in X}{p(x)\\log q(x)}} \\] 实际应用 交叉熵的实际应用，设\\(y\\)是预测的概率分布，\\(y^\\prime\\)为真实的概率分布。则用交叉熵去判断估计的准确程度 \\[ H(y^{\\prime}, y)= - \\sum_i y_i^{\\prime}\\log y_i = \\color {red} {-\\sum_i y_{真实} \\log y_{预测}} \\] n元文法模型的交叉熵 设测试集\\(T=(t_1, t_2, \\ldots, t_l)\\)包含\\(l\\)个句子，则定义测试集的概率\\(\\color {blue} {p(T)}\\)为多个句子概率的乘积 \\[ \\color {blue} {p(T)} = \\prod_{i=1}^{l} p(t_i)， \\, \\text{其中}\\color{blue}{p(t_i)}=\\color{red}{\\prod_{i=1}^{l_w} {p(w_i|w_{i-n+1}^{i-1})}}, \\text{见上面} \\] 其中\\(w_i^j\\)表示词\\(w_i\\cdots w_j\\)，\\(\\sum_{w}{c(read \\, w)}\\)是查找出所有以\\(read\\)开头的二元组的出现次数。 则在数据\\(T\\)上n元模型\\(\\color {green} {p(w_i|w_{i-n+1}^{i-1})}\\)的交叉熵\\(\\color {blue} {H_p(T)}\\)定义如下 \\[ \\color {blue} {H_p(T)} = \\color {red} {-\\frac {1} {W_T} \\log _2 p(T)}，其中W_T是文本T中基元(词或字)的长度 \\] 公式的推导过程如下 \\[ -\\sum_{x \\in X}{p(x)\\log q(x)} \\implies \\underbrace { -{\\frac{1} {W_T}}\\sum \\log q(x)}_{\\color{red}{使用均匀分布代替p(x)}} \\implies -{\\frac{1} {W_T}} \\log {\\prod r(w_i|w_{i-n+1}^{i-1})} \\implies -{\\frac{1} {W_T}} \\log_2p(T) \\] 可以这么理解：利用模型\\(p\\)对\\(W_T\\)个词进行编码，每一个编码所需要的平均比特位数。 困惑度 困惑度是评估语言的基本准则人，也是对测试集T中每一个词汇的概率的几何平均值的倒数。 \\[ \\color{blue}{PP_T(T)} =\\color{red}{ 2^{H_p(T)}= \\frac {1} {\\sqrt [W_T]{p(T)}}} = 2 ^{\\text{交叉熵}} \\] 当然，交叉熵和困惑度越小越好。语言模型设计的任务就是要找出困惑度最小的模型。 在英语中，n元语法模型的困惑度是\\(50 \\sim 1000\\)，交叉熵是\\(6 \\sim 10\\)个比特位。 数据平滑 问题的提出 按照上面提出的语言模型，有的句子就没有概率，但是这是不合理的，因为总有出现的可能，概率应该大于0。设\\(\\color {blue}{c(w)}\\)是\\(w\\)在语料库中的出现次数。 \\[ p(\\color{green} {read \\mid plm}) = \\frac {c(plm \\mid read)} {\\sum_{w_i}{c(plm | w_i})} = \\frac {\\color{red}{0}} {1}=\\color{red}{0， \\, 这是不对的} \\] 因此，必须分配给所有可能出现的字符串一个非0的概率值来避免这种错误的发送。 平滑技术就是用来解决这种零概率问题的。平滑指的是为了产生更准确的概率来调整最大似然估计的一种技术，也称作数据平滑。思想是劫富济贫，即提高低概率、降低高概率，尽量是概率分布趋于均匀。 数据平滑是语言模型中的核心问题 加法平滑 其实为了避免0概率，最简单的就是给统计次数加1。这里我们可以为每个单词的出现次数加上\\(\\delta，\\delta \\in [0, 1]\\)，设\\(V\\)是所有词汇的单词表，\\(|V|\\)是单词表的词汇个数，则有概率： \\[ p_{add}(w_i \\mid w_{i-n+1}^{i-1}) = \\frac {\\delta + c(w_{i-n+1}^i)} {\\sum_{w_i}{(\\delta*|V| + c(w_{i-n+1}^i)})}=\\frac {\\delta + \\overbrace {c(w_{i-n+1}^i)}^{\\color{red}{具体词串[i-n+1, i]}}} {\\delta*|V| + \\underbrace{\\sum_{w_i}{c(w_{i-n+1}^i)}}_{\\color{red}{所有以w_i结尾的词串[i-n+1, i]}}} \\] 注：这个方法很原始。 Good-Turing Good-Turing也称作古德-图灵方法，这是很多平滑技术的核心。 主要思想是重新分配概率，会得到一个剩余概率量\\(\\color {blue} {p_0}= \\color {red} {\\frac {n_1} N}\\)，设\\(n_0\\)为未出现的单词的个数，然后由这\\(n_0\\)个单词去平均分配得到\\(p_0\\)，即每个未出现的单词的概率为\\(\\frac {p_0} {n_0}\\)。 对于一个\\(n\\)元语法，设\\(\\color {blue} n_r\\)恰好出现\\(r\\)次的\\(n\\)元语法的数目，下面是一些新的定义 出现次数为\\(r\\)的\\(n\\)元语法 新的出现次数\\(\\color {blue} {r^*} = \\color {red} {(r+1)\\frac{n_{r+1}}{n_r}}\\) 设\\(N = \\sum_{r=0}^{\\infty}n_r r^* = \\sum_{r=1}^{\\infty} n_r r\\)，即\\(N\\)是这个分布中最初的所有文法出现的次数，例如所有以\\(read\\)开始的总次数 出现次数为\\(r\\)的修正概率 \\(\\color {blue}p_r = \\color {red} {\\frac {r^*} {N}}\\) 剩余概率量\\(\\color {blue} {p_0}= \\color {red} {\\frac {n_1} N}\\)的推导 \\[ 总的概率 = \\sum_{r&gt;0}{n_r p_r} = \\sum_{r&gt;0}{n_r (r+1)\\frac{n_{r+1}}{n_r N}} = \\frac {1}{N} (\\sum_{r&gt;0} (r+1)n_{r+1} = \\frac {1}{N} (\\sum_{r&gt;0} (r n_r - n_1) = 1 - \\frac {n_1} N &lt; 1 \\] 然后把\\(p_0\\)平均分配给所有未见事件(r=0的事件)。 缺点 若出现次数最大为\\(k\\)，则无法计算\\(r=k\\)的新的次数\\(r^*\\)和修正概率\\(p_r\\) 高低阶模型的结合通常能获得较好的平滑效果，但是Good-Turing不能高低阶模型结合 Jelinek-Mercer 问题引入 假如\\(c(send \\, the)=c(send \\, thou)=0\\)，则通过GT方法有\\(p(the \\mid send)=p(thou \\mid send)\\)，但是实际上却应该是\\(p(the \\mid send)&gt;p(thou \\mid send)\\)。 所以我们需要在二元语法模型中加入一个一元模型 \\[ p_{ML}(w_i) = \\frac {c(w_i)}{\\sum_w{c(w)}} \\] 二元线性插值 使用\\(r\\)将二元文法模型和一元文法模型进行线性插值 \\[ p(w_i \\mid w_{i-1}) = \\lambda p_{ML}(w_i | w_{i-1}) + (1-\\lambda)p_{ML}(w_i)，\\lambda \\in [0, 1] \\] 所以可以得到\\(p(the \\mid send)&gt;p(thou \\mid send)\\)","tags":[{"name":"语言模型","slug":"语言模型","permalink":"http://plmsmile.github.io/tags/语言模型/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"各种熵","slug":"各种熵","permalink":"http://plmsmile.github.io/tags/各种熵/"},{"name":"数据平滑","slug":"数据平滑","permalink":"http://plmsmile.github.io/tags/数据平滑/"}]},{"title":"剑指Offer(1-10)","date":"2017-07-29T03:42:07.000Z","path":"2017/07/29/aim2offer/","text":"剑指offer算法题1-10 数组中重复的数字-03 题目1 找到数组中重复的数字 一个数组存放n个数字，所有数字在[0, n-1]范围内。某些数字是随机重复的。请找出任意一个重复的数字。例如[2,3,1,0,2,5,3]，输出2或3 思路1 对数组进行排序，然后可以找出重复的数字。但是排序的时间复杂度是O(nlogn) 思路2 使用哈希表，每次存放的时候检查是否在哈希表中，如果已经存在，那么就重复了。时间复杂度O(n)，空间复杂度O(n) 最优思路 利用下标和值的关系，从头到尾依次扫描这个数组。扫描到下标为i，值为m。尽量把m放到a[m]的位置上。修改了原来的数组。 12345678910while (a[i] != i) if m == i: # 扫描下一个数字 else: # 把m和a[m]进行比较 if m == a[m]: # 找到一个相同的数字 else: # 把m放到a[m]的位置，交换 m &lt;--&gt; a[m] 尽管有两重循环，但每个数字最多交换两次就能找到自己的位置，所以总的时间复杂度是O(n)，空间复杂度为O(1) 关键代码 123456789101112131415161718192021222324252627// 找到数组中重复的值// Args:// a: 数组// alen: 数组长度// dup: 用于返回重复的数值// Returns:// True：数据合法(长度和值)并且有重复的数字，否则返回False// bool duplicate(int a[], int alen, int *dup) &#123; // 遍历数组，把i都放到a[i]上 for (int i = 0; i &lt; alen; i++) &#123; while (a[i] != i) &#123; int m = a[i]; if (a[m] == m) &#123; // a[m]已经有m *dup = m; return true; &#125; else &#123; // 把m放到a[m]上 int t = a[m]; a[m] = m; a[i] = t; &#125; &#125; &#125; return false;&#125; 题目2 不修改数组找出重复的数字 数组，长度为n+1，数字范围[1, n]，数组中至少有一个是重复的，找出任意一个重复的数字，但是不能修改数组 思路1 创建一个新数组b存放原数组a。遍历原数组，当前是m，如果b[m]已经没有值，则存放；如果有值，则重复。但是需要O(n)的辅助空间 最优思路 见二分查重描述清晰版 把\\(a[1, n]\\)的个数字，分为两部分。\\(a[1, m]\\)和\\(a[m+1, n]\\)。 在\\(a[1, m]\\)中，统计数字\\(1,2\\cdots, m\\)在\\(a[1,m]\\)中出现的次数count 如果是m次，则\\(a[1,m]\\)每个数字独一无二，重复的区间在a[m+1, n]中。则\\(\\rm{start}=m+1\\)，继续查找。 否则不独一无二，则重复在\\(a[1, m]\\)中 。则\\(\\rm{end}=m\\)， 继续查找。 直到\\(\\rm{start} == \\rm{end}\\) 。count &gt; 1，则start重复，否则没有重复的。 关键代码 1234567891011121314151617181920212223242526272829303132// 二分查找数组中重复的值// Args:// a: 数组// alen: 数组长度// Returns:// dup: 重复的数值; 没有重复时返回-1int get_duplication(const int *a, int alen) &#123; if (a == nullptr || alen &lt;= 0) &#123; return -1; &#125; int start = 1; int end = alen - 1; while (start &lt;= end) &#123; int m = ((end - start) &gt;&gt; 1) + start; int count = count_range(a, alen, start, m); // last if (start == end) &#123; if (count &gt; 1) &#123; return start; &#125; else &#123; break; &#125; &#125; // continue if (count == m - start + 1) &#123; start = m + 1; &#125; else &#123; end = m; &#125; &#125; return 0;&#125; 二维数组查找-04 ​ 一个二维数组，每一行从左到右递增，每一列，从上到下递增。输入一个整数，判断二维数组中是否有这个数字 错误思路 全盘扫描肯定不行，从左上角最小的开始也不行，应该从最大角的地方开始 思路 一行的最大元素在最右边，一列的最小元素在上边。所以从右上角开始查找最好。即向左查、向下查，这样每次都能够剔除一行或者一列。 1234567891011while# 当期右上角值是a[i, j]=m，查找的值是tif t == a[i,j]: done # 查找成功else if t &gt; a[i,j]: # 删除当前行 m = a[i+1, j]else if t &lt; a[i, j]: # 删除当前列 m = a[i, j-1]# 继续查找 关键代码 12345678910111213141516171819202122232425262728// 查找一个数，是否在一个矩阵中// Args:// target: 要查找的数字// array: 矩阵// Returns:// exists: true or falsebool find(int target, std::vector&lt;std::vector&lt;int&gt;&gt; array) &#123; int col = array.size(); int row = array[0].size(); bool exist = false; int i = 0; int j = col - 1; // 注意i,j的范围 while (exist == false &amp;&amp; (i &lt; row &amp;&amp; i &gt;= 0 &amp;&amp; j &lt; col &amp;&amp; j &gt;= 0)) &#123; int t = array[i][j]; if (target == t) &#123; exist = true; break; &#125; else if (target &lt; t) &#123; // to left --j; &#125; else if(target &gt; t) &#123; // to down ++i; &#125; &#125; return exist;&#125; 字符串替换空格-05 把字符串中的每个空格替换成&quot;%20&quot; 如果在原来的字符串上修改，则会覆盖原来字符串后面的内存 如果创建新的字符串，则要分配足够的内存 C/C++中字符串最后一个字符是\\0 不好思路 从前向后扫描，遇到一个空格替换一个。但是每次都需要大量移动后面的元素，所以时间复杂度是\\(O(n^2)\\) 最优思路 从后向前替换。使用两个指针p1和p2。先计算出替换后的长度，p2指向替换后的长度的末尾指针。p1指向之前的字符串的指针。 从p1开始向前移动 当前是普通字符，则复制到p2，p2向前移动 当前是空格，则在p2加入“%20”，p2向前移动 如果p1==p2，那么移动完毕 总的来说，先找到最终的长度，从后向后拉。时间复杂度是\\(O(n)\\) 技巧 合并两个数组/字符串，如果从前往后，则需要移动多次。从后向前，能够减少移动次数，提高效率 关键代码 1234567891011121314151617181920212223242526272829303132// 替换字符串中的空格字符，每个空格用'02%'替换// 直接修改原字符串// Args:// str: 字符串// len: 长度// Returns:// Nonevoid replace_space(char *str, int len) &#123; // 统计空格的个数 int count = 0; for (int i = 0; i &lt; len; i++) if (str[i] == ' ') ++count; int newlen = (len - count) + count * 3; int i = len - 1, j = newlen - 1; while (i &gt;= 0 &amp;&amp; j &gt;= 0) &#123; if (str[i] == ' ') &#123; // 在j处添加替换字符 str[j--] = '0'; str[j--] = '2'; str[j--] = '%'; // 向前移动 --i; &#125; else &#123; // 字符复制到后面 str[j--] = str[i--]; &#125; &#125; // 字符串结尾 str[newlen] = '0';&#125; 逆序打印链表-06 链表基础考点 链表是面试中最频繁的数据结构。动态结构很灵活，考指针、考编程功底。 链表创建 ： 链表插入： 为新节点分配内存，调整指针的指向。 删除链表中的节点 从尾到头打印链表 链表中倒数第k个节点 反转链表 合并两个排序的链表 两个链表的第一个公共节点 环形链表 ：尾节点指针指向头结点。题目62：圆圈中最后剩下的数字 双向链表 ：题目36，二叉搜索树与双向链表 复杂链表 ：指向下一个，指向任意节点的指针 从尾到头打印链表 本质上是先进后出， 可以用栈和递归。显然，栈的效率高。 关键代码 1234567891011121314151617181920212223// 使用栈逆序打印链表// Args:// head: 头指针// Returns:// res: vector&lt;int&gt;，逆序值vector&lt;int&gt; get_reverse_by_stack(ListNode *head) &#123; ListNode* pnode = head; stack&lt;int&gt; st; int count = 0; while (pnode != nullptr) &#123; st.push(pnode-&gt;val); pnode = pnode-&gt;next; count++; &#125; // 分配定长的vector，不用 vector&lt;int&gt; res(count); for (int i = 0; i &lt; count &amp;&amp; st.empty() == false; i++) &#123; res[i] = st.top(); st.pop(); &#125; return res;&#125; 重建二叉树-07 树的考点 树的遍历 叉树涉及指针，比较难。最常问遍历。需要对下面7种了如指掌。 前序 中序 后序 层次遍历 递归 无递归 循环 考题 题26，树的子结构 题34，二叉树中和为某一值的路径 题55，二叉树的深度 题7，重建二叉树 题33，二叉搜索树的后序遍历序列 题32，从上到下打印二叉树（层次遍历） 特别的二叉树 二叉搜索树 ：左节点小于根节点，根节点小于右节点。查找搜索时间复杂度\\(O(\\log n)\\)。 题36，二叉搜索树与双向链表；题68：树中两个节点的最低公共祖先。 堆 ：最大堆和最小堆。找最大值和最小值。 红黑树 ： 节点定义为红黑两种颜色。根节点到叶节点的最长路径不超过最短路径的两倍。 前序中序建立二叉树 前序序列：1, 2, 4, 7, 3, 5, 6, 8。 根 左 右。 中序序列：4, 7, 2, 1, 5, 3, 8, 6。 左 根 右。 使用递归，先找到根节点，找到左右子树，为左右子树分别创建各自的前序和中序序列，再进行递归创建左右子树。 关键是要构建下面的序列，注意下标值。 前序 中序 左子树 2, 4, 7 4, 7, 2 右子树 3, 5, 6, 8 5, 3, 8, 6 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 递归利用先序和中序重建二叉树// Args:// vpre: 先序序列// vin: 中序序列// Returns:// root: treeTreeNode * reconstruct_binary_tree(vector&lt;int&gt;vpre, vector&lt;int&gt; vin) &#123; // 1. 为空，停止递归 if (vpre.size() == 0 || vin.size() == 0) &#123; return NULL; &#125; // 2. 构建根节点 TreeNode *root = new TreeNode(vpre[0]); // 3. 找到根节点在中序中的位置 int root_index = -1; for (int i = 0; i &lt; vin.size(); i++) &#123; // cout &lt;&lt; vin[i] &lt;&lt; \" \" &lt;&lt; vpre[0] &lt;&lt; endl; if (vin[i] == vpre[0]) &#123; root_index = i; break; &#125; &#125; // 简单判断一下 if (root_index == -1) &#123; cout &lt;&lt; \"root_index is -1\" &lt;&lt; endl; return NULL; &#125; // 4. 生成左右子树的先序序列、中序序列 int leftlen = root_index; int rightlen = vin.size() - leftlen - 1; vector&lt;int&gt; leftvpre(leftlen), leftvin(leftlen); vector&lt;int&gt; rightvpre(rightlen), rightvin(rightlen); // 重点在这里，用实际例子去对照看 for (int i = 0; i &lt; vin.size(); i++) &#123; if (i &lt; root_index) &#123; // 左子树 leftvin[i] = vin[i]; leftvpre[i] = vpre[i+1]; &#125; else if (i &gt; root_index)&#123; // 右子树，条件特别重要 int right_idx = i - root_index - 1; rightvin[right_idx] = vin[i]; rightvpre[right_idx] = vpre[leftlen + 1 + right_idx]; &#125; &#125; // 5. 递归生成左右子树 root-&gt;left = reconstruct_binary_tree(leftvpre, leftvin); root-&gt;right = reconstruct_binary_tree(rightvpre, rightvin); return root;&#125; 二叉树的下一个节点-08 二叉树：值，左孩子，右孩子，父亲节点指针。 给一个节点，找出中序序列的该节点的下一个节点。重在分析中序序列。 12345678910if \"有右子树\": # 向左走 while (\"p有左孩子\") p = \"左孩子\" t = pelse: # 向上走 while (\"p有父节点 &amp;&amp; p是父节点的右节点\"): p = \"父节点\" t = p 关键代码 123456789101112131415161718192021222324252627// 找到中序遍历的下一个节点// Args:// pnode: 当前节点// Returns:// pnext: 中序中，pnode的下一个节点TreeNode* get_next_inorder(TreeNode* pnode) &#123; if (pnode == nullptr) &#123; return nullptr; &#125; TreeNode* pnext = nullptr; if (pnode-&gt;right != nullptr) &#123; TreeNode* p = pnode-&gt;right; while (p-&gt;left != nullptr) &#123; p = p-&gt;left; &#125; pnext = p; &#125; else &#123; TreeNode* p = pnode; while (p-&gt;parent != nullptr &amp;&amp; p == p-&gt;parent-&gt;right) &#123; p = p-&gt;parent; &#125; if (p-&gt;parent != nullptr) &#123; pnext = p-&gt;parent; &#125; &#125; return pnext;&#125; 两个栈实现队列-09 栈和队列 栈 ：后进先出 题31：栈的压入、弹出序列 \\(O(n)\\) 找到最大最小元素。若\\(O(1)\\) ，则题30：包含min函数的栈 队列 ：先进先出 树的层次遍历，题32： 从上到下打印二叉树 用两个栈实现队列 分为入栈（栈A）和出栈（栈B）。 入队时：直接进入入栈 出队时：若出栈为空，则把入栈里的内容放入出栈；再从出栈里面出一个元素。 [关键代码] 1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; private: stack&lt;int&gt; stackIn; stack&lt;int&gt; stackOut; public: // 入队 void push(int node) &#123; stackIn.push(node); &#125; // 出队 int pop() &#123; if (this-&gt;empty()) &#123; cout &lt;&lt; \"empty queue\" &lt;&lt; endl; return -1; &#125; int node = -1; if (stackOut.empty() == true) &#123; while (stackIn.empty() == false) &#123; node = stackIn.top(); stackIn.pop(); stackOut.push(node); &#125; &#125; node = stackOut.top(); stackOut.pop(); return node; &#125; bool empty() &#123; return stackIn.empty() == true &amp;&amp; stackOut.empty() == true; &#125;&#125;; 两个队列实现栈 分为空队列和非空队列 入栈：进入非空队列 出栈：非空队列中中前n-1个进入空队列，出非空队列最后一个元素（最新进来的） 关键代码 123456789101112131415161718192021222324int pop() &#123; if (this-&gt;empty()) &#123; return -1; &#125; // 找到哪个队列有元素，注意使用指针 queue&lt;int&gt;* qout; queue&lt;int&gt;* qin; if (q1.empty() == true) &#123; qout = &amp;q2; qin = &amp;q1; &#125; else &#123; qout = &amp;q1; qin = &amp;q2; &#125; // qout的前n-1个元素放到qin中 while (qout-&gt;size() &gt; 1) &#123; qin-&gt;push(qout-&gt;front()); qout-&gt;pop(); &#125; int res = qout-&gt;back(); qout-&gt;pop(); return res;&#125; 算法和数据操作 总览 类型 题型 备注 递归和循环 树的遍历 递归简介，循环效率高 排序和查找 二分查找、归并排序、快速排序 正确、完整写出代码 二维数组 迷宫、棋盘 回溯法；栈模拟递归 最优解 动态规划，问题分解为多个子问题 自上而下递归分析；自下而上循环代码实现，数组保存 最优解 贪心算法 分解时是否存在某个特殊选择：贪心得到最优解 与、或、异或、左移、右移 递归效率低的原因：函数调用自身，函数调用是由时间和空间的消耗；会在内存栈中分配空间以保存参数，返回地址和临时变量，往栈里弹入和弹出都需要时间。 递归和循环：题10，斐波那契数列；题60，n个骰子的点数。 动态规划 递归思路分析，递归分解的子问题中存在着大量的重复。用自下而上的循环来实现代码。题14 剪绳子， 题47礼物的最大价值 ， 题48最长不含重复字符的子字符串 斐波那契数列-递归循环-10 斐波那契数列 数列定义 \\[ f(n) = \\begin{cases} &amp;0 &amp; n=0 \\\\ &amp;1 &amp; n=1 \\\\ &amp;f(n-1) + f(n-2) &amp; n \\ge 1 \\end{cases} \\] 递归和循环两种实现策略 关键代码 1234567891011121314151617181920long long fibonacci_recursion(unsigned int n) &#123; if (n &lt;= 0) return 0; if (n == 1) return 1; return fibonacci_recursion(n-1) + fibonacci_recursion(n-2);&#125;long long fibonacci_loop(unsigned int n) &#123; if (n &lt;= 0) return 0; if (n == 1) return 1; long long f1 = 0; long long f2 = 1; long long fn = 0; for (unsigned int i = 2; i &lt;= n; i++) &#123; fn = f1 + f2; f1 = f2; f2 = fn; &#125; return fn;&#125; 青蛙跳台阶 青蛙可以一次跳1个台阶，一次跳2个台阶。问青蛙跳n个台阶有多少种跳法。 青蛙跳到第n个台阶有两种跳法：跳1个和2个。所以\\(f(n)=f(n-1)+f(n-2)\\) 。是斐波那契数列。 扩展 青蛙一次可以跳1个台阶、2个台阶、n个台阶。问有多少种跳法？ 数学归纳法证得：\\(f(n) = 2^{n-1}\\) 矩阵覆盖问题 \\(2\\times1\\)矩阵去覆盖\\(2 \\times 8\\) 矩阵，可以横着竖着覆盖，问多少种覆盖方法？ 同理，最后一个横着放或者竖着放。\\(f(8)=f(7)+f(6)\\)","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"},{"name":"排序","slug":"排序","permalink":"http://plmsmile.github.io/tags/排序/"}]},{"title":"利用tensorflow实现简版word2vec","date":"2017-07-14T12:17:50.000Z","path":"2017/07/14/word2vec/","text":"参考自Tensorflow的word2vec教程，本文主要包含了自己的实现理解。非常青涩时期写的 相关知识 传统方法 One-Hot Encoder 是一个词对应一个向量，向量中只有一个是1，其余是0，离散表达。$ $ Bag of Words 标识当前单词那一位不是1，而是变成了当前单词的出现次数。 存在的问题 需要大量的维数去表示，编码随机的，没有任何关联的信息。 向量空间模型 Vector Space Models可以把字词转化为连续值，并将意思相近的词被映射到向量空间相近的位置。 VSM在NLP中的一个重要假设是：在相同语境中出现的词，语义也相近。 有如下两种模型 计数模型 统计语料库中相邻出现的词的频率，再把这些计数结果转为小而稠密的矩阵。 预测模型 根据一个词周围相邻的词汇推测出这个词。 Word2Vec Word2Vec是一种计算非常高效的、可以从原始语料中学习字词空间向量的预测模型。 有如下两种模型 CBOW Continuous Bag of Words 从语境推测目标词汇，适合小型数据。如“中国的首都是__”推测出“北京”。把一整段上下文信息当做一个观察对象 Skip-Gram 从目标词汇推测语境，适合大型语料。 把每一对上下文-目标词汇当做一个观察对象 Word2Vec的一些优点 连续的词向量能够捕捉到更多的语义和关联信息 意思相近的词语在向量空间中的位置也会比较近。如北京-成都、狗-猫等词汇会分别聚集在一起。 能学会一些高阶语言的抽象概念。如&quot;man-woman&quot;和&quot;king-queen&quot;的向量很相似。 Word2Vec学习的抽象概念 噪声对比训练 神经概率化语言模型通常使用极大似然法进行训练，再使用Softmax函数得到在给出上下文单词\\(h\\)的情况下，目标词\\(w_t\\)出现的最大概率，设为\\(P(w_t|h)\\)。 设\\(score(w_t, h)\\)为当前词\\(w_t\\)和上下文单词\\(h\\)的相容性，通常使用向量积获得。 \\[ P(w_t|h) = Softmax(score(w_i, h))=\\frac{e^{score(w_i, h)}} {\\sum_{i=1}^v {e^{score(w_i, h)}}} \\] 通过对数似然函数max likelihood来进行训练 \\[ J_{ml}=\\ln{P(w_t|h)}=score(w_t,h)-\\ln{\\sum_{i=1}^v e^{score(w_i, h)}} \\] 这个方法看起来可行，但是消耗太大了，因为要对当前\\(h\\)与所有单词\\(w\\)的相容性\\(score(w, h)\\)。 在使用word2vec模型中，我们并不需要对所有的特征进行学习。所以在CBOW模型和Skip-Gram模型中，会构造\\(k\\)个噪声单词，而我们只需要从这k个中找出真正目标单词\\(w_t\\)即可，使用了一个二分类器（lr）。下面是CBOW模型，对于Skip-Gram模型只需要相反就行了。 设\\(Q_\\theta(D=1|w, h)\\)是二元逻辑回归的概率，即在当前条件下出现词语\\(w\\)的概率。 \\(\\theta\\) 输入的embedding vector \\(h\\) 当前上下文 \\(d\\) 输入数据集 \\(w\\) 目标词汇（就是他出现的概率） 此时，最大化目标函数如下： \\[ J_{NEG}=\\ln{Q_\\theta(D=1|w_t, h)} + \\frac {\\sum_{i=1}^{k}{\\ln Q_\\theta(D=0|w_I, h)}} {k} \\] 前半部分为词\\(w\\)出现的概率，后面为\\(k\\)个噪声概率的期望值（如果写法有错误，希望提出，再改啦），有点像蒙特卡洛。 负采样Negative Sampling 当模型预测的真实目标词汇\\(w_t\\)的概率越高，其他噪声词汇概率越低，模型就得到优化了 用编造的噪声词汇进行训练 计算loss效率非常高，只需要随机选择\\(k\\)个，而不是全部词汇 实现Skip-Gram模型 数据说明 Skip-Gram模型是通过目标词汇预测语境词汇。如数据集如下 1I hope you always find a reason to smile 从中我们可以得到很多目标单词和所对应的上下文信息（多个单词）。如假设设左右词的窗口距离为1，那么相应的信息如下 1&#123;'hope':['i', 'you'], 'you':['hope', 'alawys']...&#125; 训练时，希望给出目标词汇就能够预测出语境词汇，所以需要这样的训练数据 12345# 前面是目标单词，后面是语境词汇，实际上相当于数据的label('hope', 'i')('hope', 'you')('you', 'hope')('you', 'always') 同时在训练时，制造一些随机单词作为负样本（噪声）。我们希望预测的概率分布在正样本上尽可能大，在负样本上尽可能小。 使用随机梯度下降算法(SGD)来进行最优化求解，并且使用mini-batch的方法，这样来更新embedding中的参数\\(\\theta\\)，让损失函数(NCE loss)尽可能小。这样，每个单词的词向量就会在训练的过程中不断调整，最后会处在一个最合适的语料空间位置。 例如，假设训练第\\(t\\)步，输入目标单词hope，希望预测出you，选择一个噪声词汇reason。则目标函数如下 \\[ J_{NEG}^{(t)}=\\ln {Q_\\theta(D=1|hope, you)} + \\ln{Q_\\theta(D=0|hope, reason)} \\] 目标是更新embedding的参数\\(\\theta\\)以增大目标值，更新方式是计算损失函数对参数\\(\\theta\\)的导数，使得参数\\(\\theta\\)朝梯度方向进行调整。多次以后，模型就能够很好区别出真实语境单词和噪声词。 构建数据集 先来分析数据，对所有的词汇进行编码。对高频词汇给一个id，对于出现次数很少词汇，id就设置为0。高频是选择出现频率最高的50000个词汇。 1234567891011121314151617181920212223242526272829303132def build_dataset(self, words): ''' 构建数据集 Args: words: 单词列表 Returns: word_code: 所有word的编码，top的单词：数量；其余的：0 topword_id: topword-id id_topword: id-word topcount: 包含所有word的一个Counter对象 ''' # 获取top50000频数的单词 unk = 'UNK' topcount = [[unk, -1]] topcount.extend( collections.Counter(words).most_common( self.__vocab_size - 1)) topword_id = &#123;&#125; for word, _ in topcount: topword_id[word] = len(topword_id) # 构建单词的编码。top单词：出现次数；其余单词：0 word_code = [] unk_count = 0 for w in words: if w in topword_id: c = topword_id[w] else: c = 0 unk_count += 1 word_code.append(c) topcount[0][1] = unk_count id_topword = dict(zip(topword_id.values(), topword_id.keys())) return word_code, topword_id, id_topword, topcount 产生batch训练样本 由于是使用mini-batch的训练方法，所以每次要产生一些样本。对于每个单词，要确定要产生多少个语境单词，和最多可以左右选择多远。 12345678910111213141516171819202122232425262728293031323334353637383940414243def generate_batch(self, batch_size, single_num, skip_window, word_code): '''产生训练样本。Skip-Gram模型，从当前推测上下文 如 i love you. (love, i), (love, you) Args: batch_size: 每一个batch的大小，即多少个() single_num: 对单个单词生成多少个样本 skip_window: 单词最远可以联系的距离 word_code: 所有单词，单词以code形式表示 Returns: batch: 目标单词 labels: 语境单词 ''' # 条件判断 # 确保每个batch包含了一个词汇对应的所有样本 assert batch_size % single_num == 0 # 样本数量限制 assert single_num &lt;= 2 * skip_window # batch label batch = np.ndarray(shape=(batch_size), dtype=np.int32) labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # 目标单词和相关单词 span = 2 * skip_window + 1 word_buffer = collections.deque(maxlen=span) for _ in range(span): word_buffer.append(word_code[self.__data_index]) self.__data_index = (self.__data_index + 1) % len(word_code) # 遍历batchsize/samplenums次，每次一个目标词汇，一次samplenums个语境词汇 for i in range(batch_size // single_num): target = skip_window # 当前的单词 targets_to_void = [skip_window] # 已经选过的单词+自己本身 # 为当前单词选取样本 for j in range(single_num): while target in targets_to_void: target = random.randint(0, span - 1) targets_to_void.append(target) batch[i * single_num + j] = word_buffer[skip_window] labels[i * single_num + j, 0] = word_buffer[target] # 当前单词已经选择完毕，输入下一个单词，skip_window单词也成为下一个 self.__data_index = (self.__data_index + 1) % len(word_code) word_buffer.append(word_code[self.__data_index]) return batch, labels 一些配置信息 12345678910111213141516171819# 频率top50000个单词vocab_size = 50000# 一批样本的数量batch_size = 128# 将单词转化为稠密向量的维度embedding_size = 128# 为单词找相邻单词，向左向右最多能取得范围skip_window = 1# 每个单词的语境单词数量single_num = 2# 验证单词的数量valid_size = 16# 验证单词从频数最高的100个单词中抽取valid_window = 100# 从100个中随机选择16个valid_examples = np.random.choice(valid_window, valid_size, replace=False)# 负样本的噪声数量noise_num = 64 计算图 1234567891011121314151617181920212223242526272829303132333435363738394041424344graph = tf.Graph()with graph.as_default(): # 输入数据 train_inputs = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) valid_dataset = tf.constant(valid_examples, dtype=tf.int32) with tf.device('/cpu:0'): # 随机生成单词的词向量，50000*128 embeddings = tf.Variable( tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0)) # 查找输入inputs对应的向量 embed = tf.nn.embedding_lookup(embeddings, train_inputs) nce_weights = tf.Variable( tf.truncated_normal([vocab_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) nce_biases = tf.Variable(tf.zeros([vocab_size])) # 为每个batch计算nceloss loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, labels = train_labels, inputs=embed, num_sampled=noise_num, num_classes=vocab_size)) # sgd optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss) # 计算embeddings的L2范式，各元素的平方和然后求平方根，防止过拟合 norm = tf.sqrt( tf.reduce_sum( tf.square(embeddings), axis=1, keep_dims=True)) # 标准化词向量 normalized_embeddings = embeddings / norm valid_embeddings = tf.nn.embedding_lookup( normalized_embeddings, valid_dataset) # valid单词和所有单词的相似度计算，向量相乘 similarity = tf.matmul( valid_embeddings, normalized_embeddings, transpose_b=True) init = tf.global_variables_initializer() 训练过程 123456789101112131415161718192021222324252627282930313233num_steps = 100001with tf.Session(graph=graph) as sess: init.run() print('Initialized') avg_loss = 0 for step in range(num_steps): batch_inputs, batch_labels = wu.generate_batch( batch_size, single_num, skip_window, word_code) feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125; _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict) avg_loss += loss_val if step % 2000 == 0: if step &gt; 0: avg_loss /= 2000 print (\"avg loss at step %s : %s\" % (step, avg_loss)) avg_loss = 0 if step % 10000 == 0: # 相似度，16*50000 sim = similarity.eval() for i in range(valid_size): valid_word = id_topword[valid_examples[i]] # 选相似的前8个 top_k = 8 # 排序，获得id nearest = (-sim[i, :]).argsort()[1:top_k+1] log_str = \"Nearest to %s: \" % valid_word for k in range(top_k): close_word = id_topword[nearest[k]] log_str = \"%s %s,\" % (log_str, close_word) print log_str final_embeddings = normalized_embeddings.eval()","tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"}]},{"title":"朴素贝叶斯算法及其代码实现","date":"2017-05-06T06:36:58.000Z","path":"2017/05/06/22-ml-ch03-bayes/","text":"先介绍了朴素贝叶斯思想理论，然后用朴素贝叶斯代码实现垃圾邮件分类 条件概率 基础知识 条件概率 在\\(B\\)发生的情况下\\(A\\)的概率$ $ ​ \\(P(A|B) = \\frac{P(AB)}{P(B)}\\) ​ \\(P(c_i|x)=\\frac{P(c_ix)}{P(x)}\\)。\\(c_i\\)是类别，\\(x\\)是一个向量。\\(x\\)属于类别\\(c_i\\)的概率。 贝叶斯准则 交换条件概率中的条件与结果，得到想要的值。 \\(P(A|B) = \\frac{P(AB)}{P(B)}\\), \\(P(B|A) = \\frac{P(AB)}{P(A)}\\) \\(\\to\\) \\(P(B|A)=\\frac{P(A|B)P(B)}{P(A)}\\) 所以可以得到\\(\\color{red}{P(c_i|x)}=\\frac{P(x|c_i)P(c_i)}{P(x)}\\) 条件概率分类 贝叶斯决策理论 计算两个概率\\(x\\)属于类别1和类别2的概率\\(p_1(x)\\)和\\(p_2(x)\\)。 如果\\(p_1(x) &gt; p_2(x)\\)，则\\(x\\)属于类别1 如果\\(p_2(x) &gt; p_1(x)\\)，则\\(x\\)属于类别2 贝叶斯准则 \\(x\\)属于类别\\(c_i\\)的概率是\\(\\color{red}{P(c_i|x)}\\)。 如果\\(P(c_1|x) &gt; P(c_2|x)\\)，则\\(x\\)属于\\(c_1\\) 如果\\(P(c_2|x) &gt; P(c_1|x)\\)，则\\(x\\)属于\\(c_2\\) 朴素贝叶斯文档分类 简介 机器学习的一个重要应用就是文档的自动分类。我们可以观察文档中出现的词，并把每个词出现与否或者出现次数作为一个特征。朴素贝叶斯就是用于文档分类的常用算法，当然它可以用于任意场景的分类。 向量\\(\\color{red}{\\vec{w}}={(w_1,w_2,...,w_n)}\\)代表一篇文章。其中\\(w_i=0,1\\)，代表词汇表中第\\(i\\)个词汇出现与否。词汇表是指一个总体的全局词汇表。文章\\(\\vec{w}\\)属于第\\(i\\)类的概率\\(\\color{red}{P(c_i|\\vec{w})}=\\frac{P(\\vec{w}|c_i)P(c_i)}{P(\\vec{w})}\\)。 朴素贝叶斯分类器的两个假设： 特征之间相互独立 每个特征同等重要 尽管这有瑕疵，但是朴素贝叶斯的实际效果却很好了。 朴素贝叶斯分类器的两种实现： 伯努利模型：只考虑出现或者不出现 多项式模型：考虑词在文档中的出现次数 文档分类中的独立：每个单词出现的可能性和其他单词没有关系。独立的好处在下面概率计算中会体现出来。 概率计算 对每一个文章的各个分类概率计算，其实只需要计算上式的分母就行了。 对于\\(P(c_i)=\\frac{c_i数量}{总数量}\\)，即\\(c_i\\)类文章的数量除以所有类别的文章的总数量。 对于\\(P(\\vec{w}|c_i)\\)，要稍微复杂一些。由于各个特征（单词出现否）独立，则有如下推导公式： \\[P(\\vec{w}|c_i)=P(w_1,w_2,...,w_n|c_i)=P(w_1|c_i)P(w_2|c_i)\\cdots P(w_n|c_i)\\] 其中\\(\\color{red}{P(w_i|c_i)}\\)代表第\\(i\\)个单词在\\(c_i\\)类别文章的总词汇里出现的概率。 实际操作的一个小技巧，由于概率都很小多个小值做乘法会导致下溢出，所以决定对概率取对数做加法，最后再比较对数的大小。 \\[\\ln(P(\\vec{w}|c_i))=\\ln(P(w_1|c_i))+\\ln(P(w_2|c_i))+\\dots+\\ln(P(w_n|c_i))\\] 如上，可以求得每个单词在各个类别文章里出现的概率。用\\(\\color{red}{\\vec{wp_0}}\\)、\\(\\color{red}{\\vec{wp_1}}\\)来分别表示所有单词在类别0、类别1中总词汇中的概率。当然，在程序中实际上这个概率是取对数了的。 当要求一篇新的文章\\(\\color{red}{\\vec{w}}={(0,1,0,0,\\dots)}\\)，此时为出现或者不出现，当然也可以统计出现次数，属于哪个类别的时候，要先求出\\(\\color{red}{P(w|c_0)}\\)和\\(\\color{red}{P(w|c_1)}\\)，然后根据贝叶斯准则选择概率大的分类为结果。 \\[P(w|c_0)=\\vec{w}\\cdot\\vec{wp_0}, P(w|c_1)=\\vec{w}\\cdot\\vec{wp_1}\\] 程序实现 朴素贝叶斯的实例应有很多，这里主要是介绍垃圾邮件分类。数据集中的邮件有两种：垃圾邮件和正常邮件。每个类型都有25个样本，一共是50个样本。我们对数据集进行划分为训练集和测试集。训练集用来训练获得\\(\\vec{wp_0}\\)、\\(\\vec{wp_1}\\)和\\(p(c_1)\\)。然后用测试集去进行朴素贝叶斯分类，计算错误率，查看效果。 加载数据 数据是存放在两个文件夹中的，以txt格式的形式存储。取出来后要进行单词切割。然后得到邮件列表email_list和它对应的分类列表class_list。 1234567891011121314151617181920212223242526272829303132333435def parse_str(big_str): ''' 解析文本为单词列表 Args: big_str: 长文本 Returns: 单词列表 ''' # 以任何非单词字符切割 word_list = re.split(r'\\W*', big_str) # 只保留长度大于3的单词，并且全部转化为小写 return [word.lower() for word in word_list if len(word) &gt; 2]def load_dataset(spam_dir, ham_dir): ''' 从文件夹中加载文件 Args: spam_dir: 垃圾邮件文件夹 ham_dir: 正常邮件文件夹 Returns: email_list: 邮件列表 class_list: 分类好的列表 ''' email_list = [] class_list = [] txt_num = 25 # 每个文件夹有25个文件 for i in range(1, txt_num + 1): for j in range(2): file_dir = spam_dir if j == 1 else ham_dir f = open(('&#123;&#125;/&#123;&#125;.txt').format(file_dir, i)) f_str = f.read() f.close() words = parse_str(f_str) email_list.append(words) # 邮件列表 class_list.append(j) # 分类标签，1垃圾邮件，0非垃圾邮件 return email_list, class_list 划分数据集 由于前面email_list包含所有的邮件，下标是从0-49，所以我们划分数据集只需要获得对应的索引集合就可以了。 123456789101112131415def get_train_test_indices(data_num): ''' 划分训练集和测试集 Args: data_num: 数据集的数量 Returns: train_indices: 训练集的索引列表 test_indices: 测试集的索引列表 ''' train_indices = range(data_num) test_ratio = 0.3 # 测试数据的比例 test_num = int(data_num * test_ratio) test_indices = random.sample(train_indices, test_num) # 随机抽样选择 for i in test_indices: train_indices.remove(i) return train_indices, test_indices 获得训练矩阵 获得训练数据之后，要把训练数据转化为训练矩阵。 获得所有的词汇 1234567891011def get_vocab_list(post_list): ''' 从数据集中获取所有的不重复的词汇列表 Args: post_list: 多个文章的列表，一篇文章：由单词组成的list Returns: vocab_list: 单词列表 ''' vocab_set = set([]) for post in post_list: vocab_set = vocab_set | set(post) return list(vocab_set) 获得一篇文章的文档向量 12345678910111213141516171819202122def get_doc_vec(doc, vocab_list, is_bag = False): ''' 获得一篇doc的文档向量 词集模型：每个词出现为1，不出现为0。每个词出现1次 词袋模型：每个词出现次数，可以多次出现。 Args: vocab_list: 总的词汇表 doc: 一篇文档，由word组成的list is_bag: 是否是词袋模型，默认为Fasle Returns: doc_vec: 文档向量，1出现，0未出现 ''' doc_vec = [0] * len(vocab_list) for word in doc: if word in vocab_list: idx = vocab_list.index(word) if is_bag == False: # 词集模型 doc_vec[idx] = 1 else: doc_vec[idx] += 1 # 词袋模型 else: print '词汇表中没有 %s ' % word return doc_vec 获得训练矩阵 1234567891011121314151617181920def go_bayes_email(): ''' 贝叶斯垃圾邮件过滤主程序 Returns: error_rate: 错误率 ''' # 源数据 email_list, class_list = load_dataset('email/spam', 'email/ham') # 总的词汇表 vocab_list = bys.get_vocab_list(email_list) # 训练数据，测试数据的索引列表 data_num = len(email_list) train_indices, test_indices = get_train_test_indices(data_num) # 训练数据的矩阵和分类列表 train_mat = [] train_class = [] for i in train_indices: vec = bys.get_doc_vec(email_list[i], vocab_list) train_mat.append(vec) train_class.append(class_list[i]) # 后续还有训练数据和测试数据，在下文给出 贝叶斯算法 贝叶斯训练算法 通过训练数据去计算上文提到的\\(\\vec{wp_0}\\)、\\(\\vec{wp_1}\\)和\\(p(c_1)\\)。 1234567891011121314151617181920212223242526272829303132333435def train_nb0(train_mat, class_list): ''' 朴素贝叶斯训练算法，二分类问题 Args: train_mat: 训练矩阵，文档向量组成的矩阵 class_list: 每一篇文档对应的分类结果 Returns: p0_vec: c0中各个word占c0总词汇的概率 p1_vec: c1中各个word占c1总词汇的概率 p1: 文章是c1的概率 ''' # 文档数目，单词数目 doc_num = len(train_mat) word_num = len(train_mat[0]) # 两个类别的总单词数量 c0_word_count = 2.0 c1_word_count = 2.0 # 向量累加 c0_vec_sum = np.ones(word_num) c1_vec_sum = np.ones(word_num) for i in range(doc_num): if class_list[i] == 0: c0_word_count += sum(train_mat[i]) c0_vec_sum += train_mat[i] else: c1_word_count += sum(train_mat[i]) c1_vec_sum += train_mat[i] c1_num = sum(class_list) p1 = c1_num / float(doc_num) p0_vec = c0_vec_sum / c0_word_count p1_vec = c1_vec_sum / c1_word_count # 由于后面做乘法会下溢出，所以取对数做加法 for i in range(word_num): p0_vec[i] = math.log(p0_vec[i]) p1_vec[i] = math.log(p1_vec[i]) return p0_vec, p1_vec, p1 贝叶斯分类 123456789101112131415def classify_nb(w_vec, p0_vec, p1_vec, p1): ''' 使用朴素贝叶斯分类 Args: w_vec: 要测试的向量 p0_vec: c0中所有词汇占c0的总词汇的概率 p1_vec: c1中所有词汇占c1的总词汇的概率 p1: 文章为类型1的概率，即P(c1) ''' # P(w|c0)*P(c0) = P(w1|c0)*...*P(wn|c0)*P(c0) # 由于下溢出，所以上文取了对数，来做加法 w_p0 = sum(w_vec * p0_vec) + math.log(1 - p1) w_p1 = sum(w_vec * p1_vec) + math.log(p1) if w_p0 &gt; w_p1: return 0 return 1 训练数据 1p0_vec, p1_vec, p1 = bys.train_nb0(train_mat, train_class) 测试数据 一次执行 12345678910111213141516def go_bayes_email(): # 此处省略上文的部分内容 # 训练数据 p0_vec, p1_vec, p1 = bys.train_nb0(train_mat, train_class) # 测试数据 error_count = 0 for i in test_indices: vec = bys.get_doc_vec(email_list[i], vocab_list) res = bys.classify_nb(vec, p0_vec, p1_vec, p1) if res != class_list[i]: error_count += 1 error_rate = error_count / float(data_num) print 'error=%d, rate=%s, test=%d, all=%d' % (error_count, error_rate, len(test_indices), data_num) return error_rate 多次执行，取平均值 1234567def test_bayes_email(): ''' 执行多次go_bayes_email，计算平均错误率 ''' times = 100 error_rate_sum = 0.0 for i in range(10): error_rate_sum += go_bayes_email() print 'average_rate = %s' % (error_rate_sum / 10) 源代码","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","permalink":"http://plmsmile.github.io/tags/朴素贝叶斯/"}]},{"title":"NumPy","date":"2017-04-15T07:32:52.000Z","path":"2017/04/15/20-numpy/","text":"记录了一些常用的numpy方法，参考自NumPy教程 基础 简单demo NumPy中最重要的对象是ndarray，是一个N维数组。它存储着相同类型的元素集合。通过dtype来获取类型，索引来获取值。 通过numpy.array来创建ndarray。 123456789101112# 1. numpy定义numpy.array(object, dtype = None, copy = True, order = None, subok = False, ndmin = 0)# 2. demoimport numpy as npa = np.array([[1, 2], [3, 4]])# 3. 使用dtype#int8, int16, int32, int64 可替换为等价的字符串 'i1', 'i2', 'i4', 以及其他。dt = np.dtype(np.int32)student = np.dtype([('name','S20'), ('age', 'i1'), ('marks', 'f4')])a = np.array([('tom', 23, 89), ('sara', 22, 97)], dtype=student) ndarray.shape 和reshape 获取数组维度 ，也可以调整大小 123456789a = np.array([[1,2,3], [4,5,6]]) print a.shape# (2, 3)b = a.reshape(3,2) b.shapeprint b[[1, 2] [3, 4] [5, 6]] ndarray.ndim 数组的维数 123import numpy as npa = np.arange(24).reshape(2, 12) # 2b = a.reshape(2, 3, 4) # 3 创建数组 输入数组建立 1a = np.array([[1,2,3], [4,5,6]]) zeros, ones, empty 123456# zeros创建0矩阵np.zeros((3, 4))# ones创建1矩阵np.ones((2, 3, 4), dtype=np.int16)# empty不初始化数组，值随机np.empty((2, 3)) arange, linspace 创建随机数，整数和浮点数，步长 12345678# 创建[0, n-1]的数组np.arange(3)# 创建1-10范围类，3个数np.arange(1, 10, 3)# 取均值步长np.linspace(0, 1.5, 3)# array([ 0. , 0.75, 1.5 ]) 基本操作 基本数学操作 1234567891011121314151617181920212223242526272829303132333435363738394041a = np.array([20, 30, 40, 50])b = np.arrange(4)# 减法c = b - a # 乘法b * 2 # 新建一个矩阵b *= 2 # 直接改变b，不会新建一个矩阵 a += 2 同理# 次方b ** 2# 判断a &lt; 30 # array([ True, False, False, False], dtype=bool)10 * np.sin(a)# 矩阵乘法A = np.array([[1, 1], [0, 1]])B = np.array([[2, 0], [3, 4]])A.dot(B)B.dot(A)np.dot(A, B)# sum, max, mina = np.arange(12).reshape(3, 4)array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])# 所有元素sum, min, maxa.sum()a.max()# 使用axis=0按列, axis=1按行a.sum(axis=0)array([12, 15, 18, 21])a.sum(axis=1)array([ 6, 22, 38])# 通用函数B = np.arange(3)np.exp(B) # 求e的次方np.sqrt(B) # 开方C = np.array([2, -1, 4])np.add(B, C) # 相加 访问元素，index, slice, iterator 12345678910111213141516171819202122232425262728293031# 1. 一维数组a = np.arange(4)**2 # array([0, 1, 4, 9])# 下标访问a[1] # 从0开始 # 1# 切片，同python切片a[1:3] # array([1, 4])# 迭代for i in a: print (i*2) # 2. 多维数组a = np.fromfunction(lambda i, j: i + j, (3, 3), dtype=int) # 对下标进行操作array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])a[1, 2] # 访问到 3# 访问第2列a[0:3, 1] # array([1, 2, 3])a[:, 1] # array([1, 2, 3]) # 第i+1行a[1] # 第2行a[-1] # 最后一行a[1, ...] # 第2行，多维的时候这样写# 第2、3行a[1:3, :]a[1:3]for row in a: print rowfor e in a.flat: print e 切片(start, end, step) 123456789101112131415161718# 1. (start, end, step)a = np.arange(10)s = slice(2, 7, 2) b = a[s]# 2. 1-7, step=3, 不包括7b = a[1:7:3]# 3. 从idx开始向后切，包括idxb = a[2:]# 4. start, end, 不包括endb = a[2:5]# 5. a = np.array([[1,2,3],[3,4,5],[4,5,6]]) a[..., 1] #第2列 [2 4 5]a[1, ...] #第2行 [3 4 5]a[...,2:] #第2列及其剩余元素[[2 3] [4 5] [5 6]] 索引 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 1. 一维时a = np.arange(5)**2 # array([ 0, 1, 4, 9, 16])# 索引1i = np.array([1, 3, 4]) # idx 为1,3,4的元素a[i] # 访问元素 array([ 1, 9, 16])# 索引2j = np.array([ [1, 2], [3, 4]])a[j]array([[ 1, 4], [ 9, 16]])# 2. 二维时x = np.array([[1, 2], [3, 4], [5, 6]]) [[1 2] [3 4] [5 6]]# 索引y = x[ [0, 1, 2], [0, 1, 0]] # [0, 1, 2]是行, [0, 1, 0]是对应行的列[1 4 5]# 3. 没看懂x = np.array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11]]) [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]# 索引 rows = np.array([ [0,0], [3,3] ])cols = np.array([ [0,2], [0,2] ])i = [rows, cols]y = x[i][[ 0 2] [ 9 11]]# 4. 切片+索引x = np.array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11]]) # 切片, 1-3行, 1-2列z = x[1:4,1:3] [[ 4 5] [ 7 8] [10 11]]# 高级索引来切片, 1-3行,1、2列y = x[1:4, [1,2]][[ 4 5] [ 7 8] [10 11]] Shape Manipulation 改变形状 1234567891011121314151617a = np.floor(10*np.random.random((3,4))) # &lt;1的小数*10，取整array([[ 9., 6., 3., 8.], [ 2., 8., 4., 2.], [ 5., 3., 3., 1.]])# 形状a.shape(3, 4)# 打平，返回arraya.ravel()array([ 9., 6., 3., 8., 2., 8., 4., 2., 5., 3., 3., 1.])# reshape 生成新的a.reshape(2, 6) a.reshape(3, -1) # 给定一个，自动计算另外的# resize 改变自己a.resize(4, 3)# 转置a.T 堆积不同的阵列 123456789101112131415161718192021a = np.floor(10*np.random.random((2, 2)))b = np.floor(10*np.random.random((2, 2)))# 垂直堆积 (4, 2)np.vstack((a, b))[[ 1., 7.], [ 9., 8.], [ 9., 0.], [ 8., 6.]]# 水平堆积 (2, 4)np.hstack((a, b))array([[ 1., 7., 9., 0.], [ 9., 8., 8., 6.]])# 特别的，针对一维的列堆积a = np.array((1,2,3))b = np.array((2,3,4))np.column_stack((a,b))[[1, 2], [2, 3], [3, 4]]","tags":[{"name":"Python","slug":"Python","permalink":"http://plmsmile.github.io/tags/Python/"},{"name":"NumPy","slug":"NumPy","permalink":"http://plmsmile.github.io/tags/NumPy/"}]},{"title":"Spark基础编程核心思想介绍","date":"2017-03-25T10:07:35.000Z","path":"2017/03/25/19-spark-programming/","text":"Spark基础编程核心思想介绍。参考自官方文档 总览 Spark程序 有一个驱动程序，会运行用户的主要功能，并且在集群上执行各种并行操作。 RDD RDD是跨集群节点分区的、并且可以并行计算的分布式数据集合。可以通过外部文件系统或者内部集合来创建。可以在内存中持久化一个RDD，并且在并行计算中有效地重用。RDD可以从节点故障中自动恢复。 共享变量 当一组任务在不同的节点上并行运行一个函数时，Spark会为函数中的每个变量发送一个副本到各个任务中去(低效)。有时，变量需要在任务与任务、任务与驱动程序间共享。Spark有两种共享变量。 累加器：将工作节点中的值聚合到驱动程序中 广播变量：在各个节点中cache一个只读变量 SparkContext Spark的主要入口点。使用它可以连接到集群、创建RDD和广播变量。 RDD RDD是Spark中最核心的概念。 这是一个分布式的、容忍错误的、能并行操作的数据集合。 RDD是一个分布式的不可变的对象集合，可以包含任意对象。 每个RDD都会被分为多个分区，这些分区运行在不同的节点上。 Spark会自动把RDD的数据分发到集群上，并且并行化执行相关操作。 记录如何转化、计算数据的指令列表。 Spark中对数据的所有操作都是创建RDD、转化已有RDD、调用RDD操作进行求值。 创建RDD 创建RDD有两种方式：驱动程序内部的集合，外部系统的数据集(如HDFS, HBase等)。 集合 从集合中创建RDD，会把集合中的元素复制去创建一个可以并行执行的分布式数据集。 Spark可以对这些并行集合进行分区，把这些数据切割到多个分区。Spark会为集群的每个分区运行一个Task。一般，我们需要为集群中的每个CPU分配2-4个分区。默认，Spark会根据集群尝试自动设置分区数。但我们也可以手动地设置分区数。(有的代码中也称partition为slice) 123rdd = sc.parallelize([1, 2, 3, 4])rdd.reduce(lambda x, y: x + y) # 求和rdd2 = sc.parallelize(['Spark', 'Hadoop', 'ML', 'Python', 'Data'], 2) # 设置2个分区 外部数据集 Spark可以从本地文件系统、HDFS、Cassandra、HBase、Amazon S3等创建数据。支持Text、SequenceFile和任何其他Hadoop的Input Format。 Spark读取文件textFile的一些说明： 本地文件使用本地路径读取文件时，该文件也得在其它的worker node的相同路径上访问到。可以把文件复制过去或者使用network-mounted的文件共享系统。 支持文件 、文件夹、通配符、压缩文件(.gz)。 可以设置分区数。默认，Spark为文件的每一个块创建一个分区。(HDFS的block是128MB)。可以传递一个更大的值来请求更多的分区。 RDD操作 RDD主要有2个操作。 转化操作：由一个RDD生成一个新的RDD(Dataset)。惰性求值。 行动操作：会对RDD(Dataset)计算出一个结果或者写到外部系统。会触发实际的计算。 Spark会惰性计算这些RDD，只有第一次在一个行动操作中用到时才会真正计算。 一般，Spark会在每次行动操作时重新计算转换RDD。如果想复用，则用persist把RDD持久化缓存下来。可以持久化到内存、到磁盘、在多个节点上进行复制。这样，在下次查询时，集群可以更快地访问。 Spark程序大体步骤如下。 从外部数据创建输入RDD。如textFile 使用转化操作得到新的RDD。如map，filter 对重用的中间结果RDD进行持久化。如persist 使用行动操作来触发一次并行计算。如count, first 123456789# 从外部创建一个rdd。此时并没有把数据加载到内存中。lines只是一个指向文件的指针lines = sc.textFile(\"data.txt\")# 转化。没有进行真实的计算，因为惰性求值lineLengths = lines.map(lambda s: len(s))# 持久化lineLengths.persist()# 行动。Spark把计算分解为一些任务，这些任务在单独的机器上进行运算。# 每个机器只做属于自己map的部分，并且在本地reduce。返一个结果给DriverProgramtotalLength = lineLengths.reduce(lambda a, b: a + b) 传递函数给Spark Spark的API很多都依赖于传递函数来在集群上面运行。有下面3种方式可以使用： Lambda表达式：简单功能。不支持多语句函数、不支持没有返回值的语句。 本地def函数，调用spark。 模块的顶级函数。 代码较多时 1234def my_func(s): words = s.split(\" \") return len(words)len_rdd = sc.textFile(\"word.txt\").map(my_func) 对象方法时 千万不要引用self，这样会把整个对象序列化发送过去。而我们其实只需要一个方法或者属性就可以了，我们可以copy一份局部变量传递过去。 123456789101112131415161718class SearchFunctions(object): def __init__(self, query): self.query = query def is_match(self, s): return self.query in s def get_matches_func_ref(self, rdd): \"\"\"问题: self.is_match引用了整个self \"\"\" return rdd.filter(self.is_match) def get_matches_attr_ref(self, rdd): \"\"\"问题：self.query引用了整个self \"\"\" return rdd.filter(lambda s: self.query in s) def get_matches_no_ref(self, rdd): \"\"\"正确做法：使用局部变量 \"\"\" query = self.query return rdd.filter(lambda s: query in s) 理解闭包 当在集群上面执行代码时，理解变量和方法的范围和生命周期是很重要并且困难的。先看一段代码。 12345678910counter = 0rdd = sc.parallelize(data)# Wrong: Don't do this!!请使用Accumulatordef increment_counter(x): global counter counter += xrdd.foreach(increment_counter)print(\"Counter value: \", counter) 执行job的时候，Spark会把处理RDD的操作分解为多个任务，每个任务会由一个执行器executor执行。执行前，Spark会计算任务的闭包。闭包其实就是一些变量和方法，为了计算RDD，它们对于执行器是可见的。Spark会把闭包序列化并且发送到每一个执行器。 发送给执行器的闭包里的变量其实是一个副本，这些执行器程序却看不到驱动器程序节点的内存中的变量(counter)，只能看到自己的副本。当foreach函数引用counter的时候，它使用的不是驱动器程序中的counter，而是自己的副本。 本地执行时，有时候foreach函数会在和driver同一个JVM里面执行，那么访问的就是最初的counter，也会对其进行修改。 一般，我们可以使用累加器Accumulator，它可以安全地修改一个变量。闭包不应该修改全局变量。如果要进行全局聚合，则应该使用累加器。 在本地模式，rdd.foreach(println)的时候，会打印出所有的RDD。但是在集群模式的时候，执行器会打印出它自己的那一部分，在driver中并没有。如果要在driver中打印，则需要collect().foreach()，但是只适用于数据量小的情况。因为collect会拿出所有的数据。 键值对RDD 详细的知识参见Spark的键值对RDD。 Shuffle操作 shuffle说明 Shuffle是Spark中重新分布数据的机制，因此它在分区之间分组也不同。主要是复制数据到执行器和机器上，这个很复杂而且很耗费。 以reduceByKey为例，一个key的所有value不一定在同一个partition甚至不在同一个machine，但是却需要把这些values放在一起进行计算。单个任务会在单个分区上执行。为了reduceByKey的reduce任务，需要获得所有的数据。Spark执行一个all-to-all操作，会在所有分区上，查找所有key的所有value，然后跨越分区汇总，去执行reduce任务。这就是shuffle。 shuffle后，分区的顺序和分区里的元素是确定的，但是分区里元素的顺序却不是确定的。可以去设置确定顺序。 性能影响 Shuffle涉及到磁盘IO、数据序列化、网络IO。组织data：一系列map任务；shuffle这些data；聚合data：一系列reduce任务。 一些map的结果会写到内存里，当太大时，会以分区排好序，然后写到单个文件里。在reduce端，task会读取相关的有序的block。 Shuffle操作会占用大量的堆内存，在传输data之前或者之后，都会使用内存中的数据结构去组织这些record。也就是说，在map端，会创建这些structures，在reduce端会生成这些structures。在内存中存不下时，就会写到磁盘中。 Shuffle操作会在磁盘上生成大量的中间文件，并且在RDD不再被使用并且被垃圾回收之前，这些文件都将被一直保留。因为lineage(血统,DAG图)要被重新计算的话，就不会再次shuffle了。如果保留RDD的引用或者垃圾回收不频繁，那么Spark会占用大量的磁盘空间。文件目录可由spark.local.dir配置。 我们可以在Spark的配配置指南中配置各种参数。 RDD持久化 介绍 Spark一个重要的特性是可以在操作的时候持久化缓存RDD到内存中。Persist一个RDD后，每个节点都会将这个RDD计算的所有分区存储在内存中，并且会在后续的计算中进行复用。这可以让future actions快很多(一般是10倍)。缓存是迭代算法和快速交互使用的关键工具。 持久化RDD可以使用persist或cache方法。会先进行行动操作计算，然后缓存到各个节点的内存中。Spark的缓存是fault-tolerant的，如果RDD的某些分区丢失了，它会自动使用产生这个RDD的transformation进行重新计算。 类别 出于不同的目的，持久化可以设置不同的级别。例如可以缓存到磁盘，缓存到内存(以序列化对象存储，节省空间)等，然后会复制到其他节点上。可以对persist传递StorageLevel对象进行设置缓存级别，而cache方法默认的是MEMORY_ONLY，下面是几个常用的。 MEMORY_ONLY(default): RDD作为反序列化的Java对象存储在JVM中。如果not fit in memory，那么一些分区就不会存储，并且会在每次使用的时候重新计算。CPU时间快，但耗内存。 MEMORY_ONLY_SER: RDD作为序列化的Java对象存储在JVM中，每个分区一个字节数组。很省内存，可以选择一个快速的序列化器。CPU计算时间多。只是Java和Scala。 MEMORY_AND_DISK: 反序列化的Java对象存在内存中。如果not fit in memory，那么把不适合在磁盘中存放的分区存放在内存中。 MEMORY_AND_DISK_SER: 和MEMORY_ONLY_SER差不多，只是存不下的再存储到磁盘中，而不是再重新计算。只是Java和Scala。 名字 占用空间 CPU时间 在内存 在磁盘 MEMORY_ONLY 高 低 是 否 MEMORY_ONLY_SER 低 高 是 否 MEMORY_AND_DISK 高 中等 部分 部分 MEMORY_AND_DISK_SER 低 高 部分 部分 所有的类别都通过重新计算丢失的数据来保证容错能力。完整的配置见官方RDD持久化。 在Python中，我们会始终序列化要存储的数据，使用的是Pickle，所以不用担心选择serialized level。 在shuffle中，Spark会自动持久化一些中间结果，即使用户没有使用persist。这样是因为，如果一个节点failed，可以避免重新计算整个input。如果要reuse一个RDD的话，推荐使用persist这个RDD。 选择 Spark的不同storage level是为了在CPU和内存的效率之间不同的权衡，按照如下去选择： 如果适合MEMORY_ONLY，那么就这个。CPU效率最高了。RDD的操作速度会很快！ 如果不适合MEMORY_ONLY，则尽量使用MEMORY_ONLY_SER，然后选个快速序列化库。这样更加节省空间，理论上也能够快速访问。 不要溢写到磁盘。只有这两种才溢写到磁盘：计算数据集非常耗费资源；会过滤掉大量的数据。 如果要快速故障恢复，那么使用复制的storage level。虽然有容错能力，但是复制了，却可以直接继续执行任务，而不需要等待重新计算丢失的分区。 移除数据 Spark会自动监视每个节点上的缓存使用情况，并且以LRU最近最少使用的策略把最老的分区从内存中移除。当然也可以使用rdd.unpersist手动移除。 内存策略：移除分区，再次使用的时候，就需要重新计算。 内存和磁盘策略：移除的分区会写入磁盘。 共享变量 一般，把一个函数f传给Spark的操作，f会在远程集群节点上执行。当函数f在节点上执行的时候，会对所有的变量复制一份副本到该节点，然后利用这些副本单独地工作。对这些副本变量的更新修改不会传回驱动程序，只是修改这些副本。如果要在任务之间支持一般读写共享的变量是很低效的。 Spark支持两种共享变量： 广播变量：用来高效地分发较大的只读对象 累加器：用来对信息进行聚合 广播变量 简介 广播变量可以让程序高效地向所有工作节点发送一个较大的只读值，供一个或多个Spark操作共同使用。 例如较大的只读查询表、机器学习中的一个很大的特征向量，使用广播变量就很方便。这会在每台机器上cache这个变量，而不是发送一个副本。 Spark的Action操作由一组stage组成，由分布式的&quot;shuffle&quot;操作隔离。Spark会自动广播每个stage的tasks需要的common data。这种广播的数据，是以序列化格式缓存的，并且会在每个任务运行之前反序列化。 创建广播变量只有下面两种情况有用： 多个stage的task需要相同的数据 以反序列化形式缓存数据很重要 存在的问题： Spark会自动把闭包中引用到的变量发送到工作节点。方便但是低效。 可能在并行操作中使用同一个变量，但是Spark会为每个操作都发送一次这个变量。 有的变量可能很大，为每个任务都发送一次代价很大。后面再用的话，则还要重新发送。 广播变量来解决： 其实就是一个类型为spark.broadcast.BroadCast[T]的变量。 可以在Task中进行访问。 广播变量只会发送到节点一次，只读。 一种高效地类似BitTorrent的通信机制。 使用方法 对于一个类型为T的对象，使用SparkContext.broadcast创建一个BroadCast[T]。要可以序列化 通过value属性访问值 变量作为只读值会发送到各个节点一次，在自己的节点上修改不会影响到其他变量。 累加器 简介 累加器可以把工作节点中的数据聚合到驱动程序中。类似于reduce，但是更简单。常用作对事件进行计数。累加器仅仅通过关联和交换的操作来实现累加。可以有效地支持并行操作。Spark本身支持数值类型的累加器，我们也可以添加新的类型。 用法 在驱动器程序中，调用SparkContext.accumulator(initialValue)创建一个有初始值的累加器。返回值为org.apache.spark.Accumulator[T] Spark的闭包里的执行器代码可以用累加器的+=来累加。 驱动器程序中，调用累加器的value属性来访问累加器的值 工作节点上的任务不能访问累加器的值 例子 累加空行 123456789101112131415file = sc.textFile(\"callsign_file\")# 创建累加器Accumulator[Int]并且赋初值0blank_line_count = sc.accumulator(0)def extract_callsigns(line): \"\"\"提取callsigns\"\"\" global blank_line_count # 访问全局变量 if line == \"\": blank_line_count += 1 # 累加 return line.split(\" \")callsigns = file.flatMap(extract_callsigns)callsigns.saveAsTextFile(output_dir + \"/callsigns\")# 读取累加器的值 由于惰性求值，只有callsigns的action发生后，才能读取到值print \"Blank lines count: %d\" % blank_line_count.value 进行错误计数 1234567891011121314151617181920212223# 创建用来验证呼号的累加器valid_signcount = sc.accumulator(0)invalid_signcount = sc.accumulator(0)def valid_datesign(sign): global valid_signcount, invalid_sign_count if re.match(r\"\\A\\d?[a-zA-Z]&#123;1,2&#125;\\d&#123;1,4&#125;[a-zA-Z]&#123;1, 3&#125;\\Z\", sign): valid_signcount += 1 return True else: invalid_signcount += 1 return False# 对每个呼号的联系次数进行计数valid_signs = callsigns.filter(valid_datesign)contact_count = valid_signs.map(lambda sign: (sign, 1)).reduceByKey(lambda (x, y): x+y)# 强制求值计算计数contact_count.count()if invalid_signcount.value &lt; 0.1 * valid_signcount.value: contact_count.saveAsTextFile(output_dir + \"/contactcount\")else: print \"Too many errors: %d in %d\" % (invalid_signcount.value, valid_signcount.value) 12345678sign_prefixes = sc.broadcast(load_callsign_table())def process_sign_count(sign_count, sign_prefixes): country = lookup_country(sign_count[0], sign_prefixes.value) count = sign_count[1] return (country, count)country_contack_counts =","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"RDD","slug":"RDD","permalink":"http://plmsmile.github.io/tags/RDD/"}]},{"title":"Spark-SQL的简略笔记","date":"2017-03-19T14:34:44.000Z","path":"2017/03/19/18-spark-sql/","text":"几个基本的Spark-SQL概念。没多少东西 基础 概念 Spark SQL是用来处理结构化数据的模块。与基本RDD相比，Spark SQL提供了更多关于数据结构和计算方面的信息(在内部有优化效果)。通常通过SQL和Dataset API来和Spark SQL进行交互。 SQL: 进行SQL查询，从各种结构化数据源(Json, Hive, Parquet)读取数据。返回Dataset/DataFrame。 Dataset: 分布式的数据集合。 DataFrame 是一个组织有列名的Dataset。类似于关系型数据库中的表。 可以使用结构化数据文件、Hive表、外部数据库、RDD等创建。 在Scala和Java中，DataFrame由Rows和Dataset组成。在Scala中，DataFrame只是Dataset[Row]的类型别名。在Java中，用Dataset表示DataFrame 开始 SparkSession SparkSession是Spark所有功能的入口点，用SparkSession.builder()就可以。 1234567from pyspark.sql import SparkSessionspark = SparkSession \\ .builder \\ .appName(\"Python Spark SQL basic example\") \\ .config(\"spark.some.config.option\", \"some-value\") \\ .getOrCreate() DataFrameReader 从外部系统加载数据，返回DataFrame。例如文件系统、键值对等等。通过spark.read来获取Reader。 123456789101112131415# 1. json 键值对df1 = spark.read.json(\"python/test_support/sql/people.json\")df1.dtypes# [('age', 'bigint'), ('name', 'string')]df2 = sc.textFile(\"python/test_support/sql/people.json\")# df1.dtypes 和 df2.dtypes是一样的# 2. text 文本文件 # 每一行就是一个Row，默认的列名是Valuedf = spark.read.text(\"python/test_support/sql/text-test.txt\")df.collect()# [Row(value=u'hello'), Row(value=u'this')]# 3. load# 从数据源中读取数据 创建DataFrames 从RDD、Hive Table、Spark data source、外部文件中都可以创建DataFrames。 通过DataFrameReader，读取外部文件 1234# spark.read返回一个DataFrameReaderdf = spark.read.json(\"examples/src/main/resources/people.json\")df.show()df.dtypes 通过spark.createDataFrame()，读取RDD、List或pandas.DataFrame 12345678910111213141516171819202122232425262728person_list = [('AA', 18), ('PLM', 23)]rdd = sc.parallelize(person_list) # 1. listdf = spark.createDataFrame(person_list) # 没有指定列名，默认为_1 _2df = spark.createDataFrame(person_list, ['name', 'age']) # 指定了列名df.collect() # df.show()#[Row(name='AA', age=18), Row(name='PLM', age=23)]# 2. RDDrdd = sc.parallelize(person_list)df = spark.createDataFrame(rdd, ['name', 'age'])# 3. Rowfrom pyspark.sql import RowPerson = Row('name', 'age') # 指定模板属性persons = rdd.map(lambda r: Person(*r)) # 把每一行变成Persondf = spark.createDataFrame(persons)df.collect()# 4. 指定类型StructTypefrom pyspark.sql.types import *field_name = StructField('name', StringType(), True) # 名，类型，非空field_age = StructField('age', IntegerType, True)person_type = StructType([field_name, field_age])# 通过链式add也可以# person_type = StructType.add(\"name\", StringType(), True).add(...)df = spark.createDataFrame(rdd, person_type) Row Row是DataFrame中的，它可以定义一些属性，这些属性在DataFrame里面可以被访问。比如：row.key(像属性)和row['key'](像dict) 1234567891011121314151617181920from pyspark import Row# 1. 创建一个模板Person = Row('name', 'age') # &lt;Row(name, age)&gt;'name' in Person # True，有这个属性'sex' in Person # False# 2. 以Person为模板，创建alice, plmalice = Person('Alice', 22) # Row(name='Alice', age=22)plm = Person('PLM', 23)# 访问属性name, age = alice['name'], alice['age']# 返回dictplm.asDict()# &#123;'age': 23, 'name': 'PLM'&#125;# 3. 多个person创建一个DataFramep_list = [alice, plm]p_df = spark.createDataFrame(p_list)p_df.collect()# [Row(name=u'Alice', age=22), Row(name=u'PLM', age=23)] DataFrame的操作 在2.0中，DataFrames只是Scala和Java API中的Rows数据集。它的操作称为非类型转换，与带有强类型Scala和Java数据集的类型转换相反。 Python tips: df.age和df['age']都可以使用，前者在命令行里面方便，但是建议使用后者。 12345df.printSchema()df.select(\"name\").show()df.select(df['name'], df['age'] + 1).show()df.filter(df['age'] &gt; 21).show()df.groupBy(\"age\").count().show() DataFrame的Python Api，DataFrame的函数API 编程方式运行SQL 通过spark.sql()执行，返回一个DataFrame 12345678910111213# 通过df创建一个本地临时视图，与创建这个df的SparkSession同生命周期df.createOrReplaceTempview(\"people\")sqlDF = spark.sql(\"SELECT * FROM people\")sqlDF.show()# 展示为TablesqlDf.collect()# [Row(age=None, name=u'Michael'), Row(age=30, name=u'Andy'), Row(age=19, name=u'Justin')]# 全局临时视图# 在所有session中共享，直到spark application停止df.createGlobalTempView(\"people\")spark.sql(\"SELECT * FROM global_temp.people\").show()spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"Spark SQL","slug":"Spark-SQL","permalink":"http://plmsmile.github.io/tags/Spark-SQL/"}]},{"title":"旧版博客搭建过程及其问题","date":"2017-03-13T14:52:41.000Z","path":"2017/03/13/13-old-blog-problems/","text":"一直都想搭建一个博客，今天终于把博客给初步搭建好了。搭建的过程其实不那么顺利，所以简答记录一下。 搭建过程 根据手把手搭建博客教程这篇文章来进行搭建，其中目前只看了它的第一页 后期在hexo主题里面选择了even主题 依照even-wiki来添加标签、分类和about页面 修改主题目录下的_config.yml的menu，把home、tags等手动改成中文了 遇到的坑 没有看wiki，自己去谷歌建立tags、categories等页面 建立好tags，却在tags页面没有看到相应的标签。是因为没有为tags/index.md设置layout为tags 中文语言，在站点目录下的_config.yml中设置language: zh-cn。 博客重新搭建 配置及源文件 因为经常重装系统，所以博客也需要重新恢复。先配置git相关信息 1234567git config --global user.name \"plmsmile\"git config --global user.email \"pulimingspark@163.com\"ssh-keygen -t rsa -C \"pulimingspark@163.com\"# 去GitHub上添加sshkeycat ~/.ssh/id_rsa.pub# 完成后，进行测试ssh -T git@github.com 然后把之前的PLMBlogs拷贝到D盘，一般目录是d/PLMBlogs 1234567cd PLMBlogs# 安装hexonpm install hexo-cli -g# 安装插件npm install hexo-deployer-git --save# 安装依赖npm install 数学公式渲染 执行完上面的操作后，执行如下 1234hexo cleanhexo generate # 这一步会报错 hexo serverhexo deploy 错误信息如下 123456789101112131415161718192021222324252627282930313233FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: write EPIPE at exports._errnoException (util.js:1020:11) at Socket._writeGeneric (net.js:711:26) at Socket._write (net.js:730:8) at doWrite (_stream_writable.js:331:12) at writeOrBuffer (_stream_writable.js:317:5) at Socket.Writable.write (_stream_writable.js:243:11) at Socket.write (net.js:657:40) at Hexo.pandocRenderer (D:\\PLMBlogs\\node_modules\\hexo-renderer-pandoc\\index.js:64:15) at Hexo.tryCatcher (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\util.js:16:23) at Hexo.ret (eval at makeNodePromisifiedEval (C:\\Users\\PLM\\AppData\\Roaming\\npm\\node_modules\\hexo-cli\\node_modules\\bluebird\\js\\release\\promisify.js:184:12), &lt;anonymous&gt;:13:39) at D:\\PLMBlogs\\node_modules\\hexo\\lib\\hexo\\render.js:61:21 at tryCatcher (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\util.js:16:23) at Promise._settlePromiseFromHandler (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\promise.js:512:31) at Promise._settlePromise (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\promise.js:569:18) at Promise._settlePromiseCtx (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\promise.js:606:10) at Async._drainQueue (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\async.js:138:12) at Async._drainQueues (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\async.js:143:10) at Immediate.Async.drainQueues (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\async.js:17:14) at runCallback (timers.js:672:20) at tryOnImmediate (timers.js:645:5) at processImmediate [as _immediateCallback] (timers.js:617:5)events.js:160 throw er; // Unhandled 'error' event ^Error: spawn pandoc ENOENT at exports._errnoException (util.js:1020:11) at Process.ChildProcess._handle.onexit (internal/child_process.js:193:32) at onErrorNT (internal/child_process.js:367:16) at _combinedTickCallback (internal/process/next_tick.js:80:11) at process._tickCallback (internal/process/next_tick.js:104:9) 原因是有大量的数学公式，所以需要对网页进行渲染。 一般是使用pandoc进行渲染，先去官网下载，然后下一步安装即可。最后，执行下面的命令，安装就好了。 123npm install hexo-renderer-pandoc --save# 再次应该就不会报错了hexo generate 配置git 12345678git config --global user.name plmsmilegit config --global user.email plmspark@163.com# 生成Key，一路回车ssh-keygen -t rsa -C \"plmspark@163.com\" cat ~/.ssh/id_rsa.pub# 到github上添加该sshkey# 测试ssh -T git@github.com 潜在问题 本站没有搜索功能，安装插件失败了 tags页面，标签数量错误 期望 认真学习 好好做笔记 要更新博客 自己常来看看之前的知识点","tags":[{"name":"hexo搭建","slug":"hexo搭建","permalink":"http://plmsmile.github.io/tags/hexo搭建/"},{"name":"配置Git","slug":"配置Git","permalink":"http://plmsmile.github.io/tags/配置Git/"}]},{"title":"Spark键值对RDD的常用API","date":"2017-03-13T11:37:06.000Z","path":"2017/03/13/17-spark-pairrdd/","text":"Spark-KV-RDD的API使用 PairRDD及其创建 键值对RDD称为PairRDD，通常用来做聚合计算。Spark为Pair RDD提供了许多专有的操作。 1234# 创建pair rdd: map 或者 读取键值对格式自动转成pairrdd# 每行的第一个单词作为key，line作为valuepairs = lines.map(lambda line: (line.split(' ')[0], line)) 转化操作 Pair RDD 的转化操作分为单个和多个RDD的转化操作。 单个Pair RDD转化 reduceByKey(func) 合并含有相同键的值，也称作聚合 123456from operator import addrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])sorted(rdd.reduceByKey(add).collect())# [('a', 2), ('b', 1)]# 这种写法也可以rdd.reduceByKey(lambda x, y: x+y).collect() groupByKey 对具有相同键的值进行分组。会生成hash分区的RDD 12345rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])sorted(rdd.groupByKey().mapValues(len).collect())# [('a', 2), ('b', 1)]sorted(rdd.groupByKey().mapValues(list).collect())[('a', [1, 1]), ('b', [1])] 说明：如果对键进行分组以便对每个键进行聚合（如sum和average），则用reduceByKey和aggregateByKey性能更好 combineByKey 合并具有相同键的值，但是返回不同类型 (K, V) - (K, C)。最常用的聚合操作。 1234x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])def add(a, b): return a + str(b)sorted(x.combineByKey(str, add, add).collect())[('a', '11'), ('b', '1')] 下面是combineByKey的源码和参数说明 123456789101112131415161718def combineByKey[C]( createCombiner: V =&gt; C, // V =&gt; C的转变 / 初始值 / 创建one-element的list mergeValue: (C, V) =&gt; C, // 将原V累加到新的C mergeCombiners: (C, C) =&gt; C, // 两个C合并成一个 partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null): RDD[(K, C)] = &#123; //实现略 &#125;// 求平均值val scores = sc.parallelize( List((\"chinese\", 88.0) , (\"chinese\", 90.5) , (\"math\", 60.0), (\"math\", 87.0)))val avg = scores.combineByKey( (v) =&gt; (v, 1), (acc: (Float, Int), V) =&gt; (acc._1 + v, acc._2 + 1), (acc1: (Float, Int), acc2: (Float, Int) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2))).map&#123;case (key, value) =&gt; (key, value._1 / value._2.toFloat)&#125; 1234567891011# 求平均值nums = sc.parallelize([('c', 90), ('m', 95), ('c', 80)])sum_count = nums.combineByKey( lambda x: (x, 1), lambda acc, x: (acc[0] + x, acc[1] + 1), lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))# [('c', (170, 2)), ('m', (95, 1))]avg_map = sum_count.mapValues(lambda (sum, count): sum/count).collectAsMap()# &#123;'c': 85, 'm': 95&#125;avg_map = sum_count.map(lambda key, s_c: (key, s_c[0]/s_c[1])).collectAsMap() mapValues(f) 对每个pair RDD中的每个Value应用一个func，不改变Key。其实也是对value做map操作。一般我们只想访问pair的值的时候，可以用mapValues。类似于map{case (x, y): (x, func(y))} 1234x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])def f(x): return len(x)x.mapValues(f).collect()[('a', 3), ('b', 1)] mapPartitions(f) 是map的一个变种，都需要传入一个函数f，去处理数据。不同点如下： map: f应用于每一个元素。 mapPartitions: f应用于每一个分区。分区的内容以Iterator[T]传入f，f的输出结果是Iterator[U]。最终RDD的由所有分区经过输入函数处理后的结果得到的。 优点：我们可以为每一个分区做一些初始化操作，而不用为每一个元素做初始化。例如，初始化数据库，次数n。map时：n=元素数量，mapPartitions时：n=分区数量。 1234567891011121314151617181920212223# 1. 每个元素加1def f(iterator): print (\"called f\") return map(lambda x: x + 1, iterator)rdd = sc.parallelize([1, 2, 3, 4, 5], 2)rdd.mapPartitions(f).collect() # 只调用2次f\"\"\"called fcalled f[2, 3, 4, 5, 6]\"\"\"# 2. 分区求和rdd = sc.parallelize([1, 2, 3, 4, 5, 6], 2)def f(iterator): print \"call f\" yield sum(iterator)rdd.mapPartitions(f).collect() # 调用2次f，分区求和\"\"\"call fcall f[6, 15]\"\"\" mapPartitionsWithIndex(f) 和mapPartitions一样，只是多了个partition的index。 12345678910111213rdd = sc.parallelize([\"yellow\",\"red\",\"blue\",\"cyan\",\"black\"], 3)def g(index, item): return \"id-&#123;&#125;, &#123;&#125;\".format(index, item)def f(index, iterator): print 'called f' return map(lambda x: g(index, x), iterator)rdd.mapPartitionsWithIndex(f).collect()\"\"\"called fcalled fcalled f['id-0, yellow', 'id-1, red', 'id-1, blue', 'id-2, cyan', 'id-2, black']\"\"\" repartition(n) 生成新的RDD，分区数目为n。会增加或者减少 RDD的并行度。会对分布式数据集进行shuffle操作，效率低。如果只是想减少分区数，则使用coalesce，不会进行shuffle操作。 1234567&gt;&gt;&gt; rdd = sc.parallelize([1,2,3,4,5,6,7], 4)&gt;&gt;&gt; sorted(rdd.glom().collect())[[1], [2, 3], [4, 5], [6, 7]]&gt;&gt;&gt; len(rdd.repartition(2).glom().collect())2&gt;&gt;&gt; len(rdd.repartition(10).glom().collect())10 coalesce(n) 合并，减少分区数，默认不执行shuffle操作。 1234sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()# [[1], [2, 3], [4, 5]]sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()# [[1, 2, 3, 4, 5]] flatMapValues(f) 打平values，[(&quot;k&quot;, [&quot;v1&quot;, &quot;v2&quot;])] -- [(&quot;k&quot;,&quot;v1&quot;), (&quot;k&quot;, &quot;v2&quot;)] 1234x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])def f(x): return xx.flatMapValues(f).collect()# [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')] keys values sortByKey 返回一个对键进行排序的RDD。会生成范围分区的RDD 123456789101112tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]sc.parallelize(tmp).sortByKey().first()# ('1', 3)sc.parallelize(tmp).sortByKey(True, 1).collect()# [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]sc.parallelize(tmp).sortByKey(True, 2).collect()# [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()# [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)] 两个Pair RDD转化 substract 留下在x中却不在y中的元素 1234x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])y = sc.parallelize([(\"a\", 3), (\"c\", None)])sorted(x.subtract(y).collect())#[('b', 4), ('b', 5)] substractByKey 删掉X中在Y中也存在的Key所包含的所有元素 join 内连接，从x中去和y中一个一个的匹配，(k, v1), (k, v2) -- (k, (v1, v2)) 1234x = sc.parallelize([(\"a\", 1), (\"b\", 4)])y = sc.parallelize([(\"a\", 2), (\"a\", 3)])sorted(x.join(y).collect())# [('a', (1, 2)), ('a', (1, 3))] leftOuterJoin 左边RDD的键都有，右边没有的配None 1234x = sc.parallelize([('a', 1), ('b', 4)])y = sc.parallelize([('a', 2)])sorted(x.leftOuterJoin(y).collect())# [('a', (1, 2)), ('b', (4, None))] rightOuterJoin 右边RDD的键都有，左边没有的配None 123456x = sc.parallelize([('a', 1), ('b', 4)])y = sc.parallelize([('a', 2)])sorted(x.rightOuterJoin(y).collect())# [('a', (1, 2))]sorted(y.rightOuterJoin(x).collect())# [('a', (2, 1)), ('b', (None, 4))] cogroup 将两个RDD中拥有相同键的value分组到一起，即使两个RDD的V不一样 123456x = sc.parallelize([('a', 1), ('b', 4)])y = sc.parallelize([('a', 2)])x.cogroup(y).collect()# 上面显示的是16进制[(x, tuple(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]# [('a', ([1], [2])), ('b', ([4], []))] 行动操作 countByKey 对每个键对应的元素分别计数 123rdd = sc.parallelize([('a', 1), ('b', 1), ('a', 1)])rdd.countByKey().items() # 转换成一个dict，再取所有元素# [('a', 2), ('b', 1)] collectAsMap 返回一个map 123m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()m[1] - 2 # 当做map操作即可m[3] - 4 lookup(key) 返回键的RDD中的值列表。如果RDD具有已知的分区器，则通过仅搜索key映射到的分区来高效地完成该操作。 12345678910l = range(1000) # 1,2,3,...,1000rdd = sc.parallelize(zip(l, l), 10) # 键和值一样，10个数据分片，10个并行度，10个taskrdd.lookup(42) # slow# [42]sorted_rdd = rdd.sortByKey()sorted_rdd.lookup(42) # fast# [42]rdd = sc.parallelize([('a', 'a1'), ('a', 'a2'), ('b', 'b1')])rdd.lookup('a')[0]# 'a1' 聚合操作 当数据是键值对组织的时候，聚合具有相同键的元素是很常见的操作。基础RDD有fold(), combine(), reduce()，Pair RDD有combineByKey()最常用,reduceByKey(), foldByKey()等。 计算均值 1234567891011121314151617181920## 方法一：mapValues和reduceByKeyrdd = sc.parallelize([('a', 1), ('a', 3), ('b', 4)])maprdd = rdd.mapValues(lambda x : (x, 1))# [('a', (1, 1)), ('a', (3, 1)), ('b', (4, 1))]reducerdd = maprdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))# [('a', (4, 2)), ('b', (4, 1))]reducerdd.mapValues(lambda x : x[0]/x[1]).collect()# [('a', 2), ('b', 4)] ## 方法二 combineByKey 最常用的nums = sc.parallelize([('c', 90), ('m', 95), ('c', 80)])sum_count = nums.combineByKey( lambda x: (x, 1), lambda acc, x: (acc[0] + x, acc[1] + 1), lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))# [('c', (170, 2)), ('m', (95, 1))]avg_map = sum_count.mapValues(lambda (sum, count): sum/count).collectAsMap()# &#123;'c': 85, 'm': 95&#125;avg_map = sum_count.map(lambda key, s_c: (key, s_c[0]/s_c[1])).collectAsMap() 统计单词计数 1234567rdd = sc.textFile('README.md')words = rdd.flatMap(lambda x: x.split(' '))# 568个result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)# 289result.top(2)# [('your', 1), ('you', 4)] 数据分区 分区说明 在分布式程序中，通信的代价是很大的。因此控制数据分布以获得最少的网络传输可以极大地提升整体性能。Spark程序通过控制RDD分区方式来减少通信开销。使用partitionBy进行分区 不需分区：给定RDD只需要被扫描一次 需要分区：数据集在多次基于键的操作中使用，比如连接操作。partitionBy是转化操作，生成新的RDD，为了多次计算，一般要进行持久化persist Spark中所有的键值对RDD都可以进行分区。系统会根据一个针对键的函数对元素进行分组。Spark不能显示控制具体每个键落在哪一个工作节点上，但是Spark可以确保同一组的键出现在同一个节点上。 Hash分区：将一个RDD分成了100个分区，hashcode(key)%100 相同的，会在同一个节点上面 范围分区：将key在同一个范围区间内的记录放在同一个节点上 一个简单的例子，内存中有一张很大的用户信息表 -- 即(UserId, UserInfo)组成的RDD，UserInfo包含用户订阅了的所有Topics。还有一张(UserId, LinkInfo)存放着过去5分钟用户浏览的Topic。现在要找出用户浏览了但是没有订阅的Topic数量。 123456789101112val sc = new SparkContext(...)val userData = sc.sequenceFile[UserId, UserInfo](\"hdfs://...\").persist()def processNewLog(logFileName: String) &#123; val events = sc.sequenceFile[UserId, LinkInfo](logFileName) val joined = userData.join(events) // (UserId, (UserInfo, LinkInfo)) val offTopicVisits = joined.filter&#123; case (UserId, (UserInfo, LinkInfo)) =&gt; !UserInfo.topics.contains(LinkInfo.topic) &#125;.count() print (\"浏览了且未订阅的数量：\" + offTopicVisits)&#125; 这段代码不够高效。因为每次调用processNewLog都会用join操作，但我们却不知道数据集是如何分区的。 连接操作，会将两个数据集的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在机器上对所有键相同的记录进行操作。 因为userData比events要大的多并且基本不会变化，所以有很多浪费效率的事情：每次调用时都对userData表进行计算hash值计算和跨节点数据混洗。 解决方案：在程序开始的时候，对userData表使用partitionBy()转换操作，将这张表转换为哈希分区 123val userData = sc.sequenceFile[UserId, UserInfo](\"hdfs://...\") .partitionBy(new HashPartioner(100)) // 构造100个分区 .persist() // 持久化当前结果 events是本地变量，并且只使用一次，所以为它指定分区方式没有什么用处。 userData使用了partitionBy()，Spark就知道该RDD是根据键的hash值来分区的。在userData.join(events)时，Spark只会对events进行数据混洗操作。将events中特定UserId的记录发送到userData的对应分区所在的那台机器上。需要网络传输的数据就大大减少了，速度也就显著提升了。 分区相关的操作 Spark的许多操作都有将数据根据跨节点进行混洗的过程。所有这些操作都会从数据分区中获益。类似join这样的二元操作，预先进行数据分区会导致其中至少一个RDD不发生数据混洗。 获取好处的操作：cogroup, groupWith, join, leftOuterJoin , rightOuterJoin, groupByKey, reduceByKey , combineByKey, lookup 为结果设好分区的操作：cogroup, groupWith, join, leftOuterJoin , rightOuterJoin, groupByKey, reduceByKey , combineByKey, partitionBy, sort, （mapValues, flatMapValues, filter 如果父RDD有分区方式的话） 其他所有的操作的结果都不会存在特定的分区方式。对于二元操作，输出数据的分区方式取决于父RDD的分区方式。默认情况结果会采取hash分区。 PageRank PageRank的python版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\" PageRank算法author = PuLiming运行: bin/spark-submit files/pagerank.py data/mllib/pagerank_data.txt 10\"\"\"from __future__ import print_functionimport reimport sysfrom operator import addfrom pyspark import SparkConf, SparkContextdef compute_contribs(urls, rank): \"\"\" 给urls计算 Args: urls: 目标url相邻的urls集合 rank: 目标url的当前rank Returns: url: 相邻urls中的一个url rank: 当前url的新的rank \"\"\" num_urls = len(urls) for url in urls: yield (url, rank / num_urls)def split_url(url_line): \"\"\" 把一行url切分开来 Args: url_line: 一行url，如 1 2 Returns: url, neighbor_url \"\"\" parts = re.split(r'\\s+', url_line) # 正则 return parts[0], parts[1]def compute_pagerank(sc, url_data_file, iterations): \"\"\" 计算各个page的排名 Args: sc: SparkContext url_data_file: 测试数据文件 iterations: 迭代次数 Returns: status: 成功就返回0 \"\"\" # 读取url文件 ['1 2', '1 3', '2 1', '3 1'] lines = sc.textFile(url_data_file).map(lambda line: line.encode('utf8')) # 建立Pair RDD (url, neighbor_urls) [(1,[2,3]), (2,[1]), (3, [1])] links = lines.map(lambda line : split_url(line)).distinct().groupByKey().mapValues(lambda x: list(x)).cache() # 初始化所有url的rank为1 [(1, 1), (2, 1), (3, 1)] ranks = lines.map(lambda line : (line[0], 1)) for i in range(iterations): # (url, [(neighbor_urls), rank]) join neighbor_urls and rank # 把当前url的rank分别contribute到其他相邻的url (url, rank) contribs = links.join(ranks).flatMap( lambda url_urls_rank: compute_contribs(url_urls_rank[1][0], url_urls_rank[1][1]) ) # 把url的所有rank加起来，再赋值新的 ranks = contribs.reduceByKey(add).mapValues(lambda rank : rank * 0.85 + 0.15) for (link, rank) in ranks.collect(): print(\"%s has rank %s.\" % (link, rank)) return 0if __name__ == '__main__': if len(sys.argv) != 3: print(\"Usage: python pagerank.py &lt;data.txt&gt; &lt;iterations&gt;\", file = sys.stderr) sys.exit(-1) # 数据文件和迭代次数 url_data_file = sys.argv[1] iterations = int(sys.argv[2]) # 配置 SparkContext conf = SparkConf().setAppName('PythonPageRank') conf.setMaster('local') sc = SparkContext(conf=conf) ret = compute_pagerank(sc, url_data_file, iterations) sys.exit(ret) PageRank的scala版本 12345678910111213141516val sc = new SparkContext(...)val links = sc.objectFile[(String, Seq[String])](\"links\") .partitionBy(new HashPartitioner(100)) .persist()var ranks = links.mapValues(_ =&gt; 1.0)// 迭代10次for (i &lt;- 0 until 10) &#123; val contributions = links.join(ranks).flatMap &#123; case (pageId, (links, rank)) =&gt; links.map(dest =&gt; (dest, rank / links.size)) &#125; ranks = contributions.reduceByKey(_ + _).mapValues(0.15 + 0.85* _)&#125;ranks.saveAsTextFile(\"ranks\") 当前scala版本的PageRank算法的优点： links每次都会和ranks发生连接操作，所以一开始就对它进行分区partitionBy，就不会通过网络进行数据混洗了，节约了相当可观的网络通信开销 对links进行persist，留在内存中，每次迭代使用 第一次创建ranks，使用mapValues保留了父RDD的分区方式，第一次连接开销就会很小 reduceByKey后已经是分区了，再使用mapValues分区方式，再次和links进行join就会更加高效 所以对分区后的RDD尽量使用mapValues保留父分区方式，而不要用map丢失分区方式。","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"PairRDD","slug":"PairRDD","permalink":"http://plmsmile.github.io/tags/PairRDD/"}]},{"title":"Spark基础RDD的常用API","date":"2017-03-13T11:26:18.000Z","path":"2017/03/13/16-Spark-BaseRDD/","text":"Spark基础RDD的常用API总结 RDD基础 RDD是Spark中的核心抽象——弹性分布式数据集(Resilient Distributed Dataset)。其实RDD是分布式的元素集合，是一个不可变的分布式对象集合。每个RDD都会被分为多个分区，这些分区运行在不同的节点上。RDD可以包含任意对象。Spark会自动将这些RDD的数据分发到集群上，并将操作并行化执行。 RDD当做我们通过转化操作构建出来的、记录如何计算数据的指令列表。 对数据的所有操作都是创建RDD、转化已有RDD、调用RDD操作进行求值。 RDD主要有2个操作。 转化操作：由一个RDD生成一个新的RDD。惰性求值。 行动操作：会对RDD计算出一个结果或者写到外部系统。会触发实际的计算。 Spark会惰性计算这些RDD，只有第一次在一个行动操作中用到时才会真正计算。 Spark的RDD会在每次行动操作时重新计算。如果想复用，则用persist把RDD持久化缓存下来。 下面是总的大体步骤 从外部数据创建输入RDD。如textFile 使用转化操作得到新的RDD。如map，filter 对重用的中间结果RDD进行持久化。如persist 使用行动操作来触发一次并行计算。如count, first 创建RDD 创建RDD主要有两个方法：读取集合，读取外部数据。 1234# 读取集合words = sc.parallelize([\"hello\", \"spark\", \"good\", \"study\"])# 读取外部数据lines = sc.textFile(\"README.md\") 转化操作 RDD的转化操作会返回新的RDD，是惰性求值的。即只有真正调用这些RDD的行动操作这些RDD才会被计算。许多转化操作是针对各个元素的，即每次只会操作RDD中的一个元素。 通过转化操作，会从已有RDD派生出新的RDD。Spark会使用谱系图(lineage graph)来记录这些不同RDD之间的依赖关系。Spark会利用这些关系按需计算每个RDD，或者恢复所丢失的数据。 最常用的转化操作是map()和filter()。下面说明一下常用的转化操作。 map(f) 对每个元素使用func函数，将返回值构成新的RDD。不会保留父RDD的分区。 1234567rdd = sc.parallelize([\"b\", \"a\", \"c\"])rddnew = rdd.map(lambda x: (x, 1))# [('b', 1), ('a', 1), ('c', 1)]# 可使用sorted()进行排序sorted(rddnew.collect())# [('a', 1), ('b', 1), ('c', 1)] flatMap(f) 对每个元素使用func函数，然后展平结果。通常用来切分单词 1234567lines = sc.textFile(\"README.md\")# 104个words = lines.flatMap(lambda line : line.split(\" \"))# 568个rdd = sc.parallelize([2, 3, 4])rdd2 = rdd.flatMap(lambda x: range(1, x)) # range(1, x) 生成1-x的数，不包括x# [1, 1, 2, 1, 2, 3] filter(f) 元素满足f函数，则放到新的RDD里 123rdd = sc.parallelize([1, 2, 3, 4, 5])rdd.filter(lambda x: x % 2 == 0).collect()# [2, 4] distinct 去重。开销很大，会进行数据混洗。 12sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())# [1, 2, 3] union 合并两个RDD。会包含重复的元素 intersection 求两个RDD的共同的元素。会去掉重复的元素 subtract 留下在自己却不在other里面的元素 cartesian 两个RDD的笛卡尔积 行动操作 行动操作会把最终求得的结果返回到驱动程序，或者写入外部存储系统中。 collect 返回RDD中的所有元素。只适用于数据小的情况，因为会把所有数据加载到驱动程序的内存中。通常只在单元测试中使用 count RDD中元素的个数 countByValue 各元素在RDD中出现的次数，返回一个dictionary。在pair RDD中有countByKey方法 12sc.parallelize([1, 2, 1, 2, 2]).countByValue().items()# [(1, 2), (2, 3)] take(num) 返回RDD中的n个元素。它会首先扫描一个分区，在这个分区里面尽量满足n个元素，不够再去查别的分区。只能用于数据量小的情况下。得到的顺序可能和你预期的不一样 takeOrdered(num, key=None) 获取n个元素，按照升序或者按照传入的key function。只适用于数据小的RDD 1234sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)# [1, 2, 3, 4, 5, 6]sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)# [10, 9, 7, 6, 5, 4] top(num, key=None) 从RDD只获取前N个元素。降序排列。只适用于数据量小的RDD 1234sc.parallelize([2, 3, 4, 5, 6]).top(2)# [6, 5]sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)# [4, 3, 2] reduce(f) 并行整合RDD中的所有数据，得到一个结果。接收一个f函数。目前在本地reduce partitions。返回结果类型不变。 1234from operator import addsc.parallelize([1, 2, 3, 4, 5]).reduce(add)sc.parallelize([1, 2, 3, 4, 5]).reduce(lambda x, y: x+y)# 15 fold(zeroValue, op) 和reduce一样，但是需要提供初始值。op(t1, t2)，t1可以变，t2不能变 123from operator import addsc.parallelize([1, 2, 3, 4, 5]).fold(0, add)# 15 aggregate(zeroValue, seqOp, combOp) 聚合所有分区的元素，然后得到一个结果。和reduce相似，但是通常返回不同的类型。 1234seqOp = (lambda x, y: (x[0] + y, x[1] + 1)) # 累加combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1])) # combine多个sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)# (10, 4) foreach(f) 对rdd中的每个元素使用f函数 123def f(x): print (x)sc.parallelize([1, 2, 3, 4]).foreach(f) glom 将分区中的元素合并到一个list 123rdd = sc.parallelize([1, 2, 3, 4, 5], 2)rdd.glom().collect()# [[1, 2], [3, 4, 5]]","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"RDD","slug":"RDD","permalink":"http://plmsmile.github.io/tags/RDD/"}]}]
<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>SWE 总结索引 | 📚 plmblog</title>
    <meta name="description" content="记录一些学习笔记。">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.CuIOXX7t.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.CpFa0R23.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.DrNSUXfy.js">
    <link rel="modulepreload" href="/assets/chunks/framework.CvbyeFFO.js">
    <link rel="modulepreload" href="/assets/posts_llm_industry_codellm_10-swe-summary.md.CmzPFSW7.lean.js">
    <link rel="icon" href="/plm.png">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-cecb633e><!--[--><!--]--><!--[--><span tabindex="-1" data-v-c979f278></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-c979f278>Skip to content</a><!--]--><!----><header class="VPNav" data-v-cecb633e data-v-0ad68676><div class="VPNavBar" data-v-0ad68676 data-v-ce71e4ef><div class="wrapper" data-v-ce71e4ef><div class="container" data-v-ce71e4ef><div class="title" data-v-ce71e4ef><div class="VPNavBarTitle has-sidebar" data-v-ce71e4ef data-v-7e906684><a class="title" href="/" data-v-7e906684><!--[--><!--]--><!----><span data-v-7e906684>📚 plmblog</span><!--[--><!--]--></a></div></div><div class="content" data-v-ce71e4ef><div class="content-body" data-v-ce71e4ef><!--[--><!--]--><div class="VPNavBarSearch search" data-v-ce71e4ef><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-ce71e4ef data-v-e0cd9371><span id="main-nav-aria-label" class="visually-hidden" data-v-e0cd9371> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-e0cd9371 data-v-4aa19863><!--[--><span data-v-4aa19863>首页</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e0cd9371 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-ffaaba87><span class="text" data-v-ffaaba87><!----><span data-v-ffaaba87>🐲LLM</span><span class="vpi-chevron-down text-icon" data-v-ffaaba87></span></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><div class="items" data-v-798b97ca><!--[--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>Basic</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/basic/01-lm-define-information-theory.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🦋基础知识</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/infra/01-parrallel.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🛠基建框架</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>强化学习</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/rl/theory/01-reinforce-learning.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🎓RL理论基础</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/agent/rl/02-agent-tool.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🚄Agent-RL</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>行业方向</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/industry/mainllm/01-kimi-series.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🚀主流模型</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/industry/codellm/01-survey.html" data-v-dc987abe><!--[--><span data-v-dc987abe>💻代码模型</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>Agent</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/agent/basic.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🤖概念及应用</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e0cd9371 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-ffaaba87><span class="text" data-v-ffaaba87><!----><span data-v-ffaaba87>📙旧文章</span><span class="vpi-chevron-down text-icon" data-v-ffaaba87></span></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><div class="items" data-v-798b97ca><!--[--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍓NLP</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/nlp.html" data-v-dc987abe><!--[--><span data-v-dc987abe>自然语言处理</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍑基础知识</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/dl.html" data-v-dc987abe><!--[--><span data-v-dc987abe>深度学习</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/rl.html" data-v-dc987abe><!--[--><span data-v-dc987abe>强化学习</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/ml.html" data-v-dc987abe><!--[--><span data-v-dc987abe>机器学习</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍎算法</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/algo/" data-v-dc987abe><!--[--><span data-v-dc987abe>算法题</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/bigdata/" data-v-dc987abe><!--[--><span data-v-dc987abe>大数据</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍒其他</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/env/" data-v-dc987abe><!--[--><span data-v-dc987abe>环境搭建</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/other/" data-v-dc987abe><!--[--><span data-v-dc987abe>其他</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e0cd9371 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-ffaaba87><span class="text" data-v-ffaaba87><!----><span data-v-ffaaba87>经验</span><span class="vpi-chevron-down text-icon" data-v-ffaaba87></span></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><div class="items" data-v-798b97ca><!--[--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>环境</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/exps/env/01-blog-env.html" data-v-dc987abe><!--[--><span data-v-dc987abe>环境搭建</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>心得</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/exps/mind.html" data-v-dc987abe><!--[--><span data-v-dc987abe>心得体会</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/posts/archive.html" tabindex="0" data-v-e0cd9371 data-v-4aa19863><!--[--><span data-v-4aa19863>归档</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/posts/me.html" tabindex="0" data-v-e0cd9371 data-v-4aa19863><!--[--><span data-v-4aa19863>关于我</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-ce71e4ef data-v-b59ebfac><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-b59ebfac data-v-809b7594 data-v-3591f5e2><span class="check" data-v-3591f5e2><span class="icon" data-v-3591f5e2><!--[--><span class="vpi-sun sun" data-v-809b7594></span><span class="vpi-moon moon" data-v-809b7594></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-ce71e4ef data-v-e0f2db57 data-v-7a4bfff1><!--[--><a class="VPSocialLink no-icon" href="https://github.com/plmsmile" aria-label="github" target="_blank" rel="noopener" data-v-7a4bfff1 data-v-8e5dce54><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-ce71e4ef data-v-8897e953 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-ffaaba87><span class="vpi-more-horizontal icon" data-v-ffaaba87></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><!----><!--[--><!--[--><!----><div class="group" data-v-8897e953><div class="item appearance" data-v-8897e953><p class="label" data-v-8897e953>Appearance</p><div class="appearance-action" data-v-8897e953><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-8897e953 data-v-809b7594 data-v-3591f5e2><span class="check" data-v-3591f5e2><span class="icon" data-v-3591f5e2><!--[--><span class="vpi-sun sun" data-v-809b7594></span><span class="vpi-moon moon" data-v-809b7594></span><!--]--></span></span></button></div></div></div><div class="group" data-v-8897e953><div class="item social-links" data-v-8897e953><div class="VPSocialLinks social-links-list" data-v-8897e953 data-v-7a4bfff1><!--[--><a class="VPSocialLink no-icon" href="https://github.com/plmsmile" aria-label="github" target="_blank" rel="noopener" data-v-7a4bfff1 data-v-8e5dce54><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-ce71e4ef data-v-37e0f734><span class="container" data-v-37e0f734><span class="top" data-v-37e0f734></span><span class="middle" data-v-37e0f734></span><span class="bottom" data-v-37e0f734></span></span></button></div></div></div></div><div class="divider" data-v-ce71e4ef><div class="divider-line" data-v-ce71e4ef></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-cecb633e data-v-1b409c8b><div class="container" data-v-1b409c8b><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-1b409c8b><span class="vpi-align-left menu-icon" data-v-1b409c8b></span><span class="menu-text" data-v-1b409c8b>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-1b409c8b data-v-a203161a><button data-v-a203161a>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-cecb633e data-v-18f7b5ca><div class="curtain" data-v-18f7b5ca></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18f7b5ca><span class="visually-hidden" id="sidebar-aria-label" data-v-18f7b5ca> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-7f5b9a39><section class="VPSidebarItem level-0 has-active" data-v-7f5b9a39 data-v-a4affe07><div class="item" role="button" tabindex="0" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><h2 class="text" data-v-a4affe07>💻代码模型</h2><!----></div><div class="items" data-v-a4affe07><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/11-swe-data-series.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>SWE 合成数据 系列</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/10-swe-summary.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>SWE 总结索引</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/07-code-fulltrain-reading.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code 全训练 论文阅读</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/06-code-taskrl-reading.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code TaskRL 论文阅读</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/05-open-codellm.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>CodeLLM 索引简记</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/04-safety-code.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code 安全相关</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/03-rl-task.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code RL 任务</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/02-eval-task-benchmark.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code 任务Bench相关</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/01-survey.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code Survey</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/09-swe-series.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>SWE 训练方法 系列</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/08-code-pretrain-summary.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code 预训练相关</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-cecb633e data-v-53a9cb18><div class="VPDoc has-sidebar has-aside" data-v-53a9cb18 data-v-5a64a79a><!--[--><!--]--><div class="container" data-v-5a64a79a><div class="aside" data-v-5a64a79a><div class="aside-curtain" data-v-5a64a79a></div><div class="aside-container" data-v-5a64a79a><div class="aside-content" data-v-5a64a79a><div class="VPDocAside" data-v-5a64a79a data-v-f8ea3c28><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-f8ea3c28 data-v-b94d89ac><div class="content" data-v-b94d89ac><div class="outline-marker" data-v-b94d89ac></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b94d89ac>当前页大纲</div><ul class="VPDocOutlineItem root" data-v-b94d89ac data-v-80b46526><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-f8ea3c28></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-5a64a79a><div class="content-container" data-v-5a64a79a><!--[--><!--[--><!--[--><article class="post-header" data-v-a99fd7c9><h1 class="title" data-v-a99fd7c9>SWE 总结索引</h1><div class="stats-container" data-v-a99fd7c9><div class="stat-divider" data-v-a99fd7c9></div><div class="stat-item" data-v-a99fd7c9> 📅 发表于 <span class="stat-text" data-v-a99fd7c9>2026/01/05</span></div><div class="stat-item" data-v-a99fd7c9> 🔄 更新于 <span class="stat-text" data-v-a99fd7c9>2026/01/05</span></div><div class="stat-divider" data-v-a99fd7c9></div><div class="stat-item" data-v-a99fd7c9> 👁️ <span class="stat-text" data-v-a99fd7c9>-- 次访问</span><span id="busuanzi_value_page_pv" style="display:none;" data-page="/posts/llm/industry/codellm/10-swe-summary.html" data-v-a99fd7c9></span></div><div class="stat-divider" data-v-a99fd7c9></div><div class="stat-item" data-v-a99fd7c9> 📝 <span class="stat-text" data-v-a99fd7c9>0 字</span></div><div class="stat-divider" data-v-a99fd7c9></div><div class="stat-item" data-v-a99fd7c9> ⏳ <span class="stat-text" data-v-a99fd7c9>0 分钟</span></div><div class="stat-divider" data-v-a99fd7c9></div></div><div class="tag-group" data-v-a99fd7c9><!--[--><div class="category-item" data-v-a99fd7c9>swe</div><!--]--><!--[--><div class="tag-item" data-v-a99fd7c9> #Self-play SWE-RL</div><div class="tag-item" data-v-a99fd7c9> #写Bug修Bug自我博弈JointRL</div><div class="tag-item" data-v-a99fd7c9> #SKyRL-Agent</div><div class="tag-item" data-v-a99fd7c9> #AST工具增强</div><div class="tag-item" data-v-a99fd7c9> #增加环境提示信息</div><div class="tag-item" data-v-a99fd7c9> #留一法估计优势</div><div class="tag-item" data-v-a99fd7c9> #InfoCode</div><div class="tag-item" data-v-a99fd7c9> #对抗生成代码和测试</div><div class="tag-item" data-v-a99fd7c9> #Kimi-Dev</div><div class="tag-item" data-v-a99fd7c9> #Agentless训练</div><div class="tag-item" data-v-a99fd7c9> #SWE-Agent适配</div><div class="tag-item" data-v-a99fd7c9> #MidTrain</div><div class="tag-item" data-v-a99fd7c9> #CodeEditRL</div><div class="tag-item" data-v-a99fd7c9> #SWE-Swiss</div><div class="tag-item" data-v-a99fd7c9> #3任务SFT</div><div class="tag-item" data-v-a99fd7c9> #2阶段课程RL</div><div class="tag-item" data-v-a99fd7c9> #NEBIUS-SWE</div><div class="tag-item" data-v-a99fd7c9> #Mask错误动作SFT</div><div class="tag-item" data-v-a99fd7c9> #DeepSWE</div><div class="tag-item" data-v-a99fd7c9> #GRPO++</div><div class="tag-item" data-v-a99fd7c9> #Devstral2</div><div class="tag-item" data-v-a99fd7c9> #Devstral</div><div class="tag-item" data-v-a99fd7c9> #SWE-RL</div><div class="tag-item" data-v-a99fd7c9> #Patch相似度奖励信号</div><div class="tag-item" data-v-a99fd7c9> #SWE-Agent</div><div class="tag-item" data-v-a99fd7c9> #ACI</div><div class="tag-item" data-v-a99fd7c9> #Agent-Computer-Interface</div><div class="tag-item" data-v-a99fd7c9> #SWE-Lego</div><div class="tag-item" data-v-a99fd7c9> #Mask错误动作</div><div class="tag-item" data-v-a99fd7c9> #SFT课程学习</div><div class="tag-item" data-v-a99fd7c9> #BugPilot</div><div class="tag-item" data-v-a99fd7c9> #FeatAddBug</div><div class="tag-item" data-v-a99fd7c9> #SWE-Mirror</div><div class="tag-item" data-v-a99fd7c9> #Issue迁移</div><div class="tag-item" data-v-a99fd7c9> #生成测试用例</div><div class="tag-item" data-v-a99fd7c9> #生成Bug源码，Issue描述生成</div><div class="tag-item" data-v-a99fd7c9> #AgentSFT 数据蒸馏</div><div class="tag-item" data-v-a99fd7c9> #SWE-Mirror-LM-32B</div><div class="tag-item" data-v-a99fd7c9> #Skywork-SWE</div><div class="tag-item" data-v-a99fd7c9> #SWE-rebench</div><div class="tag-item" data-v-a99fd7c9> #自动Issue-PR 收集</div><div class="tag-item" data-v-a99fd7c9> #SWE-smith</div><div class="tag-item" data-v-a99fd7c9> #SWE-Agent-LM-32B</div><div class="tag-item" data-v-a99fd7c9> #Agent安装环境</div><div class="tag-item" data-v-a99fd7c9> #4策略合成Bug</div><div class="tag-item" data-v-a99fd7c9> #PR Mirror</div><div class="tag-item" data-v-a99fd7c9> #执行验证</div><div class="tag-item" data-v-a99fd7c9> #逆向合成Issue</div><div class="tag-item" data-v-a99fd7c9> #R2E-Gym</div><div class="tag-item" data-v-a99fd7c9> #Hybrid TTS</div><div class="tag-item" data-v-a99fd7c9> #挖掘Commit数据</div><div class="tag-item" data-v-a99fd7c9> #SWE-Gym</div><div class="tag-item" data-v-a99fd7c9> #tts</div><div class="tag-item" data-v-a99fd7c9> #scaffold</div><!--]--></div></article><!--]--><!--]--><!--]--><main class="main" data-v-5a64a79a><div style="position:relative;" class="vp-doc _posts_llm_industry_codellm_10-swe-summary" data-v-5a64a79a><div><h2 id="swe-训练工作" tabindex="-1">SWE 训练工作 <a class="header-anchor" href="#swe-训练工作" aria-label="Permalink to &quot;SWE 训练工作&quot;">​</a></h2><h3 id="_2512-self-play-swe-rl-51-4分-meta" tabindex="-1">(2512) Self-Play SWE-RL (51.4分, Meta) <a class="header-anchor" href="#_2512-self-play-swe-rl-51-4分-meta" aria-label="Permalink to &quot;(2512) Self-Play SWE-RL (51.4分, Meta)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">Self-Play SWE-RL 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2512.18552" target="_blank" rel="noreferrer">paper</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2512-self-play-swe-rl-51-4%E5%88%86-meta" target="_blank" rel="noreferrer">论文笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>Self-Play SWE-RL框架</code><ul><li>给定<code>仓库+环境</code>，通过<code>写Bug</code>+<code>修Bug</code> <code>自我博弈联合RL训练</code>。<code>无需人工Issue</code></li></ul></li><li><code>仓库数据</code>：<code>未知</code></li><li><code>CWM scaffold</code>：<code>bash</code> + <code>search-replace 编辑器</code></li></ul><p><strong>模型效果(CWM-32B-sft)</strong></p><ul><li>在SWE-V和SWE-Pro上，<code>SSR方法</code>都超过<code>RL+人类Issue</code>训练的模型，但<code>也没高多少</code>。</li><li><code>SWE-V</code>达<code>51.4分</code>，<code>SWE-P</code>达<code>28.9分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>Self-Play RL</code>比<code>Repair/Injection-Only RL</code> 性能<code>更好</code>，<code>Inject-Only</code> 效果最差。</li><li><code>大幅删除代码的Bug</code>更好比<code>仅改一行代码的Bug</code>的好。<code>后者太简单</code>，学习信号弱。</li><li>由于共享1个Policy，<code>Solver解决率信号</code> <code>对训练效果影响不大</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li><code>Self-Play SWE-RL 思想</code>，很有<code>启发意义的工作</code>。</li></ul></div><h3 id="_2511-skyrl-agent-39分" tabindex="-1">(2511) SkyRL-Agent(39分) <a class="header-anchor" href="#_2511-skyrl-agent-39分" aria-label="Permalink to &quot;(2511) SkyRL-Agent(39分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SkyRL-Agent 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2511.16108" target="_blank" rel="noreferrer">paper</a>, <a href="https://github.com/NovaSky-AI/skyrl/tree/main/skyrl-train/examples/mini_swe_agent" target="_blank" rel="noreferrer">SkyRL代码</a>, <a href="https://huggingface.co/NovaSky-AI/SA-SWE-32B" target="_blank" rel="noreferrer">SA-SWE-32B</a>, <a href="https://skyrl.readthedocs.io/en/latest/examples/mini_swe_agent.html" target="_blank" rel="noreferrer">doc</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-r2e-gym-34-4%E5%88%86" target="_blank" rel="noreferrer">R2E-Gym</a>, <a href="http://plmsmile.github.io/posts/llm/industry/mainllm/15-skywork-series.html#_2511-skyrl-agent" target="_blank" rel="noreferrer">论文笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SkyRL-Agent 框架</code>：<code>Tool-接口</code> + <code>异步Dispatcher</code> + <code>桥接后端</code></li><li><code>SWE-RL实验</code>：<code>AST工具增强 鼓励检索</code> + <code>增加环境提示信息</code> + <code>On-Policy</code> + <code>留一法优势估计</code></li><li><code>数据</code>：<code>4.5k R2E-Gym</code> ，<code>Scaffold</code>：<code>Simple ReAct Agent</code></li></ul><p><strong>模型效果(Qwen3-32B + RL)</strong></p><ul><li><code>纯RL</code>，SWE <code>pass@1 达 39分</code>，相比基模<code>提升15pt</code>。</li><li>超过DeepSWE <code>36分</code> (报告42分)，训练成本降一半。</li><li><code>弱于蒸馏模型</code> SWE-Agent-LM-32B <code>38分</code>。</li><li>泛化性：Terminal-Bench(+2.5%), BrowseComp-Plus(+1.3%), WebArena(+1.2 turns)</li></ul><p><strong>重要结论</strong></p><p><strong>关键贡献</strong></p><ul><li><code>SKyRL-Agent</code> 框架。<code>SkyRL-Agent-SWE 开源实现</code>。</li></ul></div><h3 id="_2511-infcode-没训练模型" tabindex="-1">(2511) InfCode(没训练模型) <a class="header-anchor" href="#_2511-infcode-没训练模型" aria-label="Permalink to &quot;(2511) InfCode(没训练模型)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">InfCode 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2511.16004" target="_blank" rel="noreferrer">paper</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2511-infcode-%E6%B2%A1%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B" target="_blank" rel="noreferrer">论文笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><p><code>框架</code>：<code>对抗式PatchGeneration</code> + <code>Patch Selection</code></p><ul><li><code>对抗</code>生成<code>代码</code>和<code>单元测试</code>：<code>TestGenerator</code> + <code>CodeGenerator</code>，</li></ul></li><li><p><code>没有训练模型</code>。</p></li></ul><p><strong>模型效果</strong></p><ul><li><code>Claude4.5</code> + <code>InfCode</code>：SWE-Verified <code>79.4分</code>。不知尝试了多少次。</li><li>轻微超过<code>TRAE</code>+<code>DoubaoSeedCode</code> <code>78.8分</code></li></ul><p><strong>重要结论</strong></p><ul><li>对抗生成贡献4pt，选择贡献8pt。</li></ul><p><strong>关键贡献</strong></p><ul><li>对抗<code>Bug修复</code>和<code>测试生成</code>的迭代修复框架。</li><li>虽然没有训练模型，但思路挺好的。</li><li>后来的<code>Self-Play SWE-RL</code> 就和其思路相同，但区别是<code>使用了RL训练</code>。</li></ul></div><h3 id="_2509-kimi-dev-48分" tabindex="-1">(2509) Kimi-Dev(48分) <a class="header-anchor" href="#_2509-kimi-dev-48分" aria-label="Permalink to &quot;(2509) Kimi-Dev(48分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">Kimi-Dev 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2509.23045" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/moonshotai/Kimi-Dev-72B" target="_blank" rel="noreferrer">Kimi-Dev-72B</a>, <a href="https://github.com/MoonshotAI/Kimi-Dev" target="_blank" rel="noreferrer">Kimi-Dev</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2509-kimi-dev-48%E5%88%86" target="_blank" rel="noreferrer">论文笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><p><code>Agentless 训练</code>(3阶段) + <code>SWE-Agent适配</code>(SFT)。</p></li><li><p>Agentless训练：<code>BugFixer</code> + <code>TestWriter</code></p><ul><li><code>MidTrain</code>：<code>Diff Patch</code> + <code>PR Commit</code> + <code>定位推理合成数据</code> +<code>agent交互合成数据</code></li><li><code>CoT SFT</code> ：DeepSeek-R1 蒸馏(SWE-Gym, SWE-bench-extra)</li><li><code>CodeEdit RL</code>：<code>执行结果奖励</code> + <code>难度课程学习</code> + <code>正样本强化</code></li></ul></li><li><p>SWE-Agent适配：<a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-swe-smith-swe-agent-lm" target="_blank" rel="noreferrer">5.7k SWE-smith 轨迹数据</a> 做SFT</p></li><li><p><code>训练数据</code>：<code>是不可能开源的</code>。</p></li></ul><p><strong>模型效果(Qwen2.5-72B-Base)</strong></p><ul><li><code>Agentless 训练</code> SWE-verified <code>Pass@1 48分</code>，<code>TTS(40) 达60分</code>。</li><li><code>SWE-Agent SFT适配</code><ul><li><code>Pass@1 48分</code>，优于<a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#%E5%85%B3%E9%94%AE%E7%BB%93%E6%9E%9C-swe-agent-lm-32b" target="_blank" rel="noreferrer">SWE-Agent-LM-32B </a><code>40.2分</code>；</li><li><code>Pass@10达74分</code>，优于Agentless <code>Pass@30 73.8分</code>，推理次数仅1/3。</li></ul></li></ul><p><strong>重要结论</strong></p><ul><li>Agentless训练可以带来<code>Skill Priors</code>，更好<code>适配SWE-Agent</code></li><li><code>RL的先验最强</code>：做SFT学的快好、做RL效果也更好。</li></ul><p><strong>关键贡献</strong></p><ul><li><code>多阶段CodeAgent训练方法论</code><ul><li><code>Agentless 训练</code>(MT+SFT+RL) + <code>SWE-Agent适配</code>(SFT)。</li><li>先从Agentless打基础，再逐步做Agent，模型不偏科、适应性强。</li></ul></li></ul></div><h3 id="_2508-swe-swiss-45分" tabindex="-1">(2508) SWE-Swiss(45分) <a class="header-anchor" href="#_2508-swe-swiss-45分" aria-label="Permalink to &quot;(2508) SWE-Swiss(45分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Swiss 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.notion.so/SWE-Swiss-A-Multi-Task-Fine-Tuning-and-RL-Recipe-for-High-Performance-Issue-Resolution-21e174dedd4880ea829ed4c861c44f88" target="_blank" rel="noreferrer">SWE-Swiss Blog</a>, <a href="https://github.com/zhenyuhe00/SWE-Swiss" target="_blank" rel="noreferrer">SWE-Siwss</a>, <a href="https://huggingface.co/SWE-Swiss/datasets" target="_blank" rel="noreferrer">datasets</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2508-swe-swiss-45%E5%88%86" target="_blank" rel="noreferrer">论文笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>3任务SFT数据构建</code>：<code>问题定位</code>+ <code>问题修复</code>+ <code>测试生成</code></li><li><code>2阶段训练方法</code>：<code>3任务SFT</code> + <code>2阶段RL 课程学习</code>，难样本：过滤<code>正确率&gt;90</code>的数据。</li><li><code>3任务-SFT</code> <code>10k轨迹</code> (<code>蒸馏DSR1</code>)，<code>Bug修复-RL</code> <code>12k</code>，来自<code>SWE-Gym</code>,<code>SWE-smith</code>等。</li><li><code>TTS方法</code>：EM + <code>GT代码相似度</code>。</li><li><code>Scaffold</code>：<code>Agentless</code>，<code>不是Agent</code></li></ul><p><strong>模型效果(Qwen2.5-32B-Instruct, SFT+RL)</strong></p><ul><li>SWE-Verified <code>SFT达36</code>，<code>RL达45</code>，<code>RL提升9pt</code>，增加TTS(best-120) 达60分。</li><li>在<code>通用任务</code>、<code>Math任务</code>、<code>代码生成任务</code>上，<code>均有提升</code>。</li></ul><p><strong>重要结论</strong></p><ul><li>虽然<code>训练3任务用SFT</code>，但也<code>可用RL做定位</code>，<code>也很有效果</code>，后续可以基于此。</li></ul><p><strong>关键贡献</strong></p><ul><li><code>开源数据代码</code></li></ul></div><h3 id="_2508-nebius-swe-agent-39分-筛选swe-rebench数据" tabindex="-1">(2508) NEBIUS SWE-Agent (39分, 筛选SWE-rebench数据) <a class="header-anchor" href="#_2508-nebius-swe-agent-39分-筛选swe-rebench数据" aria-label="Permalink to &quot;(2508) NEBIUS SWE-Agent (39分, 筛选SWE-rebench数据)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">NEBIUS-SWE论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2508.03501" target="_blank" rel="noreferrer">paper</a>, <a href="https://nebius.com/blog/posts/training-and-search-for-software-engineering-agents" target="_blank" rel="noreferrer">blog</a>, <a href="https://huggingface.co/nebius" target="_blank" rel="noreferrer">nebius datasets</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2508-nebius-swe-agent-39%E5%88%86-%E7%AD%9B%E9%80%89swe-rebench%E6%95%B0%E6%8D%AE" target="_blank" rel="noreferrer">笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><p><code>SWE-rebench数据筛选</code>：<code>过滤有误数据</code>+<code>控制复杂度</code>+<code>LLM质量评估</code>+<code>确定性测试</code></p></li><li><p><code>数据</code>：<code>7k任务</code> + <code>自蒸馏6.5k轨迹数据</code> + <code>Verified-50</code>做快速验证</p></li><li><p><code>RFT冷启动</code>： <code>Mask错误格式动作</code>，仅<code>学习有效动作</code>。</p></li><li><p><code>2阶段RL课程学习</code></p><ul><li><code>65k</code> -&gt; <code>131k</code>，<code>7k全部样本</code> -&gt; <code>2k难度样本</code></li><li><code>难样本</code>：过滤阶段1 <code>正确率 &gt; 2/3</code>、<code>正确率=0</code>的样本</li></ul></li><li><p><code>DAPO技巧</code></p><ul><li><code>超长步数惩罚</code> + <code>去掉0优势样本</code> + <code>Token-level Loss</code>，<code>阶段2减小CLIP-Higher</code></li><li><code>步数惩罚</code>：鼓励高效和<code>惩罚死循环</code>动作</li></ul></li><li><p><code>Scaffold</code>：<code>SWE-Agent</code></p></li></ul><p><strong>模型效果 (Qwen2.5-72B-Inst, SFT+2RL)</strong></p><ul><li>训练后，SWE <code>pass@1达39分</code>，<code>pass@10达58分</code>，<code>持平DeeepSeek-V3-0324</code></li></ul><p><strong>重要结论</strong></p><ul><li><code>不要过滤超长样本</code>，<code>要惩罚死循环</code>。</li><li><code>训推不一致</code>：采样<code>topk, topp</code>导致<code>词表被截断</code>，解法：<code>关闭filter</code>。</li><li>未来难题方向：<code>长程信用分配问题</code>、<code>盲目自信问题</code>。</li></ul></div><h3 id="_2508-deepswe-42分-agentic" tabindex="-1">(2508) DeepSWE (42分, Agentic) <a class="header-anchor" href="#_2508-deepswe-42分-agentic" aria-label="Permalink to &quot;(2508) DeepSWE (42分, Agentic)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">DeepSWE 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33" target="_blank" rel="noreferrer">blog</a>, <a href="https://huggingface.co/agentica-org/DeepSWE-Preview" target="_blank" rel="noreferrer">DeepSWE</a>, <a href="https://rllm-project.readthedocs.io/en/latest/examples/swe/" target="_blank" rel="noreferrer">rllm-deepswe</a>, <a href="https://huggingface.co/datasets/R2E-Gym/R2E-Gym-Subset" target="_blank" rel="noreferrer">R2E-Gym-Subset</a>, <a href="https://github.com/agentica-project/rllm" target="_blank" rel="noreferrer">rllm</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2508-deepswe-42%E5%88%86-agentic" target="_blank" rel="noreferrer">笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>Kubernates R2E环境集群</code> + <code>R2E-Gym 4.5k数据</code> + <code>环境执行反馈</code></li><li><code>GRPO++算法</code>： <ul><li>DAPO技巧：<code>Clip-Higher</code>+<code>去除KLloss</code>+ <code>去除熵loss</code> + <code>compact过滤</code></li><li>Dr.GRPO技巧：<code>优势不除以标准差</code> + <code>去掉序列内Token平均</code></li><li>RLOO技巧：<code>留一法计算优势</code></li></ul></li><li><code>Hybrid TTS</code>：执行验证 + 免执行验证</li><li><code>SWE-Agent</code></li></ul><p><strong>模型效果(Qwen3-32B, RL)</strong></p><ul><li>Qwen3-32B 经<code>GRPO++</code>优化后，SWE-verified 达<code>42分</code>，<code>TTS达59分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li>用Claude蒸馏来<code>SFT模型</code>，<code>SWE仅34分</code>，低于<a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#%E5%85%B3%E9%94%AE%E7%BB%93%E6%9E%9C-swe-agent-lm-32b" target="_blank" rel="noreferrer">SWE-Agent-LM 40分</a>。</li><li>用<code>SWE-Smith</code>和<code>SWE-Gym</code>数据做RL，<code>提升有限</code>。</li><li><code>R2E-Gym</code> 很适合做RL，<code>较好课程学习</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li>开源。</li></ul></div><h3 id="_2512-devstral2-72-2分" tabindex="-1">(2512) Devstral2(72.2分) <a class="header-anchor" href="#_2512-devstral2-72-2分" aria-label="Permalink to &quot;(2512) Devstral2(72.2分)&quot;">​</a></h3><div class="custom-block danger"><div class="custom-block-title">Devstral2 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512" target="_blank" rel="noreferrer">Devstral-Small-2-24B-Instruct-2512</a>, <a href="https://mistral.ai/news/devstral-2-vibe-cli" target="_blank" rel="noreferrer">devstral-2-vibe-cli</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2512-devstral2-72-2%E5%88%86" target="_blank" rel="noreferrer">笔记</a></li></ul><p><strong>模型效果</strong></p><ul><li><code>模型小</code>且<code>效果好</code><ul><li><code>256k</code>、<code>Dense模型</code>，比Kimi/DeepSeek<code>都小很多</code>。</li><li>Devstral2：<code>123B</code>，<code>72.2 SWE-verified</code>。</li><li>Devstral Small2：<code>24B</code>，<code>68 SWE-verified</code>。</li></ul></li><li>但<code>仍落后于闭源模型</code>。</li></ul><p><strong>关键结论</strong></p><ul><li>支持<code>探索代码库</code>、<code>跨文件协调更改</code>、<code>架构级上下文</code></li><li>支持 <code>Mistral Vibe CLI 工具</code>。</li></ul></div><h3 id="_2505-devstral-46分-tts3指标" tabindex="-1">(2505) Devstral(46分, tts3指标) <a class="header-anchor" href="#_2505-devstral-46分-tts3指标" aria-label="Permalink to &quot;(2505) Devstral(46分, tts3指标)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">Devstral 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://mistral.ai/news/devstral" target="_blank" rel="noreferrer">devstral</a>, <a href="https://huggingface.co/mistralai/Devstral-Small-2505" target="_blank" rel="noreferrer">mistralai/Devstral-Small-2505</a>, <a href="https://www.alphaxiv.org/abs/2509.25193" target="_blank" rel="noreferrer">paper</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2505-devstral-46%E5%88%86-tts3%E6%8C%87%E6%A0%87" target="_blank" rel="noreferrer">笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li>SFT轨迹数据合成方法：基于<code>环境探索</code>+<code>单元测试验证</code>， 保留<code>正确轨迹</code><ul><li>模式：<code>CoT</code>+<code>代码执行</code>，<code>OpenHands</code> + <code>SWE-Gym</code></li><li>具体数据没细讲，类似 <a href="http://plmsmile.github.io/posts/llm/industry/mainllm/02-deepseek-series.html#%E8%87%AA%E8%92%B8%E9%A6%8F%E5%86%B7%E5%90%AF%E5%8A%A8" target="_blank" rel="noreferrer">DeepSeekV3.2 自蒸馏冷启动</a></li></ul></li><li><code>Post-Training方法</code>：<code>简单过滤SFT</code>、<code>严格过滤SFT</code>、<code>RL训练</code>。</li><li><code>OpenHands</code></li></ul><p><strong>模型效果</strong></p><ul><li>Devstral-small-24B模型，<code>SWE达46分</code>，<code>迭代式 Best-of-3</code>指标。</li></ul></div><h3 id="_2502-swe-rl-meta" tabindex="-1">(2502) SWE-RL (Meta) <a class="header-anchor" href="#_2502-swe-rl-meta" aria-label="Permalink to &quot;(2502) SWE-RL (Meta)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-RL 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://www.alphaxiv.org/abs/2502.18449" target="_blank" rel="noreferrer">paper</a>, <a href="https://github.com/facebookresearch/swe-rl" target="_blank" rel="noreferrer">swe-rl</a>, <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2502-swe-rl-meta" target="_blank" rel="noreferrer">笔记</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>GithubPR数据收集构建方法</code>：<code>仓库事件克隆</code> + <code>PR聚合</code> + <code>预测相关文件</code> + <code>数据过滤</code><ul><li>SWE-RL PR数据：<code>27.3w </code></li></ul></li><li><code>AgentSFT数据合成方法</code>：<code>PR种子筛选</code> + <code>定位数据合成</code> + <code>编辑数据合成</code></li><li>SWE-RL方法：<code>LLama3-70B</code> + <code>GRPO</code>，<code>不执行环境</code>，采用<code>Patch相似度</code>来做<code>奖励信号</code></li><li><code>Agentless Scaffold</code></li></ul><p><strong>模型效果(LLaMA3-70B, RL, SWE-Verified)</strong></p><ul><li>LLama3-SWE-RL-70B：<code>SWE-Verified 41分</code>，在<code>100B模型下效果最好</code>，</li><li><code>SFT 达36.2分</code>，效果也不错。</li><li><code>未使用闭源LLM蒸馏技术</code>，<code>纯开源数据</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>RL比SFT效果好</code>。</li><li><code>Best-of-N</code> 越大越好，但后期逐渐收敛。</li><li><code>DenseReward</code> 比Sparse Reward好。</li></ul></div><h3 id="_2405-swe-agent" tabindex="-1">(2405) SWE-agent <a class="header-anchor" href="#_2405-swe-agent" aria-label="Permalink to &quot;(2405) SWE-agent&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-agent 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2405-swe-agent" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2405.15793" target="_blank" rel="noreferrer">paper</a></li></ul><p><strong>核心方法</strong></p><ul><li>设计<code>Agent-Computer-Interface 范式</code></li></ul><p><strong>模型效果</strong></p><ul><li>基于<code>SWE-Agent框架</code>，GPT4-Turbo，<code>SWE-Full-12分</code>，<code>Light-18分</code></li><li><code>SWE-Agent</code>比<code>标准Shell提高7pt</code>，<code>比RAG提高16pt</code>。</li></ul></div><h2 id="swe-数据工作" tabindex="-1">SWE 数据工作 <a class="header-anchor" href="#swe-数据工作" aria-label="Permalink to &quot;SWE 数据工作&quot;">​</a></h2><h3 id="_2601-swe-lego-52-6分" tabindex="-1">(2601) SWE-Lego (52.6分) <a class="header-anchor" href="#_2601-swe-lego-52-6分" aria-label="Permalink to &quot;(2601) SWE-Lego (52.6分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Lego 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2601-swe-lego-52-6%E5%88%86" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2601.01426?chatId=019bc120-7bc9-7898-9d4a-d66201136d67" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/SWE-Lego" target="_blank" rel="noreferrer">SWE-lego</a>, <a href="https://github.com/SWE-Lego/SWE-Lego" target="_blank" rel="noreferrer">代码</a></li></ul><p><strong>核心方法</strong></p><ul><li><p><code>SWE-lego数据集</code>：<code>3.2k仓库</code>+<code>32k任务</code>+<code>18k轨迹</code>，来源<code>SWE-rebench</code></p></li><li><p><code>数据集构造方法</code>：<code>真实PR</code> + <code>合成任务</code> + <code>Qwen3Coder蒸馏轨迹</code></p></li><li><p><code>Refine SFT方法</code>：<code>Mask错误动作</code> + <code>3难度课程学习</code>，<code>难度为交互轮次</code></p></li></ul><p><strong>模型效果(Qwen3-32B + SFT)</strong></p><ul><li><code>SWE-V</code> <code>达52.6分</code>，<code>TTS-16</code> <code>达58.8分</code>，<code>8B</code> <code>达42.2分</code>。</li><li><code>Refine SFT</code> 比<code>普通 SFT(48.8分)</code> <code>高 3.8pt</code></li><li><code>没有Git Hacking</code>的结果，让Agent <code>不能查看git log</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>精细化SFT数据</code> 效果可以<code>超过复杂训练方法</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li><code>SWE-lego数据集</code>，<code>开源代码</code></li></ul></div><h3 id="_2510-bugpilot-54-9分" tabindex="-1">(2510) BugPilot(54.9分) <a class="header-anchor" href="#_2510-bugpilot-54-9分" aria-label="Permalink to &quot;(2510) BugPilot(54.9分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">BugPilot 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2510-bugpilot" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2510.19898?chatId=019bb759-e3a3-7c3c-8a37-b430712af950" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/microsoft/FrogBoss-32B-2510" target="_blank" rel="noreferrer">microsoft/FrogBoss-32B-2510</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>1套Bug合成框架</code>：<code>SWE-Agent</code>开发Feature，<code>引入无意的FeatAdd-Bug</code></li><li><code>数据集-9k轨迹</code>：<code>R2E-Gym</code> + <code>SWE-Smith</code> + <code>FeatAdd轨迹/任务</code>(<code>未开源</code>)</li><li><code>2种训练方法</code>：<code>SFT全数据训练</code>，<code>SFT冷启动</code>+<code>RL训练</code>。</li><li><code>R2E-Gym 脚手架</code></li></ul><p><strong>模型效果(Qwen3-32B + SFT, SWE-Verified)</strong></p><ul><li><code>BaseMix5.8k-SFT</code> <code>pass@1</code> <code>达49分</code>，即<code>SWE-Gym</code> + <code>SWE-smith</code> <code>蒸馏数据</code></li><li>增加<code>FeatAdd-1.2k-轨迹 SFT</code> 达<code>51.9分</code>；增加<code>FeatAdd-Bug RL</code>达<code>52.4分</code>。</li><li>使用<code>全9k蒸馏数据 SFT </code>达<code>54.9分</code>，高于<code>SWE-Mirror-60k-SFT 52分</code>。<code>14B也达45分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>FeatAdd-Bug比较好</code><ul><li><code>解决率低</code>(相比规则SWE-Smith)，平均<code>修改4.2个文件</code>，<code>Bug类型更均匀</code>。</li><li><code>无意Bug</code>比<code>故意Bug</code> <code>效果好</code>。</li></ul></li></ul><p><strong>关键贡献</strong></p><ul><li><code>FeatAdd 无意引入的Bug</code> <code>这种思想</code></li><li>仅开源模型，并<code>未开源</code> <code>数据集</code>和<code>代码</code>。</li></ul></div><h3 id="_2509-swe-mirror-52分-seed" tabindex="-1">(2509) SWE-Mirror(52分, Seed) <a class="header-anchor" href="#_2509-swe-mirror-52分-seed" aria-label="Permalink to &quot;(2509) SWE-Mirror(52分, Seed)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Mirror 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2509-swe-mirror-seed" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2509.08724" target="_blank" rel="noreferrer">paper</a></li></ul><p><strong>核心方法</strong></p><ul><li><p>1套<code>SWE任务合成移植方法</code>：<code>任务选择</code> + <code>任务移植</code> + <code>任务验证</code></p><ul><li><code>Bug移植</code>：<code>生成测试用例</code> + <code>生成Bug源代码</code> + <code>生成Issue描述</code></li></ul></li><li><p><code>SWE-mirror-60k 数据</code>：<code>4语言</code>+<code>40 仓库</code>+<code>60k任务</code>+<code>6.3k蒸馏轨迹</code></p><ul><li><code>数据未开源</code>，python为主，来自<code>SWE-Gym</code>, <code>SWE-rebench</code>, <code>Multi-SWE-RL</code></li></ul></li><li><p><code>SFT方法</code>：<code>Mask错误动作</code></p></li><li><p><code>Scaffold</code>：<code>OpenHands</code>+<code>MopenHands</code></p></li></ul><p><strong>模型效果(Qwen2.5-Coder-Instruct-32B + SFT)</strong></p><ul><li>SWE-verified 达<code>52分</code>。Multi-SWE-Bench-Flash 达21分。</li></ul><p><strong>重要结论</strong></p><ul><li><code>Mask错误动作</code> SFT 效果比不Mask或片段剪辑掉的好。</li><li>SFT <code>Data Scaling有效</code>：<code>4k</code>轨迹训练，6-&gt;<code>35分</code>；<code>12k</code>训练，达<code>52分</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li>SWE-Mirror-60k 任务，没开源，也不算贡献吧。</li></ul></div><h3 id="_2506-skywork-swe-36分" tabindex="-1">(2506) Skywork-SWE(36分) <a class="header-anchor" href="#_2506-skywork-swe-36分" aria-label="Permalink to &quot;(2506) Skywork-SWE(36分)&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">Skywork-SWE 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/mainllm/15-skywork-series.html#_2506-skywork-swe" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2506.19290" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/Skywork/Skywork-SWE-32B" target="_blank" rel="noreferrer">Skywork-SWE-32B</a>, <a href="https://quixotic-sting-239.notion.site/Skywork-SWE-Unveiling-Data-Scaling-Laws-for-Software-Engineering-in-LLMs-eb17f379610040ceb54da5d5d24065bd" target="_blank" rel="noreferrer">blog</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SWE任务收集构建方法</code><ul><li><code>Repo+PR 收集</code> + <code>统一环境安装</code> + <code>执行验证</code>等。</li><li>基于<code>真实环境执行</code>来做<code>数据验证</code>，<code>3层增量式镜像</code> (基础+环境+实例镜像)。</li></ul></li><li><code>Skywork-SWE数据</code>：<code>10k任务</code> + <code>2.5k仓库</code> + <code>8k蒸馏轨迹</code>。<code>没开源数据</code></li><li><code>Scaffold</code>：<code>Openhands</code></li></ul><p><strong>模型效果 (Qwen-2.5-Coder-32B + SFT)</strong></p><ul><li>SWE-verified 达<code>36分</code>，<code>TTS-3</code> 达<code>47分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li>SWE <code>Data-Scaling</code>, <code>Test-Time-Scaling</code>, <code>轮数Scaling</code> Law 得到验证。</li><li>经过<code>单元测试验证的数据</code>比<code>SWE-smith合成数据</code> 靠谱，提升6.8%</li></ul><p><strong>关键贡献</strong></p><ul><li>仅开源模型，<code>未开源代码和数据</code>。</li></ul></div><h3 id="_2505-swe-rebench" tabindex="-1">(2505) SWE-rebench <a class="header-anchor" href="#_2505-swe-rebench" aria-label="Permalink to &quot;(2505) SWE-rebench&quot;">​</a></h3><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-rebench 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2505-swe-rebench" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2505.20411" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/datasets/nebius/SWE-rebench-openhands-trajectories" target="_blank" rel="noreferrer">NEBIUS-SWE-rebench-轨迹数据</a>, <a href="https://huggingface.co/datasets/nebius/SWE-rebench-openhands-trajectories" target="_blank" rel="noreferrer">nebius/SWE-rebench</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>自动</code> <code>SWE Issue-PR任务</code> <code>收集工具</code></li></ul><p><strong>关键贡献</strong></p><ul><li>SWE-rebench 数据集：<code>21k python任务</code></li><li><a href="https://swe-rebench.com/" target="_blank" rel="noreferrer">SWE-rebench Benchmark 排行榜</a></li></ul></div><h3 id="_2504-swe-smith-40分-swe-agent-lm" tabindex="-1">(2504) SWE-smith (40分, SWE-Agent-LM) <a class="header-anchor" href="#_2504-swe-smith-40分-swe-agent-lm" aria-label="Permalink to &quot;(2504) SWE-smith (40分, SWE-Agent-LM)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-smith 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-swe-smith-swe-agent-lm" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2504.21798" target="_blank" rel="noreferrer">paper</a>, <a href="https://github.com/SWE-bench/SWE-smith" target="_blank" rel="noreferrer">SWE-smith</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SWE任务合成方法</code>：<code>Agent安装环境</code> + <code>4策略合成候选任务</code> + <code>执行验证</code> + <code>逆向合成Issue</code></li><li><code>SWE-smith数据</code>：<code>128仓库</code>+<code>50k任务</code>+<code>5k蒸馏轨迹</code></li><li><code>SWE-Agent</code></li></ul><p><strong>模型效果 (Qwen2.5-Coder-32B)</strong></p><ul><li>使用<code>轨迹数据SFT</code>，<code>SWE-verified 达40</code>，<code>提升33pt</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>任务Scaling有效</code>，<code>多样性很重要</code>，<code>PR-Mirror</code>, <code>LM-Rewrite</code>的任务比较好。</li></ul><p><strong>关键贡献</strong></p><ul><li>开源代码、<code>任务</code>、<code>环境</code>、<code>轨迹</code>，<code>真开源！</code></li><li><a href="https://huggingface.co/datasets/SWE-bench/SWE-smith" target="_blank" rel="noreferrer">SWE-smith 52k任务</a>，<a href="https://huggingface.co/datasets/SWE-bench/SWE-smith-trajectories" target="_blank" rel="noreferrer">26k SWE-smith-轨迹</a>，<a href="https://github.com/SWE-bench/SWE-smith-envs" target="_blank" rel="noreferrer">SWE-smith-env</a></li></ul></div><h3 id="_2504-r2e-gym-34-4分" tabindex="-1">(2504) R2E-Gym(34.4分) <a class="header-anchor" href="#_2504-r2e-gym-34-4分" aria-label="Permalink to &quot;(2504) R2E-Gym(34.4分)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">R2E-Gym 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-r2e-gym" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2504.07164?chatId=019b8c06-0a6c-7d41-b722-443e9539be96" target="_blank" rel="noreferrer">paper</a>, <a href="https://r2e-gym.github.io/" target="_blank" rel="noreferrer">r2e-gym</a>, <a href="https://huggingface.co/R2E-Gym" target="_blank" rel="noreferrer">R2E-Gym</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>自动合成SWE任务方法</code>：<code>Commit挖掘</code>+<code>测试用例生成</code>+<code>反向Issue生成</code></li><li><code>R2E-Gym 数据</code>： <code>10仓库</code>+<code>8k任务</code>+<code>3.3k蒸馏轨迹</code> ，<code>R2E-Gym Sub</code>：<code>4.5k 任务</code></li><li><code>OpenHands</code></li></ul><p><strong>模型效果(Qwen-Coder-32B + SFT)</strong></p><ul><li>SWE-Verified 达<code> 34.4分</code></li></ul><p><strong>重要结论</strong></p><ul><li><code>合成数据不输人工数据</code></li><li><code>Hybrid TTS</code> 有效果，从34.4<code>提升至51分</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li></li></ul></div><h3 id="_2412-swe-gym-19-7分" tabindex="-1">(2412) SWE-Gym(19.7分) <a class="header-anchor" href="#_2412-swe-gym-19-7分" aria-label="Permalink to &quot;(2412) SWE-Gym(19.7分)&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Gym 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2412-swe-gym" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2412.21139" target="_blank" rel="noreferrer">paper</a>, <a href="https://github.com/SWE-Gym/SWE-Gym" target="_blank" rel="noreferrer">代码</a>, <a href="https://huggingface.co/SWE-Gym" target="_blank" rel="noreferrer">SWE-Gym Data</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SWE任务构建方法</code>：<code>通过脚本直接提取PR</code>，并半手动<code>构建好环境</code>(仅覆盖11仓库)</li><li><code>SWE-Gym数据集</code>：<code>2.4k任务</code>+ <code>11仓库</code></li><li><code>OpenHands</code>，<code>Moatless</code></li></ul><p><strong>模型效果(Qwen2.5-Coder-32B + SFT)</strong></p><ul><li>SWE-Verified <code>19.7分</code>，TTS-16 达32分。</li></ul><p><strong>重要结论</strong></p><ul><li><code>Best-of-16策略</code>：20.6 -&gt; <code>32分</code>，开源模型新标杆。</li></ul></div><h2 id="swe-背景" tabindex="-1">SWE 背景 <a class="header-anchor" href="#swe-背景" aria-label="Permalink to &quot;SWE 背景&quot;">​</a></h2><h3 id="swe-任务" tabindex="-1">SWE 任务 <a class="header-anchor" href="#swe-任务" aria-label="Permalink to &quot;SWE 任务&quot;">​</a></h3><h3 id="swe-挑战" tabindex="-1">SWE 挑战 <a class="header-anchor" href="#swe-挑战" aria-label="Permalink to &quot;SWE 挑战&quot;">​</a></h3><div class="custom-block warning"><div class="custom-block-title">SWE 挑战</div><p><strong>挑战</strong></p><ul><li>环境验证不足：<code>可执行环境</code> + <code>验证过的单元测试</code> + <code>代码执行套件</code>(统一执行脚本)</li><li><strong>高质量数据不足</strong>：<code>量大质低</code> + <code>质高量小</code><ul><li>SWE-Dev：数据多，但缺环境和单元测试</li><li>SWE-Gym：有环境，但仅11仓库</li></ul></li><li><strong>SWE-Scaling Law 尚不清晰</strong>：SWE数据量小，Scaling Law尚未得到验证，增加数据是否带来效果提升？</li></ul></div><p><a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#swe%E6%96%B9%E6%B3%95%E5%8F%8A%E6%8C%91%E6%88%98" target="_blank" rel="noreferrer">NEBIUS SWE挑战</a></p><div class="custom-block warning"><div class="custom-block-title">SWE 挑战</div><p><strong>SWE 存在挑战</strong></p><ul><li><code>Long-Horizon 多轮交互</code><ul><li>2阶段RL，YaRN 技术 扩展至131k</li></ul></li><li><code>反馈复杂</code>：反馈一大堆报错，可能看不懂 <ul><li>RFT 冷启动</li></ul></li><li><code>数据难以构建</code><ul><li>对策：使用ReBench做清洗，一套清洗策略</li></ul></li><li><code>奖励稀疏</code><ul><li>对策：GRPO/DAPO，Token-Level Loss</li></ul></li><li><code>评估贵且有噪声</code>：跑1次要几分钟，还学不到东西； <ul><li>对策：<code>Verified-50子集</code>、<code>去掉Noisy不稳定数据</code></li></ul></li></ul></div><h3 id="tts-方法" tabindex="-1">TTS 方法 <a class="header-anchor" href="#tts-方法" aria-label="Permalink to &quot;TTS 方法&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">TTS方法</div><p><strong>相关笔记</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#test-time-scaling-deepswe-verifier" target="_blank" rel="noreferrer">DeepSWE-TTS 笔记</a></li></ul><p><strong>环境执行验证</strong></p><ul><li>由<code>LLM生成测试用例</code>，来<code>执行验证</code>，<code>用例通过最多</code>则为<code>最优轨迹</code>。</li><li>优点：信号直接</li><li>缺点：<code>可能区分度低</code>，比如<code>测试用例都太简单</code>、<code>有bug全部都未通过</code>等。</li></ul><p><strong>免执行验证</strong></p><ul><li>不执行验证，通过<code>LLM来选择最优轨迹</code>。</li><li>缺点：<code>容易有偏见</code>，关注Agent的思考过程等，而<code>忽略了代码Patch本身</code>。</li></ul><p><strong>混合方法</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#test-time-scaling-deepswe-verifier" target="_blank" rel="noreferrer">DeepSWE-TTS 笔记</a></li></ul></div><p>R2E-Gym 混合方法：</p><img src="https://r2e-gym.github.io/assets/r2egym/overview-p2.png" style="display:block;margin:auto;" width="70%"><h3 id="scaffold-agent" tabindex="-1">Scaffold(Agent) <a class="header-anchor" href="#scaffold-agent" aria-label="Permalink to &quot;Scaffold(Agent)&quot;">​</a></h3><h4 id="agent-aci-派" tabindex="-1">Agent ACI 派 <a class="header-anchor" href="#agent-aci-派" aria-label="Permalink to &quot;Agent ACI 派&quot;">​</a></h4><div class="custom-block info"><div class="custom-block-title">AgentACI</div><p><strong>核心思想</strong></p><ul><li>核心：<code>端到端</code>，<code>多轮推理</code>。迭代plan + act + reflect等。</li></ul><p><strong>优点</strong></p><ul><li>更灵活，扩展性更好。</li></ul><p><strong>缺点</strong></p><ul><li>端到端<code>难训练</code>，<code>稳定性</code>不如Workflow</li><li><code>交互轮次长</code>，<code>上下文有压力</code>。</li><li>RL 训练不稳定 <ul><li><code>长序列信用分配</code>存在挑战：奖励稀疏</li><li>对<code>初始模型很敏感</code>，需要<code>SFT冷启动</code>。 <ul><li>如果从<code>通用模型</code>开始，可能<code>不会使用工具</code>，<code>陷入死循环</code>。</li></ul></li></ul></li></ul><p><strong>典型工作</strong></p><ul><li><p>OpenHands</p><ul><li>提供<code>编辑器</code> + <code>命令行终端</code> + <code>网页搜索</code>，agent在<code>沙箱环境</code> <code>自主迭代式完成任务</code>。</li><li>优点：上限高，能处理复杂问题，更像人。</li><li>缺点：成本高，容易陷入死循环</li></ul></li><li><p>SWE-Agent</p><ul><li>使用Agent-Computer-Interface，提供<code>编辑器</code>+<code>shell</code>+<code>测试运行器</code>给LLM。</li><li>仓库探索、写脚本复现Bug、修复Bug、测试执行、边缘case生成和测试</li></ul></li><li><p>Moatless-Tools</p></li><li><p>AutoCodeRover</p></li><li><p>SpecRover</p></li><li><p>Trae-Agent</p></li></ul></div><h4 id="workflow-派" tabindex="-1">Workflow 派 <a class="header-anchor" href="#workflow-派" aria-label="Permalink to &quot;Workflow 派&quot;">​</a></h4><div class="custom-block info"><div class="custom-block-title">Workflow 派</div><p><strong>优点</strong></p><ul><li><p><code>流程可控更稳定</code>，模块化更好</p></li><li><p><code>每一步</code>更容易使用<code>RLVR训练</code>。</p></li></ul><p><strong>缺点</strong></p><ul><li><code>探索空间</code>、<code>灵活性</code> <code>有限</code>。</li></ul><p><strong>其他Tradeoff</strong></p><ul><li><p><code>原子能力</code>可作为<code>skill priors</code>，更好的支持<code>通用Agent</code>。</p></li><li><p>定位、修复、反射、验证等。</p></li></ul><p><strong>典型工作</strong></p><ul><li><p><code>专有Pipeline</code></p><ul><li>Agentless：固定的<code>问题定位</code>-<code>Bug修复</code>-<code>执行验证</code> pipeline</li><li>Moatless：主张<code>有效上下文检索</code>才是关键。</li></ul></li><li><p>检索微调</p><ul><li>SWE-fixer：由粗到细，文件检索和编辑解耦。</li></ul></li></ul></div><h4 id="trade-off-派" tabindex="-1">Trade-off 派 <a class="header-anchor" href="#trade-off-派" aria-label="Permalink to &quot;Trade-off 派&quot;">​</a></h4><p>先Agentless训练，再适配到SWE-Agent</p><h3 id="训练流派" tabindex="-1">训练流派 <a class="header-anchor" href="#训练流派" aria-label="Permalink to &quot;训练流派&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">方法流派</div><p><strong>数据蒸馏SFT派 (主流)</strong></p><ul><li>工作：SWE-fixer,</li></ul><p><strong>RL 派 (主流)</strong></p><ul><li>不执行反馈：SWE-RL</li><li>执行反馈：主流，但成本高。</li></ul><p><strong>进化派</strong></p><ul><li>在解决问题的过程中，逐渐积累经验，</li><li>自我提升，Self-Evolution。</li><li>对抗训练。写Bug-修Bug对抗，写测试-修Bug对抗等等。</li><li>工作：SE-Agent,</li></ul></div></div></div></main><footer class="VPDocFooter" data-v-5a64a79a data-v-54a90a4a><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-54a90a4a><span class="visually-hidden" id="doc-footer-aria-label" data-v-54a90a4a>Pager</span><div class="pager" data-v-54a90a4a><a class="VPLink link pager-link prev" href="/posts/llm/industry/codellm/11-swe-data-series.html" data-v-54a90a4a><!--[--><span class="desc" data-v-54a90a4a>上一页</span><span class="title" data-v-54a90a4a>SWE 合成数据 系列</span><!--]--></a></div><div class="pager" data-v-54a90a4a><a class="VPLink link pager-link next" href="/posts/llm/industry/codellm/07-code-fulltrain-reading.html" data-v-54a90a4a><!--[--><span class="desc" data-v-54a90a4a>下一页</span><span class="title" data-v-54a90a4a>Code 全训练 论文阅读</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--[--><div class="busuanzi"> 总访客数： <span id="busuanzi_value_site_uv"></span>   ·   总访问量： <span id="busuanzi_value_site_pv"></span></div><div class="busuanzi"> PLM&#39;s Blog @ 2016 - 2026</div><!--]--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"Bt59YpwR\",\"index.md\":\"DmRGHmj9\",\"markdown-examples.md\":\"CPJSv--2\",\"posts_archive.md\":\"pZWQT7dp\",\"posts_exps_env_01-blog-env.md\":\"CcTUm41j\",\"posts_exps_env_index.md\":\"BJEZeoT-\",\"posts_exps_mind_index.md\":\"naOB3-Mb\",\"posts_llm_agent_basic_01-lhy-agent-notes.md\":\"BK_CsHNC\",\"posts_llm_agent_basic_02-evaluation-agent.md\":\"Dn02cVtP\",\"posts_llm_agent_basic_03-current-agents.md\":\"C25gmbg8\",\"posts_llm_agent_basic_04-agent-blogs.md\":\"_QbL2Plv\",\"posts_llm_agent_basic_05-comuter-agent.md\":\"D0gJpId6\",\"posts_llm_agent_basic_06-deepresearch-evaluation.md\":\"DdRDcxUJ\",\"posts_llm_agent_basic_index.md\":\"ClGtwUqg\",\"posts_llm_agent_rl_01-agent-rl.md\":\"wOF9bz66\",\"posts_llm_agent_rl_02-agent-tool.md\":\"IB_AqkSe\",\"posts_llm_agent_rl_03-agent-search.md\":\"Cim7OGxb\",\"posts_llm_agent_rl_04-agent-env.md\":\"C_O9BVi3\",\"posts_llm_agent_rl_index.md\":\"BudtoDK9\",\"posts_llm_basic_01-lm-define-information-theory.md\":\"Bl38RTdm\",\"posts_llm_basic_02-llm-components.md\":\"CGM_jRUE\",\"posts_llm_basic_03-transformer-detail.md\":\"BgLyAVDZ\",\"posts_llm_basic_04-llm-architecture.md\":\"jk4T-c21\",\"posts_llm_basic_05-llm-basic-info.md\":\"bdnM02bn\",\"posts_llm_basic_06-llm-attention.md\":\"BRJ9jR-M\",\"posts_llm_basic_07-decode.md\":\"BRytPqZo\",\"posts_llm_basic_08-llm-position-embedding.md\":\"d9dN_RuO\",\"posts_llm_basic_index.md\":\"DjX2HRXy\",\"posts_llm_industry_codellm_01-survey.md\":\"DIZQiOWr\",\"posts_llm_industry_codellm_02-eval-task-benchmark.md\":\"B-okIRAD\",\"posts_llm_industry_codellm_03-rl-task.md\":\"vQDqALFA\",\"posts_llm_industry_codellm_04-safety-code.md\":\"mkEFLxnr\",\"posts_llm_industry_codellm_05-open-codellm.md\":\"C6UU6qcZ\",\"posts_llm_industry_codellm_06-code-taskrl-reading.md\":\"BJI-7ypF\",\"posts_llm_industry_codellm_07-code-fulltrain-reading.md\":\"BjyYsnTY\",\"posts_llm_industry_codellm_08-code-pretrain-summary.md\":\"DfgRUz6a\",\"posts_llm_industry_codellm_09-swe-series.md\":\"njRlhnZH\",\"posts_llm_industry_codellm_10-swe-summary.md\":\"CmzPFSW7\",\"posts_llm_industry_codellm_11-swe-data-series.md\":\"BkWEB2x6\",\"posts_llm_industry_mainllm_01-kimi-series.md\":\"B03hc7IE\",\"posts_llm_industry_mainllm_02-deepseek-series.md\":\"B04zRtyw\",\"posts_llm_industry_mainllm_03-glm-series.md\":\"8i7VdpW8\",\"posts_llm_industry_mainllm_04-minimax-series.md\":\"DjQwki4V\",\"posts_llm_industry_mainllm_05-qwen-series.md\":\"B4DMxRmo\",\"posts_llm_industry_mainllm_06-seed-series.md\":\"BNsIBjBZ\",\"posts_llm_industry_mainllm_07-openai-series.md\":\"ZBquCwKO\",\"posts_llm_industry_mainllm_08-gemini-series.md\":\"Bdm2kU-I\",\"posts_llm_industry_mainllm_09-claude-series.md\":\"0YnEaaXS\",\"posts_llm_industry_mainllm_10-longcat-series.md\":\"DroIAFXW\",\"posts_llm_industry_mainllm_11-tencent-series.md\":\"BVkGSYEM\",\"posts_llm_industry_mainllm_12-kwai-series.md\":\"o6Hut3bE\",\"posts_llm_industry_mainllm_13-nvidia-series.md\":\"BFgUmWZk\",\"posts_llm_industry_mainllm_14-mimo-series.md\":\"DDj7XRZV\",\"posts_llm_industry_mainllm_15-skywork-series.md\":\"BMrtN_ri\",\"posts_llm_infra_01-parrallel.md\":\"2i82l-rT\",\"posts_llm_infra_02-speed-framework.md\":\"_bUH-n3t\",\"posts_llm_infra_03-inference-tech.md\":\"Hpk9YmXF\",\"posts_llm_infra_04-verl.md\":\"8XtGz01J\",\"posts_llm_infra_05-verl-practice.md\":\"CpYgON5R\",\"posts_llm_infra_06-verl-code.md\":\"D5bZg4dm\",\"posts_llm_infra_07-verl-core.md\":\"3RS___SF\",\"posts_llm_infra_08-verl-train-loop.md\":\"DzepHNlu\",\"posts_llm_rl_index.md\":\"iUgEsMU1\",\"posts_llm_rl_theory_01-reinforce-learning.md\":\"Ch0rtQCM\",\"posts_llm_rl_theory_01-rl-introduction.md\":\"CmW63EkM\",\"posts_llm_rl_theory_02-markove-process.md\":\"DVY5XgWd\",\"posts_llm_rl_theory_02-value-learning.md\":\"CeOlmbeS\",\"posts_llm_rl_theory_03-model-based-prediction-control.md\":\"BhVRmo7C\",\"posts_llm_rl_theory_03-strategy-learning.md\":\"DWNvEZXH\",\"posts_llm_rl_theory_04-model-free-prediction-control.md\":\"Cg3qo2Fk\",\"posts_llm_rl_theory_04-reinforce-conclusion-simple.md\":\"C6yTAxwQ\",\"posts_llm_rl_theory_05-dqn.md\":\"BatSdlnZ\",\"posts_llm_rl_theory_06-policy-gradient.md\":\"BXI-eY8I\",\"posts_llm_rl_theory_07-actor-critic.md\":\"B1-83sen\",\"posts_llm_rl_theory_08-deterministic-policy-gradient.md\":\"Dgvru3JE\",\"posts_llm_rl_theory_09-policy-trpo-ppo.md\":\"DU-7PSqU\",\"posts_llm_rl_theory_10-ppo-series.md\":\"B-5Hsj_J\",\"posts_llm_rl_theory_11-grpo-series.md\":\"sXtqvv0Z\",\"posts_llm_rl_theory_12-entropy.md\":\"BH45bCLH\",\"posts_llm_rl_theory_13-agentrl-algo.md\":\"EDVx1EmY\",\"posts_me.md\":\"B9W6CMag\",\"posts_olds_algo_aim2offer.md\":\"jCwzQ4BU\",\"posts_olds_algo_aim2offer2.md\":\"Ga2XS6dR\",\"posts_olds_algo_aim2offer3.md\":\"BP0rCZaJ\",\"posts_olds_algo_aim2offer4.md\":\"BXurWO8W\",\"posts_olds_algo_algorithm-dfs.md\":\"CkUFsStz\",\"posts_olds_algo_index.md\":\"CmdF0Gg2\",\"posts_olds_algo_leetcode-01.md\":\"C_2G2jJT\",\"posts_olds_algo_sort-algorithms.md\":\"tFMUVlmH\",\"posts_olds_bigdata_16-spark-baserdd.md\":\"CBxdArgI\",\"posts_olds_bigdata_17-spark-pairrdd.md\":\"C3n8_zMO\",\"posts_olds_bigdata_18-spark-sql.md\":\"tsaWQLcp\",\"posts_olds_bigdata_19-spark-programming.md\":\"DG5ZAF85\",\"posts_olds_bigdata_20-numpy.md\":\"DucCD22z\",\"posts_olds_bigdata_index.md\":\"CWrZwWPb\",\"posts_olds_dl_23-pytorch-start.md\":\"BokpNeAw\",\"posts_olds_dl_35-nerual-network-optim.md\":\"Cp2SpLoE\",\"posts_olds_dl_38-convolution.md\":\"CC_SQ57z\",\"posts_olds_dl_cs224n-assignment-1.md\":\"BZMSZqOS\",\"posts_olds_dl_cs224n-notes3-neural-networks-2.md\":\"Wd9TmSyb\",\"posts_olds_dl_cs224n-notes3-neural-networks.md\":\"CDZWc4X6\",\"posts_olds_dl_cs231n-linear-notes.md\":\"DLRyzcwJ\",\"posts_olds_dl_index.md\":\"C1dyLGNS\",\"posts_olds_dl_rnn.md\":\"CwYYWZ7N\",\"posts_olds_env_09-linux-notes.md\":\"CyjifvcN\",\"posts_olds_env_12-ide-envs.md\":\"YeELWzR2\",\"posts_olds_env_13-old-blog-problems.md\":\"qamPyucB\",\"posts_olds_env_24-hexo-problems.md\":\"CzxhyjW1\",\"posts_olds_env_index.md\":\"DQpgTXVj\",\"posts_olds_ml_10-trees.md\":\"BkNaj6fL\",\"posts_olds_ml_14-em.md\":\"DIufCP0H\",\"posts_olds_ml_21-lr.md\":\"C-1ms511\",\"posts_olds_ml_22-ml-ch03-bayes.md\":\"Q_M6Gn-a\",\"posts_olds_ml_27-svm-notes.md\":\"C96WS6CL\",\"posts_olds_ml_28-ml-interview-notes.md\":\"0bgezw3n\",\"posts_olds_ml_29-desicion-tree.md\":\"CtwmlbWl\",\"posts_olds_ml_crf.md\":\"CEE5oTqd\",\"posts_olds_ml_index.md\":\"BLMEziB1\",\"posts_olds_ml_maxentmodel.md\":\"CSbOumJx\",\"posts_olds_ml_pgm-01.md\":\"BTwybTMD\",\"posts_olds_nlp_11-nlp-labels.md\":\"Cl0Lb8OT\",\"posts_olds_nlp_25-google-nmt.md\":\"BOM-hoYN\",\"posts_olds_nlp_26-wordpieacemodel.md\":\"Q8-L5j15\",\"posts_olds_nlp_30-dynamic-memory-network.md\":\"wp5ucVxW\",\"posts_olds_nlp_31-co-attention-vqa.md\":\"lzwuVKG5\",\"posts_olds_nlp_32-dynamic-coattention-network.md\":\"Bxl4ABWd\",\"posts_olds_nlp_33-attention-summary.md\":\"DT6np0xG\",\"posts_olds_nlp_36-alime-chat.md\":\"Cu2xkcdT\",\"posts_olds_nlp_39-squard-models.md\":\"B1L3iDEv\",\"posts_olds_nlp_45-match-lstm.md\":\"DkpQKM5B\",\"posts_olds_nlp_46-rnet-selfmatch.md\":\"CzGHQ-PZ\",\"posts_olds_nlp_47-bidaf.md\":\"DFJaLh2v\",\"posts_olds_nlp_48-attention-is-all-you-need.md\":\"BY6XvFQ5\",\"posts_olds_nlp_49-qanet.md\":\"BPKWHzTf\",\"posts_olds_nlp_50-elmo.md\":\"WuOt1elN\",\"posts_olds_nlp_51-opengpt.md\":\"B9pQ3h5W\",\"posts_olds_nlp_52-bert.md\":\"5q3t5Qqh\",\"posts_olds_nlp_53-mrc-brief.md\":\"D9CkYrea\",\"posts_olds_nlp_54-mrc-models.md\":\"Ak4bDf1J\",\"posts_olds_nlp_attention-based-nmt.md\":\"BHef6tM6\",\"posts_olds_nlp_attention-model.md\":\"-xLhHhJE\",\"posts_olds_nlp_cs224n-lecture2-word2vec.md\":\"_MmBTADr\",\"posts_olds_nlp_cs224n-notes1-word2vec.md\":\"DXSi5KGh\",\"posts_olds_nlp_index.md\":\"Bfo4Lwn3\",\"posts_olds_nlp_nlp-notes.md\":\"BUmOZxzi\",\"posts_olds_nlp_nmt.md\":\"DUgy6vHF\",\"posts_olds_nlp_subword-units.md\":\"fR_3dRXr\",\"posts_olds_nlp_word2vec-math.md\":\"agvHiE1x\",\"posts_olds_nlp_word2vec.md\":\"D2TKUstm\",\"posts_olds_other_15-cpp-pointer-object-reference.md\":\"BKRTr8QZ\",\"posts_olds_other_index.md\":\"C1T-ubsz\",\"posts_olds_rl_37-reinforce-learning.md\":\"Cf2nny8k\",\"posts_olds_rl_40-value-learning.md\":\"DcnHvvpH\",\"posts_olds_rl_41-strategy-learning.md\":\"CStMrB-q\",\"posts_olds_rl_42-reinforce-conclusion-simple.md\":\"flRZUmJZ\",\"posts_olds_rl_43-intent-detection-slot-filling.md\":\"DcAUbaZU\",\"posts_olds_rl_44-reinforce-nlp.md\":\"Br2ShITL\",\"posts_olds_rl_index.md\":\"CUsxM3zO\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"📚 plmblog\",\"description\":\"记录一些学习笔记。\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"🐲LLM\",\"items\":[{\"text\":\"Basic\",\"items\":[{\"text\":\"🦋基础知识\",\"link\":\"/posts/llm/basic/01-lm-define-information-theory\"},{\"text\":\"🛠基建框架\",\"link\":\"/posts/llm/infra/01-parrallel\"}]},{\"text\":\"强化学习\",\"items\":[{\"text\":\"🎓RL理论基础\",\"link\":\"/posts/llm/rl/theory/01-reinforce-learning\"},{\"text\":\"🚄Agent-RL\",\"link\":\"/posts/llm/agent/rl/02-agent-tool\"}]},{\"text\":\"行业方向\",\"items\":[{\"text\":\"🚀主流模型\",\"link\":\"/posts/llm/industry/mainllm/01-kimi-series\"},{\"text\":\"💻代码模型\",\"link\":\"/posts/llm/industry/codellm/01-survey\"}]},{\"text\":\"Agent\",\"items\":[{\"text\":\"🤖概念及应用\",\"link\":\"/posts/llm/agent/basic\"}]}]},{\"text\":\"📙旧文章\",\"items\":[{\"text\":\"🍓NLP\",\"items\":[{\"text\":\"自然语言处理\",\"link\":\"/posts/olds/nlp\"}]},{\"text\":\"🍑基础知识\",\"items\":[{\"text\":\"深度学习\",\"link\":\"/posts/olds/dl\"},{\"text\":\"强化学习\",\"link\":\"/posts/olds/rl\"},{\"text\":\"机器学习\",\"link\":\"/posts/olds/ml\"}]},{\"text\":\"🍎算法\",\"items\":[{\"text\":\"算法题\",\"link\":\"/posts/olds/algo/\"},{\"text\":\"大数据\",\"link\":\"/posts/olds/bigdata/\"}]},{\"text\":\"🍒其他\",\"items\":[{\"text\":\"环境搭建\",\"link\":\"/posts/olds/env/\"},{\"text\":\"其他\",\"link\":\"/posts/olds/other/\"}]}]},{\"text\":\"经验\",\"items\":[{\"text\":\"环境\",\"items\":[{\"text\":\"环境搭建\",\"link\":\"/posts/exps/env/01-blog-env\"}]},{\"text\":\"心得\",\"items\":[{\"text\":\"心得体会\",\"link\":\"/posts/exps/mind\"}]}]},{\"text\":\"归档\",\"link\":\"/posts/archive.md\"},{\"text\":\"关于我\",\"link\":\"/posts/me\"}],\"outline\":{\"level\":[1,4],\"label\":\"当前页大纲\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/plmsmile\"}],\"search\":{\"provider\":\"local\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"sidebar\":{\"/posts/olds/nlp/\":{\"base\":\"/posts/olds/nlp/\",\"items\":[{\"text\":\"NLP\",\"items\":[{\"text\":\"机器阅读(二)--模型(未完成)\",\"link\":\"54-mrc-models\"},{\"text\":\"机器阅读(一)--整体概述\",\"link\":\"53-mrc-brief\"},{\"text\":\"BERT 详解\",\"link\":\"52-bert\"},{\"text\":\"OpenAI GPT：Improving Language Understanding by Generative Pre-Training\",\"link\":\"51-opengpt\"},{\"text\":\"ELMo：Deep Contextualized Word Representations\",\"link\":\"50-elmo\"},{\"text\":\"QANet\",\"link\":\"49-qanet\"},{\"text\":\"Transformer\",\"link\":\"48-attention-is-all-you-need\"},{\"text\":\"Bidirectional Attention Flow\",\"link\":\"47-bidaf\"},{\"text\":\"R-Net (Gated Self-Matching Networks)\",\"link\":\"46-rnet-selfmatch\"},{\"text\":\"Match-LSTM and Answer Pointer\",\"link\":\"45-match-lstm\"},{\"text\":\"阅读理解模型总结\",\"link\":\"39-squard-models\"},{\"text\":\"阿里小蜜论文\",\"link\":\"36-alime-chat\"},{\"text\":\"各种注意力总结\",\"link\":\"33-attention-summary\"},{\"text\":\"Dynamic Coattention Network (Plus)\",\"link\":\"32-dynamic-coattention-network\"},{\"text\":\"协同注意力简介\",\"link\":\"31-co-attention-vqa\"},{\"text\":\"使用Dynamic Memory Network实现一个简单QA\",\"link\":\"30-dynamic-memory-network\"},{\"text\":\"词性标注和句法依存的表示符号\",\"link\":\"11-nlp-labels\"},{\"text\":\"Word2vec之总体介绍\",\"link\":\"cs224n-notes1-word2vec\"},{\"text\":\"Word2vec之数学模型\",\"link\":\"word2vec-math\"},{\"text\":\"Word2vec之公式推导笔记\",\"link\":\"cs224n-lecture2-word2vec\"},{\"text\":\"subword-units\",\"link\":\"subword-units\"},{\"text\":\"Wordpiece模型\",\"link\":\"26-wordpieacemodel\"},{\"text\":\"谷歌RNN翻译模型\",\"link\":\"25-google-nmt\"},{\"text\":\"机器翻译注意力机制及其PyTorch实现\",\"link\":\"Attention-based-NMT\"},{\"text\":\"图文介绍RNN注意力机制\",\"link\":\"attention-model\"},{\"text\":\"最初RNN神经翻译简略笔记\",\"link\":\"NMT\"},{\"text\":\"语言模型和平滑方法\",\"link\":\"nlp-notes\"},{\"text\":\"利用tensorflow实现简版word2vec\",\"link\":\"word2vec\"}]}]},\"/posts/olds/dl/\":{\"base\":\"/posts/olds/dl/\",\"items\":[{\"text\":\"DL\",\"items\":[{\"text\":\"卷积神经网络总结\",\"link\":\"38-convolution\"},{\"text\":\"网络优化\",\"link\":\"35-nerual-network-optim\"},{\"text\":\"cs224n作业一\",\"link\":\"cs224n-assignment-1\"},{\"text\":\"cs231n线性分类器和损失函数\",\"link\":\"cs231n-linear-notes\"},{\"text\":\"神经网络-过拟合-预处理-BN\",\"link\":\"cs224n-notes3-neural-networks-2\"},{\"text\":\"神经网络基础-反向传播-激活函数\",\"link\":\"cs224n-notes3-neural-networks\"},{\"text\":\"循环神经网络\",\"link\":\"rnn\"},{\"text\":\"PyTorch快速上手\",\"link\":\"23-pytorch-start\"}]}]},\"/posts/olds/rl/\":{\"base\":\"/posts/olds/rl/\",\"items\":[{\"text\":\"RL\",\"items\":[{\"text\":\"强化学习在NLP中的应用\",\"link\":\"44-reinforce-nlp\"},{\"text\":\"意图识别和槽填充\",\"link\":\"43-intent-detection-slot-filling\"},{\"text\":\"强化学习算法小结\",\"link\":\"42-reinforce-conclusion-simple\"},{\"text\":\"基于策略函数的学习方法\",\"link\":\"41-strategy-learning\"},{\"text\":\"基于值函数的学习\",\"link\":\"40-value-learning\"},{\"text\":\"强化学习\",\"link\":\"37-reinforce-learning\"}]}]},\"/posts/olds/ml/\":{\"base\":\"/posts/olds/ml/\",\"items\":[{\"text\":\"ML\",\"items\":[{\"text\":\"决策树笔记\",\"link\":\"29-desicion-tree\"},{\"text\":\"机器学习知识点汇总整理\",\"link\":\"28-ml-interview-notes\"},{\"text\":\"SVM笔记\",\"link\":\"27-svm-notes\"},{\"text\":\"树的总结\",\"link\":\"10-trees\"},{\"text\":\"条件随机场\",\"link\":\"crf\"},{\"text\":\"最大熵模型\",\"link\":\"maxentmodel\"},{\"text\":\"线性回归和逻辑回归\",\"link\":\"21-lr\"},{\"text\":\"最大期望算法\",\"link\":\"14-em\"},{\"text\":\"马尔可夫模型\",\"link\":\"pgm-01\"},{\"text\":\"朴素贝叶斯算法及其代码实现\",\"link\":\"22-ml-ch03-bayes\"}]}]},\"/posts/olds/algo/\":{\"base\":\"/posts/olds/algo/\",\"items\":[{\"text\":\"ALGO\",\"items\":[{\"text\":\"剑指offer4(51-64)\",\"link\":\"aim2offer4\"},{\"text\":\"剑指offer3(21-40)\",\"link\":\"aim2offer3\"},{\"text\":\"数据结构之搜索算法\",\"link\":\"algorithm-dfs\"},{\"text\":\"leetcode-01\",\"link\":\"leetcode-01\"},{\"text\":\"剑指offer(11-20)\",\"link\":\"aim2offer2\"},{\"text\":\"排序算法总结\",\"link\":\"sort-algorithms\"},{\"text\":\"剑指Offer(1-10)\",\"link\":\"aim2offer\"}]}]},\"/posts/olds/bigdata/\":{\"base\":\"/posts/olds/bigdata/\",\"items\":[{\"text\":\"BIG Data\",\"items\":[{\"text\":\"NumPy\",\"link\":\"20-numpy\"},{\"text\":\"Spark基础编程核心思想介绍\",\"link\":\"19-spark-programming\"},{\"text\":\"Spark-SQL的简略笔记\",\"link\":\"18-spark-sql\"},{\"text\":\"Spark键值对RDD的常用API\",\"link\":\"17-spark-pairrdd\"},{\"text\":\"Spark基础RDD的常用API\",\"link\":\"16-Spark-BaseRDD\"}]}]},\"/posts/olds/env/\":{\"base\":\"/posts/olds/env/\",\"items\":[{\"text\":\"环境搭建\",\"items\":[{\"text\":\"IDE配置\",\"link\":\"12-ide-envs\"},{\"text\":\"Linux使用笔记\",\"link\":\"09-linux-notes\"},{\"text\":\"博客搭建及相关问题\",\"link\":\"24-hexo-problems\"},{\"text\":\"旧版博客搭建过程及其问题\",\"link\":\"13-old-blog-problems\"}]}]},\"/posts/olds/other/\":{\"base\":\"/posts/olds/other/\",\"items\":[{\"text\":\"其他\",\"items\":[{\"text\":\"C++类对象和指针的区别\",\"link\":\"15-cpp-pointer-object-reference\"}]}]},\"/posts/llm/agent/rl/\":{\"base\":\"/posts/llm/agent/rl/\",\"items\":[{\"text\":\"Agent-RL\",\"items\":[{\"text\":\"Agent-Interaction-RL 笔记\",\"link\":\"04-agent-env\"},{\"text\":\"Agent-Search-RL 笔记\",\"link\":\"03-agent-search\"},{\"text\":\"Agent-Tool-RL 笔记\",\"link\":\"02-agent-tool\"},{\"text\":\"Agent-RL 综述型笔记\",\"link\":\"01-agent-rl\"}]}]},\"/posts/llm/agent/basic/\":{\"base\":\"/posts/llm/agent/basic/\",\"items\":[{\"text\":\"Agent-基础\",\"items\":[{\"text\":\"DeepResearch 评估\",\"link\":\"06-deepresearch-evaluation\"},{\"text\":\"Computer-Agent\",\"link\":\"05-comuter-agent\"},{\"text\":\"Agent 思考性文章\",\"link\":\"04-agent-blogs\"},{\"text\":\"一些流行的Agents\",\"link\":\"03-current-agents\"},{\"text\":\"Agent 评估 Benchmarks\",\"link\":\"02-evaluation-agent\"},{\"text\":\"Agent基础概念 (李宏毅笔记)\",\"link\":\"01-lhy-agent-notes\"}]}]},\"/posts/llm/rl/theory/\":{\"base\":\"/posts/llm/rl/theory/\",\"items\":[{\"text\":\"RL-Theory\",\"items\":[{\"text\":\"Agent-RL 相关算法\",\"link\":\"13-agentrl-algo\"},{\"text\":\"熵和RL相关文章\",\"link\":\"12-entropy\"},{\"text\":\"GRPO 改进系列\",\"link\":\"11-grpo-series\"},{\"text\":\"PPO 改进系列\",\"link\":\"10-ppo-series\"},{\"text\":\"典型策略提升方法：TRPO+PPO+DPO+GRPO\",\"link\":\"09-policy-trpo-ppo\"},{\"text\":\"确定性策略梯度\",\"link\":\"08-deterministic-policy-gradient\"},{\"text\":\"Actor-Critic 算法\",\"link\":\"07-actor-critic\"},{\"text\":\"策略梯度算法\",\"link\":\"06-policy-gradient\"},{\"text\":\"DQN算法及进阶\",\"link\":\"05-dqn\"},{\"text\":\"免模型预测和控制\",\"link\":\"04-model-free-prediction-control\"},{\"text\":\"有模型预测和控制\",\"link\":\"03-model-based-prediction-control\"},{\"text\":\"马尔可夫决策过程\",\"link\":\"02-markove-process\"},{\"text\":\"强化学习基本概念\",\"link\":\"01-rl-introduction\"},{\"text\":\"(18年笔记)强化学习算法小结\",\"link\":\"04-reinforce-conclusion-simple\"},{\"text\":\"(18年笔记)基于策略函数的学习方法\",\"link\":\"03-strategy-learning\"},{\"text\":\"(18年笔记)基于值函数的学习\",\"link\":\"02-value-learning\"},{\"text\":\"(18年笔记)强化学习基础\",\"link\":\"01-reinforce-learning\"}]}]},\"/posts/llm/rl/rlhf/\":{\"base\":\"/posts/llm/rl/rlhf/\",\"items\":[{\"text\":\"RLHF\",\"items\":[]}]},\"/posts/llm/rl/o1llm/\":{\"base\":\"/posts/llm/rl/o1llm/\",\"items\":[{\"text\":\"推理模型\",\"items\":[]}]},\"/posts/llm/industry/mainllm/\":{\"base\":\"/posts/llm/industry/mainllm/\",\"items\":[{\"text\":\"🚀主流模型\",\"items\":[{\"text\":\"快手系列\",\"link\":\"12-kwai-series\"},{\"text\":\"腾讯系列\",\"link\":\"11-tencent-series\"},{\"text\":\"Claude 系列\",\"link\":\"09-claude-series\"},{\"text\":\"LongCat 系列\",\"link\":\"10-longcat-series\"},{\"text\":\"Gemini 系列\",\"link\":\"08-gemini-series\"},{\"text\":\"Seed 系列\",\"link\":\"06-seed-series\"},{\"text\":\"OpenAI 系列\",\"link\":\"07-openai-series\"},{\"text\":\"Qwen 系列\",\"link\":\"05-qwen-series\"},{\"text\":\"MiniMax 系列\",\"link\":\"04-minimax-series\"},{\"text\":\"GLM 系列\",\"link\":\"03-glm-series\"},{\"text\":\"DeepSeek 系列\",\"link\":\"02-deepseek-series\"},{\"text\":\"Kimi 系列\",\"link\":\"01-kimi-series\"},{\"text\":\"SkyWork 系列\",\"link\":\"15-skywork-series\"},{\"text\":\"小米系列\",\"link\":\"14-mimo-series\"},{\"text\":\"Nvidia 系列\",\"link\":\"13-nvidia-series\"}]}]},\"/posts/llm/industry/codellm/\":{\"base\":\"/posts/llm/industry/codellm/\",\"items\":[{\"text\":\"💻代码模型\",\"items\":[{\"text\":\"SWE 合成数据 系列\",\"link\":\"11-swe-data-series\"},{\"text\":\"SWE 总结索引\",\"link\":\"10-swe-summary\"},{\"text\":\"Code 全训练 论文阅读\",\"link\":\"07-code-fulltrain-reading\"},{\"text\":\"Code TaskRL 论文阅读\",\"link\":\"06-code-taskrl-reading\"},{\"text\":\"CodeLLM 索引简记\",\"link\":\"05-open-codellm\"},{\"text\":\"Code 安全相关\",\"link\":\"04-safety-code\"},{\"text\":\"Code RL 任务\",\"link\":\"03-rl-task\"},{\"text\":\"Code 任务Bench相关\",\"link\":\"02-eval-task-benchmark\"},{\"text\":\"Code Survey\",\"link\":\"01-survey\"},{\"text\":\"SWE 训练方法 系列\",\"link\":\"09-swe-series\"},{\"text\":\"Code 预训练相关\",\"link\":\"08-code-pretrain-summary\"}]}]},\"/posts/llm/basic/\":{\"base\":\"/posts/llm/basic/\",\"items\":[{\"text\":\"LLM-basic\",\"items\":[{\"text\":\"LLM位置编码和长度外推系列\",\"link\":\"08-llm-position-embedding\"},{\"text\":\"LLM 解码相关\",\"link\":\"07-decode\"},{\"text\":\"LLM Attention 系列\",\"link\":\"06-llm-attention\"},{\"text\":\"LLM 基础知识\",\"link\":\"05-llm-basic-info\"},{\"text\":\"LLM 架构相关\",\"link\":\"04-llm-architecture\"},{\"text\":\"Transformer细节\",\"link\":\"03-transformer-detail\"},{\"text\":\"语言模型重要组件\",\"link\":\"02-llm-components\"},{\"text\":\"语言模型定义及信息理论\",\"link\":\"01-lm-define-information-theory\"}]}]},\"/posts/llm/infra/\":{\"base\":\"/posts/llm/infra/\",\"items\":[{\"text\":\"🛠LLM-基建框架\",\"items\":[{\"text\":\"Verl 训练流程源代码阅读\",\"link\":\"08-verl-train-loop\"},{\"text\":\"Verl 有趣的功能\",\"link\":\"07-verl-core\"},{\"text\":\"Verl AgentLoop Rollout 相关\",\"link\":\"06-verl-code\"},{\"text\":\"Verl 常见参数配置和理解\",\"link\":\"05-verl-practice\"},{\"text\":\"Verl 早期概念型学习文章\",\"link\":\"04-verl\"},{\"text\":\"推理优化技术\",\"link\":\"03-inference-tech\"},{\"text\":\"分布式训练框架\",\"link\":\"02-speed-framework\"},{\"text\":\"分布式并行策略\",\"link\":\"01-parrallel\"}]}]},\"/posts/exps/env/\":{\"base\":\"/posts/exps/env/\",\"items\":[{\"text\":\"环境搭建\",\"items\":[{\"text\":\"环境搭建的一些坑\",\"link\":\"01-blog-env\"}]}]},\"/posts/exps/mind/\":{\"base\":\"/posts/exps/mind/\",\"items\":[{\"text\":\"心得体会\",\"items\":[]}]}}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>
<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>SWE 合成数据 系列 | 📚 plmblog</title>
    <meta name="description" content="记录一些学习笔记。">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.CuIOXX7t.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.CpFa0R23.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.DrNSUXfy.js">
    <link rel="modulepreload" href="/assets/chunks/framework.CvbyeFFO.js">
    <link rel="modulepreload" href="/assets/posts_llm_industry_codellm_11-swe-data-series.md.BkWEB2x6.lean.js">
    <link rel="icon" href="/plm.png">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-cecb633e><!--[--><!--]--><!--[--><span tabindex="-1" data-v-c979f278></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-c979f278>Skip to content</a><!--]--><!----><header class="VPNav" data-v-cecb633e data-v-0ad68676><div class="VPNavBar" data-v-0ad68676 data-v-ce71e4ef><div class="wrapper" data-v-ce71e4ef><div class="container" data-v-ce71e4ef><div class="title" data-v-ce71e4ef><div class="VPNavBarTitle has-sidebar" data-v-ce71e4ef data-v-7e906684><a class="title" href="/" data-v-7e906684><!--[--><!--]--><!----><span data-v-7e906684>📚 plmblog</span><!--[--><!--]--></a></div></div><div class="content" data-v-ce71e4ef><div class="content-body" data-v-ce71e4ef><!--[--><!--]--><div class="VPNavBarSearch search" data-v-ce71e4ef><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-ce71e4ef data-v-e0cd9371><span id="main-nav-aria-label" class="visually-hidden" data-v-e0cd9371> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-e0cd9371 data-v-4aa19863><!--[--><span data-v-4aa19863>首页</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e0cd9371 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-ffaaba87><span class="text" data-v-ffaaba87><!----><span data-v-ffaaba87>🐲LLM</span><span class="vpi-chevron-down text-icon" data-v-ffaaba87></span></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><div class="items" data-v-798b97ca><!--[--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>Basic</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/basic/01-lm-define-information-theory.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🦋基础知识</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/infra/01-parrallel.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🛠基建框架</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>强化学习</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/rl/theory/01-reinforce-learning.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🎓RL理论基础</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/agent/rl/02-agent-tool.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🚄Agent-RL</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>行业方向</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/industry/mainllm/01-kimi-series.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🚀主流模型</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/industry/codellm/01-survey.html" data-v-dc987abe><!--[--><span data-v-dc987abe>💻代码模型</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>Agent</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/agent/basic.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🤖概念及应用</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e0cd9371 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-ffaaba87><span class="text" data-v-ffaaba87><!----><span data-v-ffaaba87>📙旧文章</span><span class="vpi-chevron-down text-icon" data-v-ffaaba87></span></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><div class="items" data-v-798b97ca><!--[--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍓NLP</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/nlp.html" data-v-dc987abe><!--[--><span data-v-dc987abe>自然语言处理</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍑基础知识</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/dl.html" data-v-dc987abe><!--[--><span data-v-dc987abe>深度学习</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/rl.html" data-v-dc987abe><!--[--><span data-v-dc987abe>强化学习</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/ml.html" data-v-dc987abe><!--[--><span data-v-dc987abe>机器学习</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍎算法</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/algo/" data-v-dc987abe><!--[--><span data-v-dc987abe>算法题</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/bigdata/" data-v-dc987abe><!--[--><span data-v-dc987abe>大数据</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍒其他</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/env/" data-v-dc987abe><!--[--><span data-v-dc987abe>环境搭建</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/other/" data-v-dc987abe><!--[--><span data-v-dc987abe>其他</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e0cd9371 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-ffaaba87><span class="text" data-v-ffaaba87><!----><span data-v-ffaaba87>经验</span><span class="vpi-chevron-down text-icon" data-v-ffaaba87></span></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><div class="items" data-v-798b97ca><!--[--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>环境</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/exps/env/01-blog-env.html" data-v-dc987abe><!--[--><span data-v-dc987abe>环境搭建</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>心得</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/exps/mind.html" data-v-dc987abe><!--[--><span data-v-dc987abe>心得体会</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/posts/archive.html" tabindex="0" data-v-e0cd9371 data-v-4aa19863><!--[--><span data-v-4aa19863>归档</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/posts/me.html" tabindex="0" data-v-e0cd9371 data-v-4aa19863><!--[--><span data-v-4aa19863>关于我</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-ce71e4ef data-v-b59ebfac><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-b59ebfac data-v-809b7594 data-v-3591f5e2><span class="check" data-v-3591f5e2><span class="icon" data-v-3591f5e2><!--[--><span class="vpi-sun sun" data-v-809b7594></span><span class="vpi-moon moon" data-v-809b7594></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-ce71e4ef data-v-e0f2db57 data-v-7a4bfff1><!--[--><a class="VPSocialLink no-icon" href="https://github.com/plmsmile" aria-label="github" target="_blank" rel="noopener" data-v-7a4bfff1 data-v-8e5dce54><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-ce71e4ef data-v-8897e953 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-ffaaba87><span class="vpi-more-horizontal icon" data-v-ffaaba87></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><!----><!--[--><!--[--><!----><div class="group" data-v-8897e953><div class="item appearance" data-v-8897e953><p class="label" data-v-8897e953>Appearance</p><div class="appearance-action" data-v-8897e953><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-8897e953 data-v-809b7594 data-v-3591f5e2><span class="check" data-v-3591f5e2><span class="icon" data-v-3591f5e2><!--[--><span class="vpi-sun sun" data-v-809b7594></span><span class="vpi-moon moon" data-v-809b7594></span><!--]--></span></span></button></div></div></div><div class="group" data-v-8897e953><div class="item social-links" data-v-8897e953><div class="VPSocialLinks social-links-list" data-v-8897e953 data-v-7a4bfff1><!--[--><a class="VPSocialLink no-icon" href="https://github.com/plmsmile" aria-label="github" target="_blank" rel="noopener" data-v-7a4bfff1 data-v-8e5dce54><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-ce71e4ef data-v-37e0f734><span class="container" data-v-37e0f734><span class="top" data-v-37e0f734></span><span class="middle" data-v-37e0f734></span><span class="bottom" data-v-37e0f734></span></span></button></div></div></div></div><div class="divider" data-v-ce71e4ef><div class="divider-line" data-v-ce71e4ef></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-cecb633e data-v-1b409c8b><div class="container" data-v-1b409c8b><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-1b409c8b><span class="vpi-align-left menu-icon" data-v-1b409c8b></span><span class="menu-text" data-v-1b409c8b>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-1b409c8b data-v-a203161a><button data-v-a203161a>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-cecb633e data-v-18f7b5ca><div class="curtain" data-v-18f7b5ca></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18f7b5ca><span class="visually-hidden" id="sidebar-aria-label" data-v-18f7b5ca> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-7f5b9a39><section class="VPSidebarItem level-0 has-active" data-v-7f5b9a39 data-v-a4affe07><div class="item" role="button" tabindex="0" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><h2 class="text" data-v-a4affe07>💻代码模型</h2><!----></div><div class="items" data-v-a4affe07><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/11-swe-data-series.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>SWE 合成数据 系列</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/10-swe-summary.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>SWE 总结索引</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/07-code-fulltrain-reading.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code 全训练 论文阅读</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/06-code-taskrl-reading.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code TaskRL 论文阅读</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/05-open-codellm.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>CodeLLM 索引简记</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/04-safety-code.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code 安全相关</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/03-rl-task.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code RL 任务</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/02-eval-task-benchmark.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code 任务Bench相关</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/01-survey.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code Survey</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/09-swe-series.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>SWE 训练方法 系列</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/llm/industry/codellm/08-code-pretrain-summary.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Code 预训练相关</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-cecb633e data-v-53a9cb18><div class="VPDoc has-sidebar has-aside" data-v-53a9cb18 data-v-5a64a79a><!--[--><!--]--><div class="container" data-v-5a64a79a><div class="aside" data-v-5a64a79a><div class="aside-curtain" data-v-5a64a79a></div><div class="aside-container" data-v-5a64a79a><div class="aside-content" data-v-5a64a79a><div class="VPDocAside" data-v-5a64a79a data-v-f8ea3c28><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-f8ea3c28 data-v-b94d89ac><div class="content" data-v-b94d89ac><div class="outline-marker" data-v-b94d89ac></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b94d89ac>当前页大纲</div><ul class="VPDocOutlineItem root" data-v-b94d89ac data-v-80b46526><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-f8ea3c28></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-5a64a79a><div class="content-container" data-v-5a64a79a><!--[--><!--[--><!--[--><article class="post-header" data-v-a99fd7c9><h1 class="title" data-v-a99fd7c9>SWE 合成数据 系列</h1><div class="stats-container" data-v-a99fd7c9><div class="stat-divider" data-v-a99fd7c9></div><div class="stat-item" data-v-a99fd7c9> 📅 发表于 <span class="stat-text" data-v-a99fd7c9>2026/01/06</span></div><div class="stat-item" data-v-a99fd7c9> 🔄 更新于 <span class="stat-text" data-v-a99fd7c9>2026/01/06</span></div><div class="stat-divider" data-v-a99fd7c9></div><div class="stat-item" data-v-a99fd7c9> 👁️ <span class="stat-text" data-v-a99fd7c9>-- 次访问</span><span id="busuanzi_value_page_pv" style="display:none;" data-page="/posts/llm/industry/codellm/11-swe-data-series.html" data-v-a99fd7c9></span></div><div class="stat-divider" data-v-a99fd7c9></div><div class="stat-item" data-v-a99fd7c9> 📝 <span class="stat-text" data-v-a99fd7c9>0 字</span></div><div class="stat-divider" data-v-a99fd7c9></div><div class="stat-item" data-v-a99fd7c9> ⏳ <span class="stat-text" data-v-a99fd7c9>0 分钟</span></div><div class="stat-divider" data-v-a99fd7c9></div></div><div class="tag-group" data-v-a99fd7c9><!--[--><div class="category-item" data-v-a99fd7c9>swe-data</div><!--]--><!--[--><div class="tag-item" data-v-a99fd7c9> #SWE-Lego</div><div class="tag-item" data-v-a99fd7c9> #Mask错误动作</div><div class="tag-item" data-v-a99fd7c9> #SFT课程学习</div><div class="tag-item" data-v-a99fd7c9> #BugPilot</div><div class="tag-item" data-v-a99fd7c9> #FeatAddBug</div><div class="tag-item" data-v-a99fd7c9> #SWE-Mirror</div><div class="tag-item" data-v-a99fd7c9> #Issue迁移</div><div class="tag-item" data-v-a99fd7c9> #生成测试用例</div><div class="tag-item" data-v-a99fd7c9> #生成Bug源码，Issue描述生成</div><div class="tag-item" data-v-a99fd7c9> #AgentSFT 数据蒸馏</div><div class="tag-item" data-v-a99fd7c9> #SWE-Mirror-LM-32B</div><div class="tag-item" data-v-a99fd7c9> #Skywork-SWE</div><div class="tag-item" data-v-a99fd7c9> #SWE-rebench</div><div class="tag-item" data-v-a99fd7c9> #自动Issue-PR 收集</div><div class="tag-item" data-v-a99fd7c9> #SWE-smith</div><div class="tag-item" data-v-a99fd7c9> #SWE-Agent-LM-32B</div><div class="tag-item" data-v-a99fd7c9> #Agent安装环境</div><div class="tag-item" data-v-a99fd7c9> #4策略合成Bug</div><div class="tag-item" data-v-a99fd7c9> #PR Mirror</div><div class="tag-item" data-v-a99fd7c9> #执行验证</div><div class="tag-item" data-v-a99fd7c9> #逆向合成Issue</div><div class="tag-item" data-v-a99fd7c9> #R2E-Gym</div><div class="tag-item" data-v-a99fd7c9> #Hybrid TTS</div><div class="tag-item" data-v-a99fd7c9> #挖掘Commit数据</div><div class="tag-item" data-v-a99fd7c9> #SWE-Gym</div><!--]--></div></article><!--]--><!--]--><!--]--><main class="main" data-v-5a64a79a><div style="position:relative;" class="vp-doc _posts_llm_industry_codellm_11-swe-data-series" data-v-5a64a79a><div><h2 id="_2601-swe-lego-52-6分" tabindex="-1">(2601) SWE-Lego (52.6分) <a class="header-anchor" href="#_2601-swe-lego-52-6分" aria-label="Permalink to &quot;(2601) SWE-Lego (52.6分)&quot;">​</a></h2><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Lego 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2601-swe-lego-52-6%E5%88%86" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2601.01426?chatId=019bc120-7bc9-7898-9d4a-d66201136d67" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/SWE-Lego" target="_blank" rel="noreferrer">SWE-lego</a>, <a href="https://github.com/SWE-Lego/SWE-Lego" target="_blank" rel="noreferrer">代码</a></li></ul><p><strong>核心方法</strong></p><ul><li><p><code>SWE-lego数据集</code>：<code>3.2k仓库</code>+<code>32k任务</code>+<code>18k轨迹</code>，来源<code>SWE-rebench</code></p></li><li><p><code>数据集构造方法</code>：<code>真实PR</code> + <code>合成任务</code> + <code>Qwen3Coder蒸馏轨迹</code></p></li><li><p><code>Refine SFT方法</code>：<code>Mask错误动作</code> + <code>3难度课程学习</code>，<code>难度为交互轮次</code></p></li></ul><p><strong>模型效果(Qwen3-32B + SFT)</strong></p><ul><li><code>SWE-V</code> <code>达52.6分</code>，<code>TTS-16</code> <code>达58.8分</code>，<code>8B</code> <code>达42.2分</code>。</li><li><code>Refine SFT</code> 比<code>普通 SFT(48.8分)</code> <code>高 3.8pt</code></li><li><code>没有Git Hacking</code>的结果，让Agent <code>不能查看git log</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>精细化SFT数据</code> 效果可以<code>超过复杂训练方法</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li><code>SWE-lego数据集</code>，<code>开源代码</code></li></ul></div><img src="https://arxiv.org/html/2601.01426v2/x1.png" style="display:block;margin:auto;" width="70%"><img src="https://arxiv.org/html/2601.01426v2/x2.png" style="display:block;margin:auto;" width="70%"><h3 id="问题背景" tabindex="-1">问题背景 <a class="header-anchor" href="#问题背景" aria-label="Permalink to &quot;问题背景&quot;">​</a></h3><p>❓<strong>问题背景</strong></p><div class="custom-block warning"><div class="custom-block-title">问题背景</div><p><strong>SWE 方法现状</strong></p><ul><li>SFT：<code>数据质量低</code>，缺乏<code>可执行环境</code>和<code>真实Bug</code>来产生<code>高质量轨迹</code>。</li><li>MidTrain：数据量和Data要求高。</li><li>RL：缺乏<code>可执行环境</code>+<code>任务数量</code>，<code>计算量大</code>，<code>难度高</code>、<code>不稳定</code>。</li></ul><p><strong>缺乏数据</strong></p><ul><li><code>真实数据</code>：真实，但过滤后，<code>数据有限，难扩展</code>。</li><li><code>合成数据</code>：<code>可扩展</code>，但<code>缺乏真实任务的复杂性</code>。</li></ul><p><strong>轻量级SFT方法</strong></p><ul><li>如何<code>改进数据质量</code>和<code>训练策略</code>，超过复杂训练方法？</li></ul></div><div class="custom-block warning"><div class="custom-block-title">SWE任务失败分类</div><p><strong>Failed To Reproduce 复现失败</strong></p><ul><li>没有尝试复现、尝试了未能复现Bug。</li></ul><p><strong>Read Localization Error 文件定位错误</strong></p><ul><li>已复现Bug，但要修改的文件找错了，没有打开或检查</li></ul><p><strong>Write Localization Error 代码片段定位错误</strong></p><ul><li>找对了文件，但是没有找对应该修复的代码片段。</li></ul><p><strong>耗尽最大轮数</strong></p><ul><li>写入了位置，但耗尽最大轮数。</li></ul><p><strong>实现错误</strong></p><ul><li>去修了，但实际还是修复错了。</li></ul></div><h3 id="swe-lego-数据集" tabindex="-1">SWE-Lego 数据集 <a class="header-anchor" href="#swe-lego-数据集" aria-label="Permalink to &quot;SWE-Lego 数据集&quot;">​</a></h3><p>📕<strong>核心方法</strong></p><h4 id="数据概览" tabindex="-1">数据概览 <a class="header-anchor" href="#数据概览" aria-label="Permalink to &quot;数据概览&quot;">​</a></h4><div class="custom-block note"><div class="custom-block-title">SWE-Lego 数据</div><p><strong>核心思想</strong></p><ul><li>任务：<code>真实</code>+<code>合成</code>，<code>大规模</code>+<code>多样性</code>+<code>可执行</code>的任务，</li><li>轨迹：<code>高质量</code>+<code>验证过</code></li></ul><p><strong>数据来源</strong></p><ul><li>真实任务：<a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2505-swe-rebench" target="_blank" rel="noreferrer">SWE-rebench</a>，<code>贡献大</code>，<code>但难扩展</code></li><li>合成任务：<a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-swe-smith-40%E5%88%86-swe-agent-lm" target="_blank" rel="noreferrer">SWE-smith</a>，<code>补充增益</code></li></ul><p><strong>数据规模</strong></p><ul><li><code>仓库</code>：<code>3.2k</code></li><li><code>任务</code>：<code>32k</code>，<code>可执行</code>，高质量</li><li><code>轨迹</code>：<code>18k</code>，<code>验证过的</code>。其中有<code>4k</code>是<code>半解决</code>，<code>仅定位成功</code>。</li></ul></div><img src="https://arxiv.org/html/2601.01426v2/x3.png" style="display:block;margin:auto;" width="70%"><h4 id="swe任务-构建" tabindex="-1">SWE任务 构建 <a class="header-anchor" href="#swe任务-构建" aria-label="Permalink to &quot;SWE任务 构建&quot;">​</a></h4><div class="custom-block info"><div class="custom-block-title">Task构建(真实提质量+合成提数量)</div><p><strong>仓库收集 &amp; Sandbox 构建</strong></p><ul><li>仓库：从<code>SWE-rebench</code>选择<code>3k Python仓库</code>。</li><li>环境：<code>自动构建镜像</code>(解析setup.py等文件)</li><li>过滤：<code>环境构建成功</code> &amp; <code>通过sanity测试</code></li></ul><p><strong>真实 &amp; 合成任务 构建</strong></p><ul><li><code>真实任务(深度)</code>：<code>SWE-rebench</code>里的PR-Issue。</li><li><code>合成任务(广度)</code>：follow <a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-swe-smith-40%E5%88%86-swe-agent-lm" target="_blank" rel="noreferrer">SWE-smith 4种策略</a><ul><li><code>LLM-Rewrite</code>：根据函数头和稳定，重写代码</li><li><code>AST 程序化构建</code>：语法破坏</li></ul></li><li>注：<code>合成数据</code>质量低于<code>真实Issue-PR数据</code>，SWE-smith &lt; SWE-rebench</li></ul><p><strong>效果结论</strong></p><ul><li><code>Real Only</code> &lt; <code>Real</code> + <code>1合成</code> &lt; <code>Real</code> + <code>5合成</code></li><li>真实数据Scaling有限，<code>自己造数据混合进去</code>，<code>也有提升</code>。</li></ul></div><img src="https://paper-assets.alphaxiv.org/figures/2601.01426v2/img-2.jpeg" style="display:block;margin:auto;" width="70%"><h4 id="agentsft数据-构建" tabindex="-1">AgentSFT数据 构建 <a class="header-anchor" href="#agentsft数据-构建" aria-label="Permalink to &quot;AgentSFT数据 构建&quot;">​</a></h4><div class="custom-block info"><div class="custom-block-title">AgentSFT 数据构建</div><p><strong>基础配置</strong></p><ul><li>模型：<code>Qwen3-Coder-480B-A35B-Instruct</code></li><li>Scaffold：<code>OpenHands</code></li><li>超参：100轮</li></ul><p><strong>关键策略</strong></p><ul><li><code>防Git Hacking</code><ul><li>问题：LLM 运行Git log查看答案</li><li>真实数据：<code>删除Issue创建之后的Commit</code></li><li>合成数据：<code>删除Git历史</code></li><li>结果：过滤1%</li></ul></li><li><code>工具格式错误修正</code>： <code>Int和str 行号</code></li><li><code>修剪无效工具</code>：丢弃任务管理器 <ul><li>仅保留：<code>execute_bash</code> + <code>str_replace_editor</code> + <code>think</code> + <code>finish</code></li></ul></li><li><code>验证和过滤</code><ul><li>过滤低质量轨迹：如<code>修改测试文件作弊</code>的。</li><li>回收班解决轨迹：<code>定位正确</code> + <code>修复错误</code>的轨迹。</li></ul></li></ul></div><h3 id="refined-sft-mask错误动作-3阶段课程学习" tabindex="-1">Refined SFT(Mask错误动作+3阶段课程学习) <a class="header-anchor" href="#refined-sft-mask错误动作-3阶段课程学习" aria-label="Permalink to &quot;Refined SFT(Mask错误动作+3阶段课程学习)&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">Refined SFT</div><p><strong>Step-Level Error Mask</strong></p><ul><li><p><code>保留完整轨迹</code>，<code>Mask</code> <code>错误动作的Loss</code>，<code>只学习正确动作</code></p><ul><li>错误定义：工具调用错误、参数错误等内容。</li></ul></li><li><p>效果：<code>提升2pt</code></p></li><li><p>同 <a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-swe-smith-40%E5%88%86-swe-agent-lm" target="_blank" rel="noreferrer">SWE-Mirror SFT</a> <code>Mask错误动作</code>。</p></li></ul><p><strong>3阶段课程学习(防遗忘)</strong></p><ul><li><p><code>模型打分</code>作为难度</p><ul><li>1.5k人标数据，微调Qwen2.5-Coder-32B-Instruct，acc=70%</li><li>打分：<code>简单</code>、<code>中等</code>、<code>困难</code>，预计解决时间是<code>[0, 15min]</code>, <code>[15min, 1h]</code>, <code>[1h, ?]</code></li></ul></li><li><p><code>交互轮数作为难度</code> (<strong>选择此方法</strong>)</p><ul><li><code>简单0-50</code>、<code>中等50-70</code>、<code>困难70-100</code></li></ul></li><li><p><code>三阶段课程学习</code> + <code>防止遗忘</code></p><ul><li><code>Stage1</code>：<code>简单</code>。打基础，学会基本工具使用。</li><li><code>Stage2</code>：简单+<code>中等</code>。处理稍微复杂的逻辑。</li><li><code>Stage3</code>：简单+中等+<code>困难</code>。攻克难任务，全能模型。</li></ul></li></ul></div><img src="https://arxiv.org/html/2601.01426v2/x5.png" style="display:block;margin:auto;" width="70%"><h3 id="tts-方法" tabindex="-1">TTS 方法 <a class="header-anchor" href="#tts-方法" aria-label="Permalink to &quot;TTS 方法&quot;">​</a></h3><div class="custom-block tip"><div class="custom-block-title">TTS 方法</div><p><strong>Sequential Scaling</strong></p><ul><li><code>增大交互轮数</code>，<code>100-140</code>轮是极限。</li><li>但是<code>越往后</code>，<code>成本越贵</code>，超线性。</li></ul><p><strong>Parrallel Scaling</strong></p><ul><li><code>rollout多个结果</code> + <code>验证器选择</code> 最好的。<code>TTS@k</code></li><li>验证器： <ul><li>模型：<code>Qwen3-Coder-30B-A3B</code></li><li>训练数据：<code>5k 已解决</code>、<code>1.3k未解决</code> 轨迹。</li><li>目标：生成式，<code>输出Yes/No</code>。<code>词概率</code>作为<code>得分</code>。</li><li>效果：<code>生成式</code> &gt; <code>回归式</code>。</li></ul></li></ul><p><strong>总结</strong></p><ul><li><code>先增大轮数</code>，<code>再并行选择</code></li></ul></div><h3 id="实验设置-三阶段课程sft" tabindex="-1">实验设置(三阶段课程SFT) <a class="header-anchor" href="#实验设置-三阶段课程sft" aria-label="Permalink to &quot;实验设置(三阶段课程SFT)&quot;">​</a></h3><p>✍️<strong>实验设置</strong></p><div class="custom-block tip"><div class="custom-block-title">实验设置</div><p><strong>基础模型</strong></p><ul><li>Qwen3-8B, Qwen3-32B</li></ul><p><strong>训练任务/数据</strong></p><ul><li>SWE-Lego 轨迹数据</li></ul><p><strong>评测任务/数据</strong></p><ul><li>SWE-Verified</li></ul><p><strong>算法/策略</strong></p><ul><li><code>SWE-Agent</code>, LLaMA-Factory</li><li><code>三阶段课程学习</code> + <code>防遗忘</code>，难度：<code>交互轮次</code></li></ul><p><strong>超参</strong></p><ul><li><code>4epoch</code>，<code>bs=64</code></li><li>Adam, weight decay 0.01，cosine wamrup 0.1。</li><li>学习率：<code>7B/8B： 1e-4</code>，<code>32B：5e-5</code></li><li><code>128k</code>，<a href="https://plmsmile.github.io/posts/llm/basic/08-llm-position-embedding.html#%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81" target="_blank" rel="noreferrer">RoPE</a></li></ul></div><h3 id="关键结果-qwen3-32b-三阶段sft" tabindex="-1">关键结果(Qwen3-32B+三阶段SFT) <a class="header-anchor" href="#关键结果-qwen3-32b-三阶段sft" aria-label="Permalink to &quot;关键结果(Qwen3-32B+三阶段SFT)&quot;">​</a></h3><p>🍑<strong>关键结果</strong></p><div class="custom-block important"><div class="custom-block-title">关键结果</div><p><strong>模型效果(Qwen3-32B + SFT)</strong></p><ul><li><code>SWE-V</code> <code>达52.6分</code>，<code>TTS-16</code> <code>达58.8分</code>，<code>8B</code> <code>达42.2分</code>。</li><li><code>Refine SFT</code> 比<code>普通 SFT(48.8分)</code> <code>高 3.8pt</code></li><li><code>没有Git Hacking</code>的结果，<code>让Agent 不能查看git log -p</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>精细化SFT数据</code> 效果可以<code>超过复杂训练方法</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li>SWE-Lego数据集：<code>3.2k 仓库</code> + <code>32k 任务</code> + <code>14.1k 轨迹</code></li></ul></div><h3 id="未来方向" tabindex="-1">未来方向 <a class="header-anchor" href="#未来方向" aria-label="Permalink to &quot;未来方向&quot;">​</a></h3><p>⛳ <strong>未来方向</strong></p><div class="custom-block info"><div class="custom-block-title">未来方向</div></div><h2 id="_2510-bugpilot-54-9分" tabindex="-1">(2510) BugPilot(54.9分) <a class="header-anchor" href="#_2510-bugpilot-54-9分" aria-label="Permalink to &quot;(2510) BugPilot(54.9分)&quot;">​</a></h2><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">BugPilot 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2510-bugpilot" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2510.19898?chatId=019bb759-e3a3-7c3c-8a37-b430712af950" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/microsoft/FrogBoss-32B-2510" target="_blank" rel="noreferrer">microsoft/FrogBoss-32B-2510</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>1套Bug合成框架</code>：<code>SWE-Agent</code>开发Feature，<code>引入无意的FeatAdd-Bug</code></li><li><code>数据集-9k轨迹</code>：<code>R2E-Gym</code> + <code>SWE-Smith</code> + <code>FeatAdd轨迹/任务</code>(<code>未开源</code>)</li><li><code>2种训练方法</code>：<code>SFT全数据训练</code>，<code>SFT冷启动</code>+<code>RL训练</code>。</li><li><code>R2E-Gym 脚手架</code></li></ul><p><strong>模型效果(Qwen3-32B + SFT, SWE-Verified)</strong></p><ul><li><code>BaseMix5.8k-SFT</code> <code>pass@1</code> <code>达49分</code>，即<code>SWE-Gym</code> + <code>SWE-smith</code> <code>蒸馏数据</code></li><li>增加<code>FeatAdd-1.2k-轨迹 SFT</code> 达<code>51.9分</code>；增加<code>FeatAdd-Bug RL</code>达<code>52.4分</code>。</li><li>使用<code>全9k蒸馏数据 SFT </code>达<code>54.9分</code>，高于<code>SWE-Mirror-60k-SFT 52分</code>。<code>14B也达45分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>FeatAdd-Bug比较好</code><ul><li><code>解决率低</code>(相比规则SWE-Smith)，平均<code>修改4.2个文件</code>，<code>Bug类型更均匀</code>。</li><li><code>无意Bug</code>比<code>故意Bug</code> <code>效果好</code>。</li></ul></li></ul><p><strong>关键贡献</strong></p><ul><li><code>FeatAdd 无意引入的Bug</code> <code>这种思想</code></li><li>仅开源模型，并<code>未开源</code> <code>数据集</code>和<code>代码</code>。</li></ul></div><h3 id="问题背景-合成数据质量低" tabindex="-1">问题背景(合成数据质量低) <a class="header-anchor" href="#问题背景-合成数据质量低" aria-label="Permalink to &quot;问题背景(合成数据质量低)&quot;">​</a></h3><p>❓<strong>问题背景</strong></p><div class="custom-block warning"><div class="custom-block-title">问题背景</div><ul><li><strong>SWE 数据稀缺：</strong> 挖掘真实仓库<code>Bug数量受限</code>，<code>需复杂清洗工作</code>。</li><li><strong>现合成数据质量低：</strong> 通常<code>依赖规则</code>(如故意写错函数)，<code>合成Bug太简单</code>，<code>和实际Bug有差距</code>。</li><li><strong>目标：</strong> 需<code>大规模</code>生成 具有<code>复杂性</code>的<code>真实Bug</code>合成方法。</li></ul></div><p>📕<strong>核心方法</strong></p><h3 id="bug合成-bugpilot" tabindex="-1">Bug合成(BugPilot) <a class="header-anchor" href="#bug合成-bugpilot" aria-label="Permalink to &quot;Bug合成(BugPilot)&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">BugPilot 框架</div><p><strong>环境配置</strong></p><ul><li><p><code>Docker环境</code> + <code>SWE-Agent</code> + <code>强LLM</code>，<code>直接操作开发</code></p><ul><li>Claude 4 + GPT4o + GPT5</li></ul></li><li><p>任务：<code>128</code> <code>SWE-Smith 仓库</code>，每个都有环境。</p></li></ul><p><strong>两种Bug生成策略</strong></p><ul><li><p><code>FeatAdd-核心</code></p><ul><li><code>添加新功能</code>，<code>无意自然地制造Bug</code>。</li></ul></li><li><p><code>BugInstruct-基线</code></p><ul><li>直接让LLM <code>引入难以察觉的Bug</code>。</li></ul></li><li><p>效果：难度比R2E-Gym, SWE-SMITH高，<code>FeatADD 难度最高</code>。</p></li></ul><p><strong>Bug 验证</strong></p><ul><li>至少<code>有1个原测试失败</code>。</li></ul><p><strong>数据规模</strong></p><ul><li>合成Bug：</li></ul></div><img src="https://microsoft.github.io/debug-gym/figures/bug-pilot/bug-pilot-explanation.png" style="display:block;margin:auto;" width="70%"><p>FeatAdd Bug 比较均衡</p><img src="https://microsoft.github.io/debug-gym/figures/bug-pilot/qual_analysis.png" style="display:block;margin:auto;" width="70%"><h3 id="agentsft-数据集" tabindex="-1">AgentSFT 数据集 <a class="header-anchor" href="#agentsft-数据集" aria-label="Permalink to &quot;AgentSFT 数据集&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">AgentSFT 轨迹蒸馏</div><ul><li>任务：<code>Bug数据</code></li><li>模型：Claude Sonne4</li><li>配置：<code>64k上下文</code> + <code>10k Prompt</code></li><li>拒绝采样：仅保留正确样本。</li><li>成功轨迹：<code>BugInstruct 2.3k</code> + <code>FeatAdd 1.2k</code></li></ul></div><div class="custom-block note"><div class="custom-block-title">AgentSFT 数据集</div><p><strong>BaseMix</strong></p><ul><li><p>数据源：<code>R2E-Gym</code> + <code>SWE-Smith</code>。</p></li><li><p>数据量：<code>5.6k 训练</code> + <code>198测试</code></p></li></ul><p><strong>BaseMix + FeatAdd</strong></p><ul><li>数据量：<code>5.6k</code> + <code>1.2k</code></li><li>原文表格BugPilot合成轨迹<code>大约3.6k</code>，但这里<code>只用了1.2k</code>，可能是<code>纯FeatAdd？</code><ul><li>怎么选的以及为什么，暂时不清楚。</li></ul></li></ul><p><strong>AllData</strong></p><ul><li>数据源：<code>BaseMix</code> + <code>FeatAdd</code> + <code>剩余1k</code>(R2E-Gym,SWE-Smith)</li><li>数据量：<code>9k 轨迹</code></li></ul></div><h3 id="模型训练" tabindex="-1">模型训练 <a class="header-anchor" href="#模型训练" aria-label="Permalink to &quot;模型训练&quot;">​</a></h3><h4 id="sft训练对比配置" tabindex="-1">SFT训练对比配置 <a class="header-anchor" href="#sft训练对比配置" aria-label="Permalink to &quot;SFT训练对比配置&quot;">​</a></h4><div class="custom-block info"><div class="custom-block-title">SFT 训练</div><p><strong>SFT 对比实验</strong></p><ul><li>实验1：<code>BaseMix</code> 数据</li><li>实验2：BaseMix + <code>FeatAdd 1.2k</code></li><li>实验3：BaseMix + <code>其他1.2k</code>(来自R2E+SWE-smith)</li><li>实验4：<code>BaseMix</code> + <code>FeatAdd 1.2k</code> + <code>其他1.2k</code><ul><li>其实就是<code>R2E-Gym</code> + <code>SWE-Smith</code> + <code>FeatAdd</code></li></ul></li></ul><p><strong>SFT 超参</strong></p><ul><li>lr=1e-5，epoch=2，<code>32k</code>，<code>但只训练前32k内容</code></li><li>8卡 H100， 10小时，LLaMA-Factory</li></ul></div><h4 id="rl-训练" tabindex="-1">RL 训练 <a class="header-anchor" href="#rl-训练" aria-label="Permalink to &quot;RL 训练&quot;">​</a></h4><div class="custom-block note"><div class="custom-block-title">RL 训练</div><p><strong>基模</strong></p><ul><li>基于<code>BaseMix SFT模型</code>继续<code>RL训练</code>。</li></ul><p><strong>数据</strong></p><ul><li><code>FeatAdd Bug</code>，1.2k</li></ul><p><strong>环境</strong></p><ul><li><p><code>R2EGym Scaffold</code></p></li><li><p>同 <a href="https://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2508-deepswe-agentic" target="_blank" rel="noreferrer">DeepSWE</a>，用<a href="https://github.com/rllm-org/rllm" target="_blank" rel="noreferrer">RLLM</a>训练框架，RLVR。</p></li></ul><p><strong>超参</strong></p><ul><li><code>rollout 64k</code>，<code>训练仅 前32k</code>，<code>测试 64k</code></li><li>任务<code>最多 100步</code>；一共<code>训练25步</code></li><li><code>bs=64</code>，<code>mini_bs=8</code>，<code>rollout=8</code>，温度1，lr=1e-6</li><li><code>8*8 H100</code>，50小时</li></ul></div><h4 id="实验设置-sft-rl" tabindex="-1">实验设置(SFT+RL) <a class="header-anchor" href="#实验设置-sft-rl" aria-label="Permalink to &quot;实验设置(SFT+RL)&quot;">​</a></h4><p>✍️<strong>实验设置</strong></p><div class="custom-block tip"><div class="custom-block-title">实验设置</div><p><strong>基础模型</strong></p><ul><li>SFT模型：Qwen3-32B/14B</li><li>RL模型：<code>轨迹SFT后的模型</code>，<code>BaseMix-5.8k</code> (R2E-Gym+Swe-Smith)</li></ul><p><strong>训练任务/数据</strong></p><ul><li><p>SFT 数据：<code>BaseMix</code> + <code>FeatAdd</code> + <code>剩余1.2k</code>(R2E,Smith)。</p><ul><li>其实就是：<code>R2E-Gym</code> + <code>SWE-Smith</code> + <code>FeatAdd</code></li></ul></li><li><p>RL 数据：<code>1.2k FeatAddBug</code></p></li></ul><p><strong>评测任务/数据</strong></p><ul><li>SWE-Verified：500个</li></ul><p><strong>算法/策略</strong></p><ul><li>SFT + RL</li></ul><p><strong>超参</strong></p><ul><li>SFT + RL 各自见上文</li></ul></div><h4 id="关键结果-qwen3-32b-sft-rl" tabindex="-1">关键结果(Qwen3-32B+SFT+RL) <a class="header-anchor" href="#关键结果-qwen3-32b-sft-rl" aria-label="Permalink to &quot;关键结果(Qwen3-32B+SFT+RL)&quot;">​</a></h4><p>🍑<strong>关键结果</strong></p><div class="custom-block important"><div class="custom-block-title">关键结果</div><p><strong>模型效果(Qwen3-32B, SFT, RL) (SWE-V指标)</strong></p><ul><li><code>BaseMix5.8k-SFT</code> <code>pass@1</code> <code>达49分</code>。即<code>SWE-Gym</code> + <code>SWE-smith</code> 蒸馏数据</li><li>增加<code>FeatAdd-轨迹 SFT</code> 达<code>51.9分</code>；增加<code>FeatAdd-Bug RL</code>达<code>52.4分</code>。</li><li>使用<code>全9k蒸馏数据 SFT </code>达<code>54.9分</code>，高于<code>SWE-Mirror-60k-SFT 52分</code>。<code>14B也达45分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>FeatAdd生成的Bug比较难</code>：<code>解决率低</code>(相比规则SWE-Smith)，平均<code>修改4.2个文件</code>，<code>Bug类型更均匀</code>。</li><li><code>无意Bug</code>比<code>故意的Bug</code> <code>效果更好</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li><code>无意Bug这种思想</code>以及<code>数据集</code>。</li></ul></div><h3 id="未来方向-1" tabindex="-1">未来方向 <a class="header-anchor" href="#未来方向-1" aria-label="Permalink to &quot;未来方向&quot;">​</a></h3><p>⛳ <strong>未来方向</strong></p><div class="custom-block info"><div class="custom-block-title">合成数据是未来的方向</div><ul><li><code>Self-Improvement</code>：使用<code>微调后模型做开发来生成Bug</code>，避免闭源模型太强，导致无Bug产生。</li><li><code>特定领域训练</code>：修改 Prompt 以生成<code>特定领域/类型的Bug</code>（安全/并发Bug等）。</li><li><code>扩展任务</code>：不仅是写Bug，可扩展到其他SWE任务（<code>测试生成</code>、<code>配置环境</code>、<code>Agent协作</code>等）。</li></ul></div><h2 id="_2509-swe-mirror-52分-seed" tabindex="-1">(2509) SWE-Mirror(52分, Seed) <a class="header-anchor" href="#_2509-swe-mirror-52分-seed" aria-label="Permalink to &quot;(2509) SWE-Mirror(52分, Seed)&quot;">​</a></h2><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Mirror 论文摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2509-swe-mirror-seed" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2509.08724" target="_blank" rel="noreferrer">paper</a></li></ul><p><strong>核心方法</strong></p><ul><li><p>1套<code>SWE任务合成移植方法</code>：<code>任务选择</code> + <code>任务移植</code> + <code>任务验证</code></p><ul><li><code>Bug移植</code>：<code>生成测试用例</code> + <code>生成Bug源代码</code> + <code>生成Issue描述</code></li></ul></li><li><p><code>SWE-mirror-60k 数据</code>：<code>4语言</code>+<code>40 仓库</code>+<code>60k任务</code>+<code>6.3k蒸馏轨迹</code></p><ul><li><code>数据未开源</code>，python为主，来自<code>SWE-Gym</code>, <code>SWE-rebench</code>, <code>Multi-SWE-RL</code></li></ul></li><li><p><code>SFT方法</code>：<code>Mask错误动作</code></p></li><li><p><code>Scaffold</code>：<code>OpenHands</code>+<code>MopenHands</code></p></li></ul><p><strong>模型效果(Qwen2.5-Coder-Instruct-32B + SFT)</strong></p><ul><li>SWE-verified 达<code>52分</code>。Multi-SWE-Bench-Flash 达21分。</li></ul><p><strong>重要结论</strong></p><ul><li><code>Mask错误动作</code> SFT 效果比不Mask或片段剪辑掉的好。</li><li>SFT <code>Data Scaling有效</code>：<code>4k</code>轨迹训练，6-&gt;<code>35分</code>；<code>12k</code>训练，达<code>52分</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li>SWE-Mirror-60k 任务，没开源，也不算贡献吧。</li></ul></div><h3 id="问题背景-1" tabindex="-1">问题背景 <a class="header-anchor" href="#问题背景-1" aria-label="Permalink to &quot;问题背景&quot;">​</a></h3><p>❓<strong>​问题背景</strong></p><div class="custom-block warning"><div class="custom-block-title">SWE组件和两种任务构建方法优缺点</div><p><strong>SWE 两大组件</strong></p><ul><li><strong>Task Context</strong><ul><li><code>Issue</code> + <code>PR</code> + <code>仓库代码</code></li><li><code>测试用例</code>、<code>标准答案</code></li></ul></li><li><strong>Gym</strong><ul><li>可执行环境：测试命令、日志解析、验证正确性、提供奖励。</li></ul></li></ul><p><strong>SWE 两种任务构建方法</strong></p><ul><li><code>合成问题</code>-扩展任务 <ul><li>工作：SWE-smith，SWE-Synth，<code>程序化</code>+<code>重写</code>等方法<code>构建Bug</code>，合成数据。</li><li>优点：规模化。</li><li>缺点：未利用真实软件工程中丰富的历史数据，<code>人工制造的Bug</code></li></ul></li><li><code>搭建Gym</code>-扩展任务 <ul><li>工作：自动搭建环境、收集PR扩展数据。SWE-rebench。</li><li>优点：利用真实数据。</li><li>缺点：<code>环境成本高</code>，<code>成功率低</code>，<code>1实例-1GB</code>，10w实例 -100TB存储。</li></ul></li></ul><p><strong>PR-Mirror</strong></p><ul><li>核心：利用<code>现有代码</code>+<code>现有环境</code>，移植复现<code>之前的Bug</code>，避免为每个bug都重装环境。</li><li>扩展：把<code>原Repo</code>的<code>PR</code>，移植到<code>相似</code>的<code>目标Repo</code>，去<code>构建新任务</code>。</li><li>移植可行的3个理论 <ul><li>组件相似性：很多组件底层架构逻辑和API，有相似之处。(如Pytorch/Tensorflow)</li><li>逻辑可移植性：Bug本质往往是逻辑错误。</li><li>验证可迁移性：源仓库测试用例经过修改后，可验证目标仓库Bug。</li></ul></li></ul></div><div class="custom-block warning"><div class="custom-block-title">SWE 挑战</div><p><strong>Gym 环境构建困难</strong></p><ul><li>收集Context容易，但构建Gym很困难，需要<code>大量人工</code> <code>单独搭环境</code>。</li><li>大部分：<code>Task-Gym</code> 是<code>一对一</code>依赖关系。</li></ul></div><h3 id="swe-mirror-swe任务合成移植方法" tabindex="-1">SWE-Mirror SWE任务合成移植方法 <a class="header-anchor" href="#swe-mirror-swe任务合成移植方法" aria-label="Permalink to &quot;SWE-Mirror SWE任务合成移植方法&quot;">​</a></h3><div class="custom-block info"><div class="custom-block-title">数据源</div><p><strong>数据源(Gym环境)</strong></p><ul><li>SWE-Gym, SWE-rebench, Multi-SWE-RL</li><li>筛选条件：<code>5分钟 完成所有测试</code> + <code>1GB内存</code></li></ul><p><strong>最终数据集</strong></p><ul><li>SWE-mirror-60k：<code>4语言</code>，<code>40 仓库</code>，<code>60k任务</code>，Python大头</li></ul></div><h4 id="任务选择" tabindex="-1">任务选择 <a class="header-anchor" href="#任务选择" aria-label="Permalink to &quot;任务选择&quot;">​</a></h4><div class="custom-block note"><div class="custom-block-title">1. 任务选择</div><ul><li>目标：为<code>每个Repo+Gym</code> (目标Repo)，寻找可移植的Issue。</li><li><strong>搜索相关相似Repo</strong>： <ul><li><code>Qwen-32B 生成5个关键词</code> + <code>关键词搜索相关Top20 Repo</code></li></ul></li><li>从相关Repo<strong>收集过滤Issue-PR</strong>： <ul><li><code>手工规则</code>+<code>LLM筛选</code>，筛选<code>高质量</code> <code>可移植</code>的<code>Issue</code></li><li>100测试集，LLM准召：84%、86%</li></ul></li></ul></div><h4 id="issue-迁移" tabindex="-1">Issue 迁移 <a class="header-anchor" href="#issue-迁移" aria-label="Permalink to &quot;Issue 迁移&quot;">​</a></h4><div class="custom-block note"><div class="custom-block-title">2. 任务移植</div><p><strong>原Issue提炼</strong></p><ul><li><code>原Issue-PR 简洁抽象描述提炼</code>：包括功能、核心逻辑、当前行为、预期行为、观察症状等。</li></ul><p><strong>移植过程</strong></p><ul><li><strong>生成 测试用例</strong><ul><li><code>TestAgent</code>：<code>抽象描述</code> + 目标Repo-Gym的<code>测试套件 </code> --&gt; <code>新的测试用例</code></li><li><code>test.patch</code>：当前代码能通过，应用Bug后的代码不能通过。</li></ul></li><li><strong>生成 Bug源代码</strong><ul><li><code>Mirror Agent</code>：<code>抽象描述</code> + test.patch 包含的<code>文件路径</code> + <code>函数名称</code> --&gt; <code>修改源代码</code></li><li><code>mirror.patch</code>：有Bug的版本，任务起点。</li><li><code>fix.patch</code>：正确答案，无bug，即未应用Bug的版本。</li></ul></li><li><strong>生成 Issue描述</strong><ul><li>Prompt指令：原Issue描述 + <code>test.patch</code> + <code>fix.patch</code> + <code>few-shot 示例</code></li><li>描述：<code>连贯</code> <code>自包含</code>的任务描述。</li></ul></li></ul><p><strong>最终产出任务</strong></p><ul><li><code>mirror.patch</code>：引入目标bug的补丁</li><li><code>test.patch</code>：测试用例</li><li><code>fix.patch</code>：无bug，标准答案</li><li><code>Issue描述/problem_statement</code>：问题描述</li></ul><p><strong>效果评估</strong></p><ul><li>成功率：46% 很高了。88%任务具和原始Issue相比具有中高一致性。</li></ul></div><h4 id="任务验证" tabindex="-1">任务验证 <a class="header-anchor" href="#任务验证" aria-label="Permalink to &quot;任务验证&quot;">​</a></h4><div class="custom-block note"><div class="custom-block-title">3. 任务验证 (质量控制)</div><p><strong>应用实际测试</strong></p><ul><li>run.log：<code>mirror</code>，执行原始测试</li><li>test.log：<code>mirror</code> + <code>test</code>，执行生成的测试</li><li>fix.log：<code>mirror</code> + <code>test</code> + <code>fix</code>，应用3个补丁，执行测试</li></ul><p><strong>日志检查及过滤标准</strong></p><ul><li><code>不能导致原测试挂掉</code>：检查run.log+test.log，只能有pass2pass, fail2fail, skip2skip, none2fail。</li><li>必须<code>真的修复Bug</code>：必须有 Fail2Pass</li><li><code>不能产生新Bug</code>：不能有 Fail2Pass</li><li>测试不能不稳定：有时fail、有时pass，必须过滤</li></ul></div><h4 id="agentsft-数据蒸馏" tabindex="-1">AgentSFT 数据蒸馏 <a class="header-anchor" href="#agentsft-数据蒸馏" aria-label="Permalink to &quot;AgentSFT 数据蒸馏&quot;">​</a></h4><div class="custom-block info"><div class="custom-block-title">SWE-Mirror AgentSFT 轨迹数据蒸馏</div><p><strong>概览</strong></p><ul><li>模型：Claude3.7-Sonnet, Claude4-Sonnet</li><li>任务：SWE-Mirror-60k，选择 <code>15k任务</code></li><li>Agent：<code>OpenHands</code></li><li>超参：100轮，每任务3轨迹，温度=1</li><li>成功蒸馏：<code>6.3k 轨迹</code></li><li>和 <a href="https://huggingface.co/datasets/nebius/SWE-rebench" target="_blank" rel="noreferrer">SWE-rebench</a>轨迹合并，组成<code>12k轨迹</code>。</li></ul></div><h3 id="实验设置-sft-mask错误动作" tabindex="-1">实验设置(SFT, Mask错误动作) <a class="header-anchor" href="#实验设置-sft-mask错误动作" aria-label="Permalink to &quot;实验设置(SFT, Mask错误动作)&quot;">​</a></h3><p>✍️<strong>实验设置</strong></p><div class="custom-block tip"><div class="custom-block-title">实验设置</div><p><strong>基础模型</strong></p><ul><li>Qwen2.5-Coder-Instruct-7B/32B</li></ul><p><strong>训练任务/数据</strong></p><ul><li>SWE-Mirror 蒸馏轨迹 + SWE-rebench轨迹，合计<code>12k轨迹</code></li></ul><p><strong>评测任务/数据</strong></p><ul><li><code>SWE-verified</code>, <code>Multi-SWE-Bench-Flash</code></li></ul><p><strong>算法/策略</strong></p><ul><li>SFT：<code>Mask错误动作</code>，只学习有效轮次。同 <a href="http://plmsmile.github.io/posts/llm/industry/codellm/09-swe-series.html#_2508-nebius-%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%A4%9A%E8%BD%AEswe-%E7%AD%9B%E9%80%89swe-rebench%E6%95%B0%E6%8D%AE" target="_blank" rel="noreferrer">NEBIUS RFT冷启动 </a>一致。</li><li>Scaffold：<code>OpenHands</code>，<code>MopenHands</code>(多语言) <ul><li>OpenHands：编辑文件、shell命令、浏览网页等。</li></ul></li></ul><p><strong>超参</strong></p><ul><li>3epoch，AdamW cosine decay, decay weight=0.01, warmpup=0.1, 峰值lr=5e-5</li><li>如果数据小于4k，则：5epoch，lr=1e-4</li></ul></div><h3 id="关键结果-qwen2-5-coder-instruct-32b-sft" tabindex="-1">关键结果(Qwen2.5-Coder-Instruct-32B + SFT) <a class="header-anchor" href="#关键结果-qwen2-5-coder-instruct-32b-sft" aria-label="Permalink to &quot;关键结果(Qwen2.5-Coder-Instruct-32B + SFT)&quot;">​</a></h3><p>🍑<strong>关键结果</strong></p><div class="custom-block important"><div class="custom-block-title">关键结果</div><p><strong>模型效果(Qwen2.5-Coder-Instruct-32B)</strong></p><ul><li>SWE-Mirror-LM-32B <code>swe达52分</code>，7B达 22分。MSB-Flash 32B达21分。</li><li>32B 超过DeepSeekR1-0528(45分)，低于Qwen3-Coder-480B-A35B(69.6)</li></ul><p><strong>重要结论</strong></p><ul><li><code>Mask错误动作</code> SFT 效果比不Mask或片段剪辑掉的好。</li><li>SFT <code>Data Scaling有效</code>：<code>4k</code>轨迹训练，6-&gt;<code>35分</code>；<code>12k</code>训练，达<code>52分</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li>SWE-Mirror-60k 任务。</li></ul></div><h3 id="未来方向-2" tabindex="-1">未来方向 <a class="header-anchor" href="#未来方向-2" aria-label="Permalink to &quot;未来方向&quot;">​</a></h3><p>⛳ <strong>未来方向</strong></p><div class="custom-block info"><div class="custom-block-title">未来方向</div><ul><li>数据扩展：</li><li>多语言扩展：</li></ul></div><h2 id="_2506-skywork-swe-36分" tabindex="-1">(2506) Skywork-SWE(36分) <a class="header-anchor" href="#_2506-skywork-swe-36分" aria-label="Permalink to &quot;(2506) Skywork-SWE(36分)&quot;">​</a></h2><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">Skywork-SWE 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="http://plmsmile.github.io/posts/llm/industry/mainllm/15-skywork-series.html#_2506-skywork-swe" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2506.19290" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/Skywork/Skywork-SWE-32B" target="_blank" rel="noreferrer">Skywork-SWE-32B</a>, <a href="https://quixotic-sting-239.notion.site/Skywork-SWE-Unveiling-Data-Scaling-Laws-for-Software-Engineering-in-LLMs-eb17f379610040ceb54da5d5d24065bd" target="_blank" rel="noreferrer">blog</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SWE任务收集构建方法</code><ul><li><code>Repo+PR 收集</code> + <code>统一环境安装</code> + <code>执行验证</code>等。</li><li>基于<code>真实环境执行</code>来做<code>数据验证</code>，<code>3层增量式镜像</code> (基础+环境+实例镜像)。</li></ul></li><li><code>Skywork-SWE数据</code>：<code>10k任务</code> + <code>2.5k仓库</code> + <code>8k蒸馏轨迹</code>。<code>没开源数据</code></li><li><code>Scaffold</code>：<code>Openhands</code></li></ul><p><strong>模型效果 (Qwen-2.5-Coder-32B + SFT)</strong></p><ul><li>SWE-verified 达<code>36分</code>，<code>TTS-3</code> 达<code>47分</code>。</li></ul><p><strong>重要结论</strong></p><ul><li>SWE <code>Data-Scaling</code>, <code>Test-Time-Scaling</code>, <code>轮数Scaling</code> Law 得到验证。</li><li>经过<code>单元测试验证的数据</code>比<code>SWE-smith合成数据</code> 靠谱，提升6.8%</li></ul><p><strong>关键贡献</strong></p><ul><li>仅开源模型，<code>未开源代码和数据</code>。</li></ul></div><h2 id="_2505-swe-rebench" tabindex="-1">(2505) SWE-rebench <a class="header-anchor" href="#_2505-swe-rebench" aria-label="Permalink to &quot;(2505) SWE-rebench&quot;">​</a></h2><p>🌺 <strong>论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-rebench 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2505-swe-rebench" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2505.20411" target="_blank" rel="noreferrer">paper</a>, <a href="https://huggingface.co/datasets/nebius/SWE-rebench-openhands-trajectories" target="_blank" rel="noreferrer">NEBIUS-SWE-rebench-轨迹数据</a>, <a href="https://huggingface.co/datasets/nebius/SWE-rebench-openhands-trajectories" target="_blank" rel="noreferrer">nebius/SWE-rebench</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>自动</code> <code>SWE Issue-PR任务</code> <code>收集工具</code></li></ul><p><strong>关键贡献</strong></p><ul><li>SWE-rebench 数据集：<code>21k python任务</code></li><li><a href="https://swe-rebench.com/" target="_blank" rel="noreferrer">SWE-rebench Benchmark 排行榜</a></li></ul></div><p>❓<strong>问题背景</strong></p><div class="custom-block warning"><div class="custom-block-title">问题背景</div><p><strong>大规模数据促进SWE效果</strong></p><p><strong>现有数据存在局限性</strong></p><ul><li>现有数据：<code>静态Github代码</code>+<code>合成指令数据</code> 居多。 <ul><li>只能教会模型补全代码，而非解决问题。</li><li>RL需要：试错、交互式验证学习。</li></ul></li><li>SWE-Gym 改进：但需<code>人工配置环境</code>，<code>难扩展</code>，<code>仅11仓库</code>。</li></ul><p><strong>现有Bench存在局限性</strong></p><ul><li>静态数据可能被训练污染</li><li>不同Scaffold、不同对比。</li></ul></div><h3 id="自动挖掘工具" tabindex="-1">自动挖掘工具 <a class="header-anchor" href="#自动挖掘工具" aria-label="Permalink to &quot;自动挖掘工具&quot;">​</a></h3><p>📕<strong>核心方法</strong></p><h4 id="仓库收集-issue过滤" tabindex="-1">仓库收集+Issue过滤 <a class="header-anchor" href="#仓库收集-issue过滤" aria-label="Permalink to &quot;仓库收集+Issue过滤&quot;">​</a></h4><div class="custom-block note"><div class="custom-block-title">自动挖掘工具</div><p><strong>仓库收集</strong></p><ul><li>Github Archive <ul><li>每日json档案更新<code>Github事件</code>，收集Issue相关内容。 <ul><li><code>描述</code>、<code>讨论</code>、<code>关联PR</code>、<code>metadata</code>等</li><li>从PR中提取信息：合并状态、最新commit、讨论等。</li></ul></li></ul></li><li>Github <ul><li><code>Clone相关仓库</code>及<code>完整提交历史</code>。</li></ul></li><li>数据量： <ul><li><code>45w</code> <code>有Issue相关联的PR</code></li><li><code>3w</code> <code>宽松许可</code>，<code>python占比75%</code></li></ul></li></ul><p><strong>Issue 保留规则(过滤)</strong></p><ul><li><code>宽松许可</code> + <code>Python仓库</code>， <code>Issue 已解决</code>，<code>PR 已合并</code></li><li>PR 没有关联多个Issue，<code>Issue描述&gt;10字</code></li><li>PR 必须<code>修改测试文件</code>，PR必须<code>修改其他文件</code>，修改文件<code>数量在1-15之间</code></li><li>最终数据量：<code>15.3w 实例</code></li></ul></div><h4 id="自动安装环境和执行验证" tabindex="-1">自动安装环境和执行验证 <a class="header-anchor" href="#自动安装环境和执行验证" aria-label="Permalink to &quot;自动安装环境和执行验证&quot;">​</a></h4><div class="custom-block note"><div class="custom-block-title">自动环境安装</div><p><strong>自动环境安装(Agentless 方案)</strong></p><ul><li><code>寻找安装文件</code>：LLM 扫描 <code>README.md</code>, <code>Dockerfile</code>, <code>setup.py</code>等。</li><li><code>生成安装配方</code>：拼接上述文件，让<code>LLM生成安装recipe</code>。</li><li><code>迭代交互式安装</code>：LLM去执行安装，<code>出错</code>，环境反馈Bug，LLM<code>重新修改recipe</code>，<code>再重新安装</code>。</li><li><code>成功率</code>：<code>31%</code>，丢弃其余不成功的。</li><li>其中，<code>对版本做分组</code>，小版本共用1个环境，而非每个Bug一个环境。</li></ul><p><strong>基于执行的安装验证</strong></p><ul><li>确保测试正确 <ul><li>修复前有Bug：<code>test_patch</code>，至少有1个错误。</li><li>成功修复：<code>test_patch</code> + <code>solution_patch</code>，Fail2Pass，没有报错。</li><li>无新Bug：pass2pass</li></ul></li></ul></div><h4 id="模型评估任务质量" tabindex="-1">模型评估任务质量 <a class="header-anchor" href="#模型评估任务质量" aria-label="Permalink to &quot;模型评估任务质量&quot;">​</a></h4><div class="custom-block note"><div class="custom-block-title">自动任务质量评估</div><p><strong>背景</strong></p><ul><li>需要评估，不然有些问题不可解或者难以验证，<code>导致错误惩罚LLM</code>。</li><li>SWE-Bench-Verified：<code>人工验证</code></li></ul><p><strong>微调模型来评估质量(Qwen2.5-72B-Instruct)</strong></p><ul><li>输入：<code>Issue描述</code> + <code>Gold Patch</code> + <code>Test Patch</code></li><li>输出，预测3内容： <ul><li><code>Issue 清晰度</code>：Issue描述<code>是否足够详细</code>。</li><li><code>任务 复杂度</code>：预估<code>工作量</code>。</li><li><code>Test Patch正确性</code>：<code>测试是否准确</code>，能验证出修复是否达预期。</li></ul></li><li>微调数据集：SWE-Bench-Verified 标注数据。</li><li>效果：三指标准确率：81%、67%、79%。</li></ul><p><strong>启发式筛选规则</strong></p><ul><li>根据膝盖文件数量筛选：不准确。</li><li>基于<code>模型各标签</code>自行<code>筛选数据</code>。</li></ul></div><h3 id="swe-rebench-数据概览" tabindex="-1">SWE-rebench 数据概览 <a class="header-anchor" href="#swe-rebench-数据概览" aria-label="Permalink to &quot;SWE-rebench 数据概览&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">概览</div><p><strong>SWE-rebench 数据集</strong></p><ul><li><code>21k python 任务</code> + <code>环境</code> + <code>验证</code></li></ul><p><strong>rebench 排行榜</strong></p><ul><li>持续更新，去污染(新题目)，标准化对比(scaffold等)。</li></ul></div><p>⛳ <strong>未来方向</strong></p><div class="custom-block info"><div class="custom-block-title">未来方向</div><ul><li>任务质量提升：可能有的任务不可解。</li><li>语言扩展：现在仅python</li></ul></div><h2 id="_2504-swe-smith-40分-swe-agent-lm" tabindex="-1">(2504) SWE-smith (40分, SWE-Agent-LM) <a class="header-anchor" href="#_2504-swe-smith-40分-swe-agent-lm" aria-label="Permalink to &quot;(2504) SWE-smith (40分, SWE-Agent-LM)&quot;">​</a></h2><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-smith 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-swe-smith-swe-agent-lm" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2504.21798" target="_blank" rel="noreferrer">paper</a>, <a href="https://github.com/SWE-bench/SWE-smith" target="_blank" rel="noreferrer">SWE-smith</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SWE任务合成方法</code>：<code>Agent安装环境</code> + <code>4策略合成候选任务</code> + <code>执行验证</code> + <code>逆向合成Issue</code></li><li><code>SWE-smith数据</code>：<code>128仓库</code>+<code>50k任务</code>+<code>5k蒸馏轨迹</code></li><li><code>SWE-Agent</code></li></ul><p><strong>模型效果 (Qwen2.5-Coder-32B)</strong></p><ul><li>使用<code>轨迹数据SFT</code>，<code>SWE-verified 达40</code>，<code>提升33pt</code>。</li></ul><p><strong>重要结论</strong></p><ul><li><code>任务Scaling有效</code>，<code>多样性很重要</code>，<code>PR-Mirror</code>, <code>LM-Rewrite</code>的任务比较好。</li></ul><p><strong>关键贡献</strong></p><ul><li>开源代码、<code>任务</code>、<code>环境</code>、<code>轨迹</code>，<code>真开源！</code></li><li><a href="https://huggingface.co/datasets/SWE-bench/SWE-smith" target="_blank" rel="noreferrer">SWE-smith 52k任务</a>，<a href="https://huggingface.co/datasets/SWE-bench/SWE-smith-trajectories" target="_blank" rel="noreferrer">26k SWE-smith-轨迹</a>，<a href="https://github.com/SWE-bench/SWE-smith-envs" target="_blank" rel="noreferrer">SWE-smith-env</a></li></ul></div><h3 id="问题背景-缺乏数据-现2种数据方法有缺点" tabindex="-1">问题背景(缺乏数据+现2种数据方法有缺点) <a class="header-anchor" href="#问题背景-缺乏数据-现2种数据方法有缺点" aria-label="Permalink to &quot;问题背景(缺乏数据+现2种数据方法有缺点)&quot;">​</a></h3><p><strong>❓问题背景</strong></p><div class="custom-block warning"><div class="custom-block-title">问题背景</div><p><strong>SWE 任务缺数据</strong></p><ul><li>SWE-LLM 依赖闭源模型，开源<code>缺乏大规模</code>、<code>高质量训练数据</code>。</li><li><code>需要infra</code>去促进训练<code>数据scale</code></li></ul><p><strong>两种数据模式各有弊端</strong></p><ul><li><code>直接爬取 Github PR/Issue</code><ul><li>优点：简单，<code>量大管饱</code></li><li>缺点：<code>缺乏执行环境</code>、<code>缺乏测试用例</code>，<code>缺乏可靠验证信号</code>，数据质量不可靠</li><li>学习：模型只能学习<code>表面代码相似度形式</code></li></ul></li><li><code>SWE-bench 扩展模式</code><ul><li>优点：<code>执行单元测试</code> 可靠，可以用来<code>蒸馏轨迹数据</code>，<code>数据质量高</code></li><li>学习：学习<code>逻辑功能</code>。</li><li>PR 要求：<code>解决了Issue</code> + <code>需修改新增真实单元测试</code></li><li>缺点：成本高，<code>数据要求严格</code>导致扩展受限，<code>人工调试环境</code>。</li></ul></li></ul></div><h3 id="swe-smith-任务合成方法" tabindex="-1">SWE-Smith 任务合成方法 <a class="header-anchor" href="#swe-smith-任务合成方法" aria-label="Permalink to &quot;SWE-Smith 任务合成方法&quot;">​</a></h3><p><strong>📕核心方法</strong></p><h4 id="数据方法概览" tabindex="-1">数据方法概览 <a class="header-anchor" href="#数据方法概览" aria-label="Permalink to &quot;数据方法概览&quot;">​</a></h4><div class="custom-block info"><div class="custom-block-title">SWE-Smith 整体流程</div><ul><li>先定义环境，再合成任务实例。 <ul><li>给定代码库，<code>使用SWE-Agent来构建环境</code>，<code>仓库级环境</code>。</li><li>通过4种策略，<code>生成多个候选任务</code></li><li>通过执行验证，<code>过滤不合格任务</code>。</li><li>使用<code>LLM生成Issue描述</code>。</li></ul></li><li>最终： <code>128个Python仓库</code> + <code>50k任务实例</code></li><li>轨迹蒸馏：最终 <code>5k 轨迹数据</code></li></ul></div><img src="https://paper-assets.alphaxiv.org/figures/2504.21798v2/x10.png" style="display:block;margin:auto;" width="70%"><img src="https://paper-assets.alphaxiv.org/figures/2504.21798v2/x1.png" style="display:block;margin:auto;" width="70%"><h4 id="合成细节-agent搭环境-4策略合成bug-执行验证-合成issue" tabindex="-1">合成细节(Agent搭环境+4策略合成Bug+执行验证+合成Issue) <a class="header-anchor" href="#合成细节-agent搭环境-4策略合成bug-执行验证-合成issue" aria-label="Permalink to &quot;合成细节(Agent搭环境+4策略合成Bug+执行验证+合成Issue)&quot;">​</a></h4><div class="custom-block note"><div class="custom-block-title">SWE-Smith 任务构建方法</div><p><strong>环境构建</strong></p><ul><li>仓库筛选：24年11月 <code>PyPI下载量前5000</code>的包，去掉<code>stars&lt;1000</code>+<code>SWE重复</code>的仓库。</li><li><code>给定仓库</code>(最新commit)，<code>让SWE-Agent</code> 在100内 <code>安装环境</code>和<code>执行测试</code>。</li><li>人工验证安装和测试情况：<code>测试用例通过&gt;80%</code>，则算成功，为其构建<code>仓库级docker镜像</code>。</li></ul><p><strong>候选任务构建</strong></p><ul><li>核心：为每个仓库，通过4种策略<code>生成不同的候选任务</code>。</li><li><strong>大模型生成</strong>： <ul><li><code>给函数文档重新实现</code>：LLM-Rewrite，<code>效果很好</code></li><li><code>让LLM故意改错代码</code> ：LLM-Modify，效果最差</li></ul></li><li><strong>程序化制造Bug</strong>：<code>效果好</code>。 <ul><li>先<code>解析代码结构</code>再<code>故意破坏</code>。错误微小，难以觉察。</li></ul></li><li><strong>组合Bug</strong>：在同1文件里组合前面的多个bug</li><li><strong>PR Mirror(PR 反转)</strong>：<code>效果最好</code><ul><li>从<code>正确代码</code>-&gt;<code>错误代码</code>，在<code>现有版本</code>上模拟<code>之前的bug</code>，<code>避免重装环境</code></li><li>找到之前<code>修复bug的PR</code>，给LLM输入 <code>最新代码</code> + <code>diff文件</code>，让其<code>复原之前的Bug代码</code>。</li><li>优点：始终在<code>最新版本</code>和<code>环境</code>上工作，环境配置简单。</li></ul></li></ul><p><strong>执行验证</strong></p><ul><li>为每个候选patch，<code>执行测试</code>，仅保留能制造<code>Fail2Pass</code>的任务。</li><li><code>做2分钟耗时限制</code>：bug测试卡死或很慢，依然丢弃。</li></ul><p><strong>Issue 生成</strong></p><ul><li>难度控制住：描述很重要，不能太简单了。</li><li>LLM逆向生成： <ul><li>LLM输入：<code>diff patch</code>、<code>F2P test的源代码</code>、<code>测试失败的报错日志</code></li><li>LLM生成：Github <code>issue风格</code>的<code>Issue描述</code>。</li></ul></li></ul></div><p>LLM 生成Bug</p><img src="https://paper-assets.alphaxiv.org/figures/2504.21798v2/x11.png" style="display:block;margin:auto;" width="70%"><p>程序化制造Bug</p><img src="https://paper-assets.alphaxiv.org/figures/2504.21798v2/x12.png" style="display:block;margin:auto;" width="70%"><p>组合Bug</p><img src="https://paper-assets.alphaxiv.org/figures/2504.21798v2/x13.png" style="display:block;margin:auto;" width="70%"><p>PR Mirroring，从正确代码-&gt;错误代码，复现之前的bug</p><img src="https://paper-assets.alphaxiv.org/figures/2504.21798v2/x14.png" style="display:block;margin:auto;" width="70%"><h4 id="agentsft-数据蒸馏-1" tabindex="-1">AgentSFT 数据蒸馏 <a class="header-anchor" href="#agentsft-数据蒸馏-1" aria-label="Permalink to &quot;AgentSFT 数据蒸馏&quot;">​</a></h4><div class="custom-block info"><div class="custom-block-title">SFT 轨迹数据蒸馏</div><p><strong>概览</strong></p><ul><li>模型：GPT4o, Claude 3.7 SOnet</li><li>Agent：<code>SWE-Agent</code>，<code>ReAct</code></li><li>轮数：<code>75轮</code></li><li>任务：SWE-Smith <code>50k 任务</code></li><li>最终数据：<code>5k 轨迹</code></li></ul><p><strong>关键策略</strong></p><ul><li>任务：<code>共8k任务做蒸馏</code>，占比Smith任务 17.3% <ul><li>包含：<code>PR-Mirror</code> + <code>LLM Rewrite</code> 这两种策略产生的<code>轨迹最有效</code>。</li></ul></li><li><strong>轨迹</strong>：每任务执行3次，17k尝试，解决率36%，共6.4k轨迹。</li><li><strong>过滤</strong>：<code>去掉简单任务</code>，最终5k轨迹</li></ul></div><h3 id="实验设置-sft" tabindex="-1">实验设置(SFT) <a class="header-anchor" href="#实验设置-sft" aria-label="Permalink to &quot;实验设置(SFT)&quot;">​</a></h3><p><strong>✍️实验设置</strong></p><div class="custom-block tip"><div class="custom-block-title">实验设置</div><p><strong>基础模型</strong></p><ul><li>Qwen2.5-Coder-Instruct-7B/32B</li></ul><p><strong>训练任务/数据</strong></p><ul><li>在<code>SWE-smith任务</code>上，<code>合成的5k轨迹</code> (Claude/GPT4o)</li></ul><p><strong>评测任务/数据</strong></p><ul><li>SWE-verified(500)， SWE-light(300)，SWE-Multilingual(300)</li></ul><p><strong>算法/策略</strong></p><ul><li>SFT</li><li><code>SWE-Agent</code>：ReAct 风格，编辑文件、执行命令等。</li></ul><p><strong>超参</strong></p></div><h3 id="关键结果-swe-agent-lm-32b-sft" tabindex="-1">关键结果(SWE-Agent-LM-32B + SFT) <a class="header-anchor" href="#关键结果-swe-agent-lm-32b-sft" aria-label="Permalink to &quot;关键结果(SWE-Agent-LM-32B + SFT)&quot;">​</a></h3><p><strong>🍑关键结果</strong></p><div class="custom-block important"><div class="custom-block-title">关键结果</div><p><strong>模型效果</strong></p><ul><li>SFT后的SWE-Agent-LM-32B，<code>SWE-Verified 40.2pt</code>，<code>提升33pt</code></li><li>SWE-Agent-LM-32B 仅需24.9步，但Claude 3.7 Sonnet 需要29.1步。 <ul><li>但<code>32B容易陷入重复动作</code>。</li></ul></li></ul><p><strong>重要结论</strong></p><ul><li><code>任务实例</code>/<code>bug类型</code>/<code>仓库覆盖</code> 越多越好，任务<code>Scaling有效果</code>。<code>多样性&gt;难度</code>。</li><li><code>PR-Mirror</code>、<code>LM-Rewrite</code>、程序化制造Bug 效果都好，依次排序，LM Modify 效果最差。</li><li><code>合成Issue</code>已非常<code>接近人类真实水平</code></li><li>可针<code>对特定仓库优化</code>模型表现(Pandas,Sckikit-learn等)，对通用能力损害小</li></ul><p><strong>关键贡献</strong></p><ul><li>开源SWE-smith工具包：<code>生成任务实例</code> + <code>执行环境</code> + <code>专家轨迹数据</code></li></ul></div><h3 id="未来方向-3" tabindex="-1">未来方向 <a class="header-anchor" href="#未来方向-3" aria-label="Permalink to &quot;未来方向&quot;">​</a></h3><p><strong>⛳ 未来方向</strong></p><div class="custom-block info"><div class="custom-block-title">未来方向</div></div><h2 id="_2504-r2e-gym-34-4分" tabindex="-1">(2504) R2E-Gym(34.4分) <a class="header-anchor" href="#_2504-r2e-gym-34-4分" aria-label="Permalink to &quot;(2504) R2E-Gym(34.4分)&quot;">​</a></h2><h3 id="摘要背景" tabindex="-1">摘要背景 <a class="header-anchor" href="#摘要背景" aria-label="Permalink to &quot;摘要背景&quot;">​</a></h3><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">R2E-Gym 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2504-r2e-gym" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2504.07164?chatId=019b8c06-0a6c-7d41-b722-443e9539be96" target="_blank" rel="noreferrer">paper</a>, <a href="https://r2e-gym.github.io/" target="_blank" rel="noreferrer">r2e-gym</a>, <a href="https://huggingface.co/R2E-Gym" target="_blank" rel="noreferrer">R2E-Gym</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>自动合成SWE任务方法</code>：<code>Commit挖掘</code>+<code>测试用例生成</code>+<code>反向Issue生成</code></li><li><code>R2E-Gym 数据</code>： <code>10仓库</code>+<code>8k任务</code>+<code>3.3k蒸馏轨迹</code> ，<code>R2E-Gym Sub</code>：<code>4.5k 任务</code></li><li><code>OpenHands</code></li></ul><p><strong>模型效果(Qwen-Coder-32B + SFT)</strong></p><ul><li>SWE-Verified 达<code> 34.4分</code></li></ul><p><strong>重要结论</strong></p><ul><li><code>合成数据不输人工数据</code></li><li><code>Hybrid TTS</code> 有效果，从34.4<code>提升至51分</code>。</li></ul><p><strong>关键贡献</strong></p><ul><li></li></ul></div><p><strong>❓问题背景</strong></p><div class="custom-block warning"><div class="custom-block-title">问题背景</div><ul><li><strong>开源模型有差距</strong>：真实<code>GitHub-SWE问题</code>，<code>开源</code>显著<code>落后于闭源模型</code>(GPT-4, Claude 3.5)</li><li><strong>数据有瓶颈</strong>：缺乏<code>大规模</code>、<code>高质量</code>的<code>可执行训练环境</code><ul><li>现有数据：许多<code>无可执行环境</code>，或依赖<code>人类编写的Issue</code>和<code>测试用例</code>，<code>难以自动化扩充</code></li></ul></li><li><strong>推理扩展难题</strong>：现有验证方法 <code>基于测试执行</code> + <code>基于模型打分</code>，都<code>存在局限性</code><ul><li>缺乏对测试时计算（test-time compute）的最佳扩展策略的研究。</li></ul></li></ul></div><h3 id="r2e-gym-任务合成方法" tabindex="-1">R2E-Gym 任务合成方法 <a class="header-anchor" href="#r2e-gym-任务合成方法" aria-label="Permalink to &quot;R2E-Gym 任务合成方法&quot;">​</a></h3><p><strong>📕核心方法</strong></p><div class="custom-block note"><div class="custom-block-title">R2E-Gym SWE任务合成方法</div><p><strong>合成流程</strong></p><ul><li><p><code>挖掘Commit变更数据</code></p><ul><li>SEART 搜索 Python 仓库，</li><li>提取<code>Commit历史记录</code>，使<code>用LLM挖掘</code>出<code>有价值的变更</code>。<code>不依赖 Pull Requests</code>。</li></ul></li><li><p><code>提取或生成测试用例</code></p><ul><li>如Commit带测试用例，<code>直接提取Fail2Pass测试用例</code>。</li><li>若无测试用例，则<code>自动生成测试用例</code>。</li></ul></li><li><p><code>反向翻译生成 Issue</code></p><ul><li>利用<code>TestCases</code>+ <code>Commit</code> 来反向 <code>合成Issue</code>。</li></ul></li></ul><p><strong>最终数据集</strong></p><ul><li><code>R2E-Gym</code> <code>8.1k</code>任务，<code>subset</code> <code>4.5k</code> 任务，仅<code>10 python仓库</code>。</li></ul></div><div class="custom-block caution"><div class="custom-block-title">AgentSFT 数据蒸馏</div><ul><li>基于<code>Subset任务</code> + <code>Claude-3.5-Sonnet 做蒸馏</code> + <code>Openhands</code> + <code>R2E 环境</code></li><li>共<code>3.3k 轨迹</code>，<code>2k任务</code></li></ul></div><h3 id="hybrid-tts" tabindex="-1">Hybrid TTS <a class="header-anchor" href="#hybrid-tts" aria-label="Permalink to &quot;Hybrid TTS&quot;">​</a></h3><div class="custom-block note"><div class="custom-block-title">Hybrid TTS (执行验证+免执行验证)</div><ul><li>先基于<code>免执行验证</code>选出<code>top-n</code>，再基于<code>执行验证</code>做<code>最终排序</code>。</li></ul></div><h3 id="实验设置-sft-1" tabindex="-1">实验设置(SFT) <a class="header-anchor" href="#实验设置-sft-1" aria-label="Permalink to &quot;实验设置(SFT)&quot;">​</a></h3><p><strong>✍️实验设置</strong></p><div class="custom-block tip"><div class="custom-block-title">实验设置</div><p><strong>基础模型</strong></p><ul><li>Qwen2.5-Coder-7B, 14B, 32B</li></ul><p><strong>训练任务/数据</strong></p><ul><li>SWE任务：R2E-Gym-Subset 4.5k</li><li>SFT 数据：蒸馏<code>3.3k轨迹</code> + <code>2k任务</code>。</li></ul><p><strong>评测任务/数据</strong></p><ul><li>SWE-Verified</li></ul><p><strong>算法/策略</strong></p><ul><li><code>SFT 训练</code>，LLaMA-Factory</li><li><code>验证器训练</code>： <ul><li>Testing Agent：Qwen-Coder-32B</li><li><code>免执行验证器</code>：Qwen-Coder-14B，对轨迹做<code>二分类打分</code>。</li></ul></li></ul><p><strong>脚手架</strong></p><ul><li><code>OpenHands</code>的ReAct框架 <ul><li><code>file_editor</code>、<code>search_tool</code>、<code>execute_bash</code>、<code>submit</code></li></ul></li></ul><p><strong>超参</strong></p><ul><li>2epochs, lr=1e-5, bs=78 <code>seqlen=20k</code></li></ul></div><h3 id="关键结果-qwen2-5-coder-32b-sft" tabindex="-1">关键结果(Qwen2.5-Coder-32B + SFT) <a class="header-anchor" href="#关键结果-qwen2-5-coder-32b-sft" aria-label="Permalink to &quot;关键结果(Qwen2.5-Coder-32B + SFT)&quot;">​</a></h3><p><strong>🍑关键结果</strong></p><div class="custom-block important"><div class="custom-block-title">关键结果</div><ul><li><strong>数据扩展有效</strong><ul><li><code>SFT-32B</code> SWE-Verified达 <code>34.4% Pass@1</code>，比之前SOTA SWE-Gym提高13.8%。</li><li>证明了<code>合成数据的有效性</code>。</li><li><strong>合成数据不输人工数据</strong>： <ul><li>纯<code>合成数据</code>训练：<code>27分</code>；纯<code>人工数据</code>训练：<code>28分</code>。差不多。</li></ul></li><li>训练带思考比不带思考，提升4pt，思考34分，不带思考仅30分。</li></ul></li><li><strong>验证器性能互补</strong><ul><li>单独使用<code>基于执行</code>或<code>免执行</code>的方法，性能均在 <code>42-43%</code> 左右饱和。</li><li><code>混合验证器</code>，将性能提升至 <strong>51.0%</strong>。</li></ul></li><li><strong>计算效率</strong>： <ul><li>增加“测试 Agent”的采样次数比单纯增加“编辑 Agent”的采样次数更具计算性价比。</li></ul></li><li><strong>SOTA 表现</strong>：51分成绩使<code>开源模型</code>首次在SWE任务与专有模型(o1, Sonnet + Tools)竞争。</li></ul></div><img src="https://r2e-gym.github.io/assets/r2egym/data_size_ablation.png" style="display:block;margin:auto;" width="70%"><p><strong>⛳ 未来方向</strong></p><div class="custom-block info"><div class="custom-block-title">未来方向</div><ul><li><strong>环境构建自动化</strong>：目前的<code>依赖安装</code>和<code>环境构建</code>仍含手动步骤，未来<code>利用LLM实现全自动化</code>。</li><li><strong>更长上下文</strong>：训练仅20k/32k上下文，未来利用上下文并行训练处理更复杂的Agent。</li><li><strong>测试生成质量</strong>：进一步<code>减少</code>生成测试中的<code>毒性Toxic Tests</code>和<code>无效测试</code>，提高验证器的鲁棒性。</li></ul></div><h2 id="_2412-swe-gym-19-7分" tabindex="-1">(2412) SWE-Gym(19.7分) <a class="header-anchor" href="#_2412-swe-gym-19-7分" aria-label="Permalink to &quot;(2412) SWE-Gym(19.7分)&quot;">​</a></h2><p><strong>🌺 论文摘要</strong></p><div class="custom-block danger"><div class="custom-block-title">SWE-Gym 摘要</div><p><strong>参考链接</strong></p><ul><li><a href="https://plmsmile.github.io/posts/llm/industry/codellm/11-swe-data-series.html#_2412-swe-gym" target="_blank" rel="noreferrer">论文笔记</a>, <a href="https://www.alphaxiv.org/abs/2412.21139" target="_blank" rel="noreferrer">paper</a>, <a href="https://github.com/SWE-Gym/SWE-Gym" target="_blank" rel="noreferrer">代码</a>, <a href="https://huggingface.co/SWE-Gym" target="_blank" rel="noreferrer">SWE-Gym Data</a></li></ul><p><strong>核心方法</strong></p><ul><li><code>SWE任务构建方法</code>：<code>通过脚本直接提取PR</code>，并半手动<code>构建好环境</code>(仅覆盖11仓库)</li><li><code>SWE-Gym数据集</code>：<code>2.4k任务</code>+ <code>11仓库</code></li><li><code>OpenHands</code>，<code>Moatless</code></li></ul><p><strong>模型效果(Qwen2.5-Coder-32B + SFT)</strong></p><ul><li>SWE-Verified <code>19.7分</code>，TTS-16 达32分。</li></ul><p><strong>重要结论</strong></p><ul><li><code>Best-of-16策略</code>：20.6 -&gt; <code>32分</code>，开源模型新标杆。</li></ul></div><img src="https://arxiv.org/html/2412.21139v2/x1.png" style="display:block;margin:auto;" width="70%"><h3 id="swe-gym-任务收集构建方法" tabindex="-1">SWE-Gym 任务收集构建方法 <a class="header-anchor" href="#swe-gym-任务收集构建方法" aria-label="Permalink to &quot;SWE-Gym 任务收集构建方法&quot;">​</a></h3><div class="custom-block info"><div class="custom-block-title">SWE-Gym 任务构建方法</div><p><strong>仓库识别</strong></p><ul><li><code>用SEART搜索</code> + <code>选择python仓库</code></li><li>过滤条件：<code>PyPI 下载top-5k</code> + <code>500star</code> + <code>300行代码</code> + <code>500个pr</code> + <code>100个contributor</code></li><li>最终数据：<code>358 仓库</code>。时间：2022.07.01之前的</li></ul><p><strong>任务构建</strong></p><ul><li><p>SWE-bench的<code>提取脚本</code>：把<code>仓库PR</code> 转换成具体<code>任务实例</code></p></li><li><p>最终数据：<code>64k实例</code>，作为<code>SWE-Gym-Raw</code></p></li><li><p>每个任务：Issue、代码、解决方案，但是缺乏可执行环境。</p></li></ul><p><strong>环境构建</strong></p><ul><li>为<code>11个仓库</code> + <code>大量任务实例</code>，<code>半手动的创建环境</code></li><li>设置版本编号，按版本来构建实例环境，按版本分组。</li><li>手动查阅CI脚本，一个一个配置环境。</li></ul><p><strong>验证机制</strong></p><ul><li>Fail2Pass：<code>原始代码必须失败</code>，<code>打上patch后 必须通过</code>。</li></ul><p><strong>结果</strong></p><ul><li>200小时人工配置，6TB Docker镜像。</li></ul></div><div class="custom-block info"><div class="custom-block-title">SFT 数据蒸馏</div><ul><li>模型：GPT4o, Claude3.5-Sonnet</li><li>轮数：平均19轮</li><li>最终：off-policy 491条，on-policy 875条(微调Qwen2.5-Coder-Inst-32B)，失败的1318条。</li></ul></div><img src="https://arxiv.org/html/2412.21139v2/x2.png" style="display:block;margin:auto;" width="70%"></div></div></main><footer class="VPDocFooter" data-v-5a64a79a data-v-54a90a4a><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-54a90a4a><span class="visually-hidden" id="doc-footer-aria-label" data-v-54a90a4a>Pager</span><div class="pager" data-v-54a90a4a><!----></div><div class="pager" data-v-54a90a4a><a class="VPLink link pager-link next" href="/posts/llm/industry/codellm/10-swe-summary.html" data-v-54a90a4a><!--[--><span class="desc" data-v-54a90a4a>下一页</span><span class="title" data-v-54a90a4a>SWE 总结索引</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--[--><div class="busuanzi"> 总访客数： <span id="busuanzi_value_site_uv"></span>   ·   总访问量： <span id="busuanzi_value_site_pv"></span></div><div class="busuanzi"> PLM&#39;s Blog @ 2016 - 2026</div><!--]--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"Bt59YpwR\",\"index.md\":\"DmRGHmj9\",\"markdown-examples.md\":\"CPJSv--2\",\"posts_archive.md\":\"pZWQT7dp\",\"posts_exps_env_01-blog-env.md\":\"CcTUm41j\",\"posts_exps_env_index.md\":\"BJEZeoT-\",\"posts_exps_mind_index.md\":\"naOB3-Mb\",\"posts_llm_agent_basic_01-lhy-agent-notes.md\":\"BK_CsHNC\",\"posts_llm_agent_basic_02-evaluation-agent.md\":\"Dn02cVtP\",\"posts_llm_agent_basic_03-current-agents.md\":\"C25gmbg8\",\"posts_llm_agent_basic_04-agent-blogs.md\":\"_QbL2Plv\",\"posts_llm_agent_basic_05-comuter-agent.md\":\"D0gJpId6\",\"posts_llm_agent_basic_06-deepresearch-evaluation.md\":\"DdRDcxUJ\",\"posts_llm_agent_basic_index.md\":\"ClGtwUqg\",\"posts_llm_agent_rl_01-agent-rl.md\":\"wOF9bz66\",\"posts_llm_agent_rl_02-agent-tool.md\":\"IB_AqkSe\",\"posts_llm_agent_rl_03-agent-search.md\":\"Cim7OGxb\",\"posts_llm_agent_rl_04-agent-env.md\":\"C_O9BVi3\",\"posts_llm_agent_rl_index.md\":\"BudtoDK9\",\"posts_llm_basic_01-lm-define-information-theory.md\":\"Bl38RTdm\",\"posts_llm_basic_02-llm-components.md\":\"CGM_jRUE\",\"posts_llm_basic_03-transformer-detail.md\":\"BgLyAVDZ\",\"posts_llm_basic_04-llm-architecture.md\":\"jk4T-c21\",\"posts_llm_basic_05-llm-basic-info.md\":\"bdnM02bn\",\"posts_llm_basic_06-llm-attention.md\":\"BRJ9jR-M\",\"posts_llm_basic_07-decode.md\":\"BRytPqZo\",\"posts_llm_basic_08-llm-position-embedding.md\":\"d9dN_RuO\",\"posts_llm_basic_index.md\":\"DjX2HRXy\",\"posts_llm_industry_codellm_01-survey.md\":\"DIZQiOWr\",\"posts_llm_industry_codellm_02-eval-task-benchmark.md\":\"B-okIRAD\",\"posts_llm_industry_codellm_03-rl-task.md\":\"vQDqALFA\",\"posts_llm_industry_codellm_04-safety-code.md\":\"mkEFLxnr\",\"posts_llm_industry_codellm_05-open-codellm.md\":\"C6UU6qcZ\",\"posts_llm_industry_codellm_06-code-taskrl-reading.md\":\"BJI-7ypF\",\"posts_llm_industry_codellm_07-code-fulltrain-reading.md\":\"BjyYsnTY\",\"posts_llm_industry_codellm_08-code-pretrain-summary.md\":\"DfgRUz6a\",\"posts_llm_industry_codellm_09-swe-series.md\":\"njRlhnZH\",\"posts_llm_industry_codellm_10-swe-summary.md\":\"CmzPFSW7\",\"posts_llm_industry_codellm_11-swe-data-series.md\":\"BkWEB2x6\",\"posts_llm_industry_mainllm_01-kimi-series.md\":\"B03hc7IE\",\"posts_llm_industry_mainllm_02-deepseek-series.md\":\"B04zRtyw\",\"posts_llm_industry_mainllm_03-glm-series.md\":\"8i7VdpW8\",\"posts_llm_industry_mainllm_04-minimax-series.md\":\"DjQwki4V\",\"posts_llm_industry_mainllm_05-qwen-series.md\":\"B4DMxRmo\",\"posts_llm_industry_mainllm_06-seed-series.md\":\"BNsIBjBZ\",\"posts_llm_industry_mainllm_07-openai-series.md\":\"ZBquCwKO\",\"posts_llm_industry_mainllm_08-gemini-series.md\":\"Bdm2kU-I\",\"posts_llm_industry_mainllm_09-claude-series.md\":\"0YnEaaXS\",\"posts_llm_industry_mainllm_10-longcat-series.md\":\"DroIAFXW\",\"posts_llm_industry_mainllm_11-tencent-series.md\":\"BVkGSYEM\",\"posts_llm_industry_mainllm_12-kwai-series.md\":\"o6Hut3bE\",\"posts_llm_industry_mainllm_13-nvidia-series.md\":\"BFgUmWZk\",\"posts_llm_industry_mainllm_14-mimo-series.md\":\"DDj7XRZV\",\"posts_llm_industry_mainllm_15-skywork-series.md\":\"BMrtN_ri\",\"posts_llm_infra_01-parrallel.md\":\"2i82l-rT\",\"posts_llm_infra_02-speed-framework.md\":\"_bUH-n3t\",\"posts_llm_infra_03-inference-tech.md\":\"Hpk9YmXF\",\"posts_llm_infra_04-verl.md\":\"8XtGz01J\",\"posts_llm_infra_05-verl-practice.md\":\"CpYgON5R\",\"posts_llm_infra_06-verl-code.md\":\"D5bZg4dm\",\"posts_llm_infra_07-verl-core.md\":\"3RS___SF\",\"posts_llm_infra_08-verl-train-loop.md\":\"DzepHNlu\",\"posts_llm_rl_index.md\":\"iUgEsMU1\",\"posts_llm_rl_theory_01-reinforce-learning.md\":\"Ch0rtQCM\",\"posts_llm_rl_theory_01-rl-introduction.md\":\"CmW63EkM\",\"posts_llm_rl_theory_02-markove-process.md\":\"DVY5XgWd\",\"posts_llm_rl_theory_02-value-learning.md\":\"CeOlmbeS\",\"posts_llm_rl_theory_03-model-based-prediction-control.md\":\"BhVRmo7C\",\"posts_llm_rl_theory_03-strategy-learning.md\":\"DWNvEZXH\",\"posts_llm_rl_theory_04-model-free-prediction-control.md\":\"Cg3qo2Fk\",\"posts_llm_rl_theory_04-reinforce-conclusion-simple.md\":\"C6yTAxwQ\",\"posts_llm_rl_theory_05-dqn.md\":\"BatSdlnZ\",\"posts_llm_rl_theory_06-policy-gradient.md\":\"BXI-eY8I\",\"posts_llm_rl_theory_07-actor-critic.md\":\"B1-83sen\",\"posts_llm_rl_theory_08-deterministic-policy-gradient.md\":\"Dgvru3JE\",\"posts_llm_rl_theory_09-policy-trpo-ppo.md\":\"DU-7PSqU\",\"posts_llm_rl_theory_10-ppo-series.md\":\"B-5Hsj_J\",\"posts_llm_rl_theory_11-grpo-series.md\":\"sXtqvv0Z\",\"posts_llm_rl_theory_12-entropy.md\":\"BH45bCLH\",\"posts_llm_rl_theory_13-agentrl-algo.md\":\"EDVx1EmY\",\"posts_me.md\":\"B9W6CMag\",\"posts_olds_algo_aim2offer.md\":\"jCwzQ4BU\",\"posts_olds_algo_aim2offer2.md\":\"Ga2XS6dR\",\"posts_olds_algo_aim2offer3.md\":\"BP0rCZaJ\",\"posts_olds_algo_aim2offer4.md\":\"BXurWO8W\",\"posts_olds_algo_algorithm-dfs.md\":\"CkUFsStz\",\"posts_olds_algo_index.md\":\"CmdF0Gg2\",\"posts_olds_algo_leetcode-01.md\":\"C_2G2jJT\",\"posts_olds_algo_sort-algorithms.md\":\"tFMUVlmH\",\"posts_olds_bigdata_16-spark-baserdd.md\":\"CBxdArgI\",\"posts_olds_bigdata_17-spark-pairrdd.md\":\"C3n8_zMO\",\"posts_olds_bigdata_18-spark-sql.md\":\"tsaWQLcp\",\"posts_olds_bigdata_19-spark-programming.md\":\"DG5ZAF85\",\"posts_olds_bigdata_20-numpy.md\":\"DucCD22z\",\"posts_olds_bigdata_index.md\":\"CWrZwWPb\",\"posts_olds_dl_23-pytorch-start.md\":\"BokpNeAw\",\"posts_olds_dl_35-nerual-network-optim.md\":\"Cp2SpLoE\",\"posts_olds_dl_38-convolution.md\":\"CC_SQ57z\",\"posts_olds_dl_cs224n-assignment-1.md\":\"BZMSZqOS\",\"posts_olds_dl_cs224n-notes3-neural-networks-2.md\":\"Wd9TmSyb\",\"posts_olds_dl_cs224n-notes3-neural-networks.md\":\"CDZWc4X6\",\"posts_olds_dl_cs231n-linear-notes.md\":\"DLRyzcwJ\",\"posts_olds_dl_index.md\":\"C1dyLGNS\",\"posts_olds_dl_rnn.md\":\"CwYYWZ7N\",\"posts_olds_env_09-linux-notes.md\":\"CyjifvcN\",\"posts_olds_env_12-ide-envs.md\":\"YeELWzR2\",\"posts_olds_env_13-old-blog-problems.md\":\"qamPyucB\",\"posts_olds_env_24-hexo-problems.md\":\"CzxhyjW1\",\"posts_olds_env_index.md\":\"DQpgTXVj\",\"posts_olds_ml_10-trees.md\":\"BkNaj6fL\",\"posts_olds_ml_14-em.md\":\"DIufCP0H\",\"posts_olds_ml_21-lr.md\":\"C-1ms511\",\"posts_olds_ml_22-ml-ch03-bayes.md\":\"Q_M6Gn-a\",\"posts_olds_ml_27-svm-notes.md\":\"C96WS6CL\",\"posts_olds_ml_28-ml-interview-notes.md\":\"0bgezw3n\",\"posts_olds_ml_29-desicion-tree.md\":\"CtwmlbWl\",\"posts_olds_ml_crf.md\":\"CEE5oTqd\",\"posts_olds_ml_index.md\":\"BLMEziB1\",\"posts_olds_ml_maxentmodel.md\":\"CSbOumJx\",\"posts_olds_ml_pgm-01.md\":\"BTwybTMD\",\"posts_olds_nlp_11-nlp-labels.md\":\"Cl0Lb8OT\",\"posts_olds_nlp_25-google-nmt.md\":\"BOM-hoYN\",\"posts_olds_nlp_26-wordpieacemodel.md\":\"Q8-L5j15\",\"posts_olds_nlp_30-dynamic-memory-network.md\":\"wp5ucVxW\",\"posts_olds_nlp_31-co-attention-vqa.md\":\"lzwuVKG5\",\"posts_olds_nlp_32-dynamic-coattention-network.md\":\"Bxl4ABWd\",\"posts_olds_nlp_33-attention-summary.md\":\"DT6np0xG\",\"posts_olds_nlp_36-alime-chat.md\":\"Cu2xkcdT\",\"posts_olds_nlp_39-squard-models.md\":\"B1L3iDEv\",\"posts_olds_nlp_45-match-lstm.md\":\"DkpQKM5B\",\"posts_olds_nlp_46-rnet-selfmatch.md\":\"CzGHQ-PZ\",\"posts_olds_nlp_47-bidaf.md\":\"DFJaLh2v\",\"posts_olds_nlp_48-attention-is-all-you-need.md\":\"BY6XvFQ5\",\"posts_olds_nlp_49-qanet.md\":\"BPKWHzTf\",\"posts_olds_nlp_50-elmo.md\":\"WuOt1elN\",\"posts_olds_nlp_51-opengpt.md\":\"B9pQ3h5W\",\"posts_olds_nlp_52-bert.md\":\"5q3t5Qqh\",\"posts_olds_nlp_53-mrc-brief.md\":\"D9CkYrea\",\"posts_olds_nlp_54-mrc-models.md\":\"Ak4bDf1J\",\"posts_olds_nlp_attention-based-nmt.md\":\"BHef6tM6\",\"posts_olds_nlp_attention-model.md\":\"-xLhHhJE\",\"posts_olds_nlp_cs224n-lecture2-word2vec.md\":\"_MmBTADr\",\"posts_olds_nlp_cs224n-notes1-word2vec.md\":\"DXSi5KGh\",\"posts_olds_nlp_index.md\":\"Bfo4Lwn3\",\"posts_olds_nlp_nlp-notes.md\":\"BUmOZxzi\",\"posts_olds_nlp_nmt.md\":\"DUgy6vHF\",\"posts_olds_nlp_subword-units.md\":\"fR_3dRXr\",\"posts_olds_nlp_word2vec-math.md\":\"agvHiE1x\",\"posts_olds_nlp_word2vec.md\":\"D2TKUstm\",\"posts_olds_other_15-cpp-pointer-object-reference.md\":\"BKRTr8QZ\",\"posts_olds_other_index.md\":\"C1T-ubsz\",\"posts_olds_rl_37-reinforce-learning.md\":\"Cf2nny8k\",\"posts_olds_rl_40-value-learning.md\":\"DcnHvvpH\",\"posts_olds_rl_41-strategy-learning.md\":\"CStMrB-q\",\"posts_olds_rl_42-reinforce-conclusion-simple.md\":\"flRZUmJZ\",\"posts_olds_rl_43-intent-detection-slot-filling.md\":\"DcAUbaZU\",\"posts_olds_rl_44-reinforce-nlp.md\":\"Br2ShITL\",\"posts_olds_rl_index.md\":\"CUsxM3zO\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"📚 plmblog\",\"description\":\"记录一些学习笔记。\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"🐲LLM\",\"items\":[{\"text\":\"Basic\",\"items\":[{\"text\":\"🦋基础知识\",\"link\":\"/posts/llm/basic/01-lm-define-information-theory\"},{\"text\":\"🛠基建框架\",\"link\":\"/posts/llm/infra/01-parrallel\"}]},{\"text\":\"强化学习\",\"items\":[{\"text\":\"🎓RL理论基础\",\"link\":\"/posts/llm/rl/theory/01-reinforce-learning\"},{\"text\":\"🚄Agent-RL\",\"link\":\"/posts/llm/agent/rl/02-agent-tool\"}]},{\"text\":\"行业方向\",\"items\":[{\"text\":\"🚀主流模型\",\"link\":\"/posts/llm/industry/mainllm/01-kimi-series\"},{\"text\":\"💻代码模型\",\"link\":\"/posts/llm/industry/codellm/01-survey\"}]},{\"text\":\"Agent\",\"items\":[{\"text\":\"🤖概念及应用\",\"link\":\"/posts/llm/agent/basic\"}]}]},{\"text\":\"📙旧文章\",\"items\":[{\"text\":\"🍓NLP\",\"items\":[{\"text\":\"自然语言处理\",\"link\":\"/posts/olds/nlp\"}]},{\"text\":\"🍑基础知识\",\"items\":[{\"text\":\"深度学习\",\"link\":\"/posts/olds/dl\"},{\"text\":\"强化学习\",\"link\":\"/posts/olds/rl\"},{\"text\":\"机器学习\",\"link\":\"/posts/olds/ml\"}]},{\"text\":\"🍎算法\",\"items\":[{\"text\":\"算法题\",\"link\":\"/posts/olds/algo/\"},{\"text\":\"大数据\",\"link\":\"/posts/olds/bigdata/\"}]},{\"text\":\"🍒其他\",\"items\":[{\"text\":\"环境搭建\",\"link\":\"/posts/olds/env/\"},{\"text\":\"其他\",\"link\":\"/posts/olds/other/\"}]}]},{\"text\":\"经验\",\"items\":[{\"text\":\"环境\",\"items\":[{\"text\":\"环境搭建\",\"link\":\"/posts/exps/env/01-blog-env\"}]},{\"text\":\"心得\",\"items\":[{\"text\":\"心得体会\",\"link\":\"/posts/exps/mind\"}]}]},{\"text\":\"归档\",\"link\":\"/posts/archive.md\"},{\"text\":\"关于我\",\"link\":\"/posts/me\"}],\"outline\":{\"level\":[1,4],\"label\":\"当前页大纲\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/plmsmile\"}],\"search\":{\"provider\":\"local\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"sidebar\":{\"/posts/olds/nlp/\":{\"base\":\"/posts/olds/nlp/\",\"items\":[{\"text\":\"NLP\",\"items\":[{\"text\":\"机器阅读(二)--模型(未完成)\",\"link\":\"54-mrc-models\"},{\"text\":\"机器阅读(一)--整体概述\",\"link\":\"53-mrc-brief\"},{\"text\":\"BERT 详解\",\"link\":\"52-bert\"},{\"text\":\"OpenAI GPT：Improving Language Understanding by Generative Pre-Training\",\"link\":\"51-opengpt\"},{\"text\":\"ELMo：Deep Contextualized Word Representations\",\"link\":\"50-elmo\"},{\"text\":\"QANet\",\"link\":\"49-qanet\"},{\"text\":\"Transformer\",\"link\":\"48-attention-is-all-you-need\"},{\"text\":\"Bidirectional Attention Flow\",\"link\":\"47-bidaf\"},{\"text\":\"R-Net (Gated Self-Matching Networks)\",\"link\":\"46-rnet-selfmatch\"},{\"text\":\"Match-LSTM and Answer Pointer\",\"link\":\"45-match-lstm\"},{\"text\":\"阅读理解模型总结\",\"link\":\"39-squard-models\"},{\"text\":\"阿里小蜜论文\",\"link\":\"36-alime-chat\"},{\"text\":\"各种注意力总结\",\"link\":\"33-attention-summary\"},{\"text\":\"Dynamic Coattention Network (Plus)\",\"link\":\"32-dynamic-coattention-network\"},{\"text\":\"协同注意力简介\",\"link\":\"31-co-attention-vqa\"},{\"text\":\"使用Dynamic Memory Network实现一个简单QA\",\"link\":\"30-dynamic-memory-network\"},{\"text\":\"词性标注和句法依存的表示符号\",\"link\":\"11-nlp-labels\"},{\"text\":\"Word2vec之总体介绍\",\"link\":\"cs224n-notes1-word2vec\"},{\"text\":\"Word2vec之数学模型\",\"link\":\"word2vec-math\"},{\"text\":\"Word2vec之公式推导笔记\",\"link\":\"cs224n-lecture2-word2vec\"},{\"text\":\"subword-units\",\"link\":\"subword-units\"},{\"text\":\"Wordpiece模型\",\"link\":\"26-wordpieacemodel\"},{\"text\":\"谷歌RNN翻译模型\",\"link\":\"25-google-nmt\"},{\"text\":\"机器翻译注意力机制及其PyTorch实现\",\"link\":\"Attention-based-NMT\"},{\"text\":\"图文介绍RNN注意力机制\",\"link\":\"attention-model\"},{\"text\":\"最初RNN神经翻译简略笔记\",\"link\":\"NMT\"},{\"text\":\"语言模型和平滑方法\",\"link\":\"nlp-notes\"},{\"text\":\"利用tensorflow实现简版word2vec\",\"link\":\"word2vec\"}]}]},\"/posts/olds/dl/\":{\"base\":\"/posts/olds/dl/\",\"items\":[{\"text\":\"DL\",\"items\":[{\"text\":\"卷积神经网络总结\",\"link\":\"38-convolution\"},{\"text\":\"网络优化\",\"link\":\"35-nerual-network-optim\"},{\"text\":\"cs224n作业一\",\"link\":\"cs224n-assignment-1\"},{\"text\":\"cs231n线性分类器和损失函数\",\"link\":\"cs231n-linear-notes\"},{\"text\":\"神经网络-过拟合-预处理-BN\",\"link\":\"cs224n-notes3-neural-networks-2\"},{\"text\":\"神经网络基础-反向传播-激活函数\",\"link\":\"cs224n-notes3-neural-networks\"},{\"text\":\"循环神经网络\",\"link\":\"rnn\"},{\"text\":\"PyTorch快速上手\",\"link\":\"23-pytorch-start\"}]}]},\"/posts/olds/rl/\":{\"base\":\"/posts/olds/rl/\",\"items\":[{\"text\":\"RL\",\"items\":[{\"text\":\"强化学习在NLP中的应用\",\"link\":\"44-reinforce-nlp\"},{\"text\":\"意图识别和槽填充\",\"link\":\"43-intent-detection-slot-filling\"},{\"text\":\"强化学习算法小结\",\"link\":\"42-reinforce-conclusion-simple\"},{\"text\":\"基于策略函数的学习方法\",\"link\":\"41-strategy-learning\"},{\"text\":\"基于值函数的学习\",\"link\":\"40-value-learning\"},{\"text\":\"强化学习\",\"link\":\"37-reinforce-learning\"}]}]},\"/posts/olds/ml/\":{\"base\":\"/posts/olds/ml/\",\"items\":[{\"text\":\"ML\",\"items\":[{\"text\":\"决策树笔记\",\"link\":\"29-desicion-tree\"},{\"text\":\"机器学习知识点汇总整理\",\"link\":\"28-ml-interview-notes\"},{\"text\":\"SVM笔记\",\"link\":\"27-svm-notes\"},{\"text\":\"树的总结\",\"link\":\"10-trees\"},{\"text\":\"条件随机场\",\"link\":\"crf\"},{\"text\":\"最大熵模型\",\"link\":\"maxentmodel\"},{\"text\":\"线性回归和逻辑回归\",\"link\":\"21-lr\"},{\"text\":\"最大期望算法\",\"link\":\"14-em\"},{\"text\":\"马尔可夫模型\",\"link\":\"pgm-01\"},{\"text\":\"朴素贝叶斯算法及其代码实现\",\"link\":\"22-ml-ch03-bayes\"}]}]},\"/posts/olds/algo/\":{\"base\":\"/posts/olds/algo/\",\"items\":[{\"text\":\"ALGO\",\"items\":[{\"text\":\"剑指offer4(51-64)\",\"link\":\"aim2offer4\"},{\"text\":\"剑指offer3(21-40)\",\"link\":\"aim2offer3\"},{\"text\":\"数据结构之搜索算法\",\"link\":\"algorithm-dfs\"},{\"text\":\"leetcode-01\",\"link\":\"leetcode-01\"},{\"text\":\"剑指offer(11-20)\",\"link\":\"aim2offer2\"},{\"text\":\"排序算法总结\",\"link\":\"sort-algorithms\"},{\"text\":\"剑指Offer(1-10)\",\"link\":\"aim2offer\"}]}]},\"/posts/olds/bigdata/\":{\"base\":\"/posts/olds/bigdata/\",\"items\":[{\"text\":\"BIG Data\",\"items\":[{\"text\":\"NumPy\",\"link\":\"20-numpy\"},{\"text\":\"Spark基础编程核心思想介绍\",\"link\":\"19-spark-programming\"},{\"text\":\"Spark-SQL的简略笔记\",\"link\":\"18-spark-sql\"},{\"text\":\"Spark键值对RDD的常用API\",\"link\":\"17-spark-pairrdd\"},{\"text\":\"Spark基础RDD的常用API\",\"link\":\"16-Spark-BaseRDD\"}]}]},\"/posts/olds/env/\":{\"base\":\"/posts/olds/env/\",\"items\":[{\"text\":\"环境搭建\",\"items\":[{\"text\":\"IDE配置\",\"link\":\"12-ide-envs\"},{\"text\":\"Linux使用笔记\",\"link\":\"09-linux-notes\"},{\"text\":\"博客搭建及相关问题\",\"link\":\"24-hexo-problems\"},{\"text\":\"旧版博客搭建过程及其问题\",\"link\":\"13-old-blog-problems\"}]}]},\"/posts/olds/other/\":{\"base\":\"/posts/olds/other/\",\"items\":[{\"text\":\"其他\",\"items\":[{\"text\":\"C++类对象和指针的区别\",\"link\":\"15-cpp-pointer-object-reference\"}]}]},\"/posts/llm/agent/rl/\":{\"base\":\"/posts/llm/agent/rl/\",\"items\":[{\"text\":\"Agent-RL\",\"items\":[{\"text\":\"Agent-Interaction-RL 笔记\",\"link\":\"04-agent-env\"},{\"text\":\"Agent-Search-RL 笔记\",\"link\":\"03-agent-search\"},{\"text\":\"Agent-Tool-RL 笔记\",\"link\":\"02-agent-tool\"},{\"text\":\"Agent-RL 综述型笔记\",\"link\":\"01-agent-rl\"}]}]},\"/posts/llm/agent/basic/\":{\"base\":\"/posts/llm/agent/basic/\",\"items\":[{\"text\":\"Agent-基础\",\"items\":[{\"text\":\"DeepResearch 评估\",\"link\":\"06-deepresearch-evaluation\"},{\"text\":\"Computer-Agent\",\"link\":\"05-comuter-agent\"},{\"text\":\"Agent 思考性文章\",\"link\":\"04-agent-blogs\"},{\"text\":\"一些流行的Agents\",\"link\":\"03-current-agents\"},{\"text\":\"Agent 评估 Benchmarks\",\"link\":\"02-evaluation-agent\"},{\"text\":\"Agent基础概念 (李宏毅笔记)\",\"link\":\"01-lhy-agent-notes\"}]}]},\"/posts/llm/rl/theory/\":{\"base\":\"/posts/llm/rl/theory/\",\"items\":[{\"text\":\"RL-Theory\",\"items\":[{\"text\":\"Agent-RL 相关算法\",\"link\":\"13-agentrl-algo\"},{\"text\":\"熵和RL相关文章\",\"link\":\"12-entropy\"},{\"text\":\"GRPO 改进系列\",\"link\":\"11-grpo-series\"},{\"text\":\"PPO 改进系列\",\"link\":\"10-ppo-series\"},{\"text\":\"典型策略提升方法：TRPO+PPO+DPO+GRPO\",\"link\":\"09-policy-trpo-ppo\"},{\"text\":\"确定性策略梯度\",\"link\":\"08-deterministic-policy-gradient\"},{\"text\":\"Actor-Critic 算法\",\"link\":\"07-actor-critic\"},{\"text\":\"策略梯度算法\",\"link\":\"06-policy-gradient\"},{\"text\":\"DQN算法及进阶\",\"link\":\"05-dqn\"},{\"text\":\"免模型预测和控制\",\"link\":\"04-model-free-prediction-control\"},{\"text\":\"有模型预测和控制\",\"link\":\"03-model-based-prediction-control\"},{\"text\":\"马尔可夫决策过程\",\"link\":\"02-markove-process\"},{\"text\":\"强化学习基本概念\",\"link\":\"01-rl-introduction\"},{\"text\":\"(18年笔记)强化学习算法小结\",\"link\":\"04-reinforce-conclusion-simple\"},{\"text\":\"(18年笔记)基于策略函数的学习方法\",\"link\":\"03-strategy-learning\"},{\"text\":\"(18年笔记)基于值函数的学习\",\"link\":\"02-value-learning\"},{\"text\":\"(18年笔记)强化学习基础\",\"link\":\"01-reinforce-learning\"}]}]},\"/posts/llm/rl/rlhf/\":{\"base\":\"/posts/llm/rl/rlhf/\",\"items\":[{\"text\":\"RLHF\",\"items\":[]}]},\"/posts/llm/rl/o1llm/\":{\"base\":\"/posts/llm/rl/o1llm/\",\"items\":[{\"text\":\"推理模型\",\"items\":[]}]},\"/posts/llm/industry/mainllm/\":{\"base\":\"/posts/llm/industry/mainllm/\",\"items\":[{\"text\":\"🚀主流模型\",\"items\":[{\"text\":\"快手系列\",\"link\":\"12-kwai-series\"},{\"text\":\"腾讯系列\",\"link\":\"11-tencent-series\"},{\"text\":\"Claude 系列\",\"link\":\"09-claude-series\"},{\"text\":\"LongCat 系列\",\"link\":\"10-longcat-series\"},{\"text\":\"Gemini 系列\",\"link\":\"08-gemini-series\"},{\"text\":\"Seed 系列\",\"link\":\"06-seed-series\"},{\"text\":\"OpenAI 系列\",\"link\":\"07-openai-series\"},{\"text\":\"Qwen 系列\",\"link\":\"05-qwen-series\"},{\"text\":\"MiniMax 系列\",\"link\":\"04-minimax-series\"},{\"text\":\"GLM 系列\",\"link\":\"03-glm-series\"},{\"text\":\"DeepSeek 系列\",\"link\":\"02-deepseek-series\"},{\"text\":\"Kimi 系列\",\"link\":\"01-kimi-series\"},{\"text\":\"SkyWork 系列\",\"link\":\"15-skywork-series\"},{\"text\":\"小米系列\",\"link\":\"14-mimo-series\"},{\"text\":\"Nvidia 系列\",\"link\":\"13-nvidia-series\"}]}]},\"/posts/llm/industry/codellm/\":{\"base\":\"/posts/llm/industry/codellm/\",\"items\":[{\"text\":\"💻代码模型\",\"items\":[{\"text\":\"SWE 合成数据 系列\",\"link\":\"11-swe-data-series\"},{\"text\":\"SWE 总结索引\",\"link\":\"10-swe-summary\"},{\"text\":\"Code 全训练 论文阅读\",\"link\":\"07-code-fulltrain-reading\"},{\"text\":\"Code TaskRL 论文阅读\",\"link\":\"06-code-taskrl-reading\"},{\"text\":\"CodeLLM 索引简记\",\"link\":\"05-open-codellm\"},{\"text\":\"Code 安全相关\",\"link\":\"04-safety-code\"},{\"text\":\"Code RL 任务\",\"link\":\"03-rl-task\"},{\"text\":\"Code 任务Bench相关\",\"link\":\"02-eval-task-benchmark\"},{\"text\":\"Code Survey\",\"link\":\"01-survey\"},{\"text\":\"SWE 训练方法 系列\",\"link\":\"09-swe-series\"},{\"text\":\"Code 预训练相关\",\"link\":\"08-code-pretrain-summary\"}]}]},\"/posts/llm/basic/\":{\"base\":\"/posts/llm/basic/\",\"items\":[{\"text\":\"LLM-basic\",\"items\":[{\"text\":\"LLM位置编码和长度外推系列\",\"link\":\"08-llm-position-embedding\"},{\"text\":\"LLM 解码相关\",\"link\":\"07-decode\"},{\"text\":\"LLM Attention 系列\",\"link\":\"06-llm-attention\"},{\"text\":\"LLM 基础知识\",\"link\":\"05-llm-basic-info\"},{\"text\":\"LLM 架构相关\",\"link\":\"04-llm-architecture\"},{\"text\":\"Transformer细节\",\"link\":\"03-transformer-detail\"},{\"text\":\"语言模型重要组件\",\"link\":\"02-llm-components\"},{\"text\":\"语言模型定义及信息理论\",\"link\":\"01-lm-define-information-theory\"}]}]},\"/posts/llm/infra/\":{\"base\":\"/posts/llm/infra/\",\"items\":[{\"text\":\"🛠LLM-基建框架\",\"items\":[{\"text\":\"Verl 训练流程源代码阅读\",\"link\":\"08-verl-train-loop\"},{\"text\":\"Verl 有趣的功能\",\"link\":\"07-verl-core\"},{\"text\":\"Verl AgentLoop Rollout 相关\",\"link\":\"06-verl-code\"},{\"text\":\"Verl 常见参数配置和理解\",\"link\":\"05-verl-practice\"},{\"text\":\"Verl 早期概念型学习文章\",\"link\":\"04-verl\"},{\"text\":\"推理优化技术\",\"link\":\"03-inference-tech\"},{\"text\":\"分布式训练框架\",\"link\":\"02-speed-framework\"},{\"text\":\"分布式并行策略\",\"link\":\"01-parrallel\"}]}]},\"/posts/exps/env/\":{\"base\":\"/posts/exps/env/\",\"items\":[{\"text\":\"环境搭建\",\"items\":[{\"text\":\"环境搭建的一些坑\",\"link\":\"01-blog-env\"}]}]},\"/posts/exps/mind/\":{\"base\":\"/posts/exps/mind/\",\"items\":[{\"text\":\"心得体会\",\"items\":[]}]}}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>
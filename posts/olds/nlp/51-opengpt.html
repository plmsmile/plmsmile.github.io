<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenAI GPT：Improving Language Understanding by Generative Pre-Training | 📚 plmblog</title>
    <meta name="description" content="记录一些学习笔记。">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.B63gMKix.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.S_ESFZGl.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.CgdRTeBV.js">
    <link rel="modulepreload" href="/assets/chunks/framework.CvbyeFFO.js">
    <link rel="modulepreload" href="/assets/posts_olds_nlp_51-opengpt.md.DI_-PibW.lean.js">
    <link rel="icon" href="/plm.png">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-cecb633e><!--[--><!--]--><!--[--><span tabindex="-1" data-v-c979f278></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-c979f278>Skip to content</a><!--]--><!----><header class="VPNav" data-v-cecb633e data-v-0ad68676><div class="VPNavBar" data-v-0ad68676 data-v-ce71e4ef><div class="wrapper" data-v-ce71e4ef><div class="container" data-v-ce71e4ef><div class="title" data-v-ce71e4ef><div class="VPNavBarTitle has-sidebar" data-v-ce71e4ef data-v-7e906684><a class="title" href="/" data-v-7e906684><!--[--><!--]--><!----><span data-v-7e906684>📚 plmblog</span><!--[--><!--]--></a></div></div><div class="content" data-v-ce71e4ef><div class="content-body" data-v-ce71e4ef><!--[--><!--]--><div class="VPNavBarSearch search" data-v-ce71e4ef><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-ce71e4ef data-v-e0cd9371><span id="main-nav-aria-label" class="visually-hidden" data-v-e0cd9371> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-e0cd9371 data-v-4aa19863><!--[--><span data-v-4aa19863>首页</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e0cd9371 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-ffaaba87><span class="text" data-v-ffaaba87><!----><span data-v-ffaaba87>🐲LLM</span><span class="vpi-chevron-down text-icon" data-v-ffaaba87></span></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><div class="items" data-v-798b97ca><!--[--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>Basic</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/basic.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🦋基础</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>Agent</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/agent/basic.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🤖概念及应用</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/agent/rl/02-agent-tool.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🚄Agent-RL</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>rl</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/llm/rl.html" data-v-dc987abe><!--[--><span data-v-dc987abe>🐼r1相关</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e0cd9371 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-ffaaba87><span class="text" data-v-ffaaba87><!----><span data-v-ffaaba87>📙旧文章</span><span class="vpi-chevron-down text-icon" data-v-ffaaba87></span></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><div class="items" data-v-798b97ca><!--[--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍓NLP</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/nlp.html" data-v-dc987abe><!--[--><span data-v-dc987abe>自然语言处理</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍑基础知识</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/dl.html" data-v-dc987abe><!--[--><span data-v-dc987abe>深度学习</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/rl.html" data-v-dc987abe><!--[--><span data-v-dc987abe>强化学习</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/ml.html" data-v-dc987abe><!--[--><span data-v-dc987abe>机器学习</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍎算法</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/algo/" data-v-dc987abe><!--[--><span data-v-dc987abe>算法题</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/bigdata/" data-v-dc987abe><!--[--><span data-v-dc987abe>大数据</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>🍒其他</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/env/" data-v-dc987abe><!--[--><span data-v-dc987abe>环境搭建</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/olds/other/" data-v-dc987abe><!--[--><span data-v-dc987abe>其他</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e0cd9371 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-ffaaba87><span class="text" data-v-ffaaba87><!----><span data-v-ffaaba87>经验</span><span class="vpi-chevron-down text-icon" data-v-ffaaba87></span></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><div class="items" data-v-798b97ca><!--[--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>环境</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/exps/env/01-blog-env.html" data-v-dc987abe><!--[--><span data-v-dc987abe>环境搭建</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-798b97ca data-v-a2651156><p class="title" data-v-a2651156>心得</p><!--[--><!--[--><div class="VPMenuLink" data-v-a2651156 data-v-dc987abe><a class="VPLink link" href="/posts/exps/mind.html" data-v-dc987abe><!--[--><span data-v-dc987abe>心得体会</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/posts/archive.html" tabindex="0" data-v-e0cd9371 data-v-4aa19863><!--[--><span data-v-4aa19863>归档</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/posts/me.html" tabindex="0" data-v-e0cd9371 data-v-4aa19863><!--[--><span data-v-4aa19863>关于我</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-ce71e4ef data-v-b59ebfac><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-b59ebfac data-v-809b7594 data-v-3591f5e2><span class="check" data-v-3591f5e2><span class="icon" data-v-3591f5e2><!--[--><span class="vpi-sun sun" data-v-809b7594></span><span class="vpi-moon moon" data-v-809b7594></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-ce71e4ef data-v-e0f2db57 data-v-7a4bfff1><!--[--><a class="VPSocialLink no-icon" href="https://github.com/plmsmile" aria-label="github" target="_blank" rel="noopener" data-v-7a4bfff1 data-v-8e5dce54><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-ce71e4ef data-v-8897e953 data-v-ffaaba87><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-ffaaba87><span class="vpi-more-horizontal icon" data-v-ffaaba87></span></button><div class="menu" data-v-ffaaba87><div class="VPMenu" data-v-ffaaba87 data-v-798b97ca><!----><!--[--><!--[--><!----><div class="group" data-v-8897e953><div class="item appearance" data-v-8897e953><p class="label" data-v-8897e953>Appearance</p><div class="appearance-action" data-v-8897e953><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-8897e953 data-v-809b7594 data-v-3591f5e2><span class="check" data-v-3591f5e2><span class="icon" data-v-3591f5e2><!--[--><span class="vpi-sun sun" data-v-809b7594></span><span class="vpi-moon moon" data-v-809b7594></span><!--]--></span></span></button></div></div></div><div class="group" data-v-8897e953><div class="item social-links" data-v-8897e953><div class="VPSocialLinks social-links-list" data-v-8897e953 data-v-7a4bfff1><!--[--><a class="VPSocialLink no-icon" href="https://github.com/plmsmile" aria-label="github" target="_blank" rel="noopener" data-v-7a4bfff1 data-v-8e5dce54><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-ce71e4ef data-v-37e0f734><span class="container" data-v-37e0f734><span class="top" data-v-37e0f734></span><span class="middle" data-v-37e0f734></span><span class="bottom" data-v-37e0f734></span></span></button></div></div></div></div><div class="divider" data-v-ce71e4ef><div class="divider-line" data-v-ce71e4ef></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-cecb633e data-v-1b409c8b><div class="container" data-v-1b409c8b><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-1b409c8b><span class="vpi-align-left menu-icon" data-v-1b409c8b></span><span class="menu-text" data-v-1b409c8b>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-1b409c8b data-v-a203161a><button data-v-a203161a>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-cecb633e data-v-18f7b5ca><div class="curtain" data-v-18f7b5ca></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18f7b5ca><span class="visually-hidden" id="sidebar-aria-label" data-v-18f7b5ca> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-7f5b9a39><section class="VPSidebarItem level-0 has-active" data-v-7f5b9a39 data-v-a4affe07><div class="item" role="button" tabindex="0" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><h2 class="text" data-v-a4affe07>NLP</h2><!----></div><div class="items" data-v-a4affe07><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/54-mrc-models.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>机器阅读(二)--模型(未完成)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/53-mrc-brief.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>机器阅读(一)--整体概述</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/52-bert.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>BERT 详解</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/51-opengpt.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>OpenAI GPT：Improving Language Understanding by Generative Pre-Training</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/50-elmo.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>ELMo：Deep Contextualized Word Representations</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/49-qanet.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>QANet</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/48-attention-is-all-you-need.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Transformer</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/47-bidaf.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Bidirectional Attention Flow</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/46-rnet-selfmatch.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>R-Net (Gated Self-Matching Networks)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/45-match-lstm.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Match-LSTM and Answer Pointer</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/39-squard-models.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>阅读理解模型总结</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/36-alime-chat.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>阿里小蜜论文</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/33-attention-summary.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>各种注意力总结</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/32-dynamic-coattention-network.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Dynamic Coattention Network (Plus)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/31-co-attention-vqa.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>协同注意力简介</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/30-dynamic-memory-network.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>使用Dynamic Memory Network实现一个简单QA</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/11-nlp-labels.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>词性标注和句法依存的表示符号</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/word2vec-math.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Word2vec之数学模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/subword-units.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>subword-units</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/26-wordpieacemodel.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>Wordpiece模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/25-google-nmt.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>谷歌RNN翻译模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/Attention-based-NMT.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>机器翻译注意力机制及其PyTorch实现</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/NMT.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>最初RNN神经翻译简略笔记</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/nlp-notes.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>语言模型和平滑方法</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4affe07 data-v-a4affe07><div class="item" data-v-a4affe07><div class="indicator" data-v-a4affe07></div><a class="VPLink link link" href="/posts/olds/nlp/word2vec.html" data-v-a4affe07><!--[--><p class="text" data-v-a4affe07>利用tensorflow实现简版word2vec</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-cecb633e data-v-53a9cb18><div class="VPDoc has-sidebar has-aside" data-v-53a9cb18 data-v-5a64a79a><!--[--><!--]--><div class="container" data-v-5a64a79a><div class="aside" data-v-5a64a79a><div class="aside-curtain" data-v-5a64a79a></div><div class="aside-container" data-v-5a64a79a><div class="aside-content" data-v-5a64a79a><div class="VPDocAside" data-v-5a64a79a data-v-f8ea3c28><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-f8ea3c28 data-v-b94d89ac><div class="content" data-v-b94d89ac><div class="outline-marker" data-v-b94d89ac></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b94d89ac>当前页大纲</div><ul class="VPDocOutlineItem root" data-v-b94d89ac data-v-80b46526><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-f8ea3c28></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-5a64a79a><div class="content-container" data-v-5a64a79a><!--[--><!--[--><!--[--><article class="post-header" data-v-fb47b20c><h1 class="title" data-v-fb47b20c>OpenAI GPT：Improving Language Understanding by Generative Pre-Training</h1><div class="stats-container" data-v-fb47b20c><div class="stat-divider" data-v-fb47b20c></div><div class="stat-item" data-v-fb47b20c> 📅 发表于 <span class="stat-text" data-v-fb47b20c>2018/12/14</span></div><div class="stat-item" data-v-fb47b20c> 🔄 更新于 <span class="stat-text" data-v-fb47b20c>2018/12/14</span></div><div class="stat-divider" data-v-fb47b20c></div><div class="stat-item" data-v-fb47b20c> 👁️ <span class="stat-text" data-v-fb47b20c>加载中... 次访问</span><span id="busuanzi_value_page_pv" style="display:none;" data-v-fb47b20c></span></div><div class="stat-divider" data-v-fb47b20c></div><div class="stat-item" data-v-fb47b20c> 📝 <span class="stat-text" data-v-fb47b20c>0 字</span></div><div class="stat-divider" data-v-fb47b20c></div><div class="stat-item" data-v-fb47b20c> ⏳ <span class="stat-text" data-v-fb47b20c>0 分钟</span></div><div class="stat-divider" data-v-fb47b20c></div></div><div class="tag-group" data-v-fb47b20c><!--[--><div class="category-item" data-v-fb47b20c>论文笔记</div><!--]--><!--[--><div class="tag-item" data-v-fb47b20c> #transformer</div><div class="tag-item" data-v-fb47b20c> #语言模型</div><div class="tag-item" data-v-fb47b20c> #迁移学习</div><!--]--></div></article><!--]--><!--]--><!--]--><main class="main" data-v-5a64a79a><div style="position:relative;" class="vp-doc _posts_olds_nlp_51-opengpt" data-v-5a64a79a><div><blockquote><p>使用Transformer(Decoder)预训练单向语言模型，再进行有监督数据进行特定任务finetune</p></blockquote><img src="" style="display:block;margin:auto;" width="60%"><h1 id="背景" tabindex="-1">背景 <a class="header-anchor" href="#背景" aria-label="Permalink to &quot;背景&quot;">​</a></h1><h2 id="问题提出" tabindex="-1">问题提出 <a class="header-anchor" href="#问题提出" aria-label="Permalink to &quot;问题提出&quot;">​</a></h2><p>NLP中，无标注语料很多，有标注的数据很少。</p><ul><li>很多任务不能从头开始训练（标注数据太少），需要<strong>减轻对标注数据的依赖</strong></li><li>在大规模无标注语料中预训练的语言模型可以提升很多效果</li><li>从无标注数据中学习一个<code>good representations</code>，很流行且有效果</li></ul><p>从无标注数据中学习到词级以上的意义的难点：</p><ul><li>没有一个有效的优化目标函数</li><li>对学习到的<code>representations</code>没有通用的有效的迁移方法</li></ul><h2 id="半监督学习" tabindex="-1">半监督学习 <a class="header-anchor" href="#半监督学习" aria-label="Permalink to &quot;半监督学习&quot;">​</a></h2><p>半监督是指从无监督数据中学习一些通用表示，再做轻微的有监督<code>finetune</code>到各种各样的特定任务中。</p><p><strong>1. 无监督特征表示</strong></p><p>利用大量的语料和语言模型任务，去学习到一个神经网络。作为后面模型的网络初始参数。</p><p>语言模型网络可以使用<a href="https://plmsmile.github.io/2017/10/18/rnn/" target="_blank" rel="noreferrer">LSTM</a>和<a href="https://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/" target="_blank" rel="noreferrer">Transformer</a>。<code>Transformer</code>可以解决LSTM的<code>长依赖问题</code>，具有<strong>更好的迁移能力</strong>。</p><p><strong>2. 有监督任务训练</strong></p><p>特定任务的监督数据去finetune初始的网络。一般需要根据任务类型加上输出层。</p><h2 id="相关研究" tabindex="-1">相关研究 <a class="header-anchor" href="#相关研究" aria-label="Permalink to &quot;相关研究&quot;">​</a></h2><p><strong>1. 语义研究</strong></p><p>词向量主要是迁移具有<code>词级别的信息</code>，但更需要其一些<code>词级别以上的语义信息</code>。主要有<code>phrase-level</code>和<code>sentence-level</code> embedding。</p><p><strong>2. 无监督预训练</strong></p><p>无监督预训练的目的在于为后续任务去<strong>初始化一个好的网络参数</strong>，而不需要去改变任务的目标。预训练主要是使用<code>语言模型任务</code>，Transformer比LSTM更好，更强的迁移能力和处理长依赖能力。</p><p><code>Ruder</code>大神说NLP的ImageNet时代已经来了，足以说明预训练的重要性。</p><p><strong>3. 辅助任务</strong></p><ul><li>加辅助特征(<a href="https://plmsmile.github.io/2018/12/11/50-elmo/" target="_blank" rel="noreferrer">ELMo</a>)：网络的参数需要重新学习</li><li>辅助训练目标--语言模型和特定任务一起训练：其实无监督预训练已经学习到了语言特征，无需辅助训练目标了</li></ul><h1 id="gpt模型" tabindex="-1">GPT模型 <a class="header-anchor" href="#gpt模型" aria-label="Permalink to &quot;GPT模型&quot;">​</a></h1><h2 id="预训练单向语言模型" tabindex="-1">预训练单向语言模型 <a class="header-anchor" href="#预训练单向语言模型" aria-label="Permalink to &quot;预训练单向语言模型&quot;">​</a></h2><p>采用的是<a href="https://plmsmile.github.io/2018/12/11/50-elmo/#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B" target="_blank" rel="noreferrer">单向语言模型</a>，预测下一个词语，采用的是<a href="https://plmsmile.github.io/2018/08/29/48-attention-is-all-you-need/#decoder" target="_blank" rel="noreferrer">Tansformer的Decoder</a>。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/openai-transformer-1.png" style="display:block;margin:auto;" width="60%"><p>在Decoder之外加上线性层去预测下一个单词，训练语言模型任务。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/openai-transformer-language-modeling.png" style="display:block;margin:auto;" width="60%"><h2 id="迁移到下游监督任务" tabindex="-1">迁移到下游监督任务 <a class="header-anchor" href="#迁移到下游监督任务" aria-label="Permalink to &quot;迁移到下游监督任务&quot;">​</a></h2><p>预训练的Tansformer已经具有处理语言的能力，再加上输出层，并且监督finetune，则可以达到一个不错的效果。如下</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/openai-transformer-sentence-classification.png" style="display:block;margin:auto;" width="60%"><h2 id="各种任务组织方式" tabindex="-1">各种任务组织方式 <a class="header-anchor" href="#各种任务组织方式" aria-label="Permalink to &quot;各种任务组织方式&quot;">​</a></h2><p>利用<a href="https://arxiv.org/abs/1509.06664" target="_blank" rel="noreferrer">traversal-style</a>方法，把结构化数据处理成一个序列。每个序列都有一个开始和结束符号，也有分解符号，都是随机初始化的。</p><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/openai-input%20transformations2.png" style="display:block;margin:auto;" width="80%"><p><strong>分类任务</strong></p><p>开始符号 -- 文本 -- 结束符号</p><p><strong>文本蕴含</strong></p><p>开始 -- 前提 -- 分界 -- 假设 -- 结束</p><p><strong>相似性</strong></p><p>相似度计算与顺序无关，所以加了两个</p><p><strong>问答和常识推理</strong></p><p>文章和问题组成上下文，与每一个可能的答案作为拼接。一共有多组</p><h1 id="分析" tabindex="-1">分析 <a class="header-anchor" href="#分析" aria-label="Permalink to &quot;分析&quot;">​</a></h1><h2 id="效果" tabindex="-1">效果 <a class="header-anchor" href="#效果" aria-label="Permalink to &quot;效果&quot;">​</a></h2><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/result-1.png" style="display:block;margin:auto;" width="60%"><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/result-2.png" style="display:block;margin:auto;" width="60%"><h2 id="参数分析" tabindex="-1">参数分析 <a class="header-anchor" href="#参数分析" aria-label="Permalink to &quot;参数分析&quot;">​</a></h2><ul><li>右图：<code>zero-shot</code>上Transformer的效果是比LSTM好的</li><li>左图：可以知道Transformer层数越多效果也越好</li></ul><img src="http://plm-images.oss-cn-hongkong.aliyuncs.com/img/paper/gpt/parameter-analyze.png" style="display:block;margin:auto;" width="70%"><h1 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h1><ul><li>原始论文 <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noreferrer">Improving Language Understanding by Generative Pre-Training</a></li><li><a href="https://jalammar.github.io/illustrated-bert/" target="_blank" rel="noreferrer">Illustrated-BERT</a></li><li><a href="https://github.com/huggingface/pytorch-openai-transformer-lm" target="_blank" rel="noreferrer">pytorch-openai-transformer-lm</a></li><li><a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noreferrer">openai language-unsupervised</a></li></ul></div></div></main><footer class="VPDocFooter" data-v-5a64a79a data-v-54a90a4a><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-54a90a4a><span class="visually-hidden" id="doc-footer-aria-label" data-v-54a90a4a>Pager</span><div class="pager" data-v-54a90a4a><a class="VPLink link pager-link prev" href="/posts/olds/nlp/52-bert.html" data-v-54a90a4a><!--[--><span class="desc" data-v-54a90a4a>上一页</span><span class="title" data-v-54a90a4a>BERT 详解</span><!--]--></a></div><div class="pager" data-v-54a90a4a><a class="VPLink link pager-link next" href="/posts/olds/nlp/50-elmo.html" data-v-54a90a4a><!--[--><span class="desc" data-v-54a90a4a>下一页</span><span class="title" data-v-54a90a4a>ELMo：Deep Contextualized Word Representations</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--[--><div class="busuanzi"> 总访客数： <span id="busuanzi_value_site_uv"></span>   ·   总访问量： <span id="busuanzi_value_site_pv"></span></div><div class="busuanzi"> PLM&#39;s Blog @ 2016 - 2025</div><!--]--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"Xkffl130\",\"index.md\":\"BheGDo0s\",\"markdown-examples.md\":\"BRkVNFxB\",\"posts_archive.md\":\"CQTyVdEr\",\"posts_exps_env_01-blog-env.md\":\"Bus1IKDN\",\"posts_exps_env_index.md\":\"ViDjWzoZ\",\"posts_exps_mind_index.md\":\"CyLVrSwW\",\"posts_llm_agent_basic_01-lhy-agent-notes.md\":\"DpDHY26Z\",\"posts_llm_agent_basic_02-evaluation-agent.md\":\"BXkS4Ri2\",\"posts_llm_agent_basic_03-current-agents.md\":\"CaKTH2fO\",\"posts_llm_agent_basic_04-agent-blogs.md\":\"BqKvEFhZ\",\"posts_llm_agent_basic_05-comuter-agent.md\":\"BCBTcgFi\",\"posts_llm_agent_basic_index.md\":\"DXkNFY8D\",\"posts_llm_agent_rl_01-agent-rl.md\":\"BOySVqgY\",\"posts_llm_agent_rl_02-agent-tool.md\":\"mhnQFP9r\",\"posts_llm_agent_rl_03-agent-search.md\":\"DSgRXGnj\",\"posts_llm_agent_rl_04-agent-env.md\":\"jEKUUE26\",\"posts_llm_agent_rl_index.md\":\"BD7Q337w\",\"posts_llm_basic_index.md\":\"Dz5x496q\",\"posts_llm_rl_index.md\":\"C5bZT_bz\",\"posts_me.md\":\"BAbIfwmW\",\"posts_olds_algo_aim2offer.md\":\"Cz6K5T_f\",\"posts_olds_algo_aim2offer2.md\":\"CsDg2hmi\",\"posts_olds_algo_aim2offer3.md\":\"Bqx47LqA\",\"posts_olds_algo_aim2offer4.md\":\"oYzKNAUb\",\"posts_olds_algo_algorithm-dfs.md\":\"C65SjM61\",\"posts_olds_algo_index.md\":\"BLDaJaAX\",\"posts_olds_algo_leetcode-01.md\":\"CuTe0X9_\",\"posts_olds_algo_sort-algorithms.md\":\"BRXzDVU8\",\"posts_olds_bigdata_16-spark-baserdd.md\":\"OqXRKfbL\",\"posts_olds_bigdata_17-spark-pairrdd.md\":\"feRVt5rb\",\"posts_olds_bigdata_18-spark-sql.md\":\"D7SRDFvb\",\"posts_olds_bigdata_19-spark-programming.md\":\"CfbNRd0q\",\"posts_olds_bigdata_20-numpy.md\":\"BFNVBsp_\",\"posts_olds_bigdata_index.md\":\"C4Y8Qm6R\",\"posts_olds_dl_23-pytorch-start.md\":\"zJr7Di2v\",\"posts_olds_dl_35-nerual-network-optim.md\":\"BR7Pojf3\",\"posts_olds_dl_38-convolution.md\":\"BAE2Eqmr\",\"posts_olds_dl_attention-model.md\":\"u_f5k2ZA\",\"posts_olds_dl_cs224n-assignment-1.md\":\"2iJxHFWC\",\"posts_olds_dl_cs224n-lecture2-word2vec.md\":\"ClthZPeA\",\"posts_olds_dl_cs224n-notes1-word2vec.md\":\"DrgwSD0P\",\"posts_olds_dl_cs224n-notes3-neural-networks-2.md\":\"DEOKMjM4\",\"posts_olds_dl_cs224n-notes3-neural-networks.md\":\"uHRveaKW\",\"posts_olds_dl_cs231n-linear-notes.md\":\"NDaJ9Gz1\",\"posts_olds_dl_index.md\":\"14-y5uZG\",\"posts_olds_dl_rnn.md\":\"jefV1kO8\",\"posts_olds_env_09-linux-notes.md\":\"hexIjQEN\",\"posts_olds_env_12-ide-envs.md\":\"6Cp0-Out\",\"posts_olds_env_13-old-blog-problems.md\":\"D2qn7U-U\",\"posts_olds_env_24-hexo-problems.md\":\"YvaoXoMV\",\"posts_olds_env_index.md\":\"DHT-S0xK\",\"posts_olds_ml_10-trees.md\":\"BvaIT7ZX\",\"posts_olds_ml_14-em.md\":\"Br7akr6s\",\"posts_olds_ml_21-lr.md\":\"DWpXnFoO\",\"posts_olds_ml_22-ml-ch03-bayes.md\":\"C3MWhhR1\",\"posts_olds_ml_27-svm-notes.md\":\"CiaAcANg\",\"posts_olds_ml_28-ml-interview-notes.md\":\"DbX9BXZo\",\"posts_olds_ml_29-desicion-tree.md\":\"B0D7-yOJ\",\"posts_olds_ml_crf.md\":\"Cq6uRF2F\",\"posts_olds_ml_index.md\":\"CMAVKRwK\",\"posts_olds_ml_maxentmodel.md\":\"BhI-D-St\",\"posts_olds_ml_pgm-01.md\":\"B98vMEDt\",\"posts_olds_nlp_11-nlp-labels.md\":\"D_2zV9zh\",\"posts_olds_nlp_25-google-nmt.md\":\"BDd-XjAk\",\"posts_olds_nlp_26-wordpieacemodel.md\":\"B5QvDwRt\",\"posts_olds_nlp_30-dynamic-memory-network.md\":\"CJduMr_x\",\"posts_olds_nlp_31-co-attention-vqa.md\":\"YqUwFeuX\",\"posts_olds_nlp_32-dynamic-coattention-network.md\":\"DzJVI_o7\",\"posts_olds_nlp_33-attention-summary.md\":\"CXBYEWYo\",\"posts_olds_nlp_36-alime-chat.md\":\"BLDf7EoM\",\"posts_olds_nlp_39-squard-models.md\":\"BLS_8SRG\",\"posts_olds_nlp_45-match-lstm.md\":\"B6IkL5cR\",\"posts_olds_nlp_46-rnet-selfmatch.md\":\"BEm_VY9n\",\"posts_olds_nlp_47-bidaf.md\":\"_vw-uiAU\",\"posts_olds_nlp_48-attention-is-all-you-need.md\":\"CWi6xOzR\",\"posts_olds_nlp_49-qanet.md\":\"GE4Cd3RX\",\"posts_olds_nlp_50-elmo.md\":\"DOa6jfpi\",\"posts_olds_nlp_51-opengpt.md\":\"DI_-PibW\",\"posts_olds_nlp_52-bert.md\":\"BZCUgFMu\",\"posts_olds_nlp_53-mrc-brief.md\":\"DguP_qcH\",\"posts_olds_nlp_54-mrc-models.md\":\"Bh_-c6qs\",\"posts_olds_nlp_attention-based-nmt.md\":\"DX7shIIn\",\"posts_olds_nlp_index.md\":\"fbJN1i8j\",\"posts_olds_nlp_nlp-notes.md\":\"BJpNHEg_\",\"posts_olds_nlp_nmt.md\":\"BcCiPtrz\",\"posts_olds_nlp_subword-units.md\":\"M6KRaDoT\",\"posts_olds_nlp_word2vec-math.md\":\"ptv0LLxb\",\"posts_olds_nlp_word2vec.md\":\"C3mQIIHl\",\"posts_olds_other_15-cpp-pointer-object-reference.md\":\"CpWjxBD8\",\"posts_olds_other_index.md\":\"_S-bEwIt\",\"posts_olds_rl_37-reinforce-learning.md\":\"BU1rDhhw\",\"posts_olds_rl_40-value-learning.md\":\"8DiVSefv\",\"posts_olds_rl_41-strategy-learning.md\":\"Cl0FdxtC\",\"posts_olds_rl_42-reinforce-conclusion-simple.md\":\"DIw2tPlF\",\"posts_olds_rl_43-intent-detection-slot-filling.md\":\"CbEf3hcQ\",\"posts_olds_rl_44-reinforce-nlp.md\":\"Cn2wpki_\",\"posts_olds_rl_index.md\":\"DrdmSwsd\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"📚 plmblog\",\"description\":\"记录一些学习笔记。\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"🐲LLM\",\"items\":[{\"text\":\"Basic\",\"items\":[{\"text\":\"🦋基础\",\"link\":\"/posts/llm/basic\"}]},{\"text\":\"Agent\",\"items\":[{\"text\":\"🤖概念及应用\",\"link\":\"/posts/llm/agent/basic\"},{\"text\":\"🚄Agent-RL\",\"link\":\"/posts/llm/agent/rl/02-agent-tool\"}]},{\"text\":\"rl\",\"items\":[{\"text\":\"🐼r1相关\",\"link\":\"/posts/llm/rl\"}]}]},{\"text\":\"📙旧文章\",\"items\":[{\"text\":\"🍓NLP\",\"items\":[{\"text\":\"自然语言处理\",\"link\":\"/posts/olds/nlp\"}]},{\"text\":\"🍑基础知识\",\"items\":[{\"text\":\"深度学习\",\"link\":\"/posts/olds/dl\"},{\"text\":\"强化学习\",\"link\":\"/posts/olds/rl\"},{\"text\":\"机器学习\",\"link\":\"/posts/olds/ml\"}]},{\"text\":\"🍎算法\",\"items\":[{\"text\":\"算法题\",\"link\":\"/posts/olds/algo/\"},{\"text\":\"大数据\",\"link\":\"/posts/olds/bigdata/\"}]},{\"text\":\"🍒其他\",\"items\":[{\"text\":\"环境搭建\",\"link\":\"/posts/olds/env/\"},{\"text\":\"其他\",\"link\":\"/posts/olds/other/\"}]}]},{\"text\":\"经验\",\"items\":[{\"text\":\"环境\",\"items\":[{\"text\":\"环境搭建\",\"link\":\"/posts/exps/env/01-blog-env\"}]},{\"text\":\"心得\",\"items\":[{\"text\":\"心得体会\",\"link\":\"/posts/exps/mind\"}]}]},{\"text\":\"归档\",\"link\":\"/posts/archive.md\"},{\"text\":\"关于我\",\"link\":\"/posts/me\"}],\"outline\":{\"level\":[1,4],\"label\":\"当前页大纲\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/plmsmile\"}],\"search\":{\"provider\":\"local\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"},\"sidebar\":{\"/posts/olds/nlp/\":{\"base\":\"/posts/olds/nlp/\",\"items\":[{\"text\":\"NLP\",\"items\":[{\"text\":\"机器阅读(二)--模型(未完成)\",\"link\":\"54-mrc-models\"},{\"text\":\"机器阅读(一)--整体概述\",\"link\":\"53-mrc-brief\"},{\"text\":\"BERT 详解\",\"link\":\"52-bert\"},{\"text\":\"OpenAI GPT：Improving Language Understanding by Generative Pre-Training\",\"link\":\"51-opengpt\"},{\"text\":\"ELMo：Deep Contextualized Word Representations\",\"link\":\"50-elmo\"},{\"text\":\"QANet\",\"link\":\"49-qanet\"},{\"text\":\"Transformer\",\"link\":\"48-attention-is-all-you-need\"},{\"text\":\"Bidirectional Attention Flow\",\"link\":\"47-bidaf\"},{\"text\":\"R-Net (Gated Self-Matching Networks)\",\"link\":\"46-rnet-selfmatch\"},{\"text\":\"Match-LSTM and Answer Pointer\",\"link\":\"45-match-lstm\"},{\"text\":\"阅读理解模型总结\",\"link\":\"39-squard-models\"},{\"text\":\"阿里小蜜论文\",\"link\":\"36-alime-chat\"},{\"text\":\"各种注意力总结\",\"link\":\"33-attention-summary\"},{\"text\":\"Dynamic Coattention Network (Plus)\",\"link\":\"32-dynamic-coattention-network\"},{\"text\":\"协同注意力简介\",\"link\":\"31-co-attention-vqa\"},{\"text\":\"使用Dynamic Memory Network实现一个简单QA\",\"link\":\"30-dynamic-memory-network\"},{\"text\":\"词性标注和句法依存的表示符号\",\"link\":\"11-nlp-labels\"},{\"text\":\"Word2vec之数学模型\",\"link\":\"word2vec-math\"},{\"text\":\"subword-units\",\"link\":\"subword-units\"},{\"text\":\"Wordpiece模型\",\"link\":\"26-wordpieacemodel\"},{\"text\":\"谷歌RNN翻译模型\",\"link\":\"25-google-nmt\"},{\"text\":\"机器翻译注意力机制及其PyTorch实现\",\"link\":\"Attention-based-NMT\"},{\"text\":\"最初RNN神经翻译简略笔记\",\"link\":\"NMT\"},{\"text\":\"语言模型和平滑方法\",\"link\":\"nlp-notes\"},{\"text\":\"利用tensorflow实现简版word2vec\",\"link\":\"word2vec\"}]}]},\"/posts/olds/dl/\":{\"base\":\"/posts/olds/dl/\",\"items\":[{\"text\":\"DL\",\"items\":[{\"text\":\"卷积神经网络总结\",\"link\":\"38-convolution\"},{\"text\":\"网络优化\",\"link\":\"35-nerual-network-optim\"},{\"text\":\"cs224n作业一\",\"link\":\"cs224n-assignment-1\"},{\"text\":\"cs231n线性分类器和损失函数\",\"link\":\"cs231n-linear-notes\"},{\"text\":\"神经网络-过拟合-预处理-BN\",\"link\":\"cs224n-notes3-neural-networks-2\"},{\"text\":\"神经网络基础-反向传播-激活函数\",\"link\":\"cs224n-notes3-neural-networks\"},{\"text\":\"Word2vec之总体介绍\",\"link\":\"cs224n-notes1-word2vec\"},{\"text\":\"Word2vec之公式推导笔记\",\"link\":\"cs224n-lecture2-word2vec\"},{\"text\":\"循环神经网络\",\"link\":\"rnn\"},{\"text\":\"图文介绍RNN注意力机制\",\"link\":\"attention-model\"},{\"text\":\"PyTorch快速上手\",\"link\":\"23-pytorch-start\"}]}]},\"/posts/olds/rl/\":{\"base\":\"/posts/olds/rl/\",\"items\":[{\"text\":\"RL\",\"items\":[{\"text\":\"强化学习在NLP中的应用\",\"link\":\"44-reinforce-nlp\"},{\"text\":\"意图识别和槽填充\",\"link\":\"43-intent-detection-slot-filling\"},{\"text\":\"强化学习算法小结\",\"link\":\"42-reinforce-conclusion-simple\"},{\"text\":\"基于策略函数的学习方法\",\"link\":\"41-strategy-learning\"},{\"text\":\"基于值函数的学习\",\"link\":\"40-value-learning\"},{\"text\":\"强化学习\",\"link\":\"37-reinforce-learning\"}]}]},\"/posts/olds/ml/\":{\"base\":\"/posts/olds/ml/\",\"items\":[{\"text\":\"ML\",\"items\":[{\"text\":\"决策树笔记\",\"link\":\"29-desicion-tree\"},{\"text\":\"机器学习知识点汇总整理\",\"link\":\"28-ml-interview-notes\"},{\"text\":\"SVM笔记\",\"link\":\"27-svm-notes\"},{\"text\":\"树的总结\",\"link\":\"10-trees\"},{\"text\":\"条件随机场\",\"link\":\"crf\"},{\"text\":\"最大熵模型\",\"link\":\"maxentmodel\"},{\"text\":\"线性回归和逻辑回归\",\"link\":\"21-lr\"},{\"text\":\"最大期望算法\",\"link\":\"14-em\"},{\"text\":\"马尔可夫模型\",\"link\":\"pgm-01\"},{\"text\":\"朴素贝叶斯算法及其代码实现\",\"link\":\"22-ml-ch03-bayes\"}]}]},\"/posts/olds/algo/\":{\"base\":\"/posts/olds/algo/\",\"items\":[{\"text\":\"ALGO\",\"items\":[{\"text\":\"剑指offer4(51-64)\",\"link\":\"aim2offer4\"},{\"text\":\"剑指offer3(21-40)\",\"link\":\"aim2offer3\"},{\"text\":\"数据结构之搜索算法\",\"link\":\"algorithm-dfs\"},{\"text\":\"leetcode-01\",\"link\":\"leetcode-01\"},{\"text\":\"剑指offer(11-20)\",\"link\":\"aim2offer2\"},{\"text\":\"排序算法总结\",\"link\":\"sort-algorithms\"},{\"text\":\"剑指Offer(1-10)\",\"link\":\"aim2offer\"}]}]},\"/posts/olds/bigdata/\":{\"base\":\"/posts/olds/bigdata/\",\"items\":[{\"text\":\"BIG Data\",\"items\":[{\"text\":\"NumPy\",\"link\":\"20-numpy\"},{\"text\":\"Spark基础编程核心思想介绍\",\"link\":\"19-spark-programming\"},{\"text\":\"Spark-SQL的简略笔记\",\"link\":\"18-spark-sql\"},{\"text\":\"Spark键值对RDD的常用API\",\"link\":\"17-spark-pairrdd\"},{\"text\":\"Spark基础RDD的常用API\",\"link\":\"16-Spark-BaseRDD\"}]}]},\"/posts/olds/env/\":{\"base\":\"/posts/olds/env/\",\"items\":[{\"text\":\"环境搭建\",\"items\":[{\"text\":\"IDE配置\",\"link\":\"12-ide-envs\"},{\"text\":\"Linux使用笔记\",\"link\":\"09-linux-notes\"},{\"text\":\"博客搭建及相关问题\",\"link\":\"24-hexo-problems\"},{\"text\":\"旧版博客搭建过程及其问题\",\"link\":\"13-old-blog-problems\"}]}]},\"/posts/olds/other/\":{\"base\":\"/posts/olds/other/\",\"items\":[{\"text\":\"其他\",\"items\":[{\"text\":\"C++类对象和指针的区别\",\"link\":\"15-cpp-pointer-object-reference\"}]}]},\"/posts/llm/agent/rl/\":{\"base\":\"/posts/llm/agent/rl/\",\"items\":[{\"text\":\"Agent-RL\",\"items\":[{\"text\":\"Agent-Interaction-RL 笔记\",\"link\":\"04-agent-env\"},{\"text\":\"Agent-Search-RL 笔记\",\"link\":\"03-agent-search\"},{\"text\":\"Agent-Tool-RL 笔记\",\"link\":\"02-agent-tool\"},{\"text\":\"Agent-RL 综述型笔记\",\"link\":\"01-agent-rl\"}]}]},\"/posts/llm/agent/basic/\":{\"base\":\"/posts/llm/agent/basic/\",\"items\":[{\"text\":\"Agent-基础\",\"items\":[{\"text\":\"Computer-Agent\",\"link\":\"05-comuter-agent\"},{\"text\":\"Agent 思考性文章\",\"link\":\"04-agent-blogs\"},{\"text\":\"一些流行的Agents\",\"link\":\"03-current-agents\"},{\"text\":\"Agent 评估 Benchmarks\",\"link\":\"02-evaluation-agent\"},{\"text\":\"Agent基础概念 (李宏毅笔记)\",\"link\":\"01-lhy-agent-notes\"}]}]},\"/posts/llm/rl/\":{\"base\":\"/posts/llm/rl/\",\"items\":[{\"text\":\"LLM-RL\",\"items\":[]}]},\"/posts/llm/basic/\":{\"base\":\"/posts/llm/basic/\",\"items\":[{\"text\":\"LLM-basic\",\"items\":[]}]},\"/posts/exps/env/\":{\"base\":\"/posts/exps/env/\",\"items\":[{\"text\":\"环境搭建\",\"items\":[{\"text\":\"环境搭建的一些坑\",\"link\":\"01-blog-env\"}]}]},\"/posts/exps/mind/\":{\"base\":\"/posts/exps/mind/\",\"items\":[{\"text\":\"心得体会\",\"items\":[]}]}}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>